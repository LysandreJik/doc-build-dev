import{S as yn,i as wn,s as bn,e as a,k as d,w as u,t as i,M as En,c as o,d as e,m as f,a as l,x as h,h as p,b as c,G as s,g as n,y as m,q as g,o as _,B as v,v as kn}from"../chunks/vendor-hf-doc-builder.js";import{T as jn}from"../chunks/Tip-hf-doc-builder.js";import{I as D}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as y}from"../chunks/CodeBlock-hf-doc-builder.js";function qn($s){let $,M,w,j,P,b,ht,z;return{c(){$=a("p"),M=i("Remember to define your credentials in your "),w=a("a"),j=i("FileSystem instance"),P=d(),b=a("code"),ht=i("fs"),z=i(" whenever you are interacting with a private cloud storage."),this.h()},l(C){$=o(C,"P",{});var E=l($);M=p(E,"Remember to define your credentials in your "),w=o(E,"A",{href:!0});var A=l(w);j=p(A,"FileSystem instance"),A.forEach(e),P=f(E),b=o(E,"CODE",{});var ne=l(b);ht=p(ne,"fs"),ne.forEach(e),z=p(E," whenever you are interacting with a private cloud storage."),E.forEach(e),this.h()},h(){c(w,"href","#set-up-your-cloud-storage-filesystem")},m(C,E){n(C,$,E),s($,M),s($,w),s(w,j),s($,P),s($,b),s(b,ht),s($,z)},d(C){C&&e($)}}}function Sn($s){let $,M,w,j,P,b,ht,z,C,E,A,ne,ye,Ma,Ua,ys,U,we,mt,be,Ka,Wa,Ee,Ja,Qa,k,gt,ke,Va,Xa,je,_t,Za,to,vt,qe,eo,so,Se,$t,ao,oo,yt,Ae,lo,ro,De,wt,no,io,bt,Te,po,fo,Le,Et,co,uo,kt,Pe,ho,mo,ze,jt,go,ws,ie,_o,bs,N,K,Ce,qt,vo,Ne,$o,Es,O,W,Oe,St,yo,Fe,wo,ks,pe,Ie,bo,js,At,qs,Dt,xe,Eo,Ss,q,ko,He,jo,qo,Be,So,Ao,Ge,Do,To,As,Tt,Ds,Lt,Re,Lo,Ts,Pt,Ls,F,J,Ye,zt,Po,Me,zo,Ps,de,Ue,Co,zs,Ct,Cs,Nt,Ke,No,Ns,Ot,Os,Ft,We,Oo,Fs,It,Is,I,Q,Je,xt,Fo,Qe,Io,xs,fe,Ve,xo,Hs,Ht,Bs,Bt,Xe,Ho,Gs,Gt,Rs,Rt,Ze,Bo,Ys,Yt,Ms,x,V,ts,Mt,Go,es,Ro,Us,H,X,ss,Ut,Yo,as,Mo,Ks,S,Uo,os,Ko,Wo,ls,Jo,Qo,rs,Vo,Xo,Ws,Z,Zo,ce,tl,el,Js,Kt,Qs,tt,sl,ue,al,ol,Vs,Wt,Xs,et,ll,he,rl,nl,Zs,Jt,ta,st,il,ns,pl,dl,ea,at,fl,is,cl,ul,sa,Qt,aa,B,ot,ps,Vt,hl,ds,ml,oa,me,gl,la,ge,_l,ra,Xt,na,lt,vl,Zt,$l,yl,ia,G,rt,fs,te,wl,cs,bl,pa,nt,El,_e,kl,jl,da,ee,fa,it,ca,R,pt,us,se,ql,hs,Sl,ua,T,Al,ms,Dl,Tl,gs,Ll,Pl,ha,ae,ma,Y,dt,_s,oe,zl,vs,Cl,ga,ft,Nl,ve,Ol,Fl,_a,le,va;return b=new D({}),qt=new D({}),St=new D({}),At=new y({props:{code:"pip install datasets[s3]",highlighted:'&gt;&gt;&gt; pip <span class="hljs-keyword">install </span>datasets[<span class="hljs-built_in">s3</span>]'}}),Tt=new y({props:{code:`storage_options = {"anon": True}  # for anynonous connection
storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key}  # for private buckets
import botocore
s3_session = botocore.session.Session(profile="my_profile_name")
storage_options = {"session": s3_session}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;anon&quot;</span>: <span class="hljs-literal">True</span>}  <span class="hljs-comment"># for anynonous connection</span>
<span class="hljs-comment"># or use your credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;key&quot;</span>: aws_access_key_id, <span class="hljs-string">&quot;secret&quot;</span>: aws_secret_access_key}  <span class="hljs-comment"># for private buckets</span>
<span class="hljs-comment"># or use a botocore session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&quot;my_profile_name&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;session&quot;</span>: s3_session}`}}),Pt=new y({props:{code:`import s3fs
fs = s3fs.S3FileSystem(**storage_options)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> s3fs
<span class="hljs-meta">&gt;&gt;&gt; </span>fs = s3fs.S3FileSystem(**storage_options)`}}),zt=new D({}),Ct=new y({props:{code:`conda install -c conda-forge gcsfs
pip install gcsfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge gcsfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install gcsfs</span>`}}),Ot=new y({props:{code:`storage_options={"token": "anon"}  # for anonymous connection
storage_options={"project": "my-google-project"}
storage_options={"project": "my-google-project", "token": TOKEN}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-string">&quot;anon&quot;</span>}  <span class="hljs-comment"># for anonymous connection</span>
<span class="hljs-comment"># or use your credentials of your default gcloud credentials or from the google metadata service</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;project&quot;</span>: <span class="hljs-string">&quot;my-google-project&quot;</span>}
<span class="hljs-comment"># or use your credentials from elsewhere, see the documentation at https://gcsfs.readthedocs.io/</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;project&quot;</span>: <span class="hljs-string">&quot;my-google-project&quot;</span>, <span class="hljs-string">&quot;token&quot;</span>: TOKEN}`}}),It=new y({props:{code:`import gcsfs
fs = gcsfs.GCSFileSystem(**storage_options)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs
<span class="hljs-meta">&gt;&gt;&gt; </span>fs = gcsfs.GCSFileSystem(**storage_options)`}}),xt=new D({}),Ht=new y({props:{code:`conda install -c conda-forge adlfs
pip install adlfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge adlfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install adlfs</span>`}}),Gt=new y({props:{code:`storage_options = {"anon": True}  # for anonymous connection
storage_options = {"account_name": ACCOUNT_NAME, "account_key": ACCOUNT_KEY)  # gen 2 filesystem
storage_options={"tenant_id": TENANT_ID, "client_id": CLIENT_ID, "client_secret": CLIENT_SECRET}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;anon&quot;</span>: <span class="hljs-literal">True</span>}  <span class="hljs-comment"># for anonymous connection</span>
<span class="hljs-comment"># or use your credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;account_name&quot;</span>: ACCOUNT_NAME, <span class="hljs-string">&quot;account_key&quot;</span>: ACCOUNT_KEY)  <span class="hljs-comment"># gen 2 filesystem</span>
<span class="hljs-comment"># or use your credentials with the gen 1 filesystem</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;tenant_id&quot;</span>: TENANT_ID, <span class="hljs-string">&quot;client_id&quot;</span>: CLIENT_ID, <span class="hljs-string">&quot;client_secret&quot;</span>: CLIENT_SECRET}`}}),Yt=new y({props:{code:`import adlfs
fs = adlfs.AzureBlobFileSystem(**storage_options)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs
<span class="hljs-meta">&gt;&gt;&gt; </span>fs = adlfs.AzureBlobFileSystem(**storage_options)`}}),Mt=new D({}),Ut=new D({}),Kt=new y({props:{code:`cache_dir = "s3://my-bucket/datasets-cache"
builder = load_dataset_builder("imdb", cache_dir=cache_dir, storage_options=storage_options)
builder.download_and_prepare(file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>cache_dir = <span class="hljs-string">&quot;s3://my-bucket/datasets-cache&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;imdb&quot;</span>, cache_dir=cache_dir, storage_options=storage_options)
<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),Wt=new y({props:{code:`cache_dir = "s3://my-bucket/datasets-cache"
builder = load_dataset_builder("path/to/local/loading_script/loading_script.py", cache_dir=cache_dir, storage_options=storage_options)
builder.download_and_prepare(file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>cache_dir = <span class="hljs-string">&quot;s3://my-bucket/datasets-cache&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;path/to/local/loading_script/loading_script.py&quot;</span>, cache_dir=cache_dir, storage_options=storage_options)
<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),Jt=new y({props:{code:`data_files = {"train": ["path/to/train.csv"]}
cache_dir = "s3://my-bucket/datasets-cache"
builder = load_dataset_builder("csv", data_files=data_files, cache_dir=cache_dir, storage_options=storage_options)
builder.download_and_prepare(file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: [<span class="hljs-string">&quot;path/to/train.csv&quot;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>cache_dir = <span class="hljs-string">&quot;s3://my-bucket/datasets-cache&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files, cache_dir=cache_dir, storage_options=storage_options)
<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),Qt=new y({props:{code:'builder.download_and_prepare(file_format="parquet", max_shard_size="1GB")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(file_format=<span class="hljs-string">&quot;parquet&quot;</span>, max_shard_size=<span class="hljs-string">&quot;1GB&quot;</span>)'}}),Vt=new D({}),Xt=new y({props:{code:`import dask.dataframe as dd

df = dd.read_parquet(builder.cache_dir, storage_options=storage_options)

# or if your dataset is split into train/valid/test
df_train = dd.read_parquet(builder.cache_dir + f"/{builder.name}-train-*.parquet", storage_options=storage_options)
df_valid = dd.read_parquet(builder.cache_dir + f"/{builder.name}-validation-*.parquet", storage_options=storage_options)
df_test = dd.read_parquet(builder.cache_dir + f"/{builder.name}-test-*.parquet", storage_options=storage_options)`,highlighted:`<span class="hljs-keyword">import</span> dask.dataframe <span class="hljs-keyword">as</span> dd

df = dd.read_parquet(builder.cache_dir, storage_options=storage_options)

<span class="hljs-comment"># or if your dataset is split into train/valid/test</span>
df_train = dd.read_parquet(builder.cache_dir + <span class="hljs-string">f&quot;/<span class="hljs-subst">{builder.name}</span>-train-*.parquet&quot;</span>, storage_options=storage_options)
df_valid = dd.read_parquet(builder.cache_dir + <span class="hljs-string">f&quot;/<span class="hljs-subst">{builder.name}</span>-validation-*.parquet&quot;</span>, storage_options=storage_options)
df_test = dd.read_parquet(builder.cache_dir + <span class="hljs-string">f&quot;/<span class="hljs-subst">{builder.name}</span>-test-*.parquet&quot;</span>, storage_options=storage_options)`}}),te=new D({}),ee=new y({props:{code:`encoded_dataset.save_to_disk("s3://my-private-datasets/imdb/train", fs=fs)
encoded_dataset.save_to_disk("gcs://my-private-datasets/imdb/train", fs=fs)
encoded_dataset.save_to_disk("adl://my-private-datasets/imdb/train", fs=fs)`,highlighted:`<span class="hljs-comment"># saves encoded_dataset to amazon s3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;s3://my-private-datasets/imdb/train&quot;</span>, fs=fs)
<span class="hljs-comment"># saves encoded_dataset to google cloud storage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;gcs://my-private-datasets/imdb/train&quot;</span>, fs=fs)
<span class="hljs-comment"># saves encoded_dataset to microsoft azure blob/datalake</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;adl://my-private-datasets/imdb/train&quot;</span>, fs=fs)`}}),it=new jn({props:{$$slots:{default:[qn]},$$scope:{ctx:$s}}}),se=new D({}),ae=new y({props:{code:'fs.ls("my-private-datasets/imdb/train")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>fs.ls(<span class="hljs-string">&quot;my-private-datasets/imdb/train&quot;</span>)
[<span class="hljs-string">&quot;dataset_info.json.json&quot;</span>,<span class="hljs-string">&quot;dataset.arrow&quot;</span>,<span class="hljs-string">&quot;state.json&quot;</span>]`}}),oe=new D({}),le=new y({props:{code:`from datasets import load_from_disk
dataset = load_from_disk("s3://a-public-datasets/imdb/train", fs=fs)  
print(len(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-comment"># load encoded_dataset from cloud storage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&quot;s3://a-public-datasets/imdb/train&quot;</span>, fs=fs)  
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-number">25000</span>`}}),{c(){$=a("meta"),M=d(),w=a("h1"),j=a("a"),P=a("span"),u(b.$$.fragment),ht=d(),z=a("span"),C=i("Cloud storage"),E=d(),A=a("p"),ne=i("\u{1F917} Datasets supports access to cloud storage providers through a "),ye=a("code"),Ma=i("fsspec"),Ua=i(` FileSystem implementations.
You can save and load datasets from any cloud storage in a Pythonic way.
Take a look at the following table for some example of supported cloud storage providers:`),ys=d(),U=a("table"),we=a("thead"),mt=a("tr"),be=a("th"),Ka=i("Storage provider"),Wa=d(),Ee=a("th"),Ja=i("Filesystem implementation"),Qa=d(),k=a("tbody"),gt=a("tr"),ke=a("td"),Va=i("Amazon S3"),Xa=d(),je=a("td"),_t=a("a"),Za=i("s3fs"),to=d(),vt=a("tr"),qe=a("td"),eo=i("Google Cloud Storage"),so=d(),Se=a("td"),$t=a("a"),ao=i("gcsfs"),oo=d(),yt=a("tr"),Ae=a("td"),lo=i("Azure Blob/DataLake"),ro=d(),De=a("td"),wt=a("a"),no=i("adlfs"),io=d(),bt=a("tr"),Te=a("td"),po=i("Dropbox"),fo=d(),Le=a("td"),Et=a("a"),co=i("dropboxdrivefs"),uo=d(),kt=a("tr"),Pe=a("td"),ho=i("Google Drive"),mo=d(),ze=a("td"),jt=a("a"),go=i("gdrivefs"),ws=d(),ie=a("p"),_o=i(`This guide will show you how to save and load datasets with any cloud storage.
Here are examples for S3, Google Cloud Storage and Azure Blob Storage.`),bs=d(),N=a("h2"),K=a("a"),Ce=a("span"),u(qt.$$.fragment),vo=d(),Ne=a("span"),$o=i("Set up your cloud storage FileSystem"),Es=d(),O=a("h3"),W=a("a"),Oe=a("span"),u(St.$$.fragment),yo=d(),Fe=a("span"),wo=i("Amazon S3"),ks=d(),pe=a("ol"),Ie=a("li"),bo=i("Install the S3 dependency with \u{1F917} Datasets:"),js=d(),u(At.$$.fragment),qs=d(),Dt=a("ol"),xe=a("li"),Eo=i("Define your credentials"),Ss=d(),q=a("p"),ko=i("To use an anonymous connection, use "),He=a("code"),jo=i("anon=True"),qo=i(`.
Otherwise, include your `),Be=a("code"),So=i("aws_access_key_id"),Ao=i(" and "),Ge=a("code"),Do=i("aws_secret_access_key"),To=i(" whenever you are interacting with a private S3 bucket."),As=d(),u(Tt.$$.fragment),Ds=d(),Lt=a("ol"),Re=a("li"),Lo=i("Load your FileSystem instance"),Ts=d(),u(Pt.$$.fragment),Ls=d(),F=a("h3"),J=a("a"),Ye=a("span"),u(zt.$$.fragment),Po=d(),Me=a("span"),zo=i("Google Cloud Storage"),Ps=d(),de=a("ol"),Ue=a("li"),Co=i("Install the Google Cloud Storage implementation:"),zs=d(),u(Ct.$$.fragment),Cs=d(),Nt=a("ol"),Ke=a("li"),No=i("Define your credentials"),Ns=d(),u(Ot.$$.fragment),Os=d(),Ft=a("ol"),We=a("li"),Oo=i("Load your FileSystem instance"),Fs=d(),u(It.$$.fragment),Is=d(),I=a("h3"),Q=a("a"),Je=a("span"),u(xt.$$.fragment),Fo=d(),Qe=a("span"),Io=i("Azure Blob Storage"),xs=d(),fe=a("ol"),Ve=a("li"),xo=i("Install the Azure Blob Storage implementation:"),Hs=d(),u(Ht.$$.fragment),Bs=d(),Bt=a("ol"),Xe=a("li"),Ho=i("Define your credentials"),Gs=d(),u(Gt.$$.fragment),Rs=d(),Rt=a("ol"),Ze=a("li"),Bo=i("Load your FileSystem instance"),Ys=d(),u(Yt.$$.fragment),Ms=d(),x=a("h2"),V=a("a"),ts=a("span"),u(Mt.$$.fragment),Go=d(),es=a("span"),Ro=i("Load and Save your datasets using your cloud storage FileSystem"),Us=d(),H=a("h3"),X=a("a"),ss=a("span"),u(Ut.$$.fragment),Yo=d(),as=a("span"),Mo=i("Load datasets into a cloud storage"),Ks=d(),S=a("p"),Uo=i("You can load and cache a dataset into your cloud storage by specifying a remote "),os=a("code"),Ko=i("cache_dir"),Wo=i(" in "),ls=a("code"),Jo=i("load_dataset"),Qo=i(`.
Don\u2019t forget to use the previously defined `),rs=a("code"),Vo=i("storage_options"),Xo=i(" containing your credentials to write into a private cloud storage."),Ws=d(),Z=a("p"),Zo=i("Load a dataset from the Hugging Face Hub (see "),ce=a("a"),tl=i("how to load from the Hugging Face Hub"),el=i("):"),Js=d(),u(Kt.$$.fragment),Qs=d(),tt=a("p"),sl=i("Load a dataset using a loading script (see "),ue=a("a"),al=i("how to load a local loading script"),ol=i("):"),Vs=d(),u(Wt.$$.fragment),Xs=d(),et=a("p"),ll=i("Load your own data files (see "),he=a("a"),rl=i("how to load local and remote files"),nl=i("):"),Zs=d(),u(Jt.$$.fragment),ta=d(),st=a("p"),il=i("It is highly recommended to save the files as compressed sharded Parquet files to optimize I/O by specifying "),ns=a("code"),pl=i('file_format="parquet"'),dl=i(`.
Otherwize the dataset is saved as an uncompressed Arrow file.`),ea=d(),at=a("p"),fl=i("You can also specify the size of the Parquet shard using "),is=a("code"),cl=i("max_shard_size"),ul=i(" (default is 500MB):"),sa=d(),u(Qt.$$.fragment),aa=d(),B=a("h4"),ot=a("a"),ps=a("span"),u(Vt.$$.fragment),hl=d(),ds=a("span"),ml=i("Dask"),oa=d(),me=a("p"),gl=i(`Dask is a parallel computing library and it has a pandas-like API for working with larger than memory Parquet datasets in parallel.
Dask can use multiple threads or processes on a single machine, or a cluster of machines to process data in parallel.
Dask supports local data but also data from a cloud storage.`),la=d(),ge=a("p"),_l=i("Therefore you can load a dataset saved as sharded Parquet files in Dask with"),ra=d(),u(Xt.$$.fragment),na=d(),lt=a("p"),vl=i("You can find more about dask dataframes in their "),Zt=a("a"),$l=i("documentation"),yl=i("."),ia=d(),G=a("h2"),rt=a("a"),fs=a("span"),u(te.$$.fragment),wl=d(),cs=a("span"),bl=i("Saving serialized datasets"),pa=d(),nt=a("p"),El=i("After you have processed your dataset, you can save it to your cloud storage with "),_e=a("a"),kl=i("Dataset.save_to_disk()"),jl=i(":"),da=d(),u(ee.$$.fragment),fa=d(),u(it.$$.fragment),ca=d(),R=a("h2"),pt=a("a"),us=a("span"),u(se.$$.fragment),ql=d(),hs=a("span"),Sl=i("Listing serialized datasets"),ua=d(),T=a("p"),Al=i("List files from a cloud storage with your FileSystem instance "),ms=a("code"),Dl=i("fs"),Tl=i(", using "),gs=a("code"),Ll=i("fs.ls"),Pl=i(":"),ha=d(),u(ae.$$.fragment),ma=d(),Y=a("h3"),dt=a("a"),_s=a("span"),u(oe.$$.fragment),zl=d(),vs=a("span"),Cl=i("Load serialized datasets"),ga=d(),ft=a("p"),Nl=i("When you are ready to use your dataset again, reload it with "),ve=a("a"),Ol=i("Dataset.load_from_disk()"),Fl=i(":"),_a=d(),u(le.$$.fragment),this.h()},l(t){const r=En('[data-svelte="svelte-1phssyn"]',document.head);$=o(r,"META",{name:!0,content:!0}),r.forEach(e),M=f(t),w=o(t,"H1",{class:!0});var re=l(w);j=o(re,"A",{id:!0,class:!0,href:!0});var Il=l(j);P=o(Il,"SPAN",{});var xl=l(P);h(b.$$.fragment,xl),xl.forEach(e),Il.forEach(e),ht=f(re),z=o(re,"SPAN",{});var Hl=l(z);C=p(Hl,"Cloud storage"),Hl.forEach(e),re.forEach(e),E=f(t),A=o(t,"P",{});var $a=l(A);ne=p($a,"\u{1F917} Datasets supports access to cloud storage providers through a "),ye=o($a,"CODE",{});var Bl=l(ye);Ma=p(Bl,"fsspec"),Bl.forEach(e),Ua=p($a,` FileSystem implementations.
You can save and load datasets from any cloud storage in a Pythonic way.
Take a look at the following table for some example of supported cloud storage providers:`),$a.forEach(e),ys=f(t),U=o(t,"TABLE",{});var ya=l(U);we=o(ya,"THEAD",{});var Gl=l(we);mt=o(Gl,"TR",{});var wa=l(mt);be=o(wa,"TH",{});var Rl=l(be);Ka=p(Rl,"Storage provider"),Rl.forEach(e),Wa=f(wa),Ee=o(wa,"TH",{});var Yl=l(Ee);Ja=p(Yl,"Filesystem implementation"),Yl.forEach(e),wa.forEach(e),Gl.forEach(e),Qa=f(ya),k=o(ya,"TBODY",{});var L=l(k);gt=o(L,"TR",{});var ba=l(gt);ke=o(ba,"TD",{});var Ml=l(ke);Va=p(Ml,"Amazon S3"),Ml.forEach(e),Xa=f(ba),je=o(ba,"TD",{});var Ul=l(je);_t=o(Ul,"A",{href:!0,rel:!0});var Kl=l(_t);Za=p(Kl,"s3fs"),Kl.forEach(e),Ul.forEach(e),ba.forEach(e),to=f(L),vt=o(L,"TR",{});var Ea=l(vt);qe=o(Ea,"TD",{});var Wl=l(qe);eo=p(Wl,"Google Cloud Storage"),Wl.forEach(e),so=f(Ea),Se=o(Ea,"TD",{});var Jl=l(Se);$t=o(Jl,"A",{href:!0,rel:!0});var Ql=l($t);ao=p(Ql,"gcsfs"),Ql.forEach(e),Jl.forEach(e),Ea.forEach(e),oo=f(L),yt=o(L,"TR",{});var ka=l(yt);Ae=o(ka,"TD",{});var Vl=l(Ae);lo=p(Vl,"Azure Blob/DataLake"),Vl.forEach(e),ro=f(ka),De=o(ka,"TD",{});var Xl=l(De);wt=o(Xl,"A",{href:!0,rel:!0});var Zl=l(wt);no=p(Zl,"adlfs"),Zl.forEach(e),Xl.forEach(e),ka.forEach(e),io=f(L),bt=o(L,"TR",{});var ja=l(bt);Te=o(ja,"TD",{});var tr=l(Te);po=p(tr,"Dropbox"),tr.forEach(e),fo=f(ja),Le=o(ja,"TD",{});var er=l(Le);Et=o(er,"A",{href:!0,rel:!0});var sr=l(Et);co=p(sr,"dropboxdrivefs"),sr.forEach(e),er.forEach(e),ja.forEach(e),uo=f(L),kt=o(L,"TR",{});var qa=l(kt);Pe=o(qa,"TD",{});var ar=l(Pe);ho=p(ar,"Google Drive"),ar.forEach(e),mo=f(qa),ze=o(qa,"TD",{});var or=l(ze);jt=o(or,"A",{href:!0,rel:!0});var lr=l(jt);go=p(lr,"gdrivefs"),lr.forEach(e),or.forEach(e),qa.forEach(e),L.forEach(e),ya.forEach(e),ws=f(t),ie=o(t,"P",{});var rr=l(ie);_o=p(rr,`This guide will show you how to save and load datasets with any cloud storage.
Here are examples for S3, Google Cloud Storage and Azure Blob Storage.`),rr.forEach(e),bs=f(t),N=o(t,"H2",{class:!0});var Sa=l(N);K=o(Sa,"A",{id:!0,class:!0,href:!0});var nr=l(K);Ce=o(nr,"SPAN",{});var ir=l(Ce);h(qt.$$.fragment,ir),ir.forEach(e),nr.forEach(e),vo=f(Sa),Ne=o(Sa,"SPAN",{});var pr=l(Ne);$o=p(pr,"Set up your cloud storage FileSystem"),pr.forEach(e),Sa.forEach(e),Es=f(t),O=o(t,"H3",{class:!0});var Aa=l(O);W=o(Aa,"A",{id:!0,class:!0,href:!0});var dr=l(W);Oe=o(dr,"SPAN",{});var fr=l(Oe);h(St.$$.fragment,fr),fr.forEach(e),dr.forEach(e),yo=f(Aa),Fe=o(Aa,"SPAN",{});var cr=l(Fe);wo=p(cr,"Amazon S3"),cr.forEach(e),Aa.forEach(e),ks=f(t),pe=o(t,"OL",{});var ur=l(pe);Ie=o(ur,"LI",{});var hr=l(Ie);bo=p(hr,"Install the S3 dependency with \u{1F917} Datasets:"),hr.forEach(e),ur.forEach(e),js=f(t),h(At.$$.fragment,t),qs=f(t),Dt=o(t,"OL",{start:!0});var mr=l(Dt);xe=o(mr,"LI",{});var gr=l(xe);Eo=p(gr,"Define your credentials"),gr.forEach(e),mr.forEach(e),Ss=f(t),q=o(t,"P",{});var ct=l(q);ko=p(ct,"To use an anonymous connection, use "),He=o(ct,"CODE",{});var _r=l(He);jo=p(_r,"anon=True"),_r.forEach(e),qo=p(ct,`.
Otherwise, include your `),Be=o(ct,"CODE",{});var vr=l(Be);So=p(vr,"aws_access_key_id"),vr.forEach(e),Ao=p(ct," and "),Ge=o(ct,"CODE",{});var $r=l(Ge);Do=p($r,"aws_secret_access_key"),$r.forEach(e),To=p(ct," whenever you are interacting with a private S3 bucket."),ct.forEach(e),As=f(t),h(Tt.$$.fragment,t),Ds=f(t),Lt=o(t,"OL",{start:!0});var yr=l(Lt);Re=o(yr,"LI",{});var wr=l(Re);Lo=p(wr,"Load your FileSystem instance"),wr.forEach(e),yr.forEach(e),Ts=f(t),h(Pt.$$.fragment,t),Ls=f(t),F=o(t,"H3",{class:!0});var Da=l(F);J=o(Da,"A",{id:!0,class:!0,href:!0});var br=l(J);Ye=o(br,"SPAN",{});var Er=l(Ye);h(zt.$$.fragment,Er),Er.forEach(e),br.forEach(e),Po=f(Da),Me=o(Da,"SPAN",{});var kr=l(Me);zo=p(kr,"Google Cloud Storage"),kr.forEach(e),Da.forEach(e),Ps=f(t),de=o(t,"OL",{});var jr=l(de);Ue=o(jr,"LI",{});var qr=l(Ue);Co=p(qr,"Install the Google Cloud Storage implementation:"),qr.forEach(e),jr.forEach(e),zs=f(t),h(Ct.$$.fragment,t),Cs=f(t),Nt=o(t,"OL",{start:!0});var Sr=l(Nt);Ke=o(Sr,"LI",{});var Ar=l(Ke);No=p(Ar,"Define your credentials"),Ar.forEach(e),Sr.forEach(e),Ns=f(t),h(Ot.$$.fragment,t),Os=f(t),Ft=o(t,"OL",{start:!0});var Dr=l(Ft);We=o(Dr,"LI",{});var Tr=l(We);Oo=p(Tr,"Load your FileSystem instance"),Tr.forEach(e),Dr.forEach(e),Fs=f(t),h(It.$$.fragment,t),Is=f(t),I=o(t,"H3",{class:!0});var Ta=l(I);Q=o(Ta,"A",{id:!0,class:!0,href:!0});var Lr=l(Q);Je=o(Lr,"SPAN",{});var Pr=l(Je);h(xt.$$.fragment,Pr),Pr.forEach(e),Lr.forEach(e),Fo=f(Ta),Qe=o(Ta,"SPAN",{});var zr=l(Qe);Io=p(zr,"Azure Blob Storage"),zr.forEach(e),Ta.forEach(e),xs=f(t),fe=o(t,"OL",{});var Cr=l(fe);Ve=o(Cr,"LI",{});var Nr=l(Ve);xo=p(Nr,"Install the Azure Blob Storage implementation:"),Nr.forEach(e),Cr.forEach(e),Hs=f(t),h(Ht.$$.fragment,t),Bs=f(t),Bt=o(t,"OL",{start:!0});var Or=l(Bt);Xe=o(Or,"LI",{});var Fr=l(Xe);Ho=p(Fr,"Define your credentials"),Fr.forEach(e),Or.forEach(e),Gs=f(t),h(Gt.$$.fragment,t),Rs=f(t),Rt=o(t,"OL",{start:!0});var Ir=l(Rt);Ze=o(Ir,"LI",{});var xr=l(Ze);Bo=p(xr,"Load your FileSystem instance"),xr.forEach(e),Ir.forEach(e),Ys=f(t),h(Yt.$$.fragment,t),Ms=f(t),x=o(t,"H2",{class:!0});var La=l(x);V=o(La,"A",{id:!0,class:!0,href:!0});var Hr=l(V);ts=o(Hr,"SPAN",{});var Br=l(ts);h(Mt.$$.fragment,Br),Br.forEach(e),Hr.forEach(e),Go=f(La),es=o(La,"SPAN",{});var Gr=l(es);Ro=p(Gr,"Load and Save your datasets using your cloud storage FileSystem"),Gr.forEach(e),La.forEach(e),Us=f(t),H=o(t,"H3",{class:!0});var Pa=l(H);X=o(Pa,"A",{id:!0,class:!0,href:!0});var Rr=l(X);ss=o(Rr,"SPAN",{});var Yr=l(ss);h(Ut.$$.fragment,Yr),Yr.forEach(e),Rr.forEach(e),Yo=f(Pa),as=o(Pa,"SPAN",{});var Mr=l(as);Mo=p(Mr,"Load datasets into a cloud storage"),Mr.forEach(e),Pa.forEach(e),Ks=f(t),S=o(t,"P",{});var ut=l(S);Uo=p(ut,"You can load and cache a dataset into your cloud storage by specifying a remote "),os=o(ut,"CODE",{});var Ur=l(os);Ko=p(Ur,"cache_dir"),Ur.forEach(e),Wo=p(ut," in "),ls=o(ut,"CODE",{});var Kr=l(ls);Jo=p(Kr,"load_dataset"),Kr.forEach(e),Qo=p(ut,`.
Don\u2019t forget to use the previously defined `),rs=o(ut,"CODE",{});var Wr=l(rs);Vo=p(Wr,"storage_options"),Wr.forEach(e),Xo=p(ut," containing your credentials to write into a private cloud storage."),ut.forEach(e),Ws=f(t),Z=o(t,"P",{});var za=l(Z);Zo=p(za,"Load a dataset from the Hugging Face Hub (see "),ce=o(za,"A",{href:!0});var Jr=l(ce);tl=p(Jr,"how to load from the Hugging Face Hub"),Jr.forEach(e),el=p(za,"):"),za.forEach(e),Js=f(t),h(Kt.$$.fragment,t),Qs=f(t),tt=o(t,"P",{});var Ca=l(tt);sl=p(Ca,"Load a dataset using a loading script (see "),ue=o(Ca,"A",{href:!0});var Qr=l(ue);al=p(Qr,"how to load a local loading script"),Qr.forEach(e),ol=p(Ca,"):"),Ca.forEach(e),Vs=f(t),h(Wt.$$.fragment,t),Xs=f(t),et=o(t,"P",{});var Na=l(et);ll=p(Na,"Load your own data files (see "),he=o(Na,"A",{href:!0});var Vr=l(he);rl=p(Vr,"how to load local and remote files"),Vr.forEach(e),nl=p(Na,"):"),Na.forEach(e),Zs=f(t),h(Jt.$$.fragment,t),ta=f(t),st=o(t,"P",{});var Oa=l(st);il=p(Oa,"It is highly recommended to save the files as compressed sharded Parquet files to optimize I/O by specifying "),ns=o(Oa,"CODE",{});var Xr=l(ns);pl=p(Xr,'file_format="parquet"'),Xr.forEach(e),dl=p(Oa,`.
Otherwize the dataset is saved as an uncompressed Arrow file.`),Oa.forEach(e),ea=f(t),at=o(t,"P",{});var Fa=l(at);fl=p(Fa,"You can also specify the size of the Parquet shard using "),is=o(Fa,"CODE",{});var Zr=l(is);cl=p(Zr,"max_shard_size"),Zr.forEach(e),ul=p(Fa," (default is 500MB):"),Fa.forEach(e),sa=f(t),h(Qt.$$.fragment,t),aa=f(t),B=o(t,"H4",{class:!0});var Ia=l(B);ot=o(Ia,"A",{id:!0,class:!0,href:!0});var tn=l(ot);ps=o(tn,"SPAN",{});var en=l(ps);h(Vt.$$.fragment,en),en.forEach(e),tn.forEach(e),hl=f(Ia),ds=o(Ia,"SPAN",{});var sn=l(ds);ml=p(sn,"Dask"),sn.forEach(e),Ia.forEach(e),oa=f(t),me=o(t,"P",{});var an=l(me);gl=p(an,`Dask is a parallel computing library and it has a pandas-like API for working with larger than memory Parquet datasets in parallel.
Dask can use multiple threads or processes on a single machine, or a cluster of machines to process data in parallel.
Dask supports local data but also data from a cloud storage.`),an.forEach(e),la=f(t),ge=o(t,"P",{});var on=l(ge);_l=p(on,"Therefore you can load a dataset saved as sharded Parquet files in Dask with"),on.forEach(e),ra=f(t),h(Xt.$$.fragment,t),na=f(t),lt=o(t,"P",{});var xa=l(lt);vl=p(xa,"You can find more about dask dataframes in their "),Zt=o(xa,"A",{href:!0,rel:!0});var ln=l(Zt);$l=p(ln,"documentation"),ln.forEach(e),yl=p(xa,"."),xa.forEach(e),ia=f(t),G=o(t,"H2",{class:!0});var Ha=l(G);rt=o(Ha,"A",{id:!0,class:!0,href:!0});var rn=l(rt);fs=o(rn,"SPAN",{});var nn=l(fs);h(te.$$.fragment,nn),nn.forEach(e),rn.forEach(e),wl=f(Ha),cs=o(Ha,"SPAN",{});var pn=l(cs);bl=p(pn,"Saving serialized datasets"),pn.forEach(e),Ha.forEach(e),pa=f(t),nt=o(t,"P",{});var Ba=l(nt);El=p(Ba,"After you have processed your dataset, you can save it to your cloud storage with "),_e=o(Ba,"A",{href:!0});var dn=l(_e);kl=p(dn,"Dataset.save_to_disk()"),dn.forEach(e),jl=p(Ba,":"),Ba.forEach(e),da=f(t),h(ee.$$.fragment,t),fa=f(t),h(it.$$.fragment,t),ca=f(t),R=o(t,"H2",{class:!0});var Ga=l(R);pt=o(Ga,"A",{id:!0,class:!0,href:!0});var fn=l(pt);us=o(fn,"SPAN",{});var cn=l(us);h(se.$$.fragment,cn),cn.forEach(e),fn.forEach(e),ql=f(Ga),hs=o(Ga,"SPAN",{});var un=l(hs);Sl=p(un,"Listing serialized datasets"),un.forEach(e),Ga.forEach(e),ua=f(t),T=o(t,"P",{});var $e=l(T);Al=p($e,"List files from a cloud storage with your FileSystem instance "),ms=o($e,"CODE",{});var hn=l(ms);Dl=p(hn,"fs"),hn.forEach(e),Tl=p($e,", using "),gs=o($e,"CODE",{});var mn=l(gs);Ll=p(mn,"fs.ls"),mn.forEach(e),Pl=p($e,":"),$e.forEach(e),ha=f(t),h(ae.$$.fragment,t),ma=f(t),Y=o(t,"H3",{class:!0});var Ra=l(Y);dt=o(Ra,"A",{id:!0,class:!0,href:!0});var gn=l(dt);_s=o(gn,"SPAN",{});var _n=l(_s);h(oe.$$.fragment,_n),_n.forEach(e),gn.forEach(e),zl=f(Ra),vs=o(Ra,"SPAN",{});var vn=l(vs);Cl=p(vn,"Load serialized datasets"),vn.forEach(e),Ra.forEach(e),ga=f(t),ft=o(t,"P",{});var Ya=l(ft);Nl=p(Ya,"When you are ready to use your dataset again, reload it with "),ve=o(Ya,"A",{href:!0});var $n=l(ve);Ol=p($n,"Dataset.load_from_disk()"),$n.forEach(e),Fl=p(Ya,":"),Ya.forEach(e),_a=f(t),h(le.$$.fragment,t),this.h()},h(){c($,"name","hf:doc:metadata"),c($,"content",JSON.stringify(An)),c(j,"id","cloud-storage"),c(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j,"href","#cloud-storage"),c(w,"class","relative group"),c(_t,"href","https://s3fs.readthedocs.io/en/latest/"),c(_t,"rel","nofollow"),c($t,"href","https://gcsfs.readthedocs.io/en/latest/"),c($t,"rel","nofollow"),c(wt,"href","https://github.com/fsspec/adlfs"),c(wt,"rel","nofollow"),c(Et,"href","https://github.com/MarineChap/dropboxdrivefs"),c(Et,"rel","nofollow"),c(jt,"href","https://github.com/intake/gdrivefs"),c(jt,"rel","nofollow"),c(K,"id","set-up-your-cloud-storage-filesystem"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#set-up-your-cloud-storage-filesystem"),c(N,"class","relative group"),c(W,"id","amazon-s3"),c(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W,"href","#amazon-s3"),c(O,"class","relative group"),c(Dt,"start","2"),c(Lt,"start","3"),c(J,"id","google-cloud-storage"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#google-cloud-storage"),c(F,"class","relative group"),c(Nt,"start","2"),c(Ft,"start","3"),c(Q,"id","azure-blob-storage"),c(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Q,"href","#azure-blob-storage"),c(I,"class","relative group"),c(Bt,"start","2"),c(Rt,"start","3"),c(V,"id","load-and-save-your-datasets-using-your-cloud-storage-filesystem"),c(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(V,"href","#load-and-save-your-datasets-using-your-cloud-storage-filesystem"),c(x,"class","relative group"),c(X,"id","load-datasets-into-a-cloud-storage"),c(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X,"href","#load-datasets-into-a-cloud-storage"),c(H,"class","relative group"),c(ce,"href","./loading#hugging-face-hub"),c(ue,"href","./loading#local-loading-script"),c(he,"href","./loading#local-and-remote-files"),c(ot,"id","dask"),c(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ot,"href","#dask"),c(B,"class","relative group"),c(Zt,"href","https://docs.dask.org/en/stable/dataframe.html"),c(Zt,"rel","nofollow"),c(rt,"id","saving-serialized-datasets"),c(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rt,"href","#saving-serialized-datasets"),c(G,"class","relative group"),c(_e,"href","/docs/datasets/pr_4747/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),c(pt,"id","listing-serialized-datasets"),c(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pt,"href","#listing-serialized-datasets"),c(R,"class","relative group"),c(dt,"id","load-serialized-datasets"),c(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dt,"href","#load-serialized-datasets"),c(Y,"class","relative group"),c(ve,"href","/docs/datasets/pr_4747/en/package_reference/main_classes#datasets.Dataset.load_from_disk")},m(t,r){s(document.head,$),n(t,M,r),n(t,w,r),s(w,j),s(j,P),m(b,P,null),s(w,ht),s(w,z),s(z,C),n(t,E,r),n(t,A,r),s(A,ne),s(A,ye),s(ye,Ma),s(A,Ua),n(t,ys,r),n(t,U,r),s(U,we),s(we,mt),s(mt,be),s(be,Ka),s(mt,Wa),s(mt,Ee),s(Ee,Ja),s(U,Qa),s(U,k),s(k,gt),s(gt,ke),s(ke,Va),s(gt,Xa),s(gt,je),s(je,_t),s(_t,Za),s(k,to),s(k,vt),s(vt,qe),s(qe,eo),s(vt,so),s(vt,Se),s(Se,$t),s($t,ao),s(k,oo),s(k,yt),s(yt,Ae),s(Ae,lo),s(yt,ro),s(yt,De),s(De,wt),s(wt,no),s(k,io),s(k,bt),s(bt,Te),s(Te,po),s(bt,fo),s(bt,Le),s(Le,Et),s(Et,co),s(k,uo),s(k,kt),s(kt,Pe),s(Pe,ho),s(kt,mo),s(kt,ze),s(ze,jt),s(jt,go),n(t,ws,r),n(t,ie,r),s(ie,_o),n(t,bs,r),n(t,N,r),s(N,K),s(K,Ce),m(qt,Ce,null),s(N,vo),s(N,Ne),s(Ne,$o),n(t,Es,r),n(t,O,r),s(O,W),s(W,Oe),m(St,Oe,null),s(O,yo),s(O,Fe),s(Fe,wo),n(t,ks,r),n(t,pe,r),s(pe,Ie),s(Ie,bo),n(t,js,r),m(At,t,r),n(t,qs,r),n(t,Dt,r),s(Dt,xe),s(xe,Eo),n(t,Ss,r),n(t,q,r),s(q,ko),s(q,He),s(He,jo),s(q,qo),s(q,Be),s(Be,So),s(q,Ao),s(q,Ge),s(Ge,Do),s(q,To),n(t,As,r),m(Tt,t,r),n(t,Ds,r),n(t,Lt,r),s(Lt,Re),s(Re,Lo),n(t,Ts,r),m(Pt,t,r),n(t,Ls,r),n(t,F,r),s(F,J),s(J,Ye),m(zt,Ye,null),s(F,Po),s(F,Me),s(Me,zo),n(t,Ps,r),n(t,de,r),s(de,Ue),s(Ue,Co),n(t,zs,r),m(Ct,t,r),n(t,Cs,r),n(t,Nt,r),s(Nt,Ke),s(Ke,No),n(t,Ns,r),m(Ot,t,r),n(t,Os,r),n(t,Ft,r),s(Ft,We),s(We,Oo),n(t,Fs,r),m(It,t,r),n(t,Is,r),n(t,I,r),s(I,Q),s(Q,Je),m(xt,Je,null),s(I,Fo),s(I,Qe),s(Qe,Io),n(t,xs,r),n(t,fe,r),s(fe,Ve),s(Ve,xo),n(t,Hs,r),m(Ht,t,r),n(t,Bs,r),n(t,Bt,r),s(Bt,Xe),s(Xe,Ho),n(t,Gs,r),m(Gt,t,r),n(t,Rs,r),n(t,Rt,r),s(Rt,Ze),s(Ze,Bo),n(t,Ys,r),m(Yt,t,r),n(t,Ms,r),n(t,x,r),s(x,V),s(V,ts),m(Mt,ts,null),s(x,Go),s(x,es),s(es,Ro),n(t,Us,r),n(t,H,r),s(H,X),s(X,ss),m(Ut,ss,null),s(H,Yo),s(H,as),s(as,Mo),n(t,Ks,r),n(t,S,r),s(S,Uo),s(S,os),s(os,Ko),s(S,Wo),s(S,ls),s(ls,Jo),s(S,Qo),s(S,rs),s(rs,Vo),s(S,Xo),n(t,Ws,r),n(t,Z,r),s(Z,Zo),s(Z,ce),s(ce,tl),s(Z,el),n(t,Js,r),m(Kt,t,r),n(t,Qs,r),n(t,tt,r),s(tt,sl),s(tt,ue),s(ue,al),s(tt,ol),n(t,Vs,r),m(Wt,t,r),n(t,Xs,r),n(t,et,r),s(et,ll),s(et,he),s(he,rl),s(et,nl),n(t,Zs,r),m(Jt,t,r),n(t,ta,r),n(t,st,r),s(st,il),s(st,ns),s(ns,pl),s(st,dl),n(t,ea,r),n(t,at,r),s(at,fl),s(at,is),s(is,cl),s(at,ul),n(t,sa,r),m(Qt,t,r),n(t,aa,r),n(t,B,r),s(B,ot),s(ot,ps),m(Vt,ps,null),s(B,hl),s(B,ds),s(ds,ml),n(t,oa,r),n(t,me,r),s(me,gl),n(t,la,r),n(t,ge,r),s(ge,_l),n(t,ra,r),m(Xt,t,r),n(t,na,r),n(t,lt,r),s(lt,vl),s(lt,Zt),s(Zt,$l),s(lt,yl),n(t,ia,r),n(t,G,r),s(G,rt),s(rt,fs),m(te,fs,null),s(G,wl),s(G,cs),s(cs,bl),n(t,pa,r),n(t,nt,r),s(nt,El),s(nt,_e),s(_e,kl),s(nt,jl),n(t,da,r),m(ee,t,r),n(t,fa,r),m(it,t,r),n(t,ca,r),n(t,R,r),s(R,pt),s(pt,us),m(se,us,null),s(R,ql),s(R,hs),s(hs,Sl),n(t,ua,r),n(t,T,r),s(T,Al),s(T,ms),s(ms,Dl),s(T,Tl),s(T,gs),s(gs,Ll),s(T,Pl),n(t,ha,r),m(ae,t,r),n(t,ma,r),n(t,Y,r),s(Y,dt),s(dt,_s),m(oe,_s,null),s(Y,zl),s(Y,vs),s(vs,Cl),n(t,ga,r),n(t,ft,r),s(ft,Nl),s(ft,ve),s(ve,Ol),s(ft,Fl),n(t,_a,r),m(le,t,r),va=!0},p(t,[r]){const re={};r&2&&(re.$$scope={dirty:r,ctx:t}),it.$set(re)},i(t){va||(g(b.$$.fragment,t),g(qt.$$.fragment,t),g(St.$$.fragment,t),g(At.$$.fragment,t),g(Tt.$$.fragment,t),g(Pt.$$.fragment,t),g(zt.$$.fragment,t),g(Ct.$$.fragment,t),g(Ot.$$.fragment,t),g(It.$$.fragment,t),g(xt.$$.fragment,t),g(Ht.$$.fragment,t),g(Gt.$$.fragment,t),g(Yt.$$.fragment,t),g(Mt.$$.fragment,t),g(Ut.$$.fragment,t),g(Kt.$$.fragment,t),g(Wt.$$.fragment,t),g(Jt.$$.fragment,t),g(Qt.$$.fragment,t),g(Vt.$$.fragment,t),g(Xt.$$.fragment,t),g(te.$$.fragment,t),g(ee.$$.fragment,t),g(it.$$.fragment,t),g(se.$$.fragment,t),g(ae.$$.fragment,t),g(oe.$$.fragment,t),g(le.$$.fragment,t),va=!0)},o(t){_(b.$$.fragment,t),_(qt.$$.fragment,t),_(St.$$.fragment,t),_(At.$$.fragment,t),_(Tt.$$.fragment,t),_(Pt.$$.fragment,t),_(zt.$$.fragment,t),_(Ct.$$.fragment,t),_(Ot.$$.fragment,t),_(It.$$.fragment,t),_(xt.$$.fragment,t),_(Ht.$$.fragment,t),_(Gt.$$.fragment,t),_(Yt.$$.fragment,t),_(Mt.$$.fragment,t),_(Ut.$$.fragment,t),_(Kt.$$.fragment,t),_(Wt.$$.fragment,t),_(Jt.$$.fragment,t),_(Qt.$$.fragment,t),_(Vt.$$.fragment,t),_(Xt.$$.fragment,t),_(te.$$.fragment,t),_(ee.$$.fragment,t),_(it.$$.fragment,t),_(se.$$.fragment,t),_(ae.$$.fragment,t),_(oe.$$.fragment,t),_(le.$$.fragment,t),va=!1},d(t){e($),t&&e(M),t&&e(w),v(b),t&&e(E),t&&e(A),t&&e(ys),t&&e(U),t&&e(ws),t&&e(ie),t&&e(bs),t&&e(N),v(qt),t&&e(Es),t&&e(O),v(St),t&&e(ks),t&&e(pe),t&&e(js),v(At,t),t&&e(qs),t&&e(Dt),t&&e(Ss),t&&e(q),t&&e(As),v(Tt,t),t&&e(Ds),t&&e(Lt),t&&e(Ts),v(Pt,t),t&&e(Ls),t&&e(F),v(zt),t&&e(Ps),t&&e(de),t&&e(zs),v(Ct,t),t&&e(Cs),t&&e(Nt),t&&e(Ns),v(Ot,t),t&&e(Os),t&&e(Ft),t&&e(Fs),v(It,t),t&&e(Is),t&&e(I),v(xt),t&&e(xs),t&&e(fe),t&&e(Hs),v(Ht,t),t&&e(Bs),t&&e(Bt),t&&e(Gs),v(Gt,t),t&&e(Rs),t&&e(Rt),t&&e(Ys),v(Yt,t),t&&e(Ms),t&&e(x),v(Mt),t&&e(Us),t&&e(H),v(Ut),t&&e(Ks),t&&e(S),t&&e(Ws),t&&e(Z),t&&e(Js),v(Kt,t),t&&e(Qs),t&&e(tt),t&&e(Vs),v(Wt,t),t&&e(Xs),t&&e(et),t&&e(Zs),v(Jt,t),t&&e(ta),t&&e(st),t&&e(ea),t&&e(at),t&&e(sa),v(Qt,t),t&&e(aa),t&&e(B),v(Vt),t&&e(oa),t&&e(me),t&&e(la),t&&e(ge),t&&e(ra),v(Xt,t),t&&e(na),t&&e(lt),t&&e(ia),t&&e(G),v(te),t&&e(pa),t&&e(nt),t&&e(da),v(ee,t),t&&e(fa),v(it,t),t&&e(ca),t&&e(R),v(se),t&&e(ua),t&&e(T),t&&e(ha),v(ae,t),t&&e(ma),t&&e(Y),v(oe),t&&e(ga),t&&e(ft),t&&e(_a),v(le,t)}}}const An={local:"cloud-storage",sections:[{local:"set-up-your-cloud-storage-filesystem",sections:[{local:"amazon-s3",title:"Amazon S3"},{local:"google-cloud-storage",title:"Google Cloud Storage"},{local:"azure-blob-storage",title:"Azure Blob Storage"}],title:"Set up your cloud storage FileSystem"},{local:"load-and-save-your-datasets-using-your-cloud-storage-filesystem",sections:[{local:"load-datasets-into-a-cloud-storage",sections:[{local:"dask",title:"Dask"}],title:"Load datasets into a cloud storage"}],title:"Load and Save your datasets using your cloud storage FileSystem"},{local:"saving-serialized-datasets",title:"Saving serialized datasets"},{local:"listing-serialized-datasets",sections:[{local:"load-serialized-datasets",title:"Load serialized datasets"}],title:"Listing serialized datasets"}],title:"Cloud storage"};function Dn($s){return kn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Cn extends yn{constructor($){super();wn(this,$,Dn,Sn,bn,{})}}export{Cn as default,An as metadata};
