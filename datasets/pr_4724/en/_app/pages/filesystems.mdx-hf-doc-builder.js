import{S as dn,i as fn,s as cn,e as a,k as d,w as u,t as i,M as un,c as o,d as e,m as f,a as l,x as h,h as p,b as c,G as s,g as n,y as m,q as g,o as _,B as v,v as hn}from"../chunks/vendor-hf-doc-builder.js";import{T as mn}from"../chunks/Tip-hf-doc-builder.js";import{I as D}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as w}from"../chunks/CodeBlock-hf-doc-builder.js";function gn(gs){let y,M,$,j,P,b,ut,z;return{c(){y=a("p"),M=i("Remember to define your credentials in your "),$=a("a"),j=i("FileSystem instance"),P=d(),b=a("code"),ut=i("fs"),z=i(" whenever you are interacting with a private cloud storage."),this.h()},l(C){y=o(C,"P",{});var E=l(y);M=p(E,"Remember to define your credentials in your "),$=o(E,"A",{href:!0});var A=l($);j=p(A,"FileSystem instance"),A.forEach(e),P=f(E),b=o(E,"CODE",{});var le=l(b);ut=p(le,"fs"),le.forEach(e),z=p(E," whenever you are interacting with a private cloud storage."),E.forEach(e),this.h()},h(){c($,"href","#set-up-your-cloud-storage-filesystem")},m(C,E){n(C,y,E),s(y,M),s(y,$),s($,j),s(y,P),s(y,b),s(b,ut),s(y,z)},d(C){C&&e(y)}}}function _n(gs){let y,M,$,j,P,b,ut,z,C,E,A,le,ve,xa,Ha,_s,U,ye,ht,$e,Ba,Ga,we,Ra,Ya,k,mt,be,Ma,Ua,Ee,gt,Ka,Wa,_t,ke,Ja,Qa,je,vt,Va,Xa,yt,qe,Za,to,Se,$t,eo,so,wt,Ae,ao,oo,De,bt,lo,ro,Et,Te,no,io,Le,kt,po,vs,re,fo,ys,N,K,Pe,jt,co,ze,uo,$s,O,W,Ce,qt,ho,Ne,mo,ws,ne,Oe,go,bs,St,Es,At,Fe,_o,ks,q,vo,Ie,yo,$o,xe,wo,bo,He,Eo,ko,js,Dt,qs,Tt,Be,jo,Ss,Lt,As,F,J,Ge,Pt,qo,Re,So,Ds,ie,Ye,Ao,Ts,zt,Ls,Ct,Me,Do,Ps,Nt,zs,Ot,Ue,To,Cs,Ft,Ns,I,Q,Ke,It,Lo,We,Po,Os,pe,Je,zo,Fs,xt,Is,Ht,Qe,Co,xs,Bt,Hs,Gt,Ve,No,Bs,Rt,Gs,x,V,Xe,Yt,Oo,Ze,Fo,Rs,H,X,ts,Mt,Io,es,xo,Ys,S,Ho,ss,Bo,Go,as,Ro,Yo,os,Mo,Uo,Ms,Z,Ko,de,Wo,Jo,Us,Ut,Ks,tt,Qo,fe,Vo,Xo,Ws,Kt,Js,et,Zo,ce,tl,el,Qs,Wt,Vs,st,sl,ls,al,ol,Xs,B,at,rs,Jt,ll,ns,rl,Zs,ue,nl,ta,he,il,ea,Qt,sa,ot,pl,Vt,dl,fl,aa,G,lt,is,Xt,cl,ps,ul,oa,rt,hl,me,ml,gl,la,Zt,ra,nt,na,R,it,ds,te,_l,fs,vl,ia,T,yl,cs,$l,wl,us,bl,El,pa,ee,da,Y,pt,hs,se,kl,ms,jl,fa,dt,ql,ge,Sl,Al,ca,ae,ua;return b=new D({}),jt=new D({}),qt=new D({}),St=new w({props:{code:"pip install datasets[s3]",highlighted:'&gt;&gt;&gt; pip <span class="hljs-keyword">install </span>datasets[<span class="hljs-built_in">s3</span>]'}}),Dt=new w({props:{code:`storage_options = {"anon": True}  # for anynonous connection
storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key}  # for private buckets
import botocore
s3_session = botocore.session.Session(profile="my_profile_name")
storage_options = {"session": s3_session}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;anon&quot;</span>: <span class="hljs-literal">True</span>}  <span class="hljs-comment"># for anynonous connection</span>
<span class="hljs-comment"># or use your credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;key&quot;</span>: aws_access_key_id, <span class="hljs-string">&quot;secret&quot;</span>: aws_secret_access_key}  <span class="hljs-comment"># for private buckets</span>
<span class="hljs-comment"># or use a botocore session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&quot;my_profile_name&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;session&quot;</span>: s3_session}`}}),Lt=new w({props:{code:`import s3fs
fs = s3fs.S3FileSystem(**storage_options)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> s3fs
<span class="hljs-meta">&gt;&gt;&gt; </span>fs = s3fs.S3FileSystem(**storage_options)`}}),Pt=new D({}),zt=new w({props:{code:`conda install -c conda-forge gcsfs
pip install gcsfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge gcsfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install gcsfs</span>`}}),Nt=new w({props:{code:`storage_options={"token": "anon"}  # for anonymous connection
storage_options={"project": "my-google-project"}
storage_options={"project": "my-google-project", "token": TOKEN}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-string">&quot;anon&quot;</span>}  <span class="hljs-comment"># for anonymous connection</span>
<span class="hljs-comment"># or use your credentials of your default gcloud credentials or from the google metadata service</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;project&quot;</span>: <span class="hljs-string">&quot;my-google-project&quot;</span>}
<span class="hljs-comment"># or use your credentials from elsewhere, see the documentation at https://gcsfs.readthedocs.io/</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;project&quot;</span>: <span class="hljs-string">&quot;my-google-project&quot;</span>, <span class="hljs-string">&quot;token&quot;</span>: TOKEN}`}}),Ft=new w({props:{code:`import gcsfs
fs = gcsfs.GCSFileSystem(**storage_options)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs
<span class="hljs-meta">&gt;&gt;&gt; </span>fs = gcsfs.GCSFileSystem(**storage_options)`}}),It=new D({}),xt=new w({props:{code:`conda install -c conda-forge adlfs
pip install adlfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge adlfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install adlfs</span>`}}),Bt=new w({props:{code:`storage_options = {"anon": True}  # for anonymous connection
storage_options = {"account_name": ACCOUNT_NAME, "account_key": ACCOUNT_KEY)  # gen 2 filesystem
storage_options={"tenant_id": TENANT_ID, "client_id": CLIENT_ID, "client_secret": CLIENT_SECRET}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;anon&quot;</span>: <span class="hljs-literal">True</span>}  <span class="hljs-comment"># for anonymous connection</span>
<span class="hljs-comment"># or use your credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;account_name&quot;</span>: ACCOUNT_NAME, <span class="hljs-string">&quot;account_key&quot;</span>: ACCOUNT_KEY)  <span class="hljs-comment"># gen 2 filesystem</span>
<span class="hljs-comment"># or use your credentials with the gen 1 filesystem</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;tenant_id&quot;</span>: TENANT_ID, <span class="hljs-string">&quot;client_id&quot;</span>: CLIENT_ID, <span class="hljs-string">&quot;client_secret&quot;</span>: CLIENT_SECRET}`}}),Rt=new w({props:{code:`import adlfs
fs = adlfs.AzureBlobFileSystem(**storage_options)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs
<span class="hljs-meta">&gt;&gt;&gt; </span>fs = adlfs.AzureBlobFileSystem(**storage_options)`}}),Yt=new D({}),Mt=new D({}),Ut=new w({props:{code:`cache_dir = "s3://my-bucket/datasets-cache"
builder = load_dataset_builder("imdb", cache_dir=cache_dir, storage_options=storage_options)
builder.download_and_prepare(file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>cache_dir = <span class="hljs-string">&quot;s3://my-bucket/datasets-cache&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;imdb&quot;</span>, cache_dir=cache_dir, storage_options=storage_options)
<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),Kt=new w({props:{code:`cache_dir = "s3://my-bucket/datasets-cache"
builder = load_dataset_builder("path/to/local/loading_script/loading_script.py", cache_dir=cache_dir, storage_options=storage_options)
builder.download_and_prepare(file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>cache_dir = <span class="hljs-string">&quot;s3://my-bucket/datasets-cache&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;path/to/local/loading_script/loading_script.py&quot;</span>, cache_dir=cache_dir, storage_options=storage_options)
<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),Wt=new w({props:{code:`data_files = {"train": ["path/to/train.csv"]}
cache_dir = "s3://my-bucket/datasets-cache"
builder = load_dataset_builder("csv", data_files=data_files, cache_dir=cache_dir, storage_options=storage_options)
builder.download_and_prepare(file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: [<span class="hljs-string">&quot;path/to/train.csv&quot;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>cache_dir = <span class="hljs-string">&quot;s3://my-bucket/datasets-cache&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files, cache_dir=cache_dir, storage_options=storage_options)
<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),Jt=new D({}),Qt=new w({props:{code:`import dask.dataframe as dd

df = dd.read_parquet(builder.cache_dir, storage_options=storage_options)

# or if your dataset is split into train/valid/test
df_train = dd.read_parquet(builder.cache_dir + f"/{builder.name}-train-*.parquet", storage_options=storage_options)
df_valid = dd.read_parquet(builder.cache_dir + f"/{builder.name}-validation-*.parquet", storage_options=storage_options)
df_test = dd.read_parquet(builder.cache_dir + f"/{builder.name}-test-*.parquet", storage_options=storage_options)`,highlighted:`<span class="hljs-keyword">import</span> dask.dataframe <span class="hljs-keyword">as</span> dd

df = dd.read_parquet(builder.cache_dir, storage_options=storage_options)

<span class="hljs-comment"># or if your dataset is split into train/valid/test</span>
df_train = dd.read_parquet(builder.cache_dir + <span class="hljs-string">f&quot;/<span class="hljs-subst">{builder.name}</span>-train-*.parquet&quot;</span>, storage_options=storage_options)
df_valid = dd.read_parquet(builder.cache_dir + <span class="hljs-string">f&quot;/<span class="hljs-subst">{builder.name}</span>-validation-*.parquet&quot;</span>, storage_options=storage_options)
df_test = dd.read_parquet(builder.cache_dir + <span class="hljs-string">f&quot;/<span class="hljs-subst">{builder.name}</span>-test-*.parquet&quot;</span>, storage_options=storage_options)`}}),Xt=new D({}),Zt=new w({props:{code:`encoded_dataset.save_to_disk("s3://my-private-datasets/imdb/train", fs=fs)
encoded_dataset.save_to_disk("gcs://my-private-datasets/imdb/train", fs=fs)
encoded_dataset.save_to_disk("adl://my-private-datasets/imdb/train", fs=fs)`,highlighted:`<span class="hljs-comment"># saves encoded_dataset to amazon s3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;s3://my-private-datasets/imdb/train&quot;</span>, fs=fs)
<span class="hljs-comment"># saves encoded_dataset to google cloud storage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;gcs://my-private-datasets/imdb/train&quot;</span>, fs=fs)
<span class="hljs-comment"># saves encoded_dataset to microsoft azure blob/datalake</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;adl://my-private-datasets/imdb/train&quot;</span>, fs=fs)`}}),nt=new mn({props:{$$slots:{default:[gn]},$$scope:{ctx:gs}}}),te=new D({}),ee=new w({props:{code:'fs.ls("my-private-datasets/imdb/train")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>fs.ls(<span class="hljs-string">&quot;my-private-datasets/imdb/train&quot;</span>)
[<span class="hljs-string">&quot;dataset_info.json.json&quot;</span>,<span class="hljs-string">&quot;dataset.arrow&quot;</span>,<span class="hljs-string">&quot;state.json&quot;</span>]`}}),se=new D({}),ae=new w({props:{code:`from datasets import load_from_disk
dataset = load_from_disk("s3://a-public-datasets/imdb/train", fs=fs)  
print(len(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-comment"># load encoded_dataset from cloud storage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&quot;s3://a-public-datasets/imdb/train&quot;</span>, fs=fs)  
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-number">25000</span>`}}),{c(){y=a("meta"),M=d(),$=a("h1"),j=a("a"),P=a("span"),u(b.$$.fragment),ut=d(),z=a("span"),C=i("Cloud storage"),E=d(),A=a("p"),le=i("\u{1F917} Datasets supports access to cloud storage providers through a "),ve=a("code"),xa=i("fsspec"),Ha=i(` FileSystem implementations.
You can save and load datasets from any cloud storage in a Pythonic way.
Take a look at the following table for some example of supported cloud storage providers:`),_s=d(),U=a("table"),ye=a("thead"),ht=a("tr"),$e=a("th"),Ba=i("Storage provider"),Ga=d(),we=a("th"),Ra=i("Filesystem implementation"),Ya=d(),k=a("tbody"),mt=a("tr"),be=a("td"),Ma=i("Amazon S3"),Ua=d(),Ee=a("td"),gt=a("a"),Ka=i("s3fs"),Wa=d(),_t=a("tr"),ke=a("td"),Ja=i("Google Cloud Storage"),Qa=d(),je=a("td"),vt=a("a"),Va=i("gcsfs"),Xa=d(),yt=a("tr"),qe=a("td"),Za=i("Azure Blob/DataLake"),to=d(),Se=a("td"),$t=a("a"),eo=i("adlfs"),so=d(),wt=a("tr"),Ae=a("td"),ao=i("Dropbox"),oo=d(),De=a("td"),bt=a("a"),lo=i("dropboxdrivefs"),ro=d(),Et=a("tr"),Te=a("td"),no=i("Google Drive"),io=d(),Le=a("td"),kt=a("a"),po=i("gdrivefs"),vs=d(),re=a("p"),fo=i(`This guide will show you how to save and load datasets with any cloud storage.
Here are examples for S3, Google Cloud Storage and Azure Blob Storage.`),ys=d(),N=a("h2"),K=a("a"),Pe=a("span"),u(jt.$$.fragment),co=d(),ze=a("span"),uo=i("Set up your cloud storage FileSystem"),$s=d(),O=a("h3"),W=a("a"),Ce=a("span"),u(qt.$$.fragment),ho=d(),Ne=a("span"),mo=i("Amazon S3"),ws=d(),ne=a("ol"),Oe=a("li"),go=i("Install the S3 dependency with \u{1F917} Datasets:"),bs=d(),u(St.$$.fragment),Es=d(),At=a("ol"),Fe=a("li"),_o=i("Define your credentials"),ks=d(),q=a("p"),vo=i("To use an anonymous connection, use "),Ie=a("code"),yo=i("anon=True"),$o=i(`.
Otherwise, include your `),xe=a("code"),wo=i("aws_access_key_id"),bo=i(" and "),He=a("code"),Eo=i("aws_secret_access_key"),ko=i(" whenever you are interacting with a private S3 bucket."),js=d(),u(Dt.$$.fragment),qs=d(),Tt=a("ol"),Be=a("li"),jo=i("Load your FileSystem instance"),Ss=d(),u(Lt.$$.fragment),As=d(),F=a("h3"),J=a("a"),Ge=a("span"),u(Pt.$$.fragment),qo=d(),Re=a("span"),So=i("Google Cloud Storage"),Ds=d(),ie=a("ol"),Ye=a("li"),Ao=i("Install the Google Cloud Storage implementation:"),Ts=d(),u(zt.$$.fragment),Ls=d(),Ct=a("ol"),Me=a("li"),Do=i("Define your credentials"),Ps=d(),u(Nt.$$.fragment),zs=d(),Ot=a("ol"),Ue=a("li"),To=i("Load your FileSystem instance"),Cs=d(),u(Ft.$$.fragment),Ns=d(),I=a("h3"),Q=a("a"),Ke=a("span"),u(It.$$.fragment),Lo=d(),We=a("span"),Po=i("Azure Blob Storage"),Os=d(),pe=a("ol"),Je=a("li"),zo=i("Install the Azure Blob Storage implementation:"),Fs=d(),u(xt.$$.fragment),Is=d(),Ht=a("ol"),Qe=a("li"),Co=i("Define your credentials"),xs=d(),u(Bt.$$.fragment),Hs=d(),Gt=a("ol"),Ve=a("li"),No=i("Load your FileSystem instance"),Bs=d(),u(Rt.$$.fragment),Gs=d(),x=a("h2"),V=a("a"),Xe=a("span"),u(Yt.$$.fragment),Oo=d(),Ze=a("span"),Fo=i("Load and Save your datasets using your cloud storage FileSystem"),Rs=d(),H=a("h3"),X=a("a"),ts=a("span"),u(Mt.$$.fragment),Io=d(),es=a("span"),xo=i("Load datasets into a cloud storage"),Ys=d(),S=a("p"),Ho=i("You can load and cache a dataset into your cloud storage by specifying a remote "),ss=a("code"),Bo=i("cache_dir"),Go=i(" in "),as=a("code"),Ro=i("load_dataset"),Yo=i(`.
Don\u2019t forget to use the previously defined `),os=a("code"),Mo=i("storage_options"),Uo=i(" containing your credentials to write into a private cloud storage."),Ms=d(),Z=a("p"),Ko=i("Load a dataset from the Hugging Face Hub (see "),de=a("a"),Wo=i("how to load from the Hugging Face Hub"),Jo=i("):"),Us=d(),u(Ut.$$.fragment),Ks=d(),tt=a("p"),Qo=i("Load a dataset using a loading script (see "),fe=a("a"),Vo=i("how to load a local loading script"),Xo=i("):"),Ws=d(),u(Kt.$$.fragment),Js=d(),et=a("p"),Zo=i("Load your own data files (see "),ce=a("a"),tl=i("how to load local and remote files"),el=i("):"),Qs=d(),u(Wt.$$.fragment),Vs=d(),st=a("p"),sl=i("It is highly recommended to save the files as compressed Parquet files to optimize I/O by specifying "),ls=a("code"),al=i('file_format="parquet"'),ol=i(`.
Otherwize the dataset is saved as an uncompressed Arrow file.`),Xs=d(),B=a("h4"),at=a("a"),rs=a("span"),u(Jt.$$.fragment),ll=d(),ns=a("span"),rl=i("Dask"),Zs=d(),ue=a("p"),nl=i(`Dask is a parallel computing library and it has a pandas-like API for working with larger than memory Parquet datasets in parallel.
Dask can use multiple threads or processes on a single machine, or a cluster of machines to process data in parallel.
Dask supports local data but also data from a cloud storage.`),ta=d(),he=a("p"),il=i("Therefore you can load a dataset saved as sharded Parquet files in Dask with"),ea=d(),u(Qt.$$.fragment),sa=d(),ot=a("p"),pl=i("You can find more about dask dataframes in their "),Vt=a("a"),dl=i("documentation"),fl=i("."),aa=d(),G=a("h2"),lt=a("a"),is=a("span"),u(Xt.$$.fragment),cl=d(),ps=a("span"),ul=i("Saving serialized datasets"),oa=d(),rt=a("p"),hl=i("After you have processed your dataset, you can save it to your cloud storage with "),me=a("a"),ml=i("Dataset.save_to_disk()"),gl=i(":"),la=d(),u(Zt.$$.fragment),ra=d(),u(nt.$$.fragment),na=d(),R=a("h2"),it=a("a"),ds=a("span"),u(te.$$.fragment),_l=d(),fs=a("span"),vl=i("Listing serialized datasets"),ia=d(),T=a("p"),yl=i("List files from a cloud storage with your FileSystem instance "),cs=a("code"),$l=i("fs"),wl=i(", using "),us=a("code"),bl=i("fs.ls"),El=i(":"),pa=d(),u(ee.$$.fragment),da=d(),Y=a("h3"),pt=a("a"),hs=a("span"),u(se.$$.fragment),kl=d(),ms=a("span"),jl=i("Load serialized datasets"),fa=d(),dt=a("p"),ql=i("When you are ready to use your dataset again, reload it with "),ge=a("a"),Sl=i("Dataset.load_from_disk()"),Al=i(":"),ca=d(),u(ae.$$.fragment),this.h()},l(t){const r=un('[data-svelte="svelte-1phssyn"]',document.head);y=o(r,"META",{name:!0,content:!0}),r.forEach(e),M=f(t),$=o(t,"H1",{class:!0});var oe=l($);j=o(oe,"A",{id:!0,class:!0,href:!0});var Dl=l(j);P=o(Dl,"SPAN",{});var Tl=l(P);h(b.$$.fragment,Tl),Tl.forEach(e),Dl.forEach(e),ut=f(oe),z=o(oe,"SPAN",{});var Ll=l(z);C=p(Ll,"Cloud storage"),Ll.forEach(e),oe.forEach(e),E=f(t),A=o(t,"P",{});var ha=l(A);le=p(ha,"\u{1F917} Datasets supports access to cloud storage providers through a "),ve=o(ha,"CODE",{});var Pl=l(ve);xa=p(Pl,"fsspec"),Pl.forEach(e),Ha=p(ha,` FileSystem implementations.
You can save and load datasets from any cloud storage in a Pythonic way.
Take a look at the following table for some example of supported cloud storage providers:`),ha.forEach(e),_s=f(t),U=o(t,"TABLE",{});var ma=l(U);ye=o(ma,"THEAD",{});var zl=l(ye);ht=o(zl,"TR",{});var ga=l(ht);$e=o(ga,"TH",{});var Cl=l($e);Ba=p(Cl,"Storage provider"),Cl.forEach(e),Ga=f(ga),we=o(ga,"TH",{});var Nl=l(we);Ra=p(Nl,"Filesystem implementation"),Nl.forEach(e),ga.forEach(e),zl.forEach(e),Ya=f(ma),k=o(ma,"TBODY",{});var L=l(k);mt=o(L,"TR",{});var _a=l(mt);be=o(_a,"TD",{});var Ol=l(be);Ma=p(Ol,"Amazon S3"),Ol.forEach(e),Ua=f(_a),Ee=o(_a,"TD",{});var Fl=l(Ee);gt=o(Fl,"A",{href:!0,rel:!0});var Il=l(gt);Ka=p(Il,"s3fs"),Il.forEach(e),Fl.forEach(e),_a.forEach(e),Wa=f(L),_t=o(L,"TR",{});var va=l(_t);ke=o(va,"TD",{});var xl=l(ke);Ja=p(xl,"Google Cloud Storage"),xl.forEach(e),Qa=f(va),je=o(va,"TD",{});var Hl=l(je);vt=o(Hl,"A",{href:!0,rel:!0});var Bl=l(vt);Va=p(Bl,"gcsfs"),Bl.forEach(e),Hl.forEach(e),va.forEach(e),Xa=f(L),yt=o(L,"TR",{});var ya=l(yt);qe=o(ya,"TD",{});var Gl=l(qe);Za=p(Gl,"Azure Blob/DataLake"),Gl.forEach(e),to=f(ya),Se=o(ya,"TD",{});var Rl=l(Se);$t=o(Rl,"A",{href:!0,rel:!0});var Yl=l($t);eo=p(Yl,"adlfs"),Yl.forEach(e),Rl.forEach(e),ya.forEach(e),so=f(L),wt=o(L,"TR",{});var $a=l(wt);Ae=o($a,"TD",{});var Ml=l(Ae);ao=p(Ml,"Dropbox"),Ml.forEach(e),oo=f($a),De=o($a,"TD",{});var Ul=l(De);bt=o(Ul,"A",{href:!0,rel:!0});var Kl=l(bt);lo=p(Kl,"dropboxdrivefs"),Kl.forEach(e),Ul.forEach(e),$a.forEach(e),ro=f(L),Et=o(L,"TR",{});var wa=l(Et);Te=o(wa,"TD",{});var Wl=l(Te);no=p(Wl,"Google Drive"),Wl.forEach(e),io=f(wa),Le=o(wa,"TD",{});var Jl=l(Le);kt=o(Jl,"A",{href:!0,rel:!0});var Ql=l(kt);po=p(Ql,"gdrivefs"),Ql.forEach(e),Jl.forEach(e),wa.forEach(e),L.forEach(e),ma.forEach(e),vs=f(t),re=o(t,"P",{});var Vl=l(re);fo=p(Vl,`This guide will show you how to save and load datasets with any cloud storage.
Here are examples for S3, Google Cloud Storage and Azure Blob Storage.`),Vl.forEach(e),ys=f(t),N=o(t,"H2",{class:!0});var ba=l(N);K=o(ba,"A",{id:!0,class:!0,href:!0});var Xl=l(K);Pe=o(Xl,"SPAN",{});var Zl=l(Pe);h(jt.$$.fragment,Zl),Zl.forEach(e),Xl.forEach(e),co=f(ba),ze=o(ba,"SPAN",{});var tr=l(ze);uo=p(tr,"Set up your cloud storage FileSystem"),tr.forEach(e),ba.forEach(e),$s=f(t),O=o(t,"H3",{class:!0});var Ea=l(O);W=o(Ea,"A",{id:!0,class:!0,href:!0});var er=l(W);Ce=o(er,"SPAN",{});var sr=l(Ce);h(qt.$$.fragment,sr),sr.forEach(e),er.forEach(e),ho=f(Ea),Ne=o(Ea,"SPAN",{});var ar=l(Ne);mo=p(ar,"Amazon S3"),ar.forEach(e),Ea.forEach(e),ws=f(t),ne=o(t,"OL",{});var or=l(ne);Oe=o(or,"LI",{});var lr=l(Oe);go=p(lr,"Install the S3 dependency with \u{1F917} Datasets:"),lr.forEach(e),or.forEach(e),bs=f(t),h(St.$$.fragment,t),Es=f(t),At=o(t,"OL",{start:!0});var rr=l(At);Fe=o(rr,"LI",{});var nr=l(Fe);_o=p(nr,"Define your credentials"),nr.forEach(e),rr.forEach(e),ks=f(t),q=o(t,"P",{});var ft=l(q);vo=p(ft,"To use an anonymous connection, use "),Ie=o(ft,"CODE",{});var ir=l(Ie);yo=p(ir,"anon=True"),ir.forEach(e),$o=p(ft,`.
Otherwise, include your `),xe=o(ft,"CODE",{});var pr=l(xe);wo=p(pr,"aws_access_key_id"),pr.forEach(e),bo=p(ft," and "),He=o(ft,"CODE",{});var dr=l(He);Eo=p(dr,"aws_secret_access_key"),dr.forEach(e),ko=p(ft," whenever you are interacting with a private S3 bucket."),ft.forEach(e),js=f(t),h(Dt.$$.fragment,t),qs=f(t),Tt=o(t,"OL",{start:!0});var fr=l(Tt);Be=o(fr,"LI",{});var cr=l(Be);jo=p(cr,"Load your FileSystem instance"),cr.forEach(e),fr.forEach(e),Ss=f(t),h(Lt.$$.fragment,t),As=f(t),F=o(t,"H3",{class:!0});var ka=l(F);J=o(ka,"A",{id:!0,class:!0,href:!0});var ur=l(J);Ge=o(ur,"SPAN",{});var hr=l(Ge);h(Pt.$$.fragment,hr),hr.forEach(e),ur.forEach(e),qo=f(ka),Re=o(ka,"SPAN",{});var mr=l(Re);So=p(mr,"Google Cloud Storage"),mr.forEach(e),ka.forEach(e),Ds=f(t),ie=o(t,"OL",{});var gr=l(ie);Ye=o(gr,"LI",{});var _r=l(Ye);Ao=p(_r,"Install the Google Cloud Storage implementation:"),_r.forEach(e),gr.forEach(e),Ts=f(t),h(zt.$$.fragment,t),Ls=f(t),Ct=o(t,"OL",{start:!0});var vr=l(Ct);Me=o(vr,"LI",{});var yr=l(Me);Do=p(yr,"Define your credentials"),yr.forEach(e),vr.forEach(e),Ps=f(t),h(Nt.$$.fragment,t),zs=f(t),Ot=o(t,"OL",{start:!0});var $r=l(Ot);Ue=o($r,"LI",{});var wr=l(Ue);To=p(wr,"Load your FileSystem instance"),wr.forEach(e),$r.forEach(e),Cs=f(t),h(Ft.$$.fragment,t),Ns=f(t),I=o(t,"H3",{class:!0});var ja=l(I);Q=o(ja,"A",{id:!0,class:!0,href:!0});var br=l(Q);Ke=o(br,"SPAN",{});var Er=l(Ke);h(It.$$.fragment,Er),Er.forEach(e),br.forEach(e),Lo=f(ja),We=o(ja,"SPAN",{});var kr=l(We);Po=p(kr,"Azure Blob Storage"),kr.forEach(e),ja.forEach(e),Os=f(t),pe=o(t,"OL",{});var jr=l(pe);Je=o(jr,"LI",{});var qr=l(Je);zo=p(qr,"Install the Azure Blob Storage implementation:"),qr.forEach(e),jr.forEach(e),Fs=f(t),h(xt.$$.fragment,t),Is=f(t),Ht=o(t,"OL",{start:!0});var Sr=l(Ht);Qe=o(Sr,"LI",{});var Ar=l(Qe);Co=p(Ar,"Define your credentials"),Ar.forEach(e),Sr.forEach(e),xs=f(t),h(Bt.$$.fragment,t),Hs=f(t),Gt=o(t,"OL",{start:!0});var Dr=l(Gt);Ve=o(Dr,"LI",{});var Tr=l(Ve);No=p(Tr,"Load your FileSystem instance"),Tr.forEach(e),Dr.forEach(e),Bs=f(t),h(Rt.$$.fragment,t),Gs=f(t),x=o(t,"H2",{class:!0});var qa=l(x);V=o(qa,"A",{id:!0,class:!0,href:!0});var Lr=l(V);Xe=o(Lr,"SPAN",{});var Pr=l(Xe);h(Yt.$$.fragment,Pr),Pr.forEach(e),Lr.forEach(e),Oo=f(qa),Ze=o(qa,"SPAN",{});var zr=l(Ze);Fo=p(zr,"Load and Save your datasets using your cloud storage FileSystem"),zr.forEach(e),qa.forEach(e),Rs=f(t),H=o(t,"H3",{class:!0});var Sa=l(H);X=o(Sa,"A",{id:!0,class:!0,href:!0});var Cr=l(X);ts=o(Cr,"SPAN",{});var Nr=l(ts);h(Mt.$$.fragment,Nr),Nr.forEach(e),Cr.forEach(e),Io=f(Sa),es=o(Sa,"SPAN",{});var Or=l(es);xo=p(Or,"Load datasets into a cloud storage"),Or.forEach(e),Sa.forEach(e),Ys=f(t),S=o(t,"P",{});var ct=l(S);Ho=p(ct,"You can load and cache a dataset into your cloud storage by specifying a remote "),ss=o(ct,"CODE",{});var Fr=l(ss);Bo=p(Fr,"cache_dir"),Fr.forEach(e),Go=p(ct," in "),as=o(ct,"CODE",{});var Ir=l(as);Ro=p(Ir,"load_dataset"),Ir.forEach(e),Yo=p(ct,`.
Don\u2019t forget to use the previously defined `),os=o(ct,"CODE",{});var xr=l(os);Mo=p(xr,"storage_options"),xr.forEach(e),Uo=p(ct," containing your credentials to write into a private cloud storage."),ct.forEach(e),Ms=f(t),Z=o(t,"P",{});var Aa=l(Z);Ko=p(Aa,"Load a dataset from the Hugging Face Hub (see "),de=o(Aa,"A",{href:!0});var Hr=l(de);Wo=p(Hr,"how to load from the Hugging Face Hub"),Hr.forEach(e),Jo=p(Aa,"):"),Aa.forEach(e),Us=f(t),h(Ut.$$.fragment,t),Ks=f(t),tt=o(t,"P",{});var Da=l(tt);Qo=p(Da,"Load a dataset using a loading script (see "),fe=o(Da,"A",{href:!0});var Br=l(fe);Vo=p(Br,"how to load a local loading script"),Br.forEach(e),Xo=p(Da,"):"),Da.forEach(e),Ws=f(t),h(Kt.$$.fragment,t),Js=f(t),et=o(t,"P",{});var Ta=l(et);Zo=p(Ta,"Load your own data files (see "),ce=o(Ta,"A",{href:!0});var Gr=l(ce);tl=p(Gr,"how to load local and remote files"),Gr.forEach(e),el=p(Ta,"):"),Ta.forEach(e),Qs=f(t),h(Wt.$$.fragment,t),Vs=f(t),st=o(t,"P",{});var La=l(st);sl=p(La,"It is highly recommended to save the files as compressed Parquet files to optimize I/O by specifying "),ls=o(La,"CODE",{});var Rr=l(ls);al=p(Rr,'file_format="parquet"'),Rr.forEach(e),ol=p(La,`.
Otherwize the dataset is saved as an uncompressed Arrow file.`),La.forEach(e),Xs=f(t),B=o(t,"H4",{class:!0});var Pa=l(B);at=o(Pa,"A",{id:!0,class:!0,href:!0});var Yr=l(at);rs=o(Yr,"SPAN",{});var Mr=l(rs);h(Jt.$$.fragment,Mr),Mr.forEach(e),Yr.forEach(e),ll=f(Pa),ns=o(Pa,"SPAN",{});var Ur=l(ns);rl=p(Ur,"Dask"),Ur.forEach(e),Pa.forEach(e),Zs=f(t),ue=o(t,"P",{});var Kr=l(ue);nl=p(Kr,`Dask is a parallel computing library and it has a pandas-like API for working with larger than memory Parquet datasets in parallel.
Dask can use multiple threads or processes on a single machine, or a cluster of machines to process data in parallel.
Dask supports local data but also data from a cloud storage.`),Kr.forEach(e),ta=f(t),he=o(t,"P",{});var Wr=l(he);il=p(Wr,"Therefore you can load a dataset saved as sharded Parquet files in Dask with"),Wr.forEach(e),ea=f(t),h(Qt.$$.fragment,t),sa=f(t),ot=o(t,"P",{});var za=l(ot);pl=p(za,"You can find more about dask dataframes in their "),Vt=o(za,"A",{href:!0,rel:!0});var Jr=l(Vt);dl=p(Jr,"documentation"),Jr.forEach(e),fl=p(za,"."),za.forEach(e),aa=f(t),G=o(t,"H2",{class:!0});var Ca=l(G);lt=o(Ca,"A",{id:!0,class:!0,href:!0});var Qr=l(lt);is=o(Qr,"SPAN",{});var Vr=l(is);h(Xt.$$.fragment,Vr),Vr.forEach(e),Qr.forEach(e),cl=f(Ca),ps=o(Ca,"SPAN",{});var Xr=l(ps);ul=p(Xr,"Saving serialized datasets"),Xr.forEach(e),Ca.forEach(e),oa=f(t),rt=o(t,"P",{});var Na=l(rt);hl=p(Na,"After you have processed your dataset, you can save it to your cloud storage with "),me=o(Na,"A",{href:!0});var Zr=l(me);ml=p(Zr,"Dataset.save_to_disk()"),Zr.forEach(e),gl=p(Na,":"),Na.forEach(e),la=f(t),h(Zt.$$.fragment,t),ra=f(t),h(nt.$$.fragment,t),na=f(t),R=o(t,"H2",{class:!0});var Oa=l(R);it=o(Oa,"A",{id:!0,class:!0,href:!0});var tn=l(it);ds=o(tn,"SPAN",{});var en=l(ds);h(te.$$.fragment,en),en.forEach(e),tn.forEach(e),_l=f(Oa),fs=o(Oa,"SPAN",{});var sn=l(fs);vl=p(sn,"Listing serialized datasets"),sn.forEach(e),Oa.forEach(e),ia=f(t),T=o(t,"P",{});var _e=l(T);yl=p(_e,"List files from a cloud storage with your FileSystem instance "),cs=o(_e,"CODE",{});var an=l(cs);$l=p(an,"fs"),an.forEach(e),wl=p(_e,", using "),us=o(_e,"CODE",{});var on=l(us);bl=p(on,"fs.ls"),on.forEach(e),El=p(_e,":"),_e.forEach(e),pa=f(t),h(ee.$$.fragment,t),da=f(t),Y=o(t,"H3",{class:!0});var Fa=l(Y);pt=o(Fa,"A",{id:!0,class:!0,href:!0});var ln=l(pt);hs=o(ln,"SPAN",{});var rn=l(hs);h(se.$$.fragment,rn),rn.forEach(e),ln.forEach(e),kl=f(Fa),ms=o(Fa,"SPAN",{});var nn=l(ms);jl=p(nn,"Load serialized datasets"),nn.forEach(e),Fa.forEach(e),fa=f(t),dt=o(t,"P",{});var Ia=l(dt);ql=p(Ia,"When you are ready to use your dataset again, reload it with "),ge=o(Ia,"A",{href:!0});var pn=l(ge);Sl=p(pn,"Dataset.load_from_disk()"),pn.forEach(e),Al=p(Ia,":"),Ia.forEach(e),ca=f(t),h(ae.$$.fragment,t),this.h()},h(){c(y,"name","hf:doc:metadata"),c(y,"content",JSON.stringify(vn)),c(j,"id","cloud-storage"),c(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j,"href","#cloud-storage"),c($,"class","relative group"),c(gt,"href","https://s3fs.readthedocs.io/en/latest/"),c(gt,"rel","nofollow"),c(vt,"href","https://gcsfs.readthedocs.io/en/latest/"),c(vt,"rel","nofollow"),c($t,"href","https://github.com/fsspec/adlfs"),c($t,"rel","nofollow"),c(bt,"href","https://github.com/MarineChap/dropboxdrivefs"),c(bt,"rel","nofollow"),c(kt,"href","https://github.com/intake/gdrivefs"),c(kt,"rel","nofollow"),c(K,"id","set-up-your-cloud-storage-filesystem"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#set-up-your-cloud-storage-filesystem"),c(N,"class","relative group"),c(W,"id","amazon-s3"),c(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W,"href","#amazon-s3"),c(O,"class","relative group"),c(At,"start","2"),c(Tt,"start","3"),c(J,"id","google-cloud-storage"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#google-cloud-storage"),c(F,"class","relative group"),c(Ct,"start","2"),c(Ot,"start","3"),c(Q,"id","azure-blob-storage"),c(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Q,"href","#azure-blob-storage"),c(I,"class","relative group"),c(Ht,"start","2"),c(Gt,"start","3"),c(V,"id","load-and-save-your-datasets-using-your-cloud-storage-filesystem"),c(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(V,"href","#load-and-save-your-datasets-using-your-cloud-storage-filesystem"),c(x,"class","relative group"),c(X,"id","load-datasets-into-a-cloud-storage"),c(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X,"href","#load-datasets-into-a-cloud-storage"),c(H,"class","relative group"),c(de,"href","./loading#hugging-face-hub"),c(fe,"href","./loading#local-loading-script"),c(ce,"href","./loading#local-and-remote-files"),c(at,"id","dask"),c(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(at,"href","#dask"),c(B,"class","relative group"),c(Vt,"href","https://docs.dask.org/en/stable/dataframe.html"),c(Vt,"rel","nofollow"),c(lt,"id","saving-serialized-datasets"),c(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lt,"href","#saving-serialized-datasets"),c(G,"class","relative group"),c(me,"href","/docs/datasets/pr_4724/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),c(it,"id","listing-serialized-datasets"),c(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(it,"href","#listing-serialized-datasets"),c(R,"class","relative group"),c(pt,"id","load-serialized-datasets"),c(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pt,"href","#load-serialized-datasets"),c(Y,"class","relative group"),c(ge,"href","/docs/datasets/pr_4724/en/package_reference/main_classes#datasets.Dataset.load_from_disk")},m(t,r){s(document.head,y),n(t,M,r),n(t,$,r),s($,j),s(j,P),m(b,P,null),s($,ut),s($,z),s(z,C),n(t,E,r),n(t,A,r),s(A,le),s(A,ve),s(ve,xa),s(A,Ha),n(t,_s,r),n(t,U,r),s(U,ye),s(ye,ht),s(ht,$e),s($e,Ba),s(ht,Ga),s(ht,we),s(we,Ra),s(U,Ya),s(U,k),s(k,mt),s(mt,be),s(be,Ma),s(mt,Ua),s(mt,Ee),s(Ee,gt),s(gt,Ka),s(k,Wa),s(k,_t),s(_t,ke),s(ke,Ja),s(_t,Qa),s(_t,je),s(je,vt),s(vt,Va),s(k,Xa),s(k,yt),s(yt,qe),s(qe,Za),s(yt,to),s(yt,Se),s(Se,$t),s($t,eo),s(k,so),s(k,wt),s(wt,Ae),s(Ae,ao),s(wt,oo),s(wt,De),s(De,bt),s(bt,lo),s(k,ro),s(k,Et),s(Et,Te),s(Te,no),s(Et,io),s(Et,Le),s(Le,kt),s(kt,po),n(t,vs,r),n(t,re,r),s(re,fo),n(t,ys,r),n(t,N,r),s(N,K),s(K,Pe),m(jt,Pe,null),s(N,co),s(N,ze),s(ze,uo),n(t,$s,r),n(t,O,r),s(O,W),s(W,Ce),m(qt,Ce,null),s(O,ho),s(O,Ne),s(Ne,mo),n(t,ws,r),n(t,ne,r),s(ne,Oe),s(Oe,go),n(t,bs,r),m(St,t,r),n(t,Es,r),n(t,At,r),s(At,Fe),s(Fe,_o),n(t,ks,r),n(t,q,r),s(q,vo),s(q,Ie),s(Ie,yo),s(q,$o),s(q,xe),s(xe,wo),s(q,bo),s(q,He),s(He,Eo),s(q,ko),n(t,js,r),m(Dt,t,r),n(t,qs,r),n(t,Tt,r),s(Tt,Be),s(Be,jo),n(t,Ss,r),m(Lt,t,r),n(t,As,r),n(t,F,r),s(F,J),s(J,Ge),m(Pt,Ge,null),s(F,qo),s(F,Re),s(Re,So),n(t,Ds,r),n(t,ie,r),s(ie,Ye),s(Ye,Ao),n(t,Ts,r),m(zt,t,r),n(t,Ls,r),n(t,Ct,r),s(Ct,Me),s(Me,Do),n(t,Ps,r),m(Nt,t,r),n(t,zs,r),n(t,Ot,r),s(Ot,Ue),s(Ue,To),n(t,Cs,r),m(Ft,t,r),n(t,Ns,r),n(t,I,r),s(I,Q),s(Q,Ke),m(It,Ke,null),s(I,Lo),s(I,We),s(We,Po),n(t,Os,r),n(t,pe,r),s(pe,Je),s(Je,zo),n(t,Fs,r),m(xt,t,r),n(t,Is,r),n(t,Ht,r),s(Ht,Qe),s(Qe,Co),n(t,xs,r),m(Bt,t,r),n(t,Hs,r),n(t,Gt,r),s(Gt,Ve),s(Ve,No),n(t,Bs,r),m(Rt,t,r),n(t,Gs,r),n(t,x,r),s(x,V),s(V,Xe),m(Yt,Xe,null),s(x,Oo),s(x,Ze),s(Ze,Fo),n(t,Rs,r),n(t,H,r),s(H,X),s(X,ts),m(Mt,ts,null),s(H,Io),s(H,es),s(es,xo),n(t,Ys,r),n(t,S,r),s(S,Ho),s(S,ss),s(ss,Bo),s(S,Go),s(S,as),s(as,Ro),s(S,Yo),s(S,os),s(os,Mo),s(S,Uo),n(t,Ms,r),n(t,Z,r),s(Z,Ko),s(Z,de),s(de,Wo),s(Z,Jo),n(t,Us,r),m(Ut,t,r),n(t,Ks,r),n(t,tt,r),s(tt,Qo),s(tt,fe),s(fe,Vo),s(tt,Xo),n(t,Ws,r),m(Kt,t,r),n(t,Js,r),n(t,et,r),s(et,Zo),s(et,ce),s(ce,tl),s(et,el),n(t,Qs,r),m(Wt,t,r),n(t,Vs,r),n(t,st,r),s(st,sl),s(st,ls),s(ls,al),s(st,ol),n(t,Xs,r),n(t,B,r),s(B,at),s(at,rs),m(Jt,rs,null),s(B,ll),s(B,ns),s(ns,rl),n(t,Zs,r),n(t,ue,r),s(ue,nl),n(t,ta,r),n(t,he,r),s(he,il),n(t,ea,r),m(Qt,t,r),n(t,sa,r),n(t,ot,r),s(ot,pl),s(ot,Vt),s(Vt,dl),s(ot,fl),n(t,aa,r),n(t,G,r),s(G,lt),s(lt,is),m(Xt,is,null),s(G,cl),s(G,ps),s(ps,ul),n(t,oa,r),n(t,rt,r),s(rt,hl),s(rt,me),s(me,ml),s(rt,gl),n(t,la,r),m(Zt,t,r),n(t,ra,r),m(nt,t,r),n(t,na,r),n(t,R,r),s(R,it),s(it,ds),m(te,ds,null),s(R,_l),s(R,fs),s(fs,vl),n(t,ia,r),n(t,T,r),s(T,yl),s(T,cs),s(cs,$l),s(T,wl),s(T,us),s(us,bl),s(T,El),n(t,pa,r),m(ee,t,r),n(t,da,r),n(t,Y,r),s(Y,pt),s(pt,hs),m(se,hs,null),s(Y,kl),s(Y,ms),s(ms,jl),n(t,fa,r),n(t,dt,r),s(dt,ql),s(dt,ge),s(ge,Sl),s(dt,Al),n(t,ca,r),m(ae,t,r),ua=!0},p(t,[r]){const oe={};r&2&&(oe.$$scope={dirty:r,ctx:t}),nt.$set(oe)},i(t){ua||(g(b.$$.fragment,t),g(jt.$$.fragment,t),g(qt.$$.fragment,t),g(St.$$.fragment,t),g(Dt.$$.fragment,t),g(Lt.$$.fragment,t),g(Pt.$$.fragment,t),g(zt.$$.fragment,t),g(Nt.$$.fragment,t),g(Ft.$$.fragment,t),g(It.$$.fragment,t),g(xt.$$.fragment,t),g(Bt.$$.fragment,t),g(Rt.$$.fragment,t),g(Yt.$$.fragment,t),g(Mt.$$.fragment,t),g(Ut.$$.fragment,t),g(Kt.$$.fragment,t),g(Wt.$$.fragment,t),g(Jt.$$.fragment,t),g(Qt.$$.fragment,t),g(Xt.$$.fragment,t),g(Zt.$$.fragment,t),g(nt.$$.fragment,t),g(te.$$.fragment,t),g(ee.$$.fragment,t),g(se.$$.fragment,t),g(ae.$$.fragment,t),ua=!0)},o(t){_(b.$$.fragment,t),_(jt.$$.fragment,t),_(qt.$$.fragment,t),_(St.$$.fragment,t),_(Dt.$$.fragment,t),_(Lt.$$.fragment,t),_(Pt.$$.fragment,t),_(zt.$$.fragment,t),_(Nt.$$.fragment,t),_(Ft.$$.fragment,t),_(It.$$.fragment,t),_(xt.$$.fragment,t),_(Bt.$$.fragment,t),_(Rt.$$.fragment,t),_(Yt.$$.fragment,t),_(Mt.$$.fragment,t),_(Ut.$$.fragment,t),_(Kt.$$.fragment,t),_(Wt.$$.fragment,t),_(Jt.$$.fragment,t),_(Qt.$$.fragment,t),_(Xt.$$.fragment,t),_(Zt.$$.fragment,t),_(nt.$$.fragment,t),_(te.$$.fragment,t),_(ee.$$.fragment,t),_(se.$$.fragment,t),_(ae.$$.fragment,t),ua=!1},d(t){e(y),t&&e(M),t&&e($),v(b),t&&e(E),t&&e(A),t&&e(_s),t&&e(U),t&&e(vs),t&&e(re),t&&e(ys),t&&e(N),v(jt),t&&e($s),t&&e(O),v(qt),t&&e(ws),t&&e(ne),t&&e(bs),v(St,t),t&&e(Es),t&&e(At),t&&e(ks),t&&e(q),t&&e(js),v(Dt,t),t&&e(qs),t&&e(Tt),t&&e(Ss),v(Lt,t),t&&e(As),t&&e(F),v(Pt),t&&e(Ds),t&&e(ie),t&&e(Ts),v(zt,t),t&&e(Ls),t&&e(Ct),t&&e(Ps),v(Nt,t),t&&e(zs),t&&e(Ot),t&&e(Cs),v(Ft,t),t&&e(Ns),t&&e(I),v(It),t&&e(Os),t&&e(pe),t&&e(Fs),v(xt,t),t&&e(Is),t&&e(Ht),t&&e(xs),v(Bt,t),t&&e(Hs),t&&e(Gt),t&&e(Bs),v(Rt,t),t&&e(Gs),t&&e(x),v(Yt),t&&e(Rs),t&&e(H),v(Mt),t&&e(Ys),t&&e(S),t&&e(Ms),t&&e(Z),t&&e(Us),v(Ut,t),t&&e(Ks),t&&e(tt),t&&e(Ws),v(Kt,t),t&&e(Js),t&&e(et),t&&e(Qs),v(Wt,t),t&&e(Vs),t&&e(st),t&&e(Xs),t&&e(B),v(Jt),t&&e(Zs),t&&e(ue),t&&e(ta),t&&e(he),t&&e(ea),v(Qt,t),t&&e(sa),t&&e(ot),t&&e(aa),t&&e(G),v(Xt),t&&e(oa),t&&e(rt),t&&e(la),v(Zt,t),t&&e(ra),v(nt,t),t&&e(na),t&&e(R),v(te),t&&e(ia),t&&e(T),t&&e(pa),v(ee,t),t&&e(da),t&&e(Y),v(se),t&&e(fa),t&&e(dt),t&&e(ca),v(ae,t)}}}const vn={local:"cloud-storage",sections:[{local:"set-up-your-cloud-storage-filesystem",sections:[{local:"amazon-s3",title:"Amazon S3"},{local:"google-cloud-storage",title:"Google Cloud Storage"},{local:"azure-blob-storage",title:"Azure Blob Storage"}],title:"Set up your cloud storage FileSystem"},{local:"load-and-save-your-datasets-using-your-cloud-storage-filesystem",sections:[{local:"load-datasets-into-a-cloud-storage",sections:[{local:"dask",title:"Dask"}],title:"Load datasets into a cloud storage"}],title:"Load and Save your datasets using your cloud storage FileSystem"},{local:"saving-serialized-datasets",title:"Saving serialized datasets"},{local:"listing-serialized-datasets",sections:[{local:"load-serialized-datasets",title:"Load serialized datasets"}],title:"Listing serialized datasets"}],title:"Cloud storage"};function yn(gs){return hn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class kn extends dn{constructor(y){super();fn(this,y,yn,_n,cn,{})}}export{kn as default,vn as metadata};
