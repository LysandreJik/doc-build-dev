import{S as tu,i as au,s as su,e as s,k as d,w as u,t as i,M as ou,c as o,d as t,m as c,a as r,x as f,h as l,b as h,G as a,g as p,y as m,L as ru,q as v,o as w,B as _,v as nu}from"../../chunks/vendor-hf-doc-builder.js";import{I as O}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as H}from"../../chunks/CodeBlock-hf-doc-builder.js";function iu(sd){let C,Rs,z,ae,Yt,Ee,Mr,Xt,Cr,Vs,se,zr,Jt,Br,Gr,Os,tt,Ur,Fs,at,Yr,Ms,st,Xr,Cs,B,Jr,xe,Kr,Qr,Te,Zr,zs,ot,en,Bs,rt,tn,Gs,nt,an,Us,$,Kt,sn,on,Qt,rn,nn,Zt,ln,pn,ea,dn,cn,ta,hn,un,aa,fn,Ys,G,oe,sa,$e,mn,oa,vn,Xs,it,wn,Js,U,re,ra,je,_n,na,gn,Ks,Y,ne,ia,Se,bn,la,yn,Qs,lt,kn,Zs,X,ie,pa,Ae,En,da,xn,eo,pt,Tn,to,dt,$n,ao,ct,jn,so,ht,Sn,oo,J,le,ca,Pe,An,ha,Pn,ro,ut,qn,no,pe,ua,g,io,Dn,fa,Wn,Nn,ma,In,Hn,va,Ln,Rn,wa,Vn,On,_a,Fn,Mn,ga,Cn,zn,S,b,ba,Bn,Gn,ya,Un,Yn,ka,Xn,Jn,Ea,Kn,Qn,xa,Zn,ei,Ta,ti,ai,$a,si,oi,y,ja,ri,ni,Sa,ii,li,Aa,pi,di,Pa,ci,hi,qa,ui,fi,Da,mi,vi,Wa,wi,_i,k,Na,gi,bi,Ia,yi,ki,Ha,Ei,xi,La,Ti,$i,Ra,ji,Si,Va,Ai,Pi,Oa,qi,Di,E,Fa,Wi,Ni,Ma,Ii,Hi,Ca,Li,Ri,za,Vi,Oi,Ba,Fi,Mi,Ga,Ci,zi,Ua,Bi,Gi,x,Ya,Ui,Yi,Xa,Xi,Ji,Ja,Ki,Qi,Ka,Zi,el,Qa,tl,al,Za,sl,ol,es,rl,nl,T,ts,il,ll,as,pl,dl,ss,cl,hl,os,ul,fl,rs,ml,vl,ns,wl,_l,is,gl,lo,ft,bl,po,mt,yl,co,K,de,ls,qe,kl,ps,El,ho,ce,xl,De,ds,Tl,$l,uo,he,jl,We,Sl,Al,fo,ue,Pl,cs,ql,Dl,mo,Ne,vo,vt,Wl,wo,wt,Nl,_o,Ie,go,He,bo,F,Il,hs,Hl,Ll,us,Rl,Vl,yo,_t,Ol,ko,Le,Eo,Re,xo,gt,Fl,To,bt,Ml,$o,fe,Cl,fs,zl,Bl,jo,me,Gl,ms,Ul,Yl,So,ve,Xl,vs,Jl,Kl,Ao,we,ws,Ql,Zl,_s,ep,Po,Ve,tp,Oe,ap,qo,yt,sp,Do,Q,_e,gs,Fe,op,bs,rp,Wo,kt,np,No,Et,ys,ip,Io,xt,lp,Ho,Me,ks,pp,Lo,Tt,dp,Ro,$t,cp,Vo,jt,hp,Oo,St,up,Fo,Ce,Mo,ge,fp,Es,mp,vp,Co,At,wp,zo,Z,be,xs,ze,_p,Ts,gp,Bo,Be,Go,Pt,bp,Uo,qt,yp,Yo,Dt,kp,Xo,Wt,Ep,Jo,M,xp,$s,Tp,$p,js,jp,Sp,Ko,Nt,Ap,Qo,Ge,Zo,Ue,er,It,Pp,tr,Ye,ar,Xe,sr,j,qp,Ss,Dp,Wp,As,Np,Ip,Ps,Hp,Lp,qs,Rp,Vp,Ds,Op,Fp,or,Ht,Mp,rr,Lt,Cp,nr,Rt,zp,ir,Je,lr,Vt,Bp,pr,Ot,Gp,dr,Ft,Up,cr,Ke,hr,Mt,Yp,ur,Qe,fr,ee,ye,Ws,Ze,Xp,Ns,Jp,mr,Ct,Kp,vr,zt,Qp,wr,Bt,Zp,_r,te,ke,Is,et,ed,Hs,td,gr,br;return Ee=new O({}),$e=new O({}),je=new O({}),Se=new O({}),Ae=new O({}),Pe=new O({}),qe=new O({}),Ne=new H({props:{code:`from datasets import load_dataset, DatasetDict

raw_datasets = DatasetDict()

raw_datasets["train"] = load_dataset("librispeech_asr", "clean", split="train.100")
raw_datasets["validation"] = load_dataset("librispeech_asr", "clean", split="validation")
raw_datasets["test"] = load_dataset("librispeech_asr", "clean", split="test")`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, DatasetDict

raw_datasets = DatasetDict()

raw_datasets[<span class="hljs-string">&quot;train&quot;</span>] = load_dataset(<span class="hljs-string">&quot;librispeech_asr&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;train.100&quot;</span>)
raw_datasets[<span class="hljs-string">&quot;validation&quot;</span>] = load_dataset(<span class="hljs-string">&quot;librispeech_asr&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
raw_datasets[<span class="hljs-string">&quot;test&quot;</span>] = load_dataset(<span class="hljs-string">&quot;librispeech_asr&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)`}}),Ie=new H({props:{code:"raw_datasets",highlighted:"raw_datasets"}}),He=new H({props:{code:`DatasetDict({
    train: Dataset({
        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],
        num_rows: 28539
    })
    validation: Dataset({
        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],
        num_rows: 2703
    })
    test: Dataset({
        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],
        num_rows: 2620
    })
})`,highlighted:`DatasetDict({
    train: <span class="hljs-built_in">Dataset</span>({
        features: [<span class="hljs-string">&#x27;file&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;speaker_id&#x27;</span>, <span class="hljs-string">&#x27;chapter_id&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>],
        num_rows: <span class="hljs-number">28539</span>
    })
    validation: <span class="hljs-built_in">Dataset</span>({
        features: [<span class="hljs-string">&#x27;file&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;speaker_id&#x27;</span>, <span class="hljs-string">&#x27;chapter_id&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>],
        num_rows: <span class="hljs-number">2703</span>
    })
    test: <span class="hljs-built_in">Dataset</span>({
        features: [<span class="hljs-string">&#x27;file&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;speaker_id&#x27;</span>, <span class="hljs-string">&#x27;chapter_id&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>],
        num_rows: <span class="hljs-number">2620</span>
    })
})`}}),Le=new H({props:{code:'raw_datasets["train"][0]',highlighted:'raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]'}}),Re=new H({props:{code:`{'file': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/d2da1969fe9e7d06661b5dc370cf2e3c119a14c35950045bcb76243b264e4f01/374-180298-0000.flac',
 'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/d2da1969fe9e7d06661b5dc370cf2e3c119a14c35950045bcb76243b264e4f01/374-180298-0000.flac',
  'array': array([ 7.01904297e-04,  7.32421875e-04,  7.32421875e-04, ...,
         -2.74658203e-04, -1.83105469e-04, -3.05175781e-05]),
  'sampling_rate': 16000},
 'text': 'CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED',
 'speaker_id': 374,
 'chapter_id': 180298,
 'id': '374-180298-0000'}`,highlighted:`{&#x27;file&#x27;: &#x27;/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/d2da<span class="hljs-number">1969</span>fe9e7d<span class="hljs-number">0666</span>1b5dc370cf2e3c119a14c<span class="hljs-number">35950045</span>bcb<span class="hljs-number">7624</span>3b264e4f01/374-<span class="hljs-number">180298-0000</span>.flac&#x27;,
 &#x27;audio&#x27;: {&#x27;path&#x27;: &#x27;/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/d2da<span class="hljs-number">1969</span>fe9e7d<span class="hljs-number">0666</span>1b5dc370cf2e3c119a14c<span class="hljs-number">35950045</span>bcb<span class="hljs-number">7624</span>3b264e4f01/374-<span class="hljs-number">180298-0000</span>.flac&#x27;,
  &#x27;array&#x27;: array([ <span class="hljs-number">7.01904297</span>e-<span class="hljs-number">04</span>,  <span class="hljs-number">7.32421875</span>e-<span class="hljs-number">04</span>,  <span class="hljs-number">7.32421875</span>e-<span class="hljs-number">04</span>, ...,
         -<span class="hljs-number">2.74658203</span>e-<span class="hljs-number">04</span>, -<span class="hljs-number">1.83105469</span>e-<span class="hljs-number">04</span>, -<span class="hljs-number">3.05175781</span>e-<span class="hljs-number">05</span>]),
  &#x27;sampling_rate&#x27;: <span class="hljs-number">16000</span>},
 &#x27;text&#x27;: &#x27;CHAPTER SIXTEEN I MIGHT HAVE TOLD YOU OF THE BEGINNING OF THIS LIAISON IN A FEW LINES BUT I WANTED YOU TO SEE EVERY STEP BY WHICH WE CAME I TO AGREE TO WHATEVER MARGUERITE WISHED&#x27;,
 &#x27;speaker_id&#x27;: <span class="hljs-number">374</span>,
 &#x27;chapter_id&#x27;: <span class="hljs-number">180298</span>,
 &#x27;id&#x27;: &#x27;374-<span class="hljs-number">180298-0000</span>&#x27;}`}}),Fe=new O({}),Ce=new H({props:{code:`from transformers import SpeechEncoderDecoderModel

encoder_id = "facebook/wav2vec2-large-lv60"
decoder_id = "facebook/bart-large"

model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_add_adapter=True)

# set special tokens for generation
model.config.decoder_start_token_id = model.decoder.config.bos_token_id
model.config.pad_token_id = model.decoder.config.pad_token_id
model.config.eos_token_id = model.decoder.config.eos_token_id`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SpeechEncoderDecoderModel

encoder_id = <span class="hljs-string">&quot;facebook/wav2vec2-large-lv60&quot;</span>
decoder_id = <span class="hljs-string">&quot;facebook/bart-large&quot;</span>

model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id, encoder_add_adapter=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># set special tokens for generation</span>
model.config.decoder_start_token_id = model.decoder.config.bos_token_id
model.config.pad_token_id = model.decoder.config.pad_token_id
model.config.eos_token_id = model.decoder.config.eos_token_id`}}),ze=new O({}),Be=new H({props:{code:`from transformers import AutoFeatureExtractor, AutoTokenizer, Wav2Vec2Processor
# load feature extractor (for audio inputs) and tokenizer (for text outputs)
feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)
tokenizer = AutoTokenizer.from_pretrained(decoder_id)
# combine under one class
processor = Wav2Vec2Processor(feature_extractor, tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, AutoTokenizer, Wav2Vec2Processor
<span class="hljs-comment"># load feature extractor (for audio inputs) and tokenizer (for text outputs)</span>
feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)
tokenizer = AutoTokenizer.from_pretrained(decoder_id)
<span class="hljs-comment"># combine under one class</span>
processor = Wav2Vec2Processor(feature_extractor, tokenizer)`}}),Ge=new H({props:{code:`import numpy as np

sample = raw_datasets["train"][0]["audio"]

print(f"Mean: {np.mean(sample['array']):.3}, Variance: {np.var(sample['array']):.3}")`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

sample = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Mean: <span class="hljs-subst">{np.mean(sample[<span class="hljs-string">&#x27;array&#x27;</span>]):<span class="hljs-number">.3</span>}</span>, Variance: <span class="hljs-subst">{np.var(sample[<span class="hljs-string">&#x27;array&#x27;</span>]):<span class="hljs-number">.3</span>}</span>&quot;</span>)`}}),Ue=new H({props:{code:"Mean: -2.17e-05, Variance: 0.00379",highlighted:'<span class="hljs-attribute">Mean</span>: -<span class="hljs-number">2</span>.<span class="hljs-number">17</span>e-<span class="hljs-number">05</span>, Variance: <span class="hljs-number">0</span>.<span class="hljs-number">00379</span>'}}),Ye=new H({props:{code:`inputs = feature_extractor(sample["array"], sampling_rate=sample["sampling_rate"])

print(f"inputs keys: {list(inputs.keys())}")

print(f"Mean: {np.mean(inputs['input_values']):.3}, Variance: {np.var(inputs['input_values']):.3}")`,highlighted:`inputs = feature_extractor(sample[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sample[<span class="hljs-string">&quot;sampling_rate&quot;</span>])

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;inputs keys: <span class="hljs-subst">{<span class="hljs-built_in">list</span>(inputs.keys())}</span>&quot;</span>)

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Mean: <span class="hljs-subst">{np.mean(inputs[<span class="hljs-string">&#x27;input_values&#x27;</span>]):<span class="hljs-number">.3</span>}</span>, Variance: <span class="hljs-subst">{np.var(inputs[<span class="hljs-string">&#x27;input_values&#x27;</span>]):<span class="hljs-number">.3</span>}</span>&quot;</span>)`}}),Xe=new H({props:{code:`inputs keys: ['input_values', 'attention_mask']
Mean: -2.8e-09, Variance: 1.0`,highlighted:`<span class="hljs-attribute">inputs</span> keys:<span class="hljs-meta"> [&#x27;input_values&#x27;, &#x27;attention_mask&#x27;]</span>
<span class="hljs-attribute">Mean</span>: -<span class="hljs-number">2</span>.<span class="hljs-number">8</span>e-<span class="hljs-number">09</span>, Variance: <span class="hljs-number">1</span>.<span class="hljs-number">0</span>`}}),Je=new H({props:{code:`def prepare_dataset(batch):
    # process audio
    sample = batch["audio"]
    inputs = feature_extractor(sample["array"], sampling_rate=sample["sampling_rate"])
    # process audio length
    batch["input_values"] = inputs.input_values[0]
    batch["input_length"] = len(batch["input_values"])

    # process targets
    input_str = batch["text"].lower()
    # tokenize
    batch["labels"] = tokenizer(input_str).input_ids
    return batch


vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=next(iter(raw_datasets.values())).column_names,
    desc="preprocess dataset",
)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">batch</span>):
    <span class="hljs-comment"># process audio</span>
    sample = batch[<span class="hljs-string">&quot;audio&quot;</span>]
    inputs = feature_extractor(sample[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sample[<span class="hljs-string">&quot;sampling_rate&quot;</span>])
    <span class="hljs-comment"># process audio length</span>
    batch[<span class="hljs-string">&quot;input_values&quot;</span>] = inputs.input_values[<span class="hljs-number">0</span>]
    batch[<span class="hljs-string">&quot;input_length&quot;</span>] = <span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&quot;input_values&quot;</span>])

    <span class="hljs-comment"># process targets</span>
    input_str = batch[<span class="hljs-string">&quot;text&quot;</span>].lower()
    <span class="hljs-comment"># tokenize</span>
    batch[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(input_str).input_ids
    <span class="hljs-keyword">return</span> batch


vectorized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(
    prepare_dataset,
    remove_columns=<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(raw_datasets.values())).column_names,
    desc=<span class="hljs-string">&quot;preprocess dataset&quot;</span>,
)`}}),Ke=new H({props:{code:`MAX_INPUT_LENGTH_IN_SECONDS = 20

max_input_length = MAX_INPUT_LENGTH_IN_SECONDS * feature_extractor.sampling_rate
# filter data that is longer than max_input_length
def is_audio_in_length_range(length):
    return length < max_input_length

vectorized_datasets = vectorized_datasets.filter(
    is_audio_in_length_range,
    input_columns=["input_length"],
)`,highlighted:`MAX_INPUT_LENGTH_IN_SECONDS = <span class="hljs-number">20</span>

max_input_length = MAX_INPUT_LENGTH_IN_SECONDS * feature_extractor.sampling_rate
<span class="hljs-comment"># filter data that is longer than max_input_length</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">is_audio_in_length_range</span>(<span class="hljs-params">length</span>):
    <span class="hljs-keyword">return</span> length &lt; max_input_length

vectorized_datasets = vectorized_datasets.<span class="hljs-built_in">filter</span>(
    is_audio_in_length_range,
    input_columns=[<span class="hljs-string">&quot;input_length&quot;</span>],
)`}}),Qe=new H({props:{code:`from dataclasses import dataclass

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """
    Data collator that will dynamically pad the inputs received.
    Args:
        processor ([\`Wav2Vec2Processor\`])
            The processor used for processing the data.
        decoder_start_token_id (\`int\`)
            The begin-of-sentence of the decoder.
    """

    processor: Wav2Vec2Processor
    decoder_start_token_id: int

    def __call__(self, features):
        # split inputs and labels since they have to be of different lengths and need
        # different padding methods
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        # pad the audio inputs to max length
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # pad the token targets to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's appended later
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
        processor=processor, decoder_start_token_id=model.config.decoder_start_token_id
    )`,highlighted:`<span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass

<span class="hljs-meta">@dataclass</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">DataCollatorSpeechSeq2SeqWithPadding</span>:
    <span class="hljs-string">&quot;&quot;&quot;
    Data collator that will dynamically pad the inputs received.
    Args:
        processor ([\`Wav2Vec2Processor\`])
            The processor used for processing the data.
        decoder_start_token_id (\`int\`)
            The begin-of-sentence of the decoder.
    &quot;&quot;&quot;</span>

    processor: Wav2Vec2Processor
    decoder_start_token_id: <span class="hljs-built_in">int</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, features</span>):
        <span class="hljs-comment"># split inputs and labels since they have to be of different lengths and need</span>
        <span class="hljs-comment"># different padding methods</span>
        input_features = [{<span class="hljs-string">&quot;input_values&quot;</span>: feature[<span class="hljs-string">&quot;input_values&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]
        label_features = [{<span class="hljs-string">&quot;input_ids&quot;</span>: feature[<span class="hljs-string">&quot;labels&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]

        <span class="hljs-comment"># pad the audio inputs to max length</span>
        batch = self.processor.feature_extractor.pad(input_features, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

        <span class="hljs-comment"># pad the token targets to max length</span>
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

        <span class="hljs-comment"># replace padding with -100 to ignore loss correctly</span>
        labels = labels_batch[<span class="hljs-string">&quot;input_ids&quot;</span>].masked_fill(labels_batch.attention_mask.ne(<span class="hljs-number">1</span>), -<span class="hljs-number">100</span>)

        <span class="hljs-comment"># if bos token is appended in previous tokenization step,</span>
        <span class="hljs-comment"># cut bos token here as it&#x27;s appended later</span>
        <span class="hljs-keyword">if</span> (labels[:, <span class="hljs-number">0</span>] == self.decoder_start_token_id).<span class="hljs-built_in">all</span>().cpu().item():
            labels = labels[:, <span class="hljs-number">1</span>:]

        batch[<span class="hljs-string">&quot;labels&quot;</span>] = labels

        <span class="hljs-keyword">return</span> batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
        processor=processor, decoder_start_token_id=model.config.decoder_start_token_id
    )`}}),Ze=new O({}),et=new O({}),{c(){C=s("meta"),Rs=d(),z=s("h1"),ae=s("a"),Yt=s("span"),u(Ee.$$.fragment),Mr=d(),Xt=s("span"),Cr=i("Automatically recognising speech"),Vs=d(),se=s("p"),zr=i("In this section, we\u2019ll take a look at how Transformers can be used to convert spoken speech into text, a task known "),Jt=s("em"),Br=i("speech recognition"),Gr=i("."),Os=d(),tt=s("p"),Ur=i("TODO: insert Speech -> Model -> Text diagram"),Fs=d(),at=s("p"),Yr=i("Speech recognition, also known as automatic speech recognition (ASR) or speech-to-text (STT), is one of the most popular and exciting spoken language processing tasks. It\u2019s used in a wide range of applications, including dictation, voice assistants, video captioning and meeting transcriptions."),Ms=d(),st=s("p"),Xr=i("You\u2019ve probably made use of a speech recognition system many times before without realising! Consider the digital assistant in your smartphone device (Siri, Google Assistant, Alexa). When you use these assistants, the first thing that they do is transcribe your spoken speech to written text, ready to be used for any downstream tasks."),Cs=d(),B=s("p"),Jr=i("Image: "),xe=s("a"),Kr=i("https://images.app.goo.gl/J6SU6RYryXXVT9uj7"),Qr=i(`
Demo: `),Te=s("a"),Zr=i("https://huggingface.co/facebook/wav2vec2-base-960h"),zs=d(),ot=s("p"),en=i("Speech recognition is challenging as it requires joint knowledge of audio and text. The input audio might have lots of background noise, making it difficult to pick-out the spoken speech. The written text might have characters which don\u2019t have an acoustic sound, such as punctuation, which are difficult to infer from audio alone. These are all hurdles we\u2019ll have to tackle when building effective speech recognition systems."),Bs=d(),rt=s("p"),tn=i("[//]: # Speech recognition is challenging as it requires a model to leverage knowledge about two domains concurrently: the input audio and the output text. Systems need to extract the relevant features from the input audio, possibly disentangling the spoken speech from background noise. From these features, they must then infer the contextual relation of words and sentences to form accurate and coherent sentences."),Gs=d(),nt=s("p"),an=i("Now that we\u2019ve defined our task, we can begin looking into speech recognition in more detail. Specifically, we\u2019ll cover:"),Us=d(),$=s("ul"),Kt=s("li"),sn=i("How to choose a dataset"),on=d(),Qt=s("li"),rn=i("How to load a dataset"),nn=d(),Zt=s("li"),ln=i("What models we can use"),pn=d(),ea=s("li"),dn=i("How to prepare audio-text data"),cn=d(),ta=s("li"),hn=i("Metrics for speech recognition"),un=d(),aa=s("li"),fn=i("How to fine-tune an ASR system with the Trainer API"),Ys=d(),G=s("h3"),oe=s("a"),sa=s("span"),u($e.$$.fragment),mn=d(),oa=s("span"),vn=i("How to choose a dataset"),Xs=i(`

As with any machine learning problem, our model is only as good as the data that we train it on. Speech recognition datasets vary considerably in how they are curated and the domains that they cover. To pick the right dataset, we need to match our criteria with the features that a dataset offers.
`),it=s("p"),wn=i("Before we pick a dataset, we first need to understand some of the key defining features."),Js=d(),U=s("h4"),re=s("a"),ra=s("span"),u(je.$$.fragment),_n=d(),na=s("span"),gn=i("1. Number of hours"),Ks=i(`

Simply put, the number of training hours indicates how large the dataset is. It\u2019s analogous to the number of training examples in an NLP dataset. However, bigger datasets aren\u2019t necessarily better. If we want a model that generalises well, we want a diverse dataset with lots of different speakers, domains and speaking styles.
`),Y=s("h4"),ne=s("a"),ia=s("span"),u(Se.$$.fragment),bn=d(),la=s("span"),yn=i("2. Domain"),Qs=i(`

The domain entails where the data was sourced from, whether it be audiobooks, podcasts, YouTube or financial meetings. Each domain has a different distribution of data. For example, audiobooks are recorded in high-quality studio conditions (with no background noise) and text that is taken from written literature. Whereas for YouTube, the audio likely contains more background noise and a more informal style of writing.
`),lt=s("p"),kn=i("We need to match our domain to the conditions we anticipate at inference time. For instance, if we train our model on audiobooks, we can\u2019t expect our model to perform well in noisy environments."),Zs=d(),X=s("h4"),ie=s("a"),pa=s("span"),u(Ae.$$.fragment),En=d(),da=s("span"),xn=i("3. Speaking style"),eo=i(`

The speaking style falls into one of two categories:
* Narrated: read from a script
* Spontaneous: un-scripted, conversational speech
`),pt=s("p"),Tn=i("The audio and text data reflect the style of speaking. Since narrated text is scripted, it tends to be spoken articulately and without any errors:"),to=d(),dt=s("p"),$n=i("\u201CConsider the task of training a model on a speech recognition dataset\u201D"),ao=d(),ct=s("p"),jn=i("Whereas for spontaneous speech, we can expect a more colloquial style of speech, with the inclusion of repetitions, hesitations and false-starts:"),so=d(),ht=s("p"),Sn=i("\u201CLet\u2019s <uhm> take a look at how <silence> you\u2019d you\u2019d go about training a model on <uhm> a sp- speech recognition dataset\u201D (this is an extreme example)"),oo=d(),J=s("h4"),le=s("a"),ca=s("span"),u(Pe.$$.fragment),An=d(),ha=s("span"),Pn=i("4. Transcription style"),ro=i(`

Whether the text has punctuation, casing or both. If we want a system to generate fully formatted text that could be used for a publication or meeting transcription, we require training data with punctuation and casing. If we just require the spoken words in an unformatted structure, neither punctuation nor casing are necessary.
`),ut=s("p"),qn=i("A summary of datasets on the HF Hub:"),no=d(),pe=s("table"),ua=s("thead"),g=s("tr"),io=s("th"),Dn=d(),fa=s("th"),Wn=i("Train Hours"),Nn=d(),ma=s("th"),In=i("Domain"),Hn=d(),va=s("th"),Ln=i("Speaking Style"),Rn=d(),wa=s("th"),Vn=i("Transcription Casing"),On=d(),_a=s("th"),Fn=i("Transcription Punctuation"),Mn=d(),ga=s("th"),Cn=i("Recommended Usage"),zn=d(),S=s("tbody"),b=s("tr"),ba=s("td"),Bn=i("LibriSpeech"),Gn=d(),ya=s("td"),Un=i("960"),Yn=d(),ka=s("td"),Xn=i("Audiobooks"),Jn=d(),Ea=s("td"),Kn=i("Narrated"),Qn=d(),xa=s("td"),Zn=i("\u274C"),ei=d(),Ta=s("td"),ti=i("\u274C"),ai=d(),$a=s("td"),si=i("Academic benchmarks"),oi=d(),y=s("tr"),ja=s("td"),ri=i("Common Voice 9"),ni=d(),Sa=s("td"),ii=i("2224"),li=d(),Aa=s("td"),pi=i("Wikipedia text + crowd-sourced speech"),di=d(),Pa=s("td"),ci=i("Narrated"),hi=d(),qa=s("td"),ui=i("\u2705"),fi=d(),Da=s("td"),mi=i("\u2705"),vi=d(),Wa=s("td"),wi=i("Day-to-day speech"),_i=d(),k=s("tr"),Na=s("td"),gi=i("TEDLIUM"),bi=d(),Ia=s("td"),yi=i("452"),ki=d(),Ha=s("td"),Ei=i("TED talks"),xi=d(),La=s("td"),Ti=i("Narrated"),$i=d(),Ra=s("td"),ji=i("\u274C"),Si=d(),Va=s("td"),Ai=i("\u274C"),Pi=d(),Oa=s("td"),qi=i("Technical scientific, political and social topics"),Di=d(),E=s("tr"),Fa=s("td"),Wi=i("Voxpopuli"),Ni=d(),Ma=s("td"),Ii=i("543"),Hi=d(),Ca=s("td"),Li=i("European Parliament recordings"),Ri=d(),za=s("td"),Vi=i("Spontaneous"),Oi=d(),Ba=s("td"),Fi=i("\u274C"),Mi=d(),Ga=s("td"),Ci=i("\u274C"),zi=d(),Ua=s("td"),Bi=i("Non-native English speakers"),Gi=d(),x=s("tr"),Ya=s("td"),Ui=i("GigaSpeech"),Yi=d(),Xa=s("td"),Xi=i("10000"),Ji=d(),Ja=s("td"),Ki=i("Audiobook, podcast, youtube"),Qi=d(),Ka=s("td"),Zi=i("Narrated & spontaneous"),el=d(),Qa=s("td"),tl=i("\u274C"),al=d(),Za=s("td"),sl=i("\u2705"),ol=d(),es=s("td"),rl=i("Robustness over multiple domains"),nl=d(),T=s("tr"),ts=s("td"),il=i("SPGISpeech"),ll=d(),as=s("td"),pl=i("5000"),dl=d(),ss=s("td"),cl=i("Financial meetings"),hl=d(),os=s("td"),ul=i("Narrated & spontaneous"),fl=d(),rs=s("td"),ml=i("\u2705"),vl=d(),ns=s("td"),wl=i("\u2705"),_l=d(),is=s("td"),gl=i("Fully formatted transcriptions"),lo=d(),ft=s("p"),bl=i("Let\u2019s take an example where we want to train a speech recognition system to transcribe speech on the topic of machine learning (highly relevant!). Our speech is going to be narrated (scripted), and we\u2019re not worried about punctuation or casing on our text output. From our reference table, it looks like TED-LIUM is a good choice of dataset!"),po=d(),mt=s("p"),yl=i("[//]: # On review, this sub-section might be too involved. Happy to omit it and dive straight into how to load a dataset."),co=d(),K=s("h3"),de=s("a"),ls=s("span"),u(qe.$$.fragment),kl=d(),ps=s("span"),El=i("How to load an audio dataset"),ho=i(`

Before we can handle any audio datasets, we need to make sure we have the right dependencies installed. Specifically, we'll need the "Audio" feature from datasets: https://huggingface.co/docs/datasets/installation#audio. This should take care of the main Python packages we need to read audio files from byte form and convert them to arrays.
`),ce=s("p"),xl=i("Right! Now we\u2019re ready to go ahead and download our data. For this tutorial, we\u2019ll use the "),De=s("a"),ds=s("em"),Tl=i("LibriSpeech ASR"),$l=i(" corpus. LibriSpeech is one of the most popular datasets for benchmarking speech recognition systems in both academia and industry. It consists of approximately 1000 hours of narrated audiobooks collected from the (LibriVox)[https://librivox.org/] project."),uo=d(),he=s("p"),jl=i("Let\u2019s take a look at the all-time leaderboard on Papers with Code to get a feel for numbers: "),We=s("a"),Sl=i("https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean"),Al=i(`
We can see that there\u2019s been tremendous progress on the LibriSpeech benchmark in the last three years, with many of the top spots being occupied by Wav2Vec2 or Wav2Vec2-like systems! As mentioned in Section 4, Wav2Vec2 revolutionised the field of speech recognition by introducing an effective pre-training regime, much the same way as BERT revolutionised NLP. We can see that the word error rates (WERs) for this dataset are extremely low! We\u2019ll cover WER in more detail in this section, but for now we can see that the best performing system achieves a WER of just 1.4%, or a word accuracy rate of 100 - 1.4 = 98.6%! This is very impressive! We\u2019ll see how close we can get with our system \u{1F917}`),fo=d(),ue=s("p"),Pl=i("Now that we\u2019ve got better idea about the dataset that we\u2019re working with, let\u2019s download it using the "),cs=s("code"),ql=i("load_dataset()"),Dl=i(" function:"),mo=d(),u(Ne.$$.fragment),vo=d(),vt=s("p"),Wl=i("Note that audio datasets are quite large! For 100h of training data and 10h of validation/test data, we can expect to download ~60GB of raw data."),wo=d(),wt=s("p"),Nl=i("Let\u2019s inspect the dataset:"),_o=d(),u(Ie.$$.fragment),go=d(),u(He.$$.fragment),bo=d(),F=s("p"),Il=i("Great! We have our training, validation and test splits ready, with 28539, 2703 and 2620 samples respectively. The two data columns "),hs=s("code"),Hl=i("audio"),Ll=i(" and "),us=s("code"),Rl=i("text"),Vl=i(" contain the most important information for our task: the raw audio inputs and the target text outputs."),yo=d(),_t=s("p"),Ol=i("Let\u2019s take a look at the first example of the train split:"),ko=d(),u(Le.$$.fragment),Eo=d(),u(Re.$$.fragment),xo=d(),gt=s("p"),Fl=i("Alright! We can see that we have our target text sample ready - it looks very much like it\u2019s taken from an audiobook! But what on earth is going on with the audio? \u{1F914}"),To=d(),bt=s("p"),Ml=i("As it turns out, we represent audio data digitally as a 1-dimensional array. Each array value denotes the amplitude of our audio signal at a particular time step. From the amplitude information, we can reconstruct the frequency spectrum of the audio and recover all the acoustic features."),$o=d(),fe=s("p"),Cl=i("Since our audio input is continuous, we can\u2019t represent an amplitude value for every possible time step. Instead, we "),fs=s("em"),zl=i("sample"),Bl=i(" amplitude values at fixed time steps."),jo=d(),me=s("p"),Gl=i("The interval with which we sample our audio is known as the "),ms=s("em"),Ul=i("sampling rate"),Yl=i(". For our dataset, we can see that the sampling rate is 16000, meaning 16000 amplitude values are provided each second. Keep the sampling rate in the back of your mind, it\u2019ll be important when we come to processing our data later on!"),So=d(),ve=s("p"),Xl=i("Whilst 1-dimensional arrays are a suitable format for machines, they\u2019re not much use to us as humans! We want to be able to "),vs=s("strong"),Jl=i("listen"),Kl=i(" to our audio to get a feel for the speech and recording conditions. Here, we have two options:"),Ao=d(),we=s("ol"),ws=s("li"),Ql=i("Convert our 1-dimensional array into a human-friendly format (mp3 or wav)."),Zl=d(),_s=s("li"),ep=i("Look to the Hugging Face Hub!"),Po=d(),Ve=s("p"),tp=i("Option 2 seems much easier to me! Let\u2019s head over to the LibriSpeech ASR dataset card on the Hugging Face Hub: "),Oe=s("a"),ap=i("https://huggingface.co/datasets/librispeech_asr"),qo=d(),yt=s("p"),sp=i("Slap bang in the middle of the dataset card we have exactly what we\u2019re looking for: the dataset viewer! The dataset viewer shows us the first 100 samples of any dataset. What\u2019s more, it\u2019s loaded up the audio samples ready for us to listen to in real-time! If we hit the play button on the first sample, we can listen to the audio and see the corresponding text. Have a scroll through the samples for the train and test sets to get a better feel for the audio data that we\u2019re dealing with. You\u2019ll notice how clear the audio is and how well spoken the sentences are. This provides us with a clue as to why the top models do so well on LibriSpeech! The audio conditions are very conducive to high system performance."),Do=d(),Q=s("h3"),_e=s("a"),gs=s("span"),u(Fe.$$.fragment),op=d(),bs=s("span"),rp=i("Models for speech recognition"),Wo=d(),kt=s("p"),np=i("We can decompose speech recognition models into two parts:"),No=d(),Et=s("ol"),ys=s("li"),ip=i("Encoder: an acoustic model that maps the raw audio input into a sequence of hidden-states."),Io=d(),xt=s("p"),lp=i("diagram"),Ho=d(),Me=s("ol"),ks=s("li"),pp=i("Decoder: maps the sequence of hidden-states to logits over the vocabulary."),Lo=d(),Tt=s("p"),dp=i("diagram"),Ro=d(),$t=s("p"),cp=i("The encoder is typically of Wav2Vec2 architecture, introduced in Section 4 of this Chapter. We have some flexibility in our choice of decoder. We could either use a simple linear layer that maps the Wav2Vec2 hidden states directly to output logits over our vocabulary. Or, we could pair our Wav2Vec2 encoder with a decoder model, giving a speech encoder-decoder style model, analogous to the NLP encoder-decoder model introduced in Chapter 1."),Vo=d(),jt=s("p"),hp=i(`A simple linear layer will give a smaller, faster overall model, but will be more susceptible to spelling and grammatical errors. Adding a decoder model greatly improves the quality of transcriptions, at the cost of a larger, slower model. This is because the decoder model is pre-trained on a large corpus of text, enabling us to leverage its learned text representations.
Since we want our system to be robust to spelling and grammar, let\u2019s go ahead and define a speech encoder-decoder style model.`),Oo=d(),St=s("p"),up=i("We\u2019ll pair a pre-trained Wav2Vec2 encoder with a pre-trained BART decoder, yielding a Wav2Vev2-2-BART model:"),Fo=d(),u(Ce.$$.fragment),Mo=d(),ge=s("p"),fp=i("The "),Es=s("code"),mp=i("add_adapter"),vp=i(" argument introduces a small convolutional network between the encoder and decoder models. This adapter network helps interface the encoder and decoder by down-sampling the encoder hidden-states to better match the timescale of the decoder. In practice, including this adapter results in superior performance than the encoder and decoder models alone:"),Co=d(),At=s("p"),wp=i("diagram"),zo=d(),Z=s("h4"),be=s("a"),xs=s("span"),u(ze.$$.fragment),_p=d(),Ts=s("span"),gp=i("Preprocessing the data"),Bo=i(`

Great! Now that we've defined our model we can start preparing our data for training. For this, we'll need to define two objects: a feature extractor and a tokenizer.

	`),u(Be.$$.fragment),Go=d(),Pt=s("p"),bp=i("We\u2019re already pretty familiar with the tokenizer: it converts the text data into a set of token IDs that can be interpreted by the model. This we\u2019ll use to prepare our target labels. But the feature extractor is new! Simply put, the feature extractor prepares our input audio data."),Uo=d(),qt=s("p"),yp=i("A defining feature of Wav2Vec2 is that it accepts a float array corresponding to the raw waveform of the speech signal as an input. We mentioned that the audio data is represented as a 1-dimensional array, so it\u2019s already in the right format to be read by the model (a set of continuous inputs at discrete time steps)."),Yo=d(),Dt=s("p"),kp=i("So, what exactly does the feature extractor do?"),Xo=d(),Wt=s("p"),Ep=i("Well, the audio data is in the right format, but we\u2019ve imposed no restrictions on the values it can take. For our model to work optimally, we want to keep all the inputs within the same dynamic range. This is going to make sure we get a similar range of activations and gradients for our samples, helping with stability and convergence during training."),Jo=d(),M=s("p"),xp=i("To do this, we "),$s=s("em"),Tp=i("normalise"),$p=i(" our audio data, by rescaling each sample to zero mean and unit variance, a process called "),js=s("em"),jp=i("feature scaling"),Sp=i(". It\u2019s exactly this feature normalisation that our feature extractor performs!"),Ko=d(),Nt=s("p"),Ap=i("We can take a look at the feature extractor in operation by applying it to our first audio sample. First, let\u2019s compute the mean and standard deviation of our raw audio data:"),Qo=d(),u(Ge.$$.fragment),Zo=d(),u(Ue.$$.fragment),er=d(),It=s("p"),Pp=i("We can see that the mean is close to zero already, but the standard deviation is a long way off! This would cause our model problems, as the dynamic range of the audio data would be very small and difficult to separate. Let\u2019s apply the feature extractor and see what the outputs look like:"),tr=d(),u(Ye.$$.fragment),ar=d(),u(Xe.$$.fragment),sr=d(),j=s("p"),qp=i("Alright! Our feature extractor returns a dictionary of two arrays: "),Ss=s("code"),Dp=i("input_values"),Wp=i(" and "),As=s("code"),Np=i("attention_mask"),Ip=i(". The "),Ps=s("code"),Hp=i("input_values"),Lp=i(" are the preprocessed audio inputs that we\u2019d pass to the Wav2Vec2 model. The "),qs=s("code"),Rp=i("attention_mask"),Vp=i(" would be used if we processed a "),Ds=s("em"),Op=i("batch"),Fp=i(" of audio inputs at once."),or=d(),Ht=s("p"),Mp=i("We can see that the mean value is now very much closer to zero, and the variance bang-on one! This is exactly the form we want our audio samples in prior to feeding them to the Wav2Vec2 encoder."),rr=d(),Lt=s("p"),Cp=i("Note how we\u2019ve passed the sampling rate of our audio data to our feature extractor. This is good practice, as the feature extractor performs a check under-the-hood to make sure the sampling rate of our audio data matches the sampling rate expected by the model."),nr=d(),Rt=s("p"),zp=i("Let\u2019s combine our feature extractor and tokenizer into one function to jointly preprocess our audio and text data:"),ir=d(),u(Je.$$.fragment),lr=d(),Vt=s("p"),Bp=i("Note that it is important to align the decoder\u2019s vocabulary with the speech transcriptions of the dataset."),pr=d(),Ot=s("p"),Gp=i("E.g. Librispeech: only captilised letters in the transcriptions, whereas BART was pretrained mostly on lower-cased text."),dr=d(),Ft=s("p"),Up=i("Filter to max len 20s"),cr=d(),u(Ke.$$.fragment),hr=d(),Mt=s("p"),Yp=i("Define the datacollator:"),ur=d(),u(Qe.$$.fragment),fr=d(),ee=s("h4"),ye=s("a"),Ws=s("span"),u(Ze.$$.fragment),Xp=d(),Ns=s("span"),Jp=i("Metrics for speech recognition"),mr=d(),Ct=s("p"),Kp=i("WER"),vr=d(),zt=s("p"),Qp=i("WAR"),wr=d(),Bt=s("p"),Zp=i("CER"),_r=d(),te=s("h4"),ke=s("a"),Is=s("span"),u(et.$$.fragment),ed=d(),Hs=s("span"),td=i("Fine-tuning a speech recognition system with the Trainer API"),gr=i(`

Oh golly quite a lot to do...`),this.h()},l(e){const n=ou('[data-svelte="svelte-1phssyn"]',document.head);C=o(n,"META",{name:!0,content:!0}),n.forEach(t),Rs=c(e),z=o(e,"H1",{class:!0});var yr=r(z);ae=o(yr,"A",{id:!0,class:!0,href:!0});var od=r(ae);Yt=o(od,"SPAN",{});var rd=r(Yt);f(Ee.$$.fragment,rd),rd.forEach(t),od.forEach(t),Mr=c(yr),Xt=o(yr,"SPAN",{});var nd=r(Xt);Cr=l(nd,"Automatically recognising speech"),nd.forEach(t),yr.forEach(t),Vs=c(e),se=o(e,"P",{});var kr=r(se);zr=l(kr,"In this section, we\u2019ll take a look at how Transformers can be used to convert spoken speech into text, a task known "),Jt=o(kr,"EM",{});var id=r(Jt);Br=l(id,"speech recognition"),id.forEach(t),Gr=l(kr,"."),kr.forEach(t),Os=c(e),tt=o(e,"P",{});var ld=r(tt);Ur=l(ld,"TODO: insert Speech -> Model -> Text diagram"),ld.forEach(t),Fs=c(e),at=o(e,"P",{});var pd=r(at);Yr=l(pd,"Speech recognition, also known as automatic speech recognition (ASR) or speech-to-text (STT), is one of the most popular and exciting spoken language processing tasks. It\u2019s used in a wide range of applications, including dictation, voice assistants, video captioning and meeting transcriptions."),pd.forEach(t),Ms=c(e),st=o(e,"P",{});var dd=r(st);Xr=l(dd,"You\u2019ve probably made use of a speech recognition system many times before without realising! Consider the digital assistant in your smartphone device (Siri, Google Assistant, Alexa). When you use these assistants, the first thing that they do is transcribe your spoken speech to written text, ready to be used for any downstream tasks."),dd.forEach(t),Cs=c(e),B=o(e,"P",{});var Ls=r(B);Jr=l(Ls,"Image: "),xe=o(Ls,"A",{href:!0,rel:!0});var cd=r(xe);Kr=l(cd,"https://images.app.goo.gl/J6SU6RYryXXVT9uj7"),cd.forEach(t),Qr=l(Ls,`
Demo: `),Te=o(Ls,"A",{href:!0,rel:!0});var hd=r(Te);Zr=l(hd,"https://huggingface.co/facebook/wav2vec2-base-960h"),hd.forEach(t),Ls.forEach(t),zs=c(e),ot=o(e,"P",{});var ud=r(ot);en=l(ud,"Speech recognition is challenging as it requires joint knowledge of audio and text. The input audio might have lots of background noise, making it difficult to pick-out the spoken speech. The written text might have characters which don\u2019t have an acoustic sound, such as punctuation, which are difficult to infer from audio alone. These are all hurdles we\u2019ll have to tackle when building effective speech recognition systems."),ud.forEach(t),Bs=c(e),rt=o(e,"P",{});var fd=r(rt);tn=l(fd,"[//]: # Speech recognition is challenging as it requires a model to leverage knowledge about two domains concurrently: the input audio and the output text. Systems need to extract the relevant features from the input audio, possibly disentangling the spoken speech from background noise. From these features, they must then infer the contextual relation of words and sentences to form accurate and coherent sentences."),fd.forEach(t),Gs=c(e),nt=o(e,"P",{});var md=r(nt);an=l(md,"Now that we\u2019ve defined our task, we can begin looking into speech recognition in more detail. Specifically, we\u2019ll cover:"),md.forEach(t),Us=c(e),$=o(e,"UL",{});var L=r($);Kt=o(L,"LI",{});var vd=r(Kt);sn=l(vd,"How to choose a dataset"),vd.forEach(t),on=c(L),Qt=o(L,"LI",{});var wd=r(Qt);rn=l(wd,"How to load a dataset"),wd.forEach(t),nn=c(L),Zt=o(L,"LI",{});var _d=r(Zt);ln=l(_d,"What models we can use"),_d.forEach(t),pn=c(L),ea=o(L,"LI",{});var gd=r(ea);dn=l(gd,"How to prepare audio-text data"),gd.forEach(t),cn=c(L),ta=o(L,"LI",{});var bd=r(ta);hn=l(bd,"Metrics for speech recognition"),bd.forEach(t),un=c(L),aa=o(L,"LI",{});var yd=r(aa);fn=l(yd,"How to fine-tune an ASR system with the Trainer API"),yd.forEach(t),L.forEach(t),Ys=c(e),G=o(e,"H3",{class:!0});var Er=r(G);oe=o(Er,"A",{id:!0,class:!0,href:!0});var kd=r(oe);sa=o(kd,"SPAN",{});var Ed=r(sa);f($e.$$.fragment,Ed),Ed.forEach(t),kd.forEach(t),mn=c(Er),oa=o(Er,"SPAN",{});var xd=r(oa);vn=l(xd,"How to choose a dataset"),xd.forEach(t),Er.forEach(t),Xs=l(e,`

As with any machine learning problem, our model is only as good as the data that we train it on. Speech recognition datasets vary considerably in how they are curated and the domains that they cover. To pick the right dataset, we need to match our criteria with the features that a dataset offers.
`),it=o(e,"P",{});var Td=r(it);wn=l(Td,"Before we pick a dataset, we first need to understand some of the key defining features."),Td.forEach(t),Js=c(e),U=o(e,"H4",{class:!0});var xr=r(U);re=o(xr,"A",{id:!0,class:!0,href:!0});var $d=r(re);ra=o($d,"SPAN",{});var jd=r(ra);f(je.$$.fragment,jd),jd.forEach(t),$d.forEach(t),_n=c(xr),na=o(xr,"SPAN",{});var Sd=r(na);gn=l(Sd,"1. Number of hours"),Sd.forEach(t),xr.forEach(t),Ks=l(e,`

Simply put, the number of training hours indicates how large the dataset is. It\u2019s analogous to the number of training examples in an NLP dataset. However, bigger datasets aren\u2019t necessarily better. If we want a model that generalises well, we want a diverse dataset with lots of different speakers, domains and speaking styles.
`),Y=o(e,"H4",{class:!0});var Tr=r(Y);ne=o(Tr,"A",{id:!0,class:!0,href:!0});var Ad=r(ne);ia=o(Ad,"SPAN",{});var Pd=r(ia);f(Se.$$.fragment,Pd),Pd.forEach(t),Ad.forEach(t),bn=c(Tr),la=o(Tr,"SPAN",{});var qd=r(la);yn=l(qd,"2. Domain"),qd.forEach(t),Tr.forEach(t),Qs=l(e,`

The domain entails where the data was sourced from, whether it be audiobooks, podcasts, YouTube or financial meetings. Each domain has a different distribution of data. For example, audiobooks are recorded in high-quality studio conditions (with no background noise) and text that is taken from written literature. Whereas for YouTube, the audio likely contains more background noise and a more informal style of writing.
`),lt=o(e,"P",{});var Dd=r(lt);kn=l(Dd,"We need to match our domain to the conditions we anticipate at inference time. For instance, if we train our model on audiobooks, we can\u2019t expect our model to perform well in noisy environments."),Dd.forEach(t),Zs=c(e),X=o(e,"H4",{class:!0});var $r=r(X);ie=o($r,"A",{id:!0,class:!0,href:!0});var Wd=r(ie);pa=o(Wd,"SPAN",{});var Nd=r(pa);f(Ae.$$.fragment,Nd),Nd.forEach(t),Wd.forEach(t),En=c($r),da=o($r,"SPAN",{});var Id=r(da);xn=l(Id,"3. Speaking style"),Id.forEach(t),$r.forEach(t),eo=l(e,`

The speaking style falls into one of two categories:
* Narrated: read from a script
* Spontaneous: un-scripted, conversational speech
`),pt=o(e,"P",{});var Hd=r(pt);Tn=l(Hd,"The audio and text data reflect the style of speaking. Since narrated text is scripted, it tends to be spoken articulately and without any errors:"),Hd.forEach(t),to=c(e),dt=o(e,"P",{});var Ld=r(dt);$n=l(Ld,"\u201CConsider the task of training a model on a speech recognition dataset\u201D"),Ld.forEach(t),ao=c(e),ct=o(e,"P",{});var Rd=r(ct);jn=l(Rd,"Whereas for spontaneous speech, we can expect a more colloquial style of speech, with the inclusion of repetitions, hesitations and false-starts:"),Rd.forEach(t),so=c(e),ht=o(e,"P",{});var Vd=r(ht);Sn=l(Vd,"\u201CLet\u2019s <uhm> take a look at how <silence> you\u2019d you\u2019d go about training a model on <uhm> a sp- speech recognition dataset\u201D (this is an extreme example)"),Vd.forEach(t),oo=c(e),J=o(e,"H4",{class:!0});var jr=r(J);le=o(jr,"A",{id:!0,class:!0,href:!0});var Od=r(le);ca=o(Od,"SPAN",{});var Fd=r(ca);f(Pe.$$.fragment,Fd),Fd.forEach(t),Od.forEach(t),An=c(jr),ha=o(jr,"SPAN",{});var Md=r(ha);Pn=l(Md,"4. Transcription style"),Md.forEach(t),jr.forEach(t),ro=l(e,`

Whether the text has punctuation, casing or both. If we want a system to generate fully formatted text that could be used for a publication or meeting transcription, we require training data with punctuation and casing. If we just require the spoken words in an unformatted structure, neither punctuation nor casing are necessary.
`),ut=o(e,"P",{});var Cd=r(ut);qn=l(Cd,"A summary of datasets on the HF Hub:"),Cd.forEach(t),no=c(e),pe=o(e,"TABLE",{});var Sr=r(pe);ua=o(Sr,"THEAD",{});var zd=r(ua);g=o(zd,"TR",{});var A=r(g);io=o(A,"TH",{}),r(io).forEach(t),Dn=c(A),fa=o(A,"TH",{});var Bd=r(fa);Wn=l(Bd,"Train Hours"),Bd.forEach(t),Nn=c(A),ma=o(A,"TH",{});var Gd=r(ma);In=l(Gd,"Domain"),Gd.forEach(t),Hn=c(A),va=o(A,"TH",{});var Ud=r(va);Ln=l(Ud,"Speaking Style"),Ud.forEach(t),Rn=c(A),wa=o(A,"TH",{});var Yd=r(wa);Vn=l(Yd,"Transcription Casing"),Yd.forEach(t),On=c(A),_a=o(A,"TH",{});var Xd=r(_a);Fn=l(Xd,"Transcription Punctuation"),Xd.forEach(t),Mn=c(A),ga=o(A,"TH",{});var Jd=r(ga);Cn=l(Jd,"Recommended Usage"),Jd.forEach(t),A.forEach(t),zd.forEach(t),zn=c(Sr),S=o(Sr,"TBODY",{});var R=r(S);b=o(R,"TR",{});var P=r(b);ba=o(P,"TD",{});var Kd=r(ba);Bn=l(Kd,"LibriSpeech"),Kd.forEach(t),Gn=c(P),ya=o(P,"TD",{});var Qd=r(ya);Un=l(Qd,"960"),Qd.forEach(t),Yn=c(P),ka=o(P,"TD",{});var Zd=r(ka);Xn=l(Zd,"Audiobooks"),Zd.forEach(t),Jn=c(P),Ea=o(P,"TD",{});var ec=r(Ea);Kn=l(ec,"Narrated"),ec.forEach(t),Qn=c(P),xa=o(P,"TD",{});var tc=r(xa);Zn=l(tc,"\u274C"),tc.forEach(t),ei=c(P),Ta=o(P,"TD",{});var ac=r(Ta);ti=l(ac,"\u274C"),ac.forEach(t),ai=c(P),$a=o(P,"TD",{});var sc=r($a);si=l(sc,"Academic benchmarks"),sc.forEach(t),P.forEach(t),oi=c(R),y=o(R,"TR",{});var q=r(y);ja=o(q,"TD",{});var oc=r(ja);ri=l(oc,"Common Voice 9"),oc.forEach(t),ni=c(q),Sa=o(q,"TD",{});var rc=r(Sa);ii=l(rc,"2224"),rc.forEach(t),li=c(q),Aa=o(q,"TD",{});var nc=r(Aa);pi=l(nc,"Wikipedia text + crowd-sourced speech"),nc.forEach(t),di=c(q),Pa=o(q,"TD",{});var ic=r(Pa);ci=l(ic,"Narrated"),ic.forEach(t),hi=c(q),qa=o(q,"TD",{});var lc=r(qa);ui=l(lc,"\u2705"),lc.forEach(t),fi=c(q),Da=o(q,"TD",{});var pc=r(Da);mi=l(pc,"\u2705"),pc.forEach(t),vi=c(q),Wa=o(q,"TD",{});var dc=r(Wa);wi=l(dc,"Day-to-day speech"),dc.forEach(t),q.forEach(t),_i=c(R),k=o(R,"TR",{});var D=r(k);Na=o(D,"TD",{});var cc=r(Na);gi=l(cc,"TEDLIUM"),cc.forEach(t),bi=c(D),Ia=o(D,"TD",{});var hc=r(Ia);yi=l(hc,"452"),hc.forEach(t),ki=c(D),Ha=o(D,"TD",{});var uc=r(Ha);Ei=l(uc,"TED talks"),uc.forEach(t),xi=c(D),La=o(D,"TD",{});var fc=r(La);Ti=l(fc,"Narrated"),fc.forEach(t),$i=c(D),Ra=o(D,"TD",{});var mc=r(Ra);ji=l(mc,"\u274C"),mc.forEach(t),Si=c(D),Va=o(D,"TD",{});var vc=r(Va);Ai=l(vc,"\u274C"),vc.forEach(t),Pi=c(D),Oa=o(D,"TD",{});var wc=r(Oa);qi=l(wc,"Technical scientific, political and social topics"),wc.forEach(t),D.forEach(t),Di=c(R),E=o(R,"TR",{});var W=r(E);Fa=o(W,"TD",{});var _c=r(Fa);Wi=l(_c,"Voxpopuli"),_c.forEach(t),Ni=c(W),Ma=o(W,"TD",{});var gc=r(Ma);Ii=l(gc,"543"),gc.forEach(t),Hi=c(W),Ca=o(W,"TD",{});var bc=r(Ca);Li=l(bc,"European Parliament recordings"),bc.forEach(t),Ri=c(W),za=o(W,"TD",{});var yc=r(za);Vi=l(yc,"Spontaneous"),yc.forEach(t),Oi=c(W),Ba=o(W,"TD",{});var kc=r(Ba);Fi=l(kc,"\u274C"),kc.forEach(t),Mi=c(W),Ga=o(W,"TD",{});var Ec=r(Ga);Ci=l(Ec,"\u274C"),Ec.forEach(t),zi=c(W),Ua=o(W,"TD",{});var xc=r(Ua);Bi=l(xc,"Non-native English speakers"),xc.forEach(t),W.forEach(t),Gi=c(R),x=o(R,"TR",{});var N=r(x);Ya=o(N,"TD",{});var Tc=r(Ya);Ui=l(Tc,"GigaSpeech"),Tc.forEach(t),Yi=c(N),Xa=o(N,"TD",{});var $c=r(Xa);Xi=l($c,"10000"),$c.forEach(t),Ji=c(N),Ja=o(N,"TD",{});var jc=r(Ja);Ki=l(jc,"Audiobook, podcast, youtube"),jc.forEach(t),Qi=c(N),Ka=o(N,"TD",{});var Sc=r(Ka);Zi=l(Sc,"Narrated & spontaneous"),Sc.forEach(t),el=c(N),Qa=o(N,"TD",{});var Ac=r(Qa);tl=l(Ac,"\u274C"),Ac.forEach(t),al=c(N),Za=o(N,"TD",{});var Pc=r(Za);sl=l(Pc,"\u2705"),Pc.forEach(t),ol=c(N),es=o(N,"TD",{});var qc=r(es);rl=l(qc,"Robustness over multiple domains"),qc.forEach(t),N.forEach(t),nl=c(R),T=o(R,"TR",{});var I=r(T);ts=o(I,"TD",{});var Dc=r(ts);il=l(Dc,"SPGISpeech"),Dc.forEach(t),ll=c(I),as=o(I,"TD",{});var Wc=r(as);pl=l(Wc,"5000"),Wc.forEach(t),dl=c(I),ss=o(I,"TD",{});var Nc=r(ss);cl=l(Nc,"Financial meetings"),Nc.forEach(t),hl=c(I),os=o(I,"TD",{});var Ic=r(os);ul=l(Ic,"Narrated & spontaneous"),Ic.forEach(t),fl=c(I),rs=o(I,"TD",{});var Hc=r(rs);ml=l(Hc,"\u2705"),Hc.forEach(t),vl=c(I),ns=o(I,"TD",{});var Lc=r(ns);wl=l(Lc,"\u2705"),Lc.forEach(t),_l=c(I),is=o(I,"TD",{});var Rc=r(is);gl=l(Rc,"Fully formatted transcriptions"),Rc.forEach(t),I.forEach(t),R.forEach(t),Sr.forEach(t),lo=c(e),ft=o(e,"P",{});var Vc=r(ft);bl=l(Vc,"Let\u2019s take an example where we want to train a speech recognition system to transcribe speech on the topic of machine learning (highly relevant!). Our speech is going to be narrated (scripted), and we\u2019re not worried about punctuation or casing on our text output. From our reference table, it looks like TED-LIUM is a good choice of dataset!"),Vc.forEach(t),po=c(e),mt=o(e,"P",{});var Oc=r(mt);yl=l(Oc,"[//]: # On review, this sub-section might be too involved. Happy to omit it and dive straight into how to load a dataset."),Oc.forEach(t),co=c(e),K=o(e,"H3",{class:!0});var Ar=r(K);de=o(Ar,"A",{id:!0,class:!0,href:!0});var Fc=r(de);ls=o(Fc,"SPAN",{});var Mc=r(ls);f(qe.$$.fragment,Mc),Mc.forEach(t),Fc.forEach(t),kl=c(Ar),ps=o(Ar,"SPAN",{});var Cc=r(ps);El=l(Cc,"How to load an audio dataset"),Cc.forEach(t),Ar.forEach(t),ho=l(e,`

Before we can handle any audio datasets, we need to make sure we have the right dependencies installed. Specifically, we'll need the "Audio" feature from datasets: https://huggingface.co/docs/datasets/installation#audio. This should take care of the main Python packages we need to read audio files from byte form and convert them to arrays.
`),ce=o(e,"P",{});var Pr=r(ce);xl=l(Pr,"Right! Now we\u2019re ready to go ahead and download our data. For this tutorial, we\u2019ll use the "),De=o(Pr,"A",{href:!0,rel:!0});var zc=r(De);ds=o(zc,"EM",{});var Bc=r(ds);Tl=l(Bc,"LibriSpeech ASR"),Bc.forEach(t),zc.forEach(t),$l=l(Pr," corpus. LibriSpeech is one of the most popular datasets for benchmarking speech recognition systems in both academia and industry. It consists of approximately 1000 hours of narrated audiobooks collected from the (LibriVox)[https://librivox.org/] project."),Pr.forEach(t),uo=c(e),he=o(e,"P",{});var qr=r(he);jl=l(qr,"Let\u2019s take a look at the all-time leaderboard on Papers with Code to get a feel for numbers: "),We=o(qr,"A",{href:!0,rel:!0});var Gc=r(We);Sl=l(Gc,"https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean"),Gc.forEach(t),Al=l(qr,`
We can see that there\u2019s been tremendous progress on the LibriSpeech benchmark in the last three years, with many of the top spots being occupied by Wav2Vec2 or Wav2Vec2-like systems! As mentioned in Section 4, Wav2Vec2 revolutionised the field of speech recognition by introducing an effective pre-training regime, much the same way as BERT revolutionised NLP. We can see that the word error rates (WERs) for this dataset are extremely low! We\u2019ll cover WER in more detail in this section, but for now we can see that the best performing system achieves a WER of just 1.4%, or a word accuracy rate of 100 - 1.4 = 98.6%! This is very impressive! We\u2019ll see how close we can get with our system \u{1F917}`),qr.forEach(t),fo=c(e),ue=o(e,"P",{});var Dr=r(ue);Pl=l(Dr,"Now that we\u2019ve got better idea about the dataset that we\u2019re working with, let\u2019s download it using the "),cs=o(Dr,"CODE",{});var Uc=r(cs);ql=l(Uc,"load_dataset()"),Uc.forEach(t),Dl=l(Dr," function:"),Dr.forEach(t),mo=c(e),f(Ne.$$.fragment,e),vo=c(e),vt=o(e,"P",{});var Yc=r(vt);Wl=l(Yc,"Note that audio datasets are quite large! For 100h of training data and 10h of validation/test data, we can expect to download ~60GB of raw data."),Yc.forEach(t),wo=c(e),wt=o(e,"P",{});var Xc=r(wt);Nl=l(Xc,"Let\u2019s inspect the dataset:"),Xc.forEach(t),_o=c(e),f(Ie.$$.fragment,e),go=c(e),f(He.$$.fragment,e),bo=c(e),F=o(e,"P",{});var Gt=r(F);Il=l(Gt,"Great! We have our training, validation and test splits ready, with 28539, 2703 and 2620 samples respectively. The two data columns "),hs=o(Gt,"CODE",{});var Jc=r(hs);Hl=l(Jc,"audio"),Jc.forEach(t),Ll=l(Gt," and "),us=o(Gt,"CODE",{});var Kc=r(us);Rl=l(Kc,"text"),Kc.forEach(t),Vl=l(Gt," contain the most important information for our task: the raw audio inputs and the target text outputs."),Gt.forEach(t),yo=c(e),_t=o(e,"P",{});var Qc=r(_t);Ol=l(Qc,"Let\u2019s take a look at the first example of the train split:"),Qc.forEach(t),ko=c(e),f(Le.$$.fragment,e),Eo=c(e),f(Re.$$.fragment,e),xo=c(e),gt=o(e,"P",{});var Zc=r(gt);Fl=l(Zc,"Alright! We can see that we have our target text sample ready - it looks very much like it\u2019s taken from an audiobook! But what on earth is going on with the audio? \u{1F914}"),Zc.forEach(t),To=c(e),bt=o(e,"P",{});var eh=r(bt);Ml=l(eh,"As it turns out, we represent audio data digitally as a 1-dimensional array. Each array value denotes the amplitude of our audio signal at a particular time step. From the amplitude information, we can reconstruct the frequency spectrum of the audio and recover all the acoustic features."),eh.forEach(t),$o=c(e),fe=o(e,"P",{});var Wr=r(fe);Cl=l(Wr,"Since our audio input is continuous, we can\u2019t represent an amplitude value for every possible time step. Instead, we "),fs=o(Wr,"EM",{});var th=r(fs);zl=l(th,"sample"),th.forEach(t),Bl=l(Wr," amplitude values at fixed time steps."),Wr.forEach(t),jo=c(e),me=o(e,"P",{});var Nr=r(me);Gl=l(Nr,"The interval with which we sample our audio is known as the "),ms=o(Nr,"EM",{});var ah=r(ms);Ul=l(ah,"sampling rate"),ah.forEach(t),Yl=l(Nr,". For our dataset, we can see that the sampling rate is 16000, meaning 16000 amplitude values are provided each second. Keep the sampling rate in the back of your mind, it\u2019ll be important when we come to processing our data later on!"),Nr.forEach(t),So=c(e),ve=o(e,"P",{});var Ir=r(ve);Xl=l(Ir,"Whilst 1-dimensional arrays are a suitable format for machines, they\u2019re not much use to us as humans! We want to be able to "),vs=o(Ir,"STRONG",{});var sh=r(vs);Jl=l(sh,"listen"),sh.forEach(t),Kl=l(Ir," to our audio to get a feel for the speech and recording conditions. Here, we have two options:"),Ir.forEach(t),Ao=c(e),we=o(e,"OL",{});var Hr=r(we);ws=o(Hr,"LI",{});var oh=r(ws);Ql=l(oh,"Convert our 1-dimensional array into a human-friendly format (mp3 or wav)."),oh.forEach(t),Zl=c(Hr),_s=o(Hr,"LI",{});var rh=r(_s);ep=l(rh,"Look to the Hugging Face Hub!"),rh.forEach(t),Hr.forEach(t),Po=c(e),Ve=o(e,"P",{});var ad=r(Ve);tp=l(ad,"Option 2 seems much easier to me! Let\u2019s head over to the LibriSpeech ASR dataset card on the Hugging Face Hub: "),Oe=o(ad,"A",{href:!0,rel:!0});var nh=r(Oe);ap=l(nh,"https://huggingface.co/datasets/librispeech_asr"),nh.forEach(t),ad.forEach(t),qo=c(e),yt=o(e,"P",{});var ih=r(yt);sp=l(ih,"Slap bang in the middle of the dataset card we have exactly what we\u2019re looking for: the dataset viewer! The dataset viewer shows us the first 100 samples of any dataset. What\u2019s more, it\u2019s loaded up the audio samples ready for us to listen to in real-time! If we hit the play button on the first sample, we can listen to the audio and see the corresponding text. Have a scroll through the samples for the train and test sets to get a better feel for the audio data that we\u2019re dealing with. You\u2019ll notice how clear the audio is and how well spoken the sentences are. This provides us with a clue as to why the top models do so well on LibriSpeech! The audio conditions are very conducive to high system performance."),ih.forEach(t),Do=c(e),Q=o(e,"H3",{class:!0});var Lr=r(Q);_e=o(Lr,"A",{id:!0,class:!0,href:!0});var lh=r(_e);gs=o(lh,"SPAN",{});var ph=r(gs);f(Fe.$$.fragment,ph),ph.forEach(t),lh.forEach(t),op=c(Lr),bs=o(Lr,"SPAN",{});var dh=r(bs);rp=l(dh,"Models for speech recognition"),dh.forEach(t),Lr.forEach(t),Wo=c(e),kt=o(e,"P",{});var ch=r(kt);np=l(ch,"We can decompose speech recognition models into two parts:"),ch.forEach(t),No=c(e),Et=o(e,"OL",{});var hh=r(Et);ys=o(hh,"LI",{});var uh=r(ys);ip=l(uh,"Encoder: an acoustic model that maps the raw audio input into a sequence of hidden-states."),uh.forEach(t),hh.forEach(t),Io=c(e),xt=o(e,"P",{});var fh=r(xt);lp=l(fh,"diagram"),fh.forEach(t),Ho=c(e),Me=o(e,"OL",{start:!0});var mh=r(Me);ks=o(mh,"LI",{});var vh=r(ks);pp=l(vh,"Decoder: maps the sequence of hidden-states to logits over the vocabulary."),vh.forEach(t),mh.forEach(t),Lo=c(e),Tt=o(e,"P",{});var wh=r(Tt);dp=l(wh,"diagram"),wh.forEach(t),Ro=c(e),$t=o(e,"P",{});var _h=r($t);cp=l(_h,"The encoder is typically of Wav2Vec2 architecture, introduced in Section 4 of this Chapter. We have some flexibility in our choice of decoder. We could either use a simple linear layer that maps the Wav2Vec2 hidden states directly to output logits over our vocabulary. Or, we could pair our Wav2Vec2 encoder with a decoder model, giving a speech encoder-decoder style model, analogous to the NLP encoder-decoder model introduced in Chapter 1."),_h.forEach(t),Vo=c(e),jt=o(e,"P",{});var gh=r(jt);hp=l(gh,`A simple linear layer will give a smaller, faster overall model, but will be more susceptible to spelling and grammatical errors. Adding a decoder model greatly improves the quality of transcriptions, at the cost of a larger, slower model. This is because the decoder model is pre-trained on a large corpus of text, enabling us to leverage its learned text representations.
Since we want our system to be robust to spelling and grammar, let\u2019s go ahead and define a speech encoder-decoder style model.`),gh.forEach(t),Oo=c(e),St=o(e,"P",{});var bh=r(St);up=l(bh,"We\u2019ll pair a pre-trained Wav2Vec2 encoder with a pre-trained BART decoder, yielding a Wav2Vev2-2-BART model:"),bh.forEach(t),Fo=c(e),f(Ce.$$.fragment,e),Mo=c(e),ge=o(e,"P",{});var Rr=r(ge);fp=l(Rr,"The "),Es=o(Rr,"CODE",{});var yh=r(Es);mp=l(yh,"add_adapter"),yh.forEach(t),vp=l(Rr," argument introduces a small convolutional network between the encoder and decoder models. This adapter network helps interface the encoder and decoder by down-sampling the encoder hidden-states to better match the timescale of the decoder. In practice, including this adapter results in superior performance than the encoder and decoder models alone:"),Rr.forEach(t),Co=c(e),At=o(e,"P",{});var kh=r(At);wp=l(kh,"diagram"),kh.forEach(t),zo=c(e),Z=o(e,"H4",{class:!0});var Vr=r(Z);be=o(Vr,"A",{id:!0,class:!0,href:!0});var Eh=r(be);xs=o(Eh,"SPAN",{});var xh=r(xs);f(ze.$$.fragment,xh),xh.forEach(t),Eh.forEach(t),_p=c(Vr),Ts=o(Vr,"SPAN",{});var Th=r(Ts);gp=l(Th,"Preprocessing the data"),Th.forEach(t),Vr.forEach(t),Bo=l(e,`

Great! Now that we've defined our model we can start preparing our data for training. For this, we'll need to define two objects: a feature extractor and a tokenizer.

	`),f(Be.$$.fragment,e),Go=c(e),Pt=o(e,"P",{});var $h=r(Pt);bp=l($h,"We\u2019re already pretty familiar with the tokenizer: it converts the text data into a set of token IDs that can be interpreted by the model. This we\u2019ll use to prepare our target labels. But the feature extractor is new! Simply put, the feature extractor prepares our input audio data."),$h.forEach(t),Uo=c(e),qt=o(e,"P",{});var jh=r(qt);yp=l(jh,"A defining feature of Wav2Vec2 is that it accepts a float array corresponding to the raw waveform of the speech signal as an input. We mentioned that the audio data is represented as a 1-dimensional array, so it\u2019s already in the right format to be read by the model (a set of continuous inputs at discrete time steps)."),jh.forEach(t),Yo=c(e),Dt=o(e,"P",{});var Sh=r(Dt);kp=l(Sh,"So, what exactly does the feature extractor do?"),Sh.forEach(t),Xo=c(e),Wt=o(e,"P",{});var Ah=r(Wt);Ep=l(Ah,"Well, the audio data is in the right format, but we\u2019ve imposed no restrictions on the values it can take. For our model to work optimally, we want to keep all the inputs within the same dynamic range. This is going to make sure we get a similar range of activations and gradients for our samples, helping with stability and convergence during training."),Ah.forEach(t),Jo=c(e),M=o(e,"P",{});var Ut=r(M);xp=l(Ut,"To do this, we "),$s=o(Ut,"EM",{});var Ph=r($s);Tp=l(Ph,"normalise"),Ph.forEach(t),$p=l(Ut," our audio data, by rescaling each sample to zero mean and unit variance, a process called "),js=o(Ut,"EM",{});var qh=r(js);jp=l(qh,"feature scaling"),qh.forEach(t),Sp=l(Ut,". It\u2019s exactly this feature normalisation that our feature extractor performs!"),Ut.forEach(t),Ko=c(e),Nt=o(e,"P",{});var Dh=r(Nt);Ap=l(Dh,"We can take a look at the feature extractor in operation by applying it to our first audio sample. First, let\u2019s compute the mean and standard deviation of our raw audio data:"),Dh.forEach(t),Qo=c(e),f(Ge.$$.fragment,e),Zo=c(e),f(Ue.$$.fragment,e),er=c(e),It=o(e,"P",{});var Wh=r(It);Pp=l(Wh,"We can see that the mean is close to zero already, but the standard deviation is a long way off! This would cause our model problems, as the dynamic range of the audio data would be very small and difficult to separate. Let\u2019s apply the feature extractor and see what the outputs look like:"),Wh.forEach(t),tr=c(e),f(Ye.$$.fragment,e),ar=c(e),f(Xe.$$.fragment,e),sr=c(e),j=o(e,"P",{});var V=r(j);qp=l(V,"Alright! Our feature extractor returns a dictionary of two arrays: "),Ss=o(V,"CODE",{});var Nh=r(Ss);Dp=l(Nh,"input_values"),Nh.forEach(t),Wp=l(V," and "),As=o(V,"CODE",{});var Ih=r(As);Np=l(Ih,"attention_mask"),Ih.forEach(t),Ip=l(V,". The "),Ps=o(V,"CODE",{});var Hh=r(Ps);Hp=l(Hh,"input_values"),Hh.forEach(t),Lp=l(V," are the preprocessed audio inputs that we\u2019d pass to the Wav2Vec2 model. The "),qs=o(V,"CODE",{});var Lh=r(qs);Rp=l(Lh,"attention_mask"),Lh.forEach(t),Vp=l(V," would be used if we processed a "),Ds=o(V,"EM",{});var Rh=r(Ds);Op=l(Rh,"batch"),Rh.forEach(t),Fp=l(V," of audio inputs at once."),V.forEach(t),or=c(e),Ht=o(e,"P",{});var Vh=r(Ht);Mp=l(Vh,"We can see that the mean value is now very much closer to zero, and the variance bang-on one! This is exactly the form we want our audio samples in prior to feeding them to the Wav2Vec2 encoder."),Vh.forEach(t),rr=c(e),Lt=o(e,"P",{});var Oh=r(Lt);Cp=l(Oh,"Note how we\u2019ve passed the sampling rate of our audio data to our feature extractor. This is good practice, as the feature extractor performs a check under-the-hood to make sure the sampling rate of our audio data matches the sampling rate expected by the model."),Oh.forEach(t),nr=c(e),Rt=o(e,"P",{});var Fh=r(Rt);zp=l(Fh,"Let\u2019s combine our feature extractor and tokenizer into one function to jointly preprocess our audio and text data:"),Fh.forEach(t),ir=c(e),f(Je.$$.fragment,e),lr=c(e),Vt=o(e,"P",{});var Mh=r(Vt);Bp=l(Mh,"Note that it is important to align the decoder\u2019s vocabulary with the speech transcriptions of the dataset."),Mh.forEach(t),pr=c(e),Ot=o(e,"P",{});var Ch=r(Ot);Gp=l(Ch,"E.g. Librispeech: only captilised letters in the transcriptions, whereas BART was pretrained mostly on lower-cased text."),Ch.forEach(t),dr=c(e),Ft=o(e,"P",{});var zh=r(Ft);Up=l(zh,"Filter to max len 20s"),zh.forEach(t),cr=c(e),f(Ke.$$.fragment,e),hr=c(e),Mt=o(e,"P",{});var Bh=r(Mt);Yp=l(Bh,"Define the datacollator:"),Bh.forEach(t),ur=c(e),f(Qe.$$.fragment,e),fr=c(e),ee=o(e,"H4",{class:!0});var Or=r(ee);ye=o(Or,"A",{id:!0,class:!0,href:!0});var Gh=r(ye);Ws=o(Gh,"SPAN",{});var Uh=r(Ws);f(Ze.$$.fragment,Uh),Uh.forEach(t),Gh.forEach(t),Xp=c(Or),Ns=o(Or,"SPAN",{});var Yh=r(Ns);Jp=l(Yh,"Metrics for speech recognition"),Yh.forEach(t),Or.forEach(t),mr=c(e),Ct=o(e,"P",{});var Xh=r(Ct);Kp=l(Xh,"WER"),Xh.forEach(t),vr=c(e),zt=o(e,"P",{});var Jh=r(zt);Qp=l(Jh,"WAR"),Jh.forEach(t),wr=c(e),Bt=o(e,"P",{});var Kh=r(Bt);Zp=l(Kh,"CER"),Kh.forEach(t),_r=c(e),te=o(e,"H4",{class:!0});var Fr=r(te);ke=o(Fr,"A",{id:!0,class:!0,href:!0});var Qh=r(ke);Is=o(Qh,"SPAN",{});var Zh=r(Is);f(et.$$.fragment,Zh),Zh.forEach(t),Qh.forEach(t),ed=c(Fr),Hs=o(Fr,"SPAN",{});var eu=r(Hs);td=l(eu,"Fine-tuning a speech recognition system with the Trainer API"),eu.forEach(t),Fr.forEach(t),gr=l(e,`

Oh golly quite a lot to do...`),this.h()},h(){h(C,"name","hf:doc:metadata"),h(C,"content",JSON.stringify(lu)),h(ae,"id","automatically-recognising-speech"),h(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ae,"href","#automatically-recognising-speech"),h(z,"class","relative group"),h(xe,"href","https://images.app.goo.gl/J6SU6RYryXXVT9uj7"),h(xe,"rel","nofollow"),h(Te,"href","https://huggingface.co/facebook/wav2vec2-base-960h"),h(Te,"rel","nofollow"),h(oe,"id","how-to-choose-a-dataset"),h(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(oe,"href","#how-to-choose-a-dataset"),h(G,"class","relative group"),h(re,"id","1-number-of-hours"),h(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(re,"href","#1-number-of-hours"),h(U,"class","relative group"),h(ne,"id","2-domain"),h(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ne,"href","#2-domain"),h(Y,"class","relative group"),h(ie,"id","3-speaking-style"),h(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ie,"href","#3-speaking-style"),h(X,"class","relative group"),h(le,"id","4-transcription-style"),h(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(le,"href","#4-transcription-style"),h(J,"class","relative group"),h(de,"id","how-to-load-an-audio-dataset"),h(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(de,"href","#how-to-load-an-audio-dataset"),h(K,"class","relative group"),h(De,"href","https://huggingface.co/datasets/librispeech_asr"),h(De,"rel","nofollow"),h(We,"href","https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean"),h(We,"rel","nofollow"),h(Oe,"href","https://huggingface.co/datasets/librispeech_asr"),h(Oe,"rel","nofollow"),h(_e,"id","models-for-speech-recognition"),h(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_e,"href","#models-for-speech-recognition"),h(Q,"class","relative group"),h(Me,"start","2"),h(be,"id","preprocessing-the-data"),h(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(be,"href","#preprocessing-the-data"),h(Z,"class","relative group"),h(ye,"id","metrics-for-speech-recognition"),h(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ye,"href","#metrics-for-speech-recognition"),h(ee,"class","relative group"),h(ke,"id","finetuning-a-speech-recognition-system-with-the-trainer-api"),h(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ke,"href","#finetuning-a-speech-recognition-system-with-the-trainer-api"),h(te,"class","relative group")},m(e,n){a(document.head,C),p(e,Rs,n),p(e,z,n),a(z,ae),a(ae,Yt),m(Ee,Yt,null),a(z,Mr),a(z,Xt),a(Xt,Cr),p(e,Vs,n),p(e,se,n),a(se,zr),a(se,Jt),a(Jt,Br),a(se,Gr),p(e,Os,n),p(e,tt,n),a(tt,Ur),p(e,Fs,n),p(e,at,n),a(at,Yr),p(e,Ms,n),p(e,st,n),a(st,Xr),p(e,Cs,n),p(e,B,n),a(B,Jr),a(B,xe),a(xe,Kr),a(B,Qr),a(B,Te),a(Te,Zr),p(e,zs,n),p(e,ot,n),a(ot,en),p(e,Bs,n),p(e,rt,n),a(rt,tn),p(e,Gs,n),p(e,nt,n),a(nt,an),p(e,Us,n),p(e,$,n),a($,Kt),a(Kt,sn),a($,on),a($,Qt),a(Qt,rn),a($,nn),a($,Zt),a(Zt,ln),a($,pn),a($,ea),a(ea,dn),a($,cn),a($,ta),a(ta,hn),a($,un),a($,aa),a(aa,fn),p(e,Ys,n),p(e,G,n),a(G,oe),a(oe,sa),m($e,sa,null),a(G,mn),a(G,oa),a(oa,vn),p(e,Xs,n),p(e,it,n),a(it,wn),p(e,Js,n),p(e,U,n),a(U,re),a(re,ra),m(je,ra,null),a(U,_n),a(U,na),a(na,gn),p(e,Ks,n),p(e,Y,n),a(Y,ne),a(ne,ia),m(Se,ia,null),a(Y,bn),a(Y,la),a(la,yn),p(e,Qs,n),p(e,lt,n),a(lt,kn),p(e,Zs,n),p(e,X,n),a(X,ie),a(ie,pa),m(Ae,pa,null),a(X,En),a(X,da),a(da,xn),p(e,eo,n),p(e,pt,n),a(pt,Tn),p(e,to,n),p(e,dt,n),a(dt,$n),p(e,ao,n),p(e,ct,n),a(ct,jn),p(e,so,n),p(e,ht,n),a(ht,Sn),p(e,oo,n),p(e,J,n),a(J,le),a(le,ca),m(Pe,ca,null),a(J,An),a(J,ha),a(ha,Pn),p(e,ro,n),p(e,ut,n),a(ut,qn),p(e,no,n),p(e,pe,n),a(pe,ua),a(ua,g),a(g,io),a(g,Dn),a(g,fa),a(fa,Wn),a(g,Nn),a(g,ma),a(ma,In),a(g,Hn),a(g,va),a(va,Ln),a(g,Rn),a(g,wa),a(wa,Vn),a(g,On),a(g,_a),a(_a,Fn),a(g,Mn),a(g,ga),a(ga,Cn),a(pe,zn),a(pe,S),a(S,b),a(b,ba),a(ba,Bn),a(b,Gn),a(b,ya),a(ya,Un),a(b,Yn),a(b,ka),a(ka,Xn),a(b,Jn),a(b,Ea),a(Ea,Kn),a(b,Qn),a(b,xa),a(xa,Zn),a(b,ei),a(b,Ta),a(Ta,ti),a(b,ai),a(b,$a),a($a,si),a(S,oi),a(S,y),a(y,ja),a(ja,ri),a(y,ni),a(y,Sa),a(Sa,ii),a(y,li),a(y,Aa),a(Aa,pi),a(y,di),a(y,Pa),a(Pa,ci),a(y,hi),a(y,qa),a(qa,ui),a(y,fi),a(y,Da),a(Da,mi),a(y,vi),a(y,Wa),a(Wa,wi),a(S,_i),a(S,k),a(k,Na),a(Na,gi),a(k,bi),a(k,Ia),a(Ia,yi),a(k,ki),a(k,Ha),a(Ha,Ei),a(k,xi),a(k,La),a(La,Ti),a(k,$i),a(k,Ra),a(Ra,ji),a(k,Si),a(k,Va),a(Va,Ai),a(k,Pi),a(k,Oa),a(Oa,qi),a(S,Di),a(S,E),a(E,Fa),a(Fa,Wi),a(E,Ni),a(E,Ma),a(Ma,Ii),a(E,Hi),a(E,Ca),a(Ca,Li),a(E,Ri),a(E,za),a(za,Vi),a(E,Oi),a(E,Ba),a(Ba,Fi),a(E,Mi),a(E,Ga),a(Ga,Ci),a(E,zi),a(E,Ua),a(Ua,Bi),a(S,Gi),a(S,x),a(x,Ya),a(Ya,Ui),a(x,Yi),a(x,Xa),a(Xa,Xi),a(x,Ji),a(x,Ja),a(Ja,Ki),a(x,Qi),a(x,Ka),a(Ka,Zi),a(x,el),a(x,Qa),a(Qa,tl),a(x,al),a(x,Za),a(Za,sl),a(x,ol),a(x,es),a(es,rl),a(S,nl),a(S,T),a(T,ts),a(ts,il),a(T,ll),a(T,as),a(as,pl),a(T,dl),a(T,ss),a(ss,cl),a(T,hl),a(T,os),a(os,ul),a(T,fl),a(T,rs),a(rs,ml),a(T,vl),a(T,ns),a(ns,wl),a(T,_l),a(T,is),a(is,gl),p(e,lo,n),p(e,ft,n),a(ft,bl),p(e,po,n),p(e,mt,n),a(mt,yl),p(e,co,n),p(e,K,n),a(K,de),a(de,ls),m(qe,ls,null),a(K,kl),a(K,ps),a(ps,El),p(e,ho,n),p(e,ce,n),a(ce,xl),a(ce,De),a(De,ds),a(ds,Tl),a(ce,$l),p(e,uo,n),p(e,he,n),a(he,jl),a(he,We),a(We,Sl),a(he,Al),p(e,fo,n),p(e,ue,n),a(ue,Pl),a(ue,cs),a(cs,ql),a(ue,Dl),p(e,mo,n),m(Ne,e,n),p(e,vo,n),p(e,vt,n),a(vt,Wl),p(e,wo,n),p(e,wt,n),a(wt,Nl),p(e,_o,n),m(Ie,e,n),p(e,go,n),m(He,e,n),p(e,bo,n),p(e,F,n),a(F,Il),a(F,hs),a(hs,Hl),a(F,Ll),a(F,us),a(us,Rl),a(F,Vl),p(e,yo,n),p(e,_t,n),a(_t,Ol),p(e,ko,n),m(Le,e,n),p(e,Eo,n),m(Re,e,n),p(e,xo,n),p(e,gt,n),a(gt,Fl),p(e,To,n),p(e,bt,n),a(bt,Ml),p(e,$o,n),p(e,fe,n),a(fe,Cl),a(fe,fs),a(fs,zl),a(fe,Bl),p(e,jo,n),p(e,me,n),a(me,Gl),a(me,ms),a(ms,Ul),a(me,Yl),p(e,So,n),p(e,ve,n),a(ve,Xl),a(ve,vs),a(vs,Jl),a(ve,Kl),p(e,Ao,n),p(e,we,n),a(we,ws),a(ws,Ql),a(we,Zl),a(we,_s),a(_s,ep),p(e,Po,n),p(e,Ve,n),a(Ve,tp),a(Ve,Oe),a(Oe,ap),p(e,qo,n),p(e,yt,n),a(yt,sp),p(e,Do,n),p(e,Q,n),a(Q,_e),a(_e,gs),m(Fe,gs,null),a(Q,op),a(Q,bs),a(bs,rp),p(e,Wo,n),p(e,kt,n),a(kt,np),p(e,No,n),p(e,Et,n),a(Et,ys),a(ys,ip),p(e,Io,n),p(e,xt,n),a(xt,lp),p(e,Ho,n),p(e,Me,n),a(Me,ks),a(ks,pp),p(e,Lo,n),p(e,Tt,n),a(Tt,dp),p(e,Ro,n),p(e,$t,n),a($t,cp),p(e,Vo,n),p(e,jt,n),a(jt,hp),p(e,Oo,n),p(e,St,n),a(St,up),p(e,Fo,n),m(Ce,e,n),p(e,Mo,n),p(e,ge,n),a(ge,fp),a(ge,Es),a(Es,mp),a(ge,vp),p(e,Co,n),p(e,At,n),a(At,wp),p(e,zo,n),p(e,Z,n),a(Z,be),a(be,xs),m(ze,xs,null),a(Z,_p),a(Z,Ts),a(Ts,gp),p(e,Bo,n),m(Be,e,n),p(e,Go,n),p(e,Pt,n),a(Pt,bp),p(e,Uo,n),p(e,qt,n),a(qt,yp),p(e,Yo,n),p(e,Dt,n),a(Dt,kp),p(e,Xo,n),p(e,Wt,n),a(Wt,Ep),p(e,Jo,n),p(e,M,n),a(M,xp),a(M,$s),a($s,Tp),a(M,$p),a(M,js),a(js,jp),a(M,Sp),p(e,Ko,n),p(e,Nt,n),a(Nt,Ap),p(e,Qo,n),m(Ge,e,n),p(e,Zo,n),m(Ue,e,n),p(e,er,n),p(e,It,n),a(It,Pp),p(e,tr,n),m(Ye,e,n),p(e,ar,n),m(Xe,e,n),p(e,sr,n),p(e,j,n),a(j,qp),a(j,Ss),a(Ss,Dp),a(j,Wp),a(j,As),a(As,Np),a(j,Ip),a(j,Ps),a(Ps,Hp),a(j,Lp),a(j,qs),a(qs,Rp),a(j,Vp),a(j,Ds),a(Ds,Op),a(j,Fp),p(e,or,n),p(e,Ht,n),a(Ht,Mp),p(e,rr,n),p(e,Lt,n),a(Lt,Cp),p(e,nr,n),p(e,Rt,n),a(Rt,zp),p(e,ir,n),m(Je,e,n),p(e,lr,n),p(e,Vt,n),a(Vt,Bp),p(e,pr,n),p(e,Ot,n),a(Ot,Gp),p(e,dr,n),p(e,Ft,n),a(Ft,Up),p(e,cr,n),m(Ke,e,n),p(e,hr,n),p(e,Mt,n),a(Mt,Yp),p(e,ur,n),m(Qe,e,n),p(e,fr,n),p(e,ee,n),a(ee,ye),a(ye,Ws),m(Ze,Ws,null),a(ee,Xp),a(ee,Ns),a(Ns,Jp),p(e,mr,n),p(e,Ct,n),a(Ct,Kp),p(e,vr,n),p(e,zt,n),a(zt,Qp),p(e,wr,n),p(e,Bt,n),a(Bt,Zp),p(e,_r,n),p(e,te,n),a(te,ke),a(ke,Is),m(et,Is,null),a(te,ed),a(te,Hs),a(Hs,td),p(e,gr,n),br=!0},p:ru,i(e){br||(v(Ee.$$.fragment,e),v($e.$$.fragment,e),v(je.$$.fragment,e),v(Se.$$.fragment,e),v(Ae.$$.fragment,e),v(Pe.$$.fragment,e),v(qe.$$.fragment,e),v(Ne.$$.fragment,e),v(Ie.$$.fragment,e),v(He.$$.fragment,e),v(Le.$$.fragment,e),v(Re.$$.fragment,e),v(Fe.$$.fragment,e),v(Ce.$$.fragment,e),v(ze.$$.fragment,e),v(Be.$$.fragment,e),v(Ge.$$.fragment,e),v(Ue.$$.fragment,e),v(Ye.$$.fragment,e),v(Xe.$$.fragment,e),v(Je.$$.fragment,e),v(Ke.$$.fragment,e),v(Qe.$$.fragment,e),v(Ze.$$.fragment,e),v(et.$$.fragment,e),br=!0)},o(e){w(Ee.$$.fragment,e),w($e.$$.fragment,e),w(je.$$.fragment,e),w(Se.$$.fragment,e),w(Ae.$$.fragment,e),w(Pe.$$.fragment,e),w(qe.$$.fragment,e),w(Ne.$$.fragment,e),w(Ie.$$.fragment,e),w(He.$$.fragment,e),w(Le.$$.fragment,e),w(Re.$$.fragment,e),w(Fe.$$.fragment,e),w(Ce.$$.fragment,e),w(ze.$$.fragment,e),w(Be.$$.fragment,e),w(Ge.$$.fragment,e),w(Ue.$$.fragment,e),w(Ye.$$.fragment,e),w(Xe.$$.fragment,e),w(Je.$$.fragment,e),w(Ke.$$.fragment,e),w(Qe.$$.fragment,e),w(Ze.$$.fragment,e),w(et.$$.fragment,e),br=!1},d(e){t(C),e&&t(Rs),e&&t(z),_(Ee),e&&t(Vs),e&&t(se),e&&t(Os),e&&t(tt),e&&t(Fs),e&&t(at),e&&t(Ms),e&&t(st),e&&t(Cs),e&&t(B),e&&t(zs),e&&t(ot),e&&t(Bs),e&&t(rt),e&&t(Gs),e&&t(nt),e&&t(Us),e&&t($),e&&t(Ys),e&&t(G),_($e),e&&t(Xs),e&&t(it),e&&t(Js),e&&t(U),_(je),e&&t(Ks),e&&t(Y),_(Se),e&&t(Qs),e&&t(lt),e&&t(Zs),e&&t(X),_(Ae),e&&t(eo),e&&t(pt),e&&t(to),e&&t(dt),e&&t(ao),e&&t(ct),e&&t(so),e&&t(ht),e&&t(oo),e&&t(J),_(Pe),e&&t(ro),e&&t(ut),e&&t(no),e&&t(pe),e&&t(lo),e&&t(ft),e&&t(po),e&&t(mt),e&&t(co),e&&t(K),_(qe),e&&t(ho),e&&t(ce),e&&t(uo),e&&t(he),e&&t(fo),e&&t(ue),e&&t(mo),_(Ne,e),e&&t(vo),e&&t(vt),e&&t(wo),e&&t(wt),e&&t(_o),_(Ie,e),e&&t(go),_(He,e),e&&t(bo),e&&t(F),e&&t(yo),e&&t(_t),e&&t(ko),_(Le,e),e&&t(Eo),_(Re,e),e&&t(xo),e&&t(gt),e&&t(To),e&&t(bt),e&&t($o),e&&t(fe),e&&t(jo),e&&t(me),e&&t(So),e&&t(ve),e&&t(Ao),e&&t(we),e&&t(Po),e&&t(Ve),e&&t(qo),e&&t(yt),e&&t(Do),e&&t(Q),_(Fe),e&&t(Wo),e&&t(kt),e&&t(No),e&&t(Et),e&&t(Io),e&&t(xt),e&&t(Ho),e&&t(Me),e&&t(Lo),e&&t(Tt),e&&t(Ro),e&&t($t),e&&t(Vo),e&&t(jt),e&&t(Oo),e&&t(St),e&&t(Fo),_(Ce,e),e&&t(Mo),e&&t(ge),e&&t(Co),e&&t(At),e&&t(zo),e&&t(Z),_(ze),e&&t(Bo),_(Be,e),e&&t(Go),e&&t(Pt),e&&t(Uo),e&&t(qt),e&&t(Yo),e&&t(Dt),e&&t(Xo),e&&t(Wt),e&&t(Jo),e&&t(M),e&&t(Ko),e&&t(Nt),e&&t(Qo),_(Ge,e),e&&t(Zo),_(Ue,e),e&&t(er),e&&t(It),e&&t(tr),_(Ye,e),e&&t(ar),_(Xe,e),e&&t(sr),e&&t(j),e&&t(or),e&&t(Ht),e&&t(rr),e&&t(Lt),e&&t(nr),e&&t(Rt),e&&t(ir),_(Je,e),e&&t(lr),e&&t(Vt),e&&t(pr),e&&t(Ot),e&&t(dr),e&&t(Ft),e&&t(cr),_(Ke,e),e&&t(hr),e&&t(Mt),e&&t(ur),_(Qe,e),e&&t(fr),e&&t(ee),_(Ze),e&&t(mr),e&&t(Ct),e&&t(vr),e&&t(zt),e&&t(wr),e&&t(Bt),e&&t(_r),e&&t(te),_(et),e&&t(gr)}}}const lu={local:"automatically-recognising-speech",sections:[{local:"how-to-choose-a-dataset",sections:[{local:"1-number-of-hours",title:"1. Number of hours"},{local:"2-domain",title:"2. Domain"},{local:"3-speaking-style",title:"3. Speaking style"},{local:"4-transcription-style",title:"4. Transcription style"}],title:"How to choose a dataset"},{local:"how-to-load-an-audio-dataset",title:"How to load an audio dataset"},{local:"models-for-speech-recognition",sections:[{local:"preprocessing-the-data",title:"Preprocessing the data"},{local:"metrics-for-speech-recognition",title:"Metrics for speech recognition"},{local:"finetuning-a-speech-recognition-system-with-the-trainer-api",title:"Fine-tuning a speech recognition system with the Trainer API"}],title:"Models for speech recognition"}],title:"Automatically recognising speech"};function pu(sd){return nu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class uu extends tu{constructor(C){super();au(this,C,pu,iu,su,{})}}export{uu as default,lu as metadata};
