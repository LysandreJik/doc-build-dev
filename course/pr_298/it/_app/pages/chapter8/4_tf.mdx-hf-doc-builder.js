import{S as od,i as rd,s as id,e as r,k as c,w as h,t as n,M as td,c as i,d as e,m,x as b,a as t,h as l,b as u,G as a,g as p,y as f,q as j,o as v,B as g,v as pd}from"../../chunks/vendor-hf-doc-builder.js";import{T as Nl}from"../../chunks/Tip-hf-doc-builder.js";import{Y as cd}from"../../chunks/Youtube-hf-doc-builder.js";import{I as B}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as y}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as md}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as ud}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function dd(M){let d,w,$,z,A,C,S,D,ps,cs,ca,W,H,qs,T,F,X,ms,ma,us,ws,Y,ys;return{c(){d=r("p"),w=n("\u270F\uFE0F "),$=r("strong"),z=n("Prova tu!"),A=n(" Come sfida opzionale, dopo aver risolto gli altri problemi, puoi provare a tornare a questo passaggio e a far funzionare il modello con la loss originale calcolata da Keras invece che con la loss interna. \xC8 necessario aggiungere "),C=r("code"),S=n('"labels"'),D=n(" all\u2019argomento "),ps=r("code"),cs=n("label_cols"),ca=n(" di "),W=r("code"),H=n("to_tf_dataset()"),qs=n(" per assicurarsi che le label siano fornite correttamente, in modo da ottenere i gradienti, ma c\u2019\xE8 un altro problema con la loss che abbiamo specificato. L\u2019addestramento continuer\xE0 a funzionare con questo problema, ma l\u2019apprendimento sar\xE0 molto lento e si bloccher\xE0 a una loss di addestramento elevata. Riesci a capire di cosa si tratta?"),T=c(),F=r("p"),X=n("Un suggerimento codificato in ROT13, se sei bloccato/a: Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf "),ms=r("code"),ma=n("ybtvgf"),us=n(". Jung ner ybtvgf?"),ws=c(),Y=r("p"),ys=n("E un secondo indizio: Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf nyy gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf?")},l(q){d=i(q,"P",{});var k=t(d);w=l(k,"\u270F\uFE0F "),$=i(k,"STRONG",{});var ks=t($);z=l(ks,"Prova tu!"),ks.forEach(e),A=l(k," Come sfida opzionale, dopo aver risolto gli altri problemi, puoi provare a tornare a questo passaggio e a far funzionare il modello con la loss originale calcolata da Keras invece che con la loss interna. \xC8 necessario aggiungere "),C=i(k,"CODE",{});var Z=t(C);S=l(Z,'"labels"'),Z.forEach(e),D=l(k," all\u2019argomento "),ps=i(k,"CODE",{});var ie=t(ps);cs=l(ie,"label_cols"),ie.forEach(e),ca=l(k," di "),W=i(k,"CODE",{});var Cs=t(W);H=l(Cs,"to_tf_dataset()"),Cs.forEach(e),qs=l(k," per assicurarsi che le label siano fornite correttamente, in modo da ottenere i gradienti, ma c\u2019\xE8 un altro problema con la loss che abbiamo specificato. L\u2019addestramento continuer\xE0 a funzionare con questo problema, ma l\u2019apprendimento sar\xE0 molto lento e si bloccher\xE0 a una loss di addestramento elevata. Riesci a capire di cosa si tratta?"),k.forEach(e),T=m(q),F=i(q,"P",{});var Ps=t(F);X=l(Ps,"Un suggerimento codificato in ROT13, se sei bloccato/a: Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf "),ms=i(Ps,"CODE",{});var ua=t(ms);ma=l(ua,"ybtvgf"),ua.forEach(e),us=l(Ps,". Jung ner ybtvgf?"),Ps.forEach(e),ws=m(q),Y=i(q,"P",{});var ss=t(Y);ys=l(ss,"E un secondo indizio: Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf nyy gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf?"),ss.forEach(e)},m(q,k){p(q,d,k),a(d,w),a(d,$),a($,z),a(d,A),a(d,C),a(C,S),a(d,D),a(d,ps),a(ps,cs),a(d,ca),a(d,W),a(W,H),a(d,qs),p(q,T,k),p(q,F,k),a(F,X),a(F,ms),a(ms,ma),a(F,us),p(q,ws,k),p(q,Y,k),a(Y,ys)},d(q){q&&e(d),q&&e(T),q&&e(F),q&&e(ws),q&&e(Y)}}}function hd(M){let d,w,$,z,A;return{c(){d=r("p"),w=n("\u{1F4A1} \xC8 anche possibile importare la funzione "),$=r("code"),z=n("create_optimizer()"),A=n(" da \u{1F917} Transformers, che fornir\xE0 un optimizer AdamW con un corretto weight decay insieme a un learning rate warmup e decay. Questo ottimizzatore spesso produce risultati leggermente migliori di quelli ottenuti con l\u2019ottimizzatore Adam predefinito.")},l(C){d=i(C,"P",{});var S=t(d);w=l(S,"\u{1F4A1} \xC8 anche possibile importare la funzione "),$=i(S,"CODE",{});var D=t($);z=l(D,"create_optimizer()"),D.forEach(e),A=l(S," da \u{1F917} Transformers, che fornir\xE0 un optimizer AdamW con un corretto weight decay insieme a un learning rate warmup e decay. Questo ottimizzatore spesso produce risultati leggermente migliori di quelli ottenuti con l\u2019ottimizzatore Adam predefinito."),S.forEach(e)},m(C,S){p(C,d,S),a(d,w),a(d,$),a($,z),a(d,A)},d(C){C&&e(d)}}}function bd(M){let d,w;return{c(){d=r("p"),w=n("Nella prossima parte del corso, esamineremo tecniche pi\xF9 avanzate che possono aiutare a ridurre l\u2019impatto sulla memoria e ad affinare i modelli pi\xF9 grandi.")},l($){d=i($,"P",{});var z=t(d);w=l(z,"Nella prossima parte del corso, esamineremo tecniche pi\xF9 avanzate che possono aiutare a ridurre l\u2019impatto sulla memoria e ad affinare i modelli pi\xF9 grandi."),z.forEach(e)},m($,z){p($,d,z),a(d,w)},d($){$&&e(d)}}}function fd(M){let d,w;return{c(){d=r("p"),w=n("\u{1F4A1} Se i dati di addestramento sono sbilanciati, assicurati di creare un batch di dati di addestramento contenente tutte le label.")},l($){d=i($,"P",{});var z=t(d);w=l(z,"\u{1F4A1} Se i dati di addestramento sono sbilanciati, assicurati di creare un batch di dati di addestramento contenente tutte le label."),z.forEach(e)},m($,z){p($,d,z),a(d,w)},d($){$&&e(d)}}}function jd(M){let d,w;return{c(){d=r("p"),w=n("\u26A0\uFE0F Sar\xE0 necessario ricreare il modello e ricompilarlo dopo questo test, poich\xE9 il modello ottenuto probabilmente non sar\xE0 in grado di recuperare e imparare qualcosa di utile sul set di dati completo.")},l($){d=i($,"P",{});var z=t(d);w=l(z,"\u26A0\uFE0F Sar\xE0 necessario ricreare il modello e ricompilarlo dopo questo test, poich\xE9 il modello ottenuto probabilmente non sar\xE0 in grado di recuperare e imparare qualcosa di utile sul set di dati completo."),z.forEach(e)},m($,z){p($,d,z),a(d,w)},d($){$&&e(d)}}}function vd(M){let d,w,$,z,A,C,S,D,ps,cs,ca,W,H,qs,T,F,X,ms,ma,us,ws,Y,ys,q,k,ks,Z,ie,Cs,Ps,ua,ss,Il,As,Wr,Ue,Xr,Yr,Ml,Ss,Zr,Ge,si,ai,Fl,Ts,ei,da,ni,li,Ql,ha,Ul,Ds,oi,xe,ri,ii,Gl,te,ti,xl,ba,Ll,pe,pi,Kl,ds,Os,Le,fa,ci,Ke,mi,Bl,ce,ui,Hl,P,di,Be,hi,bi,He,fi,ji,Re,vi,gi,Ve,$i,_i,Je,Ei,zi,We,qi,wi,Rl,ja,Vl,R,Xe,yi,ki,Ye,Ci,Pi,Ze,Ai,Si,Jl,va,Wl,O,Ti,sn,Di,Oi,an,Ni,Ii,en,Mi,Fi,nn,Qi,Ui,Xl,Ns,Gi,ln,xi,Li,Yl,as,Ki,on,Bi,Hi,rn,Ri,Vi,Zl,ga,so,me,Ji,ao,Is,eo,Ms,Wi,tn,Xi,Yi,no,$a,lo,ue,Zi,oo,_a,pn,st,at,ro,hs,Fs,cn,Ea,et,mn,nt,io,V,un,lt,ot,dn,rt,it,hn,tt,pt,to,es,ct,bn,mt,ut,fn,dt,ht,po,za,co,qa,mo,_,bt,jn,ft,jt,vn,vt,gt,gn,$t,_t,$n,Et,zt,_n,qt,wt,En,yt,kt,zn,Ct,Pt,qn,At,St,wn,Tt,Dt,yn,Ot,Nt,kn,It,Mt,Cn,Ft,Qt,Pn,Ut,Gt,An,xt,Lt,Sn,Kt,Bt,Tn,Ht,Rt,uo,Q,Vt,Dn,Jt,Wt,On,Xt,Yt,Nn,Zt,sp,ho,wa,bo,de,ap,fo,ya,jo,J,In,ep,np,Mn,lp,op,Fn,rp,ip,vo,ka,go,Ca,$o,he,tp,_o,Pa,Eo,Aa,zo,be,pp,qo,Sa,wo,Ta,yo,ns,cp,Qn,mp,up,Un,dp,hp,ko,Da,Co,Oa,Po,Qs,bp,Gn,fp,jp,Ao,Na,So,Ia,To,Us,vp,xn,gp,$p,Do,bs,Gs,Ln,Ma,_p,Kn,Ep,Oo,U,zp,Bn,qp,wp,Hn,yp,kp,Rn,Cp,Pp,No,xs,Ap,Fa,Sp,Tp,Io,G,Dp,Vn,Op,Np,Jn,Ip,Mp,Wn,Fp,Qp,Mo,Qa,Fo,Ls,Qo,fe,Up,Uo,Ua,Go,Ga,xo,je,Gp,Lo,fs,Ks,Xn,xa,xp,Yn,Lp,Ko,ve,Kp,Bo,js,Bs,Zn,La,Bp,sl,Hp,Ho,ls,Rp,al,Vp,Jp,el,Wp,Xp,Ro,Hs,Vo,vs,Rs,nl,Ka,Yp,ll,Zp,Jo,os,sc,ol,ac,ec,rl,nc,lc,Wo,ge,oc,Xo,Vs,rc,il,ic,tc,Yo,gs,Js,tl,Ba,pc,pl,cc,Zo,N,mc,cl,uc,dc,ml,hc,bc,ul,fc,jc,dl,vc,gc,sr,Ha,ar,$e,$c,er,Ra,nr,_e,_c,lr,x,hl,Ec,zc,bl,qc,wc,fl,yc,kc,jl,Cc,or,Ws,Pc,vl,Ac,Sc,rr,Ee,Tc,ir,ze,Dc,tr,$s,Xs,gl,Va,Oc,$l,Nc,pr,qe,Ic,cr,rs,Mc,_l,Fc,Qc,El,Uc,Gc,mr,Ja,ur,Ys,dr,Zs,xc,zl,Lc,Kc,hr,we,Bc,br,sa,fr,_s,aa,ql,Wa,Hc,wl,Rc,jr,L,Vc,yl,Jc,Wc,kl,Xc,Yc,Cl,Zc,sm,vr,ye,am,gr,ke,em,$r,Es,ea,Pl,Xa,nm,Al,lm,_r,na,om,Ya,rm,im,Er,Ce,tm,zr,K,Pe,Za,pm,cm,mm,Ae,se,um,dm,hm,Se,ae,bm,fm,jm,Te,ee,vm,gm,qr,De,$m,wr;return $=new ud({props:{fw:M[0]}}),D=new B({}),H=new md({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4_tf.ipynb"}]}}),Z=new B({}),ss=new cd({props:{id:"N9kO52itd0Q"}}),ha=new y({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

validation_dataset = tokenized_datasets["validation_matched"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.fit(train_dataset)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)

train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>
)

validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>, optimizer=<span class="hljs-string">&quot;adam&quot;</span>)

model.fit(train_dataset)`}}),ba=new y({props:{code:"ValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']",highlighted:'ValueError: No gradients provided <span class="hljs-keyword">for</span> <span class="hljs-built_in">any</span> variable: [<span class="hljs-string">&#x27;tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0&#x27;</span>, <span class="hljs-string">&#x27;...&#x27;</span>]'}}),fa=new B({}),ja=new y({props:{code:`for batch in train_dataset:
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataset:
    <span class="hljs-keyword">break</span>`}}),va=new y({props:{code:`{'attention_mask': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        ...,
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0]])>,
 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1])>,
 'input_ids': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[ 101, 2174, 1010, ...,    0,    0,    0],
        [ 101, 3174, 2420, ...,    0,    0,    0],
        [ 101, 2044, 2048, ...,    0,    0,    0],
        ...,
        [ 101, 3398, 3398, ..., 2051, 2894,  102],
        [ 101, 1996, 4124, ...,    0,    0,    0],
        [ 101, 1999, 2070, ...,    0,    0,    0]])>}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">76</span>), dtype=int64, numpy=
 array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        ...,
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])&gt;,
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">76</span>), dtype=int64, numpy=
 array([[ <span class="hljs-number">101</span>, <span class="hljs-number">2174</span>, <span class="hljs-number">1010</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">3174</span>, <span class="hljs-number">2420</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">2044</span>, <span class="hljs-number">2048</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        ...,
        [ <span class="hljs-number">101</span>, <span class="hljs-number">3398</span>, <span class="hljs-number">3398</span>, ..., <span class="hljs-number">2051</span>, <span class="hljs-number">2894</span>,  <span class="hljs-number">102</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">4124</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
        [ <span class="hljs-number">101</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">2070</span>, ...,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>]])&gt;}`}}),ga=new y({props:{code:'model.compile(optimizer="adam")',highlighted:'model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>)'}}),Is=new Nl({props:{$$slots:{default:[dd]},$$scope:{ctx:M}}}),$a=new y({props:{code:"  246/24543 [..............................] - ETA: 15:52 - loss: nan",highlighted:'  <span class="hljs-number">246</span>/<span class="hljs-number">24543</span> [..............................] - ETA: <span class="hljs-number">15</span>:<span class="hljs-number">52</span> - loss: nan'}}),Ea=new B({}),za=new y({props:{code:"model(batch)",highlighted:"model(batch)"}}),qa=new y({props:{code:`TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)>, hidden_states=None, attentions=None)`,highlighted:`TFSequenceClassifierOutput(loss=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)&gt;, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}),wa=new y({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)`}}),ya=new y({props:{code:`TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([0.6844486 ,        nan,        nan, 0.67127866, 0.7068601 ,
              nan, 0.69309855,        nan, 0.65531296,        nan,
              nan,        nan, 0.675402  ,        nan,        nan,
       0.69831556], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[-0.04761693, -0.06509043],
       [-0.0481936 , -0.04556257],
       [-0.0040929 , -0.05848458],
       [-0.02417453, -0.0684005 ],
       [-0.02517801, -0.05241832],
       [-0.04514256, -0.0757378 ],
       [-0.02656011, -0.02646275],
       [ 0.00766164, -0.04350497],
       [ 0.02060014, -0.05655622],
       [-0.02615328, -0.0447021 ],
       [-0.05119278, -0.06928903],
       [-0.02859691, -0.04879177],
       [-0.02210129, -0.05791225],
       [-0.02363213, -0.05962167],
       [-0.05352269, -0.0481673 ],
       [-0.08141848, -0.07110836]], dtype=float32)>, hidden_states=None, attentions=None)`,highlighted:`TFSequenceClassifierOutput(loss=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>,), dtype=float32, numpy=
array([<span class="hljs-number">0.6844486</span> ,        nan,        nan, <span class="hljs-number">0.67127866</span>, <span class="hljs-number">0.7068601</span> ,
              nan, <span class="hljs-number">0.69309855</span>,        nan, <span class="hljs-number">0.65531296</span>,        nan,
              nan,        nan, <span class="hljs-number">0.675402</span>  ,        nan,        nan,
       <span class="hljs-number">0.69831556</span>], dtype=float32)&gt;, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[-<span class="hljs-number">0.04761693</span>, -<span class="hljs-number">0.06509043</span>],
       [-<span class="hljs-number">0.0481936</span> , -<span class="hljs-number">0.04556257</span>],
       [-<span class="hljs-number">0.0040929</span> , -<span class="hljs-number">0.05848458</span>],
       [-<span class="hljs-number">0.02417453</span>, -<span class="hljs-number">0.0684005</span> ],
       [-<span class="hljs-number">0.02517801</span>, -<span class="hljs-number">0.05241832</span>],
       [-<span class="hljs-number">0.04514256</span>, -<span class="hljs-number">0.0757378</span> ],
       [-<span class="hljs-number">0.02656011</span>, -<span class="hljs-number">0.02646275</span>],
       [ <span class="hljs-number">0.00766164</span>, -<span class="hljs-number">0.04350497</span>],
       [ <span class="hljs-number">0.02060014</span>, -<span class="hljs-number">0.05655622</span>],
       [-<span class="hljs-number">0.02615328</span>, -<span class="hljs-number">0.0447021</span> ],
       [-<span class="hljs-number">0.05119278</span>, -<span class="hljs-number">0.06928903</span>],
       [-<span class="hljs-number">0.02859691</span>, -<span class="hljs-number">0.04879177</span>],
       [-<span class="hljs-number">0.02210129</span>, -<span class="hljs-number">0.05791225</span>],
       [-<span class="hljs-number">0.02363213</span>, -<span class="hljs-number">0.05962167</span>],
       [-<span class="hljs-number">0.05352269</span>, -<span class="hljs-number">0.0481673</span> ],
       [-<span class="hljs-number">0.08141848</span>, -<span class="hljs-number">0.07110836</span>]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}),ka=new y({props:{code:`import numpy as np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices`}}),Ca=new y({props:{code:"array([ 1,  2,  5,  7,  9, 10, 11, 13, 14])",highlighted:'array([ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>])'}}),Pa=new y({props:{code:`input_ids = batch["input_ids"].numpy()
input_ids[indices]`,highlighted:`input_ids = batch[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()
input_ids[indices]`}}),Aa=new y({props:{code:`array([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,
        23212,  3070,  2214, 10170,  1010,  2012,  4356,  1997,  3183,
         6838, 12953,  2039,  2000,  1996,  6147,  1997,  2010,  2606,
         1012,   102,  6838,  2001,  3294,  6625,  3773,  1996,  2214,
         2158,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  6814,  2016,  2234,  2461,  2153,  1998, 13322,
         2009,  1012,   102,  2045,  1005,  1055,  2053,  3382,  2008,
         2016,  1005,  2222,  3046,  8103,  2075,  2009,  2153,  1012,
          102,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  2007,  1996,  3712,  4634,  1010,  2057,  8108,
         2025,  3404,  2028,  1012,  1996,  2616, 18449,  2125,  1999,
         1037,  9666,  1997,  4100,  8663, 11020,  6313,  2791,  1998,
         2431,  1011,  4301,  1012,   102,  2028,  1005,  1055,  5177,
         2110,  1998,  3977,  2000,  2832,  2106,  2025,  2689,  2104,
         2122,  6214,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1045,  2001,  1999,  1037, 13090,  5948,  2007,  2048,
         2308,  2006,  2026,  5001,  2043,  2026,  2171,  2001,  2170,
         1012,   102,  1045,  2001,  3564,  1999,  2277,  1012,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2195,  4279,  2191,  2039,  1996,  2181,  2124,  2004,
         1996,  2225,  7363,  1012,   102,  2045,  2003,  2069,  2028,
         2451,  1999,  1996,  2225,  7363,  1012,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2061,  2008,  1045,  2123,  1005,  1056,  2113,  2065,
         2009,  2428, 10654,  7347,  2030,  2009,  7126,  2256,  2495,
         2291,   102,  2009,  2003,  5094,  2256,  2495,  2291,  2035,
         2105,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2051,  1010,  2029,  3216,  2019,  2503,  3444,  1010,
         6732,  1996,  2265,  2038, 19840,  2098,  2125,  9906,  1998,
         2003,  2770,  2041,  1997,  4784,  1012,   102,  2051,  6732,
         1996,  2265,  2003,  9525,  1998,  4569,  1012,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1996, 10556,  2140, 11515,  2058,  1010,  2010,  2162,
         2252,  5689,  2013,  2010,  7223,  1012,   102,  2043,  1996,
        10556,  2140, 11515,  2058,  1010,  2010,  2252,  3062,  2000,
         1996,  2598,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101, 13543,  1999,  2049,  6143,  2933,  2443,   102,  2025,
        13543,  1999,  6143,  2933,  2003,  2443,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0]])`,highlighted:`array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">2032</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">16480</span>,  <span class="hljs-number">3917</span>,  <span class="hljs-number">2594</span>,  <span class="hljs-number">4135</span>,
        <span class="hljs-number">23212</span>,  <span class="hljs-number">3070</span>,  <span class="hljs-number">2214</span>, <span class="hljs-number">10170</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2012</span>,  <span class="hljs-number">4356</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">3183</span>,
         <span class="hljs-number">6838</span>, <span class="hljs-number">12953</span>,  <span class="hljs-number">2039</span>,  <span class="hljs-number">2000</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">6147</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2606</span>,
         <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">6838</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">3294</span>,  <span class="hljs-number">6625</span>,  <span class="hljs-number">3773</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2214</span>,
         <span class="hljs-number">2158</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">6814</span>,  <span class="hljs-number">2016</span>,  <span class="hljs-number">2234</span>,  <span class="hljs-number">2461</span>,  <span class="hljs-number">2153</span>,  <span class="hljs-number">1998</span>, <span class="hljs-number">13322</span>,
         <span class="hljs-number">2009</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1055</span>,  <span class="hljs-number">2053</span>,  <span class="hljs-number">3382</span>,  <span class="hljs-number">2008</span>,
         <span class="hljs-number">2016</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2222</span>,  <span class="hljs-number">3046</span>,  <span class="hljs-number">8103</span>,  <span class="hljs-number">2075</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">2153</span>,  <span class="hljs-number">1012</span>,
          <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">3712</span>,  <span class="hljs-number">4634</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2057</span>,  <span class="hljs-number">8108</span>,
         <span class="hljs-number">2025</span>,  <span class="hljs-number">3404</span>,  <span class="hljs-number">2028</span>,  <span class="hljs-number">1012</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2616</span>, <span class="hljs-number">18449</span>,  <span class="hljs-number">2125</span>,  <span class="hljs-number">1999</span>,
         <span class="hljs-number">1037</span>,  <span class="hljs-number">9666</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">4100</span>,  <span class="hljs-number">8663</span>, <span class="hljs-number">11020</span>,  <span class="hljs-number">6313</span>,  <span class="hljs-number">2791</span>,  <span class="hljs-number">1998</span>,
         <span class="hljs-number">2431</span>,  <span class="hljs-number">1011</span>,  <span class="hljs-number">4301</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2028</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1055</span>,  <span class="hljs-number">5177</span>,
         <span class="hljs-number">2110</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">3977</span>,  <span class="hljs-number">2000</span>,  <span class="hljs-number">2832</span>,  <span class="hljs-number">2106</span>,  <span class="hljs-number">2025</span>,  <span class="hljs-number">2689</span>,  <span class="hljs-number">2104</span>,
         <span class="hljs-number">2122</span>,  <span class="hljs-number">6214</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">13090</span>,  <span class="hljs-number">5948</span>,  <span class="hljs-number">2007</span>,  <span class="hljs-number">2048</span>,
         <span class="hljs-number">2308</span>,  <span class="hljs-number">2006</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">5001</span>,  <span class="hljs-number">2043</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2171</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">2170</span>,
         <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2001</span>,  <span class="hljs-number">3564</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">2277</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2195</span>,  <span class="hljs-number">4279</span>,  <span class="hljs-number">2191</span>,  <span class="hljs-number">2039</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2181</span>,  <span class="hljs-number">2124</span>,  <span class="hljs-number">2004</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2225</span>,  <span class="hljs-number">7363</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2045</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">2069</span>,  <span class="hljs-number">2028</span>,
         <span class="hljs-number">2451</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2225</span>,  <span class="hljs-number">7363</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2008</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">2123</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">1056</span>,  <span class="hljs-number">2113</span>,  <span class="hljs-number">2065</span>,
         <span class="hljs-number">2009</span>,  <span class="hljs-number">2428</span>, <span class="hljs-number">10654</span>,  <span class="hljs-number">7347</span>,  <span class="hljs-number">2030</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">7126</span>,  <span class="hljs-number">2256</span>,  <span class="hljs-number">2495</span>,
         <span class="hljs-number">2291</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2009</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">5094</span>,  <span class="hljs-number">2256</span>,  <span class="hljs-number">2495</span>,  <span class="hljs-number">2291</span>,  <span class="hljs-number">2035</span>,
         <span class="hljs-number">2105</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">2051</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2029</span>,  <span class="hljs-number">3216</span>,  <span class="hljs-number">2019</span>,  <span class="hljs-number">2503</span>,  <span class="hljs-number">3444</span>,  <span class="hljs-number">1010</span>,
         <span class="hljs-number">6732</span>,  <span class="hljs-number">1996</span>,  <span class="hljs-number">2265</span>,  <span class="hljs-number">2038</span>, <span class="hljs-number">19840</span>,  <span class="hljs-number">2098</span>,  <span class="hljs-number">2125</span>,  <span class="hljs-number">9906</span>,  <span class="hljs-number">1998</span>,
         <span class="hljs-number">2003</span>,  <span class="hljs-number">2770</span>,  <span class="hljs-number">2041</span>,  <span class="hljs-number">1997</span>,  <span class="hljs-number">4784</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2051</span>,  <span class="hljs-number">6732</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2265</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">9525</span>,  <span class="hljs-number">1998</span>,  <span class="hljs-number">4569</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">1996</span>, <span class="hljs-number">10556</span>,  <span class="hljs-number">2140</span>, <span class="hljs-number">11515</span>,  <span class="hljs-number">2058</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2162</span>,
         <span class="hljs-number">2252</span>,  <span class="hljs-number">5689</span>,  <span class="hljs-number">2013</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">7223</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2043</span>,  <span class="hljs-number">1996</span>,
        <span class="hljs-number">10556</span>,  <span class="hljs-number">2140</span>, <span class="hljs-number">11515</span>,  <span class="hljs-number">2058</span>,  <span class="hljs-number">1010</span>,  <span class="hljs-number">2010</span>,  <span class="hljs-number">2252</span>,  <span class="hljs-number">3062</span>,  <span class="hljs-number">2000</span>,
         <span class="hljs-number">1996</span>,  <span class="hljs-number">2598</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
       [  <span class="hljs-number">101</span>, <span class="hljs-number">13543</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">2049</span>,  <span class="hljs-number">6143</span>,  <span class="hljs-number">2933</span>,  <span class="hljs-number">2443</span>,   <span class="hljs-number">102</span>,  <span class="hljs-number">2025</span>,
        <span class="hljs-number">13543</span>,  <span class="hljs-number">1999</span>,  <span class="hljs-number">6143</span>,  <span class="hljs-number">2933</span>,  <span class="hljs-number">2003</span>,  <span class="hljs-number">2443</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,
            <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]])`}}),Sa=new y({props:{code:`labels = batch['labels'].numpy()
labels[indices]`,highlighted:`labels = batch[<span class="hljs-string">&#x27;labels&#x27;</span>].numpy()
labels[indices]`}}),Ta=new y({props:{code:"array([2, 2, 2, 2, 2, 2, 2, 2, 2])",highlighted:'array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])'}}),Da=new y({props:{code:"model.config.num_labels",highlighted:"model.config.num_labels"}}),Oa=new y({props:{code:"2",highlighted:'<span class="hljs-number">2</span>'}}),Na=new y({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)
model.compile(optimizer='adam')
model.fit(train_dataset)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, <span class="hljs-attribute">num_labels</span>=3)
model.compile(<span class="hljs-attribute">optimizer</span>=<span class="hljs-string">&#x27;adam&#x27;</span>)
model.fit(train_dataset)`}}),Ia=new y({props:{code:"  869/24543 [>.............................] - ETA: 15:29 - loss: 1.1032",highlighted:'  <span class="hljs-number">869</span>/<span class="hljs-number">24543</span> [&gt;.............................] - ETA: <span class="hljs-number">15</span>:<span class="hljs-number">29</span> - loss: <span class="hljs-number">1.1032</span>'}}),Ma=new B({}),Qa=new y({props:{code:`from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.compile(optimizer=Adam(5e-5))`,highlighted:`<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">5e-5</span>))`}}),Ls=new Nl({props:{$$slots:{default:[hd]},$$scope:{ctx:M}}}),Ua=new y({props:{code:"model.fit(train_dataset)",highlighted:"model.fit(train_dataset)"}}),Ga=new y({props:{code:"319/24543 [..............................] - ETA: 16:07 - loss: 0.9718",highlighted:'<span class="hljs-number">319</span>/<span class="hljs-number">24543</span> [..............................] - ETA: <span class="hljs-number">16</span>:07 - loss: <span class="hljs-number">0.9718</span>'}}),xa=new B({}),La=new B({}),Hs=new Nl({props:{$$slots:{default:[bd]},$$scope:{ctx:M}}}),Ka=new B({}),Ba=new B({}),Ha=new y({props:{code:`input_ids = batch["input_ids"].numpy()
tokenizer.decode(input_ids[0])`,highlighted:`input_ids = batch[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()
tokenizer.decode(input_ids[<span class="hljs-number">0</span>])`}}),Ra=new y({props:{code:`labels = batch["labels"].numpy()
label = labels[0]`,highlighted:`labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].numpy()
label = labels[<span class="hljs-number">0</span>]`}}),Va=new B({}),Ja=new y({props:{code:`for batch in train_dataset:
    break

# Make sure you have run model.compile() and set your optimizer,
# and your loss/metrics if you're using them

model.fit(batch, epochs=20)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataset:
    <span class="hljs-keyword">break</span>

<span class="hljs-comment"># Make sure you have run model.compile() and set your optimizer,</span>
<span class="hljs-comment"># and your loss/metrics if you&#x27;re using them</span>

model.fit(batch, epochs=<span class="hljs-number">20</span>)`}}),Ys=new Nl({props:{$$slots:{default:[fd]},$$scope:{ctx:M}}}),sa=new Nl({props:{warning:!0,$$slots:{default:[jd]},$$scope:{ctx:M}}}),Wa=new B({}),Xa=new B({}),{c(){d=r("meta"),w=c(),h($.$$.fragment),z=c(),A=r("h1"),C=r("a"),S=r("span"),h(D.$$.fragment),ps=c(),cs=r("span"),ca=n("Fare il debug di una training pipeline"),W=c(),h(H.$$.fragment),qs=c(),T=r("p"),F=n("Hai scritto un bello script per addestrare o affinare un modello su un determinato compito, seguendo scrupolosamente i consigli del "),X=r("a"),ms=n("Capitolo 7"),ma=n(". Ma quando lanci il comando "),us=r("code"),ws=n("model.fit()"),Y=n(", succede qualcosa di orribile: si ottiene un errore \u{1F631}! O peggio, tutto sembra andare bene e il training viene eseguito senza errori, ma il modello che ne risulta fa schifo. In questa sezione mostreremo cosa \xE8 possibile fare per eseguire il debug di questo tipo di problemi."),ys=c(),q=r("h2"),k=r("a"),ks=r("span"),h(Z.$$.fragment),ie=c(),Cs=r("span"),Ps=n("Debugging the training pipeline"),ua=c(),h(ss.$$.fragment),Il=c(),As=r("p"),Wr=n("Il problema quando si ha un errore da "),Ue=r("code"),Xr=n("model.fit()"),Yr=n(" \xE8 che potrebbe provenire da pi\xF9 fonti, poich\xE8 la fase di training di solito mette insieme molte cose su cui si \xE8 lavorato fino a quel momento. Il problema potrebbe essere qualcosa di sbagliato nel tuo dataset, o qualche problema nel provare a raggruppare in un batch elementi del dataset. E anche se tutto va bene per il training, qualcosa potrebbe andare storto durante la valutazione se c\u2019\xE8 un problema con la metrica selezionata."),Ml=c(),Ss=r("p"),Zr=n("Il modo migliore per eseguire il debug di un errore che si verifica in "),Ge=r("code"),si=n("model.fit()"),ai=n(" \xE8 quello di esaminare manualmente l\u2019intera pipeline per vedere dove le cose sono andate storte. L\u2019errore \xE8 spesso molto facile da risolvere."),Fl=c(),Ts=r("p"),ei=n("Per dimostrarlo, useremo il seguente script che ha lo scopo di affinare un modello DistilBERT sul "),da=r("a"),ni=n("dataset MNLI"),li=n(":"),Ql=c(),h(ha.$$.fragment),Ul=c(),Ds=r("p"),oi=n("Se si tenta di eseguirlo, si potrebbero riscontrare alcuni "),xe=r("code"),ri=n("VisibleDeprecationWarning"),ii=n(" durante la conversione del dataset \u2014 si tratta di un problema UX noto, quindi si prega di ignorarlo. Se stai leggendo il corso dopo, diciamo, novembre 2021 e il problema si ripresenta ancora, invia dei tweet di disappunto a @carrigmat finch\xE9 non lo risolve."),Gl=c(),te=r("p"),ti=n("Il problema pi\xF9 grave, per\xF2, \xE8 che riceviamo un vero e proprio errore. Ed \xE8 davvero terribilmente lungo:"),xl=c(),h(ba.$$.fragment),Ll=c(),pe=r("p"),pi=n("Che cosa significa? Abbiamo provato ad dare training sui nostri dati, ma non abbiamo ottenuto alcun gradiente? Questo \xE8 piuttosto preoccupante; come possiamo iniziare a fare il debug di una cosa del genere? Quando l\u2019errore che si ottiene non suggerisce immediatamente dove sia il problema, la soluzione migliore \xE8 spesso quella di procedere in ordine, assicurandosi in ogni fase che tutto sia corretto. Naturalmente, il punto di partenza \xE8 sempre\u2026"),Kl=c(),ds=r("h3"),Os=r("a"),Le=r("span"),h(fa.$$.fragment),ci=c(),Ke=r("span"),mi=n("Controllare i dati"),Bl=c(),ce=r("p"),ui=n("Non c\u2019\xE8 bisogno di dirlo, ma se i dati sono danneggiati, Keras non sar\xE0 in grado di risolverli per te. Quindi, per prima cosa, \xE8 necessario dare un\u2019occhiata a cosa c\u2019\xE8 nel training set."),Hl=c(),P=r("p"),di=n("Anche se c\u2019\xE8 la tentazione di guardare in "),Be=r("code"),hi=n("raw_datasets"),bi=n(" e "),He=r("code"),fi=n("tokenized_datasets"),ji=n(", si consiglia vivamente di esaminare i dati proprio nel punto in cui entrano nel modello. Ci\xF2 significa leggere un output dal "),Re=r("code"),vi=n("tf.data.Dataset"),gi=n(" creato con la funzione "),Ve=r("code"),$i=n("to_tf_dataset()"),_i=n("! Come si fa? Gli oggetti "),Je=r("code"),Ei=n("tf.data.Dataset"),zi=n(" ci forniscono volta per volta interi batch e non supportano l\u2019indicizzazione, quindi non possiamo semplicemente chiedere "),We=r("code"),qi=n("train_dataset[0]"),wi=n(". Possiamo per\xF2 chiedere gentilmente un batch:"),Rl=c(),h(ja.$$.fragment),Vl=c(),R=r("p"),Xe=r("code"),yi=n("break"),ki=n(" termina il ciclo dopo un\u2019iterazione, quindi prende il primo batch che esce da "),Ye=r("code"),Ci=n("train_dataset"),Pi=n(" e lo salva come "),Ze=r("code"),Ai=n("batch"),Si=n(". Ora, diamo un\u2019occhiata a ci\xF2 che c\u2019\xE8 dentro:"),Jl=c(),h(va.$$.fragment),Wl=c(),O=r("p"),Ti=n("Sembra corretto, non \xE8 vero? Stiamo passando al modello le "),sn=r("code"),Di=n("labels"),Oi=n(", le "),an=r("code"),Ni=n("attention_mask"),Ii=n(" e gli "),en=r("code"),Mi=n("input_ids"),Fi=n(", che dovrebbero essere tutto ci\xF2 di cui ha bisogno per calcolare gli output e la loss ("),nn=r("em"),Qi=n("funzione di perdita"),Ui=n("). Perch\xE9 non abbiamo un gradiente? Guarda meglio: stiamo passando un singolo dizionario come input, ma un batch di addestramento \xE8 di solito un tensore o un dizionario di input, pi\xF9 un tensore di label. Le label sono solo una chiave del dizionario di input."),Xl=c(),Ns=r("p"),Gi=n("\xC8 un problema? Non sempre, in realt\xE0! Ma \xE8 uno dei problemi pi\xF9 comuni che si incontrano quando si addestrano modelli Transformer con TensorFlow. Tutti i nostri modelli possono calcolare la loss internamente, ma per farlo \xE8 necessario passare le label nel dizionario di input. Questa \xE8 la funzione che viene utilizzata quando non si specifica un valore di loss in "),ln=r("code"),xi=n("compile()"),Li=n(". Keras, invece, di solito si aspetta che le label siano passate separatamente dal dizionario di input e le computazioni delle loss di solito falliscono se non lo si fa."),Yl=c(),as=r("p"),Ki=n("Il problema \xE8 ora pi\xF9 chiaro: abbiamo usato un argomento "),on=r("code"),Bi=n("loss"),Hi=n(", il che significa che stiamo chiedendo a Keras di calcolare le loss per noi, ma abbiamo passato le nostre label come input al modello, non come label nel posto in cui Keras se le aspetta! Dobbiamo scegliere l\u2019una o l\u2019altra soluzione: o usiamo la funzione interna del modello e manteniamo le label dove sono, oppure continuiamo a usare le loss di Keras, ma spostiamo le label nel punto in cui Keras se le aspetta. Per semplicit\xE0, adottiamo il primo approccio. Cambia la chiamata a "),rn=r("code"),Ri=n("compile()"),Vi=n(" in:"),Zl=c(),h(ga.$$.fragment),so=c(),me=r("p"),Ji=n("Ora utilizzeremo la funzione di perdita interna del modello e il problema dovrebbe essere risolto!"),ao=c(),h(Is.$$.fragment),eo=c(),Ms=r("p"),Wi=n("Ora proviamo ad avviare l\u2019addestramento. Ora dovremmo ottenere i gradienti, quindi, se tutto va bene (musica minacciosa), possiamo chiamare "),tn=r("code"),Xi=n("model.fit()"),Yi=n(" e tutto funzioner\xE0 bene!"),no=c(),h($a.$$.fragment),lo=c(),ue=r("p"),Zi=n("Oh no."),oo=c(),_a=r("p"),pn=r("code"),st=n("nan"),at=n(" non \xE8 un valore di loss molto incoraggiante. Tuttavia, abbiamo controllato i nostri dati e sembrano abbastanza buoni. Se il problema non \xE8 questo, come possiamo procedere? Il passo successivo pi\xF9 ovvio \xE8\u2026"),ro=c(),hs=r("h3"),Fs=r("a"),cn=r("span"),h(Ea.$$.fragment),et=c(),mn=r("span"),nt=n("Controllare il modello"),io=c(),V=r("p"),un=r("code"),lt=n("model.fit()"),ot=n(" \xE8 un\u2019ottima funzionionalit\xE0 di Keras, ma fa un sacco di cose per te e questo pu\xF2 rendere pi\xF9 difficile trovare esattamente dove si \xE8 generato un problema. Se stai facendo il debug del modello, una strategia che pu\xF2 essere molto utile \xE8 quella di passare un solo batch al modello e di esaminare in dettaglio gli output di quel batch. Un altro suggerimento molto utile se il modello produce errori \xE8 quello di "),dn=r("code"),rt=n("compile()"),it=n(" il modello con "),hn=r("code"),tt=n("run_eagerly=True"),pt=n(". Questo lo render\xE0 molto pi\xF9 lento, ma render\xE0 i messaggi di errore molto pi\xF9 comprensibili, perch\xE9 indicheranno esattamente in quale punto del codice del modello si \xE8 verificato il problema."),to=c(),es=r("p"),ct=n("Per ora, per\xF2, non abbiamo bisogno di "),bn=r("code"),mt=n("run_eagerly"),ut=n(". Passiamo il "),fn=r("code"),dt=n("batch"),ht=n(" che abbiamo ottenuto prima attraverso il modello e vediamo come sono gli output:"),po=c(),h(za.$$.fragment),co=c(),h(qa.$$.fragment),mo=c(),_=r("p"),bt=n("Beh, questo \xE8 insidioso. Tutto \xE8 "),jn=r("code"),ft=n("nan"),jt=n("! Ma \xE8 strano, non \xE8 vero? Come farebbero tutti i nostri logit a diventare "),vn=r("code"),vt=n("nan"),gt=n("? "),gn=r("code"),$t=n("nan"),_t=n(" significa \u201Cnot a number\u201D ("),$n=r("em"),Et=n("\u201Cnon un numero\u201D"),zt=n("). I valori "),_n=r("code"),qt=n("nan"),wt=n(" si verificano spesso quando si esegue un\u2019operazione vietata, come la divisione per zero. Ma una cosa molto importante da sapere su "),En=r("code"),yt=n("nan"),kt=n(" in machine learning \xE8 che questo valore tende a "),zn=r("em"),Ct=n("propagarsi"),Pt=n(". Se si moltiplica un numero per "),qn=r("code"),At=n("nan"),St=n(", anche il risultato sar\xE0 "),wn=r("code"),Tt=n("nan"),Dt=n(". E se si ottiene un "),yn=r("code"),Ot=n("nan"),Nt=n(" in un punto qualsiasi dell\u2019output, della loss o del gradiente, questo si diffonder\xE0 rapidamente in tutto il modello, perch\xE9 quando quel valore "),kn=r("code"),It=n("nan"),Mt=n(" si propagher\xE0 attraverso la rete, si otterranno gradienti "),Cn=r("code"),Ft=n("nan"),Qt=n(", e quando gli aggiornamenti dei pesi saranno calcolati con quei gradienti, si otterranno pesi "),Pn=r("code"),Ut=n("nan"),Gt=n(", e quei pesi calcoleranno ancora pi\xF9 output "),An=r("code"),xt=n("nan"),Lt=n("! Presto l\u2019intera rete sar\xE0 solo un grande blocco di "),Sn=r("code"),Kt=n("nan"),Bt=n(". Una volta che ci\xF2 accade, \xE8 piuttosto difficile capire dove sia iniziato il problema. Come possiamo isolare il punto in cui "),Tn=r("code"),Ht=n("nan"),Rt=n(" si \xE8 insinuato per la prima volta?"),uo=c(),Q=r("p"),Vt=n("La risposta \xE8 provare a "),Dn=r("em"),Jt=n("reinizializzare"),Wt=n(" il nostro modello. Una volta iniziato l\u2019addestramento, abbiamo avuto un "),On=r("code"),Xt=n("nan"),Yt=n(" da qualche parte e questo si \xE8 rapidamente propagato all\u2019intero modello. Quindi, carichiamo il modello da un checkpoint e non eseguiamo alcun aggiornamento dei pesi, e vediamo dove otteniamo un valore "),Nn=r("code"),Zt=n("nan"),sp=n(":"),ho=c(),h(wa.$$.fragment),bo=c(),de=r("p"),ap=n("Quando lo si esegue, si ottiene:"),fo=c(),h(ya.$$.fragment),jo=c(),J=r("p"),In=r("em"),ep=n("Adesso"),np=n(" s\xEC che ci capiamo! Non ci sono valori "),Mn=r("code"),lp=n("nan"),op=n(" nei nostri logit, il che \xE8 rassicurante. Ma vediamo alcuni valori "),Fn=r("code"),rp=n("nan"),ip=n(" nella nostra loss! C\u2019\xE8 qualcosa in quei campioni in particolare che sta causando questo problema? Vediamo quali sono (nota che se esegui questo codice da solo/a, potresti ottenere indici diversi perch\xE9 il dataset \xE8 stato rimescolato):"),vo=c(),h(ka.$$.fragment),go=c(),h(Ca.$$.fragment),$o=c(),he=r("p"),tp=n("Visualizziamo i campioni associati a questi indici:"),_o=c(),h(Pa.$$.fragment),Eo=c(),h(Aa.$$.fragment),zo=c(),be=r("p"),pp=n("Beh, ci sono tante cose qui dentro, ma non c\u2019\xE8 nulla che si distingua come insolito. Diamo un\u2019occhiata alle label:"),qo=c(),h(Sa.$$.fragment),wo=c(),h(Ta.$$.fragment),yo=c(),ns=r("p"),cp=n("I campioni "),Qn=r("code"),mp=n("nan"),up=n(" hanno tutti la stessa label, ed \xE8 la classe 2. Questo \xE8 un indizio molto chiaro. Il fatto che si abbia una loss di "),Un=r("code"),dp=n("nan"),hp=n(" solo quando la label \xE8 2 suggerisce che questo \xE8 un ottimo momento per verificare il numero di label nel nostro modello:"),ko=c(),h(Da.$$.fragment),Co=c(),h(Oa.$$.fragment),Po=c(),Qs=r("p"),bp=n("Ora vediamo il problema: il modello pensa che ci siano solo due classi, ma le label arrivano a 2, il che significa che in realt\xE0 ci sono tre classi (perch\xE9 anche lo 0 \xE8 una classe). Ecco come abbiamo ottenuto un "),Gn=r("code"),fp=n("nan"),jp=n(": cercando di calcolare la loss per una classe inesistente! Proviamo a cambiare il modello e ad adattarlo di nuovo:"),Ao=c(),h(Na.$$.fragment),So=c(),h(Ia.$$.fragment),To=c(),Us=r("p"),vp=n("Staimo addestrando! Non ci sono pi\xF9 "),xn=r("code"),gp=n("nan"),$p=n(" e la nostra loss sta diminuendo\u2026 pi\xF9 o meno. Se la si osserva per un po\u2019, si potrebbe iniziare a spazientirsi, perch\xE9 il valore della loss rimane ostinatamente alto. Interrompiamo il training e cerchiamo di capire quale potrebbe essere la causa di questo problema. A questo punto, siamo abbastanza sicuri che sia i dati che il modello siano a posto, ma il nostro modello non sta imparando bene. Cos\u2019altro rimane? \xC8 ora di\u2026"),Do=c(),bs=r("h3"),Gs=r("a"),Ln=r("span"),h(Ma.$$.fragment),_p=c(),Kn=r("span"),Ep=n("Controllare gli iperparametri"),Oo=c(),U=r("p"),zp=n("Se si guarda al codice precedente, \xE8 possibile che non si riesca a vedere alcun iperparametro, a parte forse il "),Bn=r("code"),qp=n("batch_size"),wp=n(", e questo non sembra un possibile problema. Non lasciarti ingannare, per\xF2: gli iperparametri ci sono sempre e se non li vedi significa che non conosci il valore a cui sono impostati. In particolare, ricorda una cosa fondamentale di Keras: se imposti una loss, un optimizer ("),Hn=r("em"),yp=n("ottimizzatore"),kp=n(") o una funzione di attivazione con una stringa, "),Rn=r("em"),Cp=n("tutti i suoi argomenti saranno impostati ai loro valori predefiniti"),Pp=n(". Ci\xF2 significa che, anche se usare le stringhe \xE8 molto comodo, bisogna fare molta attenzione, perch\xE9 questa cosa potrebbe facilmente nascondere alcuni aspetti importanti. (Chiunque si cimenti nella sfida opzionale qui sopra dovrebbe prendere nota di questo fatto)."),No=c(),xs=r("p"),Ap=n("In questo caso, dove abbiamo impostato un argomento con una stringa? Inizialmente settavamo la loss con una stringa, ma ora non lo facciamo pi\xF9. Tuttavia, impostiamo l\u2019optimizer usando una stringa. Potrebbe nasconderci qualcosa? Diamo un\u2019occhiata ai "),Fa=r("a"),Sp=n("suoi argomenti"),Tp=n("."),Io=c(),G=r("p"),Dp=n("C\u2019\xE8 qualcosa che balza all\u2019occhio? Esatto: il learning rate ("),Vn=r("em"),Op=n("tasso di apprendimento"),Np=n(")! Quando usiamo semplicemente la stringa "),Jn=r("code"),Ip=n("'adam'"),Mp=n(", otterremo il tasso di apprendimento predefinito, che \xE8 0,001, o 1e-3. Questo \xE8 decisamente troppo alto per un modello Transformer! In generale, si consiglia di provare learning rate tra 1e-5 e 1e-4 per i modelli; si tratta di un valore tra 10 e 100 volte inferiore a quello che stiamo usando qui. Questo sembra essere un problema importante, quindi proviamo a ridurlo. Per farlo, dobbiamo importare l\u2019oggetto "),Wn=r("code"),Fp=n("optimizer"),Qp=n(". Gi\xE0 che ci siamo, reinizializziamo il modello dal checkpoint, nel caso in cui il training con un learning rate elevato abbia compromesso i suoi pesi:"),Mo=c(),h(Qa.$$.fragment),Fo=c(),h(Ls.$$.fragment),Qo=c(),fe=r("p"),Up=n("Adess, possiamo tentarde di fare training del modell con il nuovo learning rate migliorato:"),Uo=c(),h(Ua.$$.fragment),Go=c(),h(Ga.$$.fragment),xo=c(),je=r("p"),Gp=n("Ora la nostra loss sta davvero andando da qualche parte! L\u2019addestramento sembra finalmente funzionare. C\u2019\xE8 una lezione da imparare: quando il modello funziona, ma la loss non diminuisce, e si \xE8 sicuri che i dati siano corretti, \xE8 una buona idea controllare gli iperparametri come il learning rate e il weight decay. Impostando uno di questi parametri troppo alto, \xE8 molto probabile che l\u2019addestramento si \u201Cblocchi\u201D a un valore di loss elevato."),Lo=c(),fs=r("h2"),Ks=r("a"),Xn=r("span"),h(xa.$$.fragment),xp=c(),Yn=r("span"),Lp=n("Altri potenziali problemi"),Ko=c(),ve=r("p"),Kp=n("Abbiamo trattato i problemi dello script di cui sopra, ma ci sono molti altri errori comuni che si possono incontrare. Vediamo un elenco (molto incompleto)."),Bo=c(),js=r("h3"),Bs=r("a"),Zn=r("span"),h(La.$$.fragment),Bp=c(),sl=r("span"),Hp=n("Gestire gli errori out-of-memory"),Ho=c(),ls=r("p"),Rp=n("Il segnale che indica che la memoria \xE8 esaurita \xE8 un errore del tipo \u201COOM when allocating tensor\u201D (OOM \xE8 l\u2019abbreviazione di \u201Cout of memory\u201D). Si tratta di un rischio molto comune quando si ha a che fare con modelli linguistici di grandi dimensioni. In questo caso, una buona strategia \xE8 quella di dimezzare le dimensioni del batch e riprovare. Tenete presente, per\xF2, che alcuni modelli sono "),al=r("em"),Vp=n("molto"),Jp=n(" grandi. Ad esempio, il modello GPT-2 completo ha 1,5B parametri, il che significa che sono necessari 6 GB di memoria solo per memorizzare il modello e altri 6 GB per i suoi gradienti! L\u2019addestramento del modello GPT-2 completo richiede di solito oltre 20 GB di VRAM, indipendentemente dalla dimensione del batch utilizzato, che solo poche GPU hanno. Modelli pi\xF9 leggeri come "),el=r("code"),Wp=n("distilbert-base-cased"),Xp=n(" sono molto pi\xF9 facili da eseguire e si addestrano molto pi\xF9 rapidamente."),Ro=c(),h(Hs.$$.fragment),Vo=c(),vs=r("h3"),Rs=r("a"),nl=r("span"),h(Ka.$$.fragment),Yp=c(),ll=r("span"),Zp=n("TensorFlow \xE8 molto affamato \u{1F99B}"),Jo=c(),os=r("p"),sc=n("Una particolarit\xE0 di TensorFlow di cui bisogna essere consapevoli \xE8 che alloca "),ol=r("em"),ac=n("tutta"),ec=n(" la memoria della GPU su se stesso non appena si carica un modello o si esegue un addestramento, e poi divide la memoria in base alle esigenze. Questo comportamento \xE8 diverso da quello di altri framework, come PyTorch, che allocano la memoria come richiesto con CUDA invece di farlo internamente. Un vantaggio dell\u2019approccio di TensorFlow \xE8 che spesso pu\xF2 produrre errori utili quando esaurisci la memoria e pu\xF2 recuperare da questo stato senza mandare in crash l\u2019intero kernel CUDA. Ma c\u2019\xE8 anche un importante aspetto negativo: se si eseguono due processi TensorFlow contemporaneamente, allora "),rl=r("strong"),nc=n("sar\xE0 un bel guaio"),lc=n("."),Wo=c(),ge=r("p"),oc=n("Se si esegue su Colab non ci si deve preoccupare di questo, ma se si lavora in locale \xE8 sicuramente qualcosa a cui si deve fare attenzione. In particolare, \xE8 bene ricordare che la chiusura della scheda di un notebook non comporta necessariamente la chiusura del notebook stesso! Potresti dover selezionare i notebook in esecuzione (quelli con l\u2019icona verde) e chiuderli manualmente nell\u2019elenco della directory. Qualsiasi notebook in esecuzione che utilizzava TensorFlow potrebbe ancora conservare una buona parte della memoria della GPU e ci\xF2 significa che qualsiasi nuovo notebook avviato potrebbe presentare problemi molto strani."),Xo=c(),Vs=r("p"),rc=n("Se inizi a ricevere errori relativi a CUDA, BLAS o cuBLAS in un codice che prima funzionava, questa \xE8 molto spesso la ragione. Si pu\xF2 usare un comando come "),il=r("code"),ic=n("nvidia-smi"),tc=n(" per controllare: quando si spegne o si riavvia il notebook usato, la maggior parte della memoria \xE8 libera o \xE8 ancora in uso? Se \xE8 ancora in uso, c\u2019\xE8 qualcos\u2019altro che la sta occupando!"),Yo=c(),gs=r("h3"),Js=r("a"),tl=r("span"),h(Ba.$$.fragment),pc=c(),pl=r("span"),cc=n("Check your data (again!)"),Zo=c(),N=r("p"),mc=n("Il tuo modello imparer\xE0 qualcosa solo se \xE8 effettivamente possibile imparare qualcosa dai tuoi dati. Se c\u2019\xE8 un bug che corrompe i dati o le label sono assegnate in modo casuale, \xE8 molto probabile che non si riesca ad addestrare il modello sul dataset. In questo caso uno strumento utile \xE8 "),cl=r("code"),uc=n("tokenizer.decode()"),dc=n(". Questo trasformer\xE0 gli "),ml=r("code"),hc=n("input_ids"),bc=n(" in stringhe, in modo da poter visualizzare i dati e vedere se i dati di training stanno addestrando ci\xF2 che si vuole. Per esempio, dopo aver ottenuto un "),ul=r("code"),fc=n("batch"),jc=n(" dal proprio "),dl=r("code"),vc=n("tf.data.Dataset"),gc=n(" come abbiamo fatto sopra, si pu\xF2 decodificare il primo elemento in questo modo:"),sr=c(),h(Ha.$$.fragment),ar=c(),$e=r("p"),$c=n("Poi si pu\xF2 confrontare con la prima label, in questo modo:"),er=c(),h(Ra.$$.fragment),nr=c(),_e=r("p"),_c=n("Una volta visualizzati i dati in questo modo, puoi porti le seguenti domande:"),lr=c(),x=r("ul"),hl=r("li"),Ec=n("I dati decodificati sono comprensibili?"),zc=c(),bl=r("li"),qc=n("Sei d\u2019accordo con le label?"),wc=c(),fl=r("li"),yc=n("C\u2019\xE8 una label pi\xF9 comune delle altre?"),kc=c(),jl=r("li"),Cc=n("Quale dovrebbe essere la funzione di perdita/metrica se il modello predicesse una risposta a caso/sempre la stessa risposta?"),or=c(),Ws=r("p"),Pc=n("Dopo aver osservato i dati, esamina alcune previsioni del modello: se il modello produce dei token, prova a decodificare anche quelli! Se il modello prevede sempre la stessa cosa, potrebbe essere perch\xE9 il tuo set di dati \xE8 influenzato verso una categoria (per i problemi di classificazione); tecniche come fare oversampling ("),vl=r("em"),Ac=n("sovra-campionamento"),Sc=n(") delle classi rare potrebbero aiutare. In alternativa, ci\xF2 pu\xF2 essere causato da problemi di addestramento, come ad esempio una scorretta impostazione degli iperparametri."),rr=c(),Ee=r("p"),Tc=n("Se la funzione di perdita/metrica ottenuta con il tuo modello iniziale \xE8 molto diversa da quella che ci si aspetterebbe per le previsioni casuali, ricontrolla il modo in cui viene calcolata la funzione o la metrica, perch\xE9 probabilmente c\u2019\xE8 un bug. Se si utilizzano diverse funzioni che aggiungi alla fine, assicurati che siano della stessa grandezza."),ir=c(),ze=r("p"),Dc=n("Quando sei sicuro/a che i dati sono perfetti, puoi verificare se il modello \xE8 in grado di addestrarsi su di essi con un semplice test."),tr=c(),$s=r("h3"),Xs=r("a"),gl=r("span"),h(Va.$$.fragment),Oc=c(),$l=r("span"),Nc=n("Fare overfitting del modello su un batch"),pr=c(),qe=r("p"),Ic=n("L\u2019overfitting \xE8 di solito qualcosa che cerchiamo di evitare durante l\u2019addestramento, poich\xE9 significa che il modello non sta imparando a riconoscere le propriet\xE0 generali che vogliamo, ma sta invece memorizzando i campioni di addestramento. Tuttavia, provare ad addestrare il modello su un batch pi\xF9 e pi\xF9 volte \xE8 un buon test per verificare se il problema cos\xEC come \xE8 stato inquadrato pu\xF2 essere risolto dal modello che si sta cercando di addestrare. Inoltre, ti aiuter\xE0 a capire se il learning rate iniziale \xE8 troppo alta."),cr=c(),rs=r("p"),Mc=n("Una volta definito il "),_l=r("code"),Fc=n("Trainer"),Qc=n(", \xE8 molto semplice: basta prendere un batch dal training set, ed eseguire un piccolo ciclo di addestramento manuale utilizzando solo quel "),El=r("code"),Uc=n("batch"),Gc=n(" per qualcosa come 20 step:"),mr=c(),h(Ja.$$.fragment),ur=c(),h(Ys.$$.fragment),dr=c(),Zs=r("p"),xc=n("Il modello risultante dovrebbe avere risultati quasi perfetti sul "),zl=r("code"),Lc=n("batch"),Kc=n(", con una loss che diminuisce rapidamente verso lo 0 (o il valore minimo per la loss che si sta utilizzando)."),hr=c(),we=r("p"),Bc=n("Se non si riesci a far s\xEC che il modello ottenga risultati perfetti come questo, significa che c\u2019\xE8 qualcosa di sbagliato nel modo in cui si \xE8 impostato il problema o con i dati, e quindi dovresti risolvere questa cosa. Solo quando riesci a superare il test di overfitting puoi essere sicuro/a che il tuo modello possa effettivamente imparare qualcosa."),br=c(),h(sa.$$.fragment),fr=c(),_s=r("h3"),aa=r("a"),ql=r("span"),h(Wa.$$.fragment),Hc=c(),wl=r("span"),Rc=n("Non calibrare niente prima di avere una prima baseline"),jr=c(),L=r("p"),Vc=n("Hyperparameter tuning ("),yl=r("em"),Jc=n("calibrazione degli iperparametri"),Wc=n(") \xE8 sempre considerato come la parte pi\xF9 difficile di machine learning, ma \xE8 solo l\u2019ultimo passo per aiutarti a migliorare un po\u2019 la metrica. Valori "),kl=r("em"),Xc=n("molto"),Yc=n(" sbagliati di iperparametri, come l\u2019uso del learning rate predefinito di Adam di 1e-3 con un modello Transformer, faranno s\xEC che l\u2019apprendimento proceda molto lentamente o si blocchi completamente, naturalmente, ma la maggior parte delle volte iperparametri \u201Cragionevoli\u201D, come un learning rate da 1e-5 a 5e-5, funzioneranno bene per darti buoni risultati. Quindi, non ci si deve lanciare in una ricerca di iperparametri dispendiosa in termini di tempo e di costi, finch\xE9 non si \xE8 ottenuto qualcosa che batta la baseline ("),Cl=r("em"),Zc=n("base di partenza"),sm=n(") che si ha sul dataset."),vr=c(),ye=r("p"),am=n("Una volta ottenuto un modello sufficientemente buono, si pu\xF2 iniziare a modificarlo un po\u2019. Non provare a eseguire l\u2019addestramento un migliaio di volte con iperparametri diversi, ma confronta un paio di esecuzioni che hanno valori diversi per un iperparametro cos\xEC da avere un\u2019idea di quale abbia il maggiore impatto."),gr=c(),ke=r("p"),em=n("Se stai modificando il modello stesso, mantieni le cose semplici e non provare nulla che non possa essere ragionevolmente giustificato. Assicurati sempre di rifare il test di overfitting per verificare che la modifica non abbia avuto conseguenze indesiderate."),$r=c(),Es=r("h3"),ea=r("a"),Pl=r("span"),h(Xa.$$.fragment),nm=c(),Al=r("span"),lm=n("Chiedere aiuto"),_r=c(),na=r("p"),om=n("Speriamo che in questa sezione tu abbia trovato qualche consiglio utile a risolvere il tuo problema, ma se cos\xEC non fosse, ricordati che puoi sempre chiedere aiuto alla community nei "),Ya=r("a"),rm=n("forum"),im=n("."),Er=c(),Ce=r("p"),tm=n("Qui di seguito sono riportate alcune risorse aggiuntive che potrebbero rivelarsi utili:"),zr=c(),K=r("ul"),Pe=r("li"),Za=r("a"),pm=n("\u201CReproducibility as a vehicle for engineering best practices\u201D"),cm=n(" di Joel Grus"),mm=c(),Ae=r("li"),se=r("a"),um=n("\u201CChecklist for debugging neural networks\u201D"),dm=n(" di Cecelia Shao"),hm=c(),Se=r("li"),ae=r("a"),bm=n("\u201CHow to unit test machine learning code\u201D"),fm=n(" di Chase Roberts"),jm=c(),Te=r("li"),ee=r("a"),vm=n("\u201CA Recipe for Training Neural Networks\u201D"),gm=n(" di Andrej Karpathy"),qr=c(),De=r("p"),$m=n("Naturalmente, non tutti i problemi che incontrerai durante l\u2019addestramento delle reti neurali sono colpa tua! Se si incontra qualcosa nella libreria \u{1F917} Transformers o \u{1F917} Datasets che non sembra corretto, \xE8 possibile che si sia trovato un bug. Dovresti assolutamente segnalarcelo e nella prossima sezione ti spiegheremo esattamente come fare."),this.h()},l(s){const o=td('[data-svelte="svelte-1phssyn"]',document.head);d=i(o,"META",{name:!0,content:!0}),o.forEach(e),w=m(s),b($.$$.fragment,s),z=m(s),A=i(s,"H1",{class:!0});var ne=t(A);C=i(ne,"A",{id:!0,class:!0,href:!0});var Sl=t(C);S=i(Sl,"SPAN",{});var Tl=t(S);b(D.$$.fragment,Tl),Tl.forEach(e),Sl.forEach(e),ps=m(ne),cs=i(ne,"SPAN",{});var Dl=t(cs);ca=l(Dl,"Fare il debug di una training pipeline"),Dl.forEach(e),ne.forEach(e),W=m(s),b(H.$$.fragment,s),qs=m(s),T=i(s,"P",{});var zs=t(T);F=l(zs,"Hai scritto un bello script per addestrare o affinare un modello su un determinato compito, seguendo scrupolosamente i consigli del "),X=i(zs,"A",{href:!0});var Ol=t(X);ms=l(Ol,"Capitolo 7"),Ol.forEach(e),ma=l(zs,". Ma quando lanci il comando "),us=i(zs,"CODE",{});var ym=t(us);ws=l(ym,"model.fit()"),ym.forEach(e),Y=l(zs,", succede qualcosa di orribile: si ottiene un errore \u{1F631}! O peggio, tutto sembra andare bene e il training viene eseguito senza errori, ma il modello che ne risulta fa schifo. In questa sezione mostreremo cosa \xE8 possibile fare per eseguire il debug di questo tipo di problemi."),zs.forEach(e),ys=m(s),q=i(s,"H2",{class:!0});var yr=t(q);k=i(yr,"A",{id:!0,class:!0,href:!0});var km=t(k);ks=i(km,"SPAN",{});var Cm=t(ks);b(Z.$$.fragment,Cm),Cm.forEach(e),km.forEach(e),ie=m(yr),Cs=i(yr,"SPAN",{});var Pm=t(Cs);Ps=l(Pm,"Debugging the training pipeline"),Pm.forEach(e),yr.forEach(e),ua=m(s),b(ss.$$.fragment,s),Il=m(s),As=i(s,"P",{});var kr=t(As);Wr=l(kr,"Il problema quando si ha un errore da "),Ue=i(kr,"CODE",{});var Am=t(Ue);Xr=l(Am,"model.fit()"),Am.forEach(e),Yr=l(kr," \xE8 che potrebbe provenire da pi\xF9 fonti, poich\xE8 la fase di training di solito mette insieme molte cose su cui si \xE8 lavorato fino a quel momento. Il problema potrebbe essere qualcosa di sbagliato nel tuo dataset, o qualche problema nel provare a raggruppare in un batch elementi del dataset. E anche se tutto va bene per il training, qualcosa potrebbe andare storto durante la valutazione se c\u2019\xE8 un problema con la metrica selezionata."),kr.forEach(e),Ml=m(s),Ss=i(s,"P",{});var Cr=t(Ss);Zr=l(Cr,"Il modo migliore per eseguire il debug di un errore che si verifica in "),Ge=i(Cr,"CODE",{});var Sm=t(Ge);si=l(Sm,"model.fit()"),Sm.forEach(e),ai=l(Cr," \xE8 quello di esaminare manualmente l\u2019intera pipeline per vedere dove le cose sono andate storte. L\u2019errore \xE8 spesso molto facile da risolvere."),Cr.forEach(e),Fl=m(s),Ts=i(s,"P",{});var Pr=t(Ts);ei=l(Pr,"Per dimostrarlo, useremo il seguente script che ha lo scopo di affinare un modello DistilBERT sul "),da=i(Pr,"A",{href:!0,rel:!0});var Tm=t(da);ni=l(Tm,"dataset MNLI"),Tm.forEach(e),li=l(Pr,":"),Pr.forEach(e),Ql=m(s),b(ha.$$.fragment,s),Ul=m(s),Ds=i(s,"P",{});var Ar=t(Ds);oi=l(Ar,"Se si tenta di eseguirlo, si potrebbero riscontrare alcuni "),xe=i(Ar,"CODE",{});var Dm=t(xe);ri=l(Dm,"VisibleDeprecationWarning"),Dm.forEach(e),ii=l(Ar," durante la conversione del dataset \u2014 si tratta di un problema UX noto, quindi si prega di ignorarlo. Se stai leggendo il corso dopo, diciamo, novembre 2021 e il problema si ripresenta ancora, invia dei tweet di disappunto a @carrigmat finch\xE9 non lo risolve."),Ar.forEach(e),Gl=m(s),te=i(s,"P",{});var Om=t(te);ti=l(Om,"Il problema pi\xF9 grave, per\xF2, \xE8 che riceviamo un vero e proprio errore. Ed \xE8 davvero terribilmente lungo:"),Om.forEach(e),xl=m(s),b(ba.$$.fragment,s),Ll=m(s),pe=i(s,"P",{});var Nm=t(pe);pi=l(Nm,"Che cosa significa? Abbiamo provato ad dare training sui nostri dati, ma non abbiamo ottenuto alcun gradiente? Questo \xE8 piuttosto preoccupante; come possiamo iniziare a fare il debug di una cosa del genere? Quando l\u2019errore che si ottiene non suggerisce immediatamente dove sia il problema, la soluzione migliore \xE8 spesso quella di procedere in ordine, assicurandosi in ogni fase che tutto sia corretto. Naturalmente, il punto di partenza \xE8 sempre\u2026"),Nm.forEach(e),Kl=m(s),ds=i(s,"H3",{class:!0});var Sr=t(ds);Os=i(Sr,"A",{id:!0,class:!0,href:!0});var Im=t(Os);Le=i(Im,"SPAN",{});var Mm=t(Le);b(fa.$$.fragment,Mm),Mm.forEach(e),Im.forEach(e),ci=m(Sr),Ke=i(Sr,"SPAN",{});var Fm=t(Ke);mi=l(Fm,"Controllare i dati"),Fm.forEach(e),Sr.forEach(e),Bl=m(s),ce=i(s,"P",{});var Qm=t(ce);ui=l(Qm,"Non c\u2019\xE8 bisogno di dirlo, ma se i dati sono danneggiati, Keras non sar\xE0 in grado di risolverli per te. Quindi, per prima cosa, \xE8 necessario dare un\u2019occhiata a cosa c\u2019\xE8 nel training set."),Qm.forEach(e),Hl=m(s),P=i(s,"P",{});var I=t(P);di=l(I,"Anche se c\u2019\xE8 la tentazione di guardare in "),Be=i(I,"CODE",{});var Um=t(Be);hi=l(Um,"raw_datasets"),Um.forEach(e),bi=l(I," e "),He=i(I,"CODE",{});var Gm=t(He);fi=l(Gm,"tokenized_datasets"),Gm.forEach(e),ji=l(I,", si consiglia vivamente di esaminare i dati proprio nel punto in cui entrano nel modello. Ci\xF2 significa leggere un output dal "),Re=i(I,"CODE",{});var xm=t(Re);vi=l(xm,"tf.data.Dataset"),xm.forEach(e),gi=l(I," creato con la funzione "),Ve=i(I,"CODE",{});var Lm=t(Ve);$i=l(Lm,"to_tf_dataset()"),Lm.forEach(e),_i=l(I,"! Come si fa? Gli oggetti "),Je=i(I,"CODE",{});var Km=t(Je);Ei=l(Km,"tf.data.Dataset"),Km.forEach(e),zi=l(I," ci forniscono volta per volta interi batch e non supportano l\u2019indicizzazione, quindi non possiamo semplicemente chiedere "),We=i(I,"CODE",{});var Bm=t(We);qi=l(Bm,"train_dataset[0]"),Bm.forEach(e),wi=l(I,". Possiamo per\xF2 chiedere gentilmente un batch:"),I.forEach(e),Rl=m(s),b(ja.$$.fragment,s),Vl=m(s),R=i(s,"P",{});var le=t(R);Xe=i(le,"CODE",{});var Hm=t(Xe);yi=l(Hm,"break"),Hm.forEach(e),ki=l(le," termina il ciclo dopo un\u2019iterazione, quindi prende il primo batch che esce da "),Ye=i(le,"CODE",{});var Rm=t(Ye);Ci=l(Rm,"train_dataset"),Rm.forEach(e),Pi=l(le," e lo salva come "),Ze=i(le,"CODE",{});var Vm=t(Ze);Ai=l(Vm,"batch"),Vm.forEach(e),Si=l(le,". Ora, diamo un\u2019occhiata a ci\xF2 che c\u2019\xE8 dentro:"),le.forEach(e),Jl=m(s),b(va.$$.fragment,s),Wl=m(s),O=i(s,"P",{});var is=t(O);Ti=l(is,"Sembra corretto, non \xE8 vero? Stiamo passando al modello le "),sn=i(is,"CODE",{});var Jm=t(sn);Di=l(Jm,"labels"),Jm.forEach(e),Oi=l(is,", le "),an=i(is,"CODE",{});var Wm=t(an);Ni=l(Wm,"attention_mask"),Wm.forEach(e),Ii=l(is," e gli "),en=i(is,"CODE",{});var Xm=t(en);Mi=l(Xm,"input_ids"),Xm.forEach(e),Fi=l(is,", che dovrebbero essere tutto ci\xF2 di cui ha bisogno per calcolare gli output e la loss ("),nn=i(is,"EM",{});var Ym=t(nn);Qi=l(Ym,"funzione di perdita"),Ym.forEach(e),Ui=l(is,"). Perch\xE9 non abbiamo un gradiente? Guarda meglio: stiamo passando un singolo dizionario come input, ma un batch di addestramento \xE8 di solito un tensore o un dizionario di input, pi\xF9 un tensore di label. Le label sono solo una chiave del dizionario di input."),is.forEach(e),Xl=m(s),Ns=i(s,"P",{});var Tr=t(Ns);Gi=l(Tr,"\xC8 un problema? Non sempre, in realt\xE0! Ma \xE8 uno dei problemi pi\xF9 comuni che si incontrano quando si addestrano modelli Transformer con TensorFlow. Tutti i nostri modelli possono calcolare la loss internamente, ma per farlo \xE8 necessario passare le label nel dizionario di input. Questa \xE8 la funzione che viene utilizzata quando non si specifica un valore di loss in "),ln=i(Tr,"CODE",{});var Zm=t(ln);xi=l(Zm,"compile()"),Zm.forEach(e),Li=l(Tr,". Keras, invece, di solito si aspetta che le label siano passate separatamente dal dizionario di input e le computazioni delle loss di solito falliscono se non lo si fa."),Tr.forEach(e),Yl=m(s),as=i(s,"P",{});var Oe=t(as);Ki=l(Oe,"Il problema \xE8 ora pi\xF9 chiaro: abbiamo usato un argomento "),on=i(Oe,"CODE",{});var su=t(on);Bi=l(su,"loss"),su.forEach(e),Hi=l(Oe,", il che significa che stiamo chiedendo a Keras di calcolare le loss per noi, ma abbiamo passato le nostre label come input al modello, non come label nel posto in cui Keras se le aspetta! Dobbiamo scegliere l\u2019una o l\u2019altra soluzione: o usiamo la funzione interna del modello e manteniamo le label dove sono, oppure continuiamo a usare le loss di Keras, ma spostiamo le label nel punto in cui Keras se le aspetta. Per semplicit\xE0, adottiamo il primo approccio. Cambia la chiamata a "),rn=i(Oe,"CODE",{});var au=t(rn);Ri=l(au,"compile()"),au.forEach(e),Vi=l(Oe," in:"),Oe.forEach(e),Zl=m(s),b(ga.$$.fragment,s),so=m(s),me=i(s,"P",{});var eu=t(me);Ji=l(eu,"Ora utilizzeremo la funzione di perdita interna del modello e il problema dovrebbe essere risolto!"),eu.forEach(e),ao=m(s),b(Is.$$.fragment,s),eo=m(s),Ms=i(s,"P",{});var Dr=t(Ms);Wi=l(Dr,"Ora proviamo ad avviare l\u2019addestramento. Ora dovremmo ottenere i gradienti, quindi, se tutto va bene (musica minacciosa), possiamo chiamare "),tn=i(Dr,"CODE",{});var nu=t(tn);Xi=l(nu,"model.fit()"),nu.forEach(e),Yi=l(Dr," e tutto funzioner\xE0 bene!"),Dr.forEach(e),no=m(s),b($a.$$.fragment,s),lo=m(s),ue=i(s,"P",{});var lu=t(ue);Zi=l(lu,"Oh no."),lu.forEach(e),oo=m(s),_a=i(s,"P",{});var _m=t(_a);pn=i(_m,"CODE",{});var ou=t(pn);st=l(ou,"nan"),ou.forEach(e),at=l(_m," non \xE8 un valore di loss molto incoraggiante. Tuttavia, abbiamo controllato i nostri dati e sembrano abbastanza buoni. Se il problema non \xE8 questo, come possiamo procedere? Il passo successivo pi\xF9 ovvio \xE8\u2026"),_m.forEach(e),ro=m(s),hs=i(s,"H3",{class:!0});var Or=t(hs);Fs=i(Or,"A",{id:!0,class:!0,href:!0});var ru=t(Fs);cn=i(ru,"SPAN",{});var iu=t(cn);b(Ea.$$.fragment,iu),iu.forEach(e),ru.forEach(e),et=m(Or),mn=i(Or,"SPAN",{});var tu=t(mn);nt=l(tu,"Controllare il modello"),tu.forEach(e),Or.forEach(e),io=m(s),V=i(s,"P",{});var oe=t(V);un=i(oe,"CODE",{});var pu=t(un);lt=l(pu,"model.fit()"),pu.forEach(e),ot=l(oe," \xE8 un\u2019ottima funzionionalit\xE0 di Keras, ma fa un sacco di cose per te e questo pu\xF2 rendere pi\xF9 difficile trovare esattamente dove si \xE8 generato un problema. Se stai facendo il debug del modello, una strategia che pu\xF2 essere molto utile \xE8 quella di passare un solo batch al modello e di esaminare in dettaglio gli output di quel batch. Un altro suggerimento molto utile se il modello produce errori \xE8 quello di "),dn=i(oe,"CODE",{});var cu=t(dn);rt=l(cu,"compile()"),cu.forEach(e),it=l(oe," il modello con "),hn=i(oe,"CODE",{});var mu=t(hn);tt=l(mu,"run_eagerly=True"),mu.forEach(e),pt=l(oe,". Questo lo render\xE0 molto pi\xF9 lento, ma render\xE0 i messaggi di errore molto pi\xF9 comprensibili, perch\xE9 indicheranno esattamente in quale punto del codice del modello si \xE8 verificato il problema."),oe.forEach(e),to=m(s),es=i(s,"P",{});var Ne=t(es);ct=l(Ne,"Per ora, per\xF2, non abbiamo bisogno di "),bn=i(Ne,"CODE",{});var uu=t(bn);mt=l(uu,"run_eagerly"),uu.forEach(e),ut=l(Ne,". Passiamo il "),fn=i(Ne,"CODE",{});var du=t(fn);dt=l(du,"batch"),du.forEach(e),ht=l(Ne," che abbiamo ottenuto prima attraverso il modello e vediamo come sono gli output:"),Ne.forEach(e),po=m(s),b(za.$$.fragment,s),co=m(s),b(qa.$$.fragment,s),mo=m(s),_=i(s,"P",{});var E=t(_);bt=l(E,"Beh, questo \xE8 insidioso. Tutto \xE8 "),jn=i(E,"CODE",{});var hu=t(jn);ft=l(hu,"nan"),hu.forEach(e),jt=l(E,"! Ma \xE8 strano, non \xE8 vero? Come farebbero tutti i nostri logit a diventare "),vn=i(E,"CODE",{});var bu=t(vn);vt=l(bu,"nan"),bu.forEach(e),gt=l(E,"? "),gn=i(E,"CODE",{});var fu=t(gn);$t=l(fu,"nan"),fu.forEach(e),_t=l(E," significa \u201Cnot a number\u201D ("),$n=i(E,"EM",{});var ju=t($n);Et=l(ju,"\u201Cnon un numero\u201D"),ju.forEach(e),zt=l(E,"). I valori "),_n=i(E,"CODE",{});var vu=t(_n);qt=l(vu,"nan"),vu.forEach(e),wt=l(E," si verificano spesso quando si esegue un\u2019operazione vietata, come la divisione per zero. Ma una cosa molto importante da sapere su "),En=i(E,"CODE",{});var gu=t(En);yt=l(gu,"nan"),gu.forEach(e),kt=l(E," in machine learning \xE8 che questo valore tende a "),zn=i(E,"EM",{});var $u=t(zn);Ct=l($u,"propagarsi"),$u.forEach(e),Pt=l(E,". Se si moltiplica un numero per "),qn=i(E,"CODE",{});var _u=t(qn);At=l(_u,"nan"),_u.forEach(e),St=l(E,", anche il risultato sar\xE0 "),wn=i(E,"CODE",{});var Eu=t(wn);Tt=l(Eu,"nan"),Eu.forEach(e),Dt=l(E,". E se si ottiene un "),yn=i(E,"CODE",{});var zu=t(yn);Ot=l(zu,"nan"),zu.forEach(e),Nt=l(E," in un punto qualsiasi dell\u2019output, della loss o del gradiente, questo si diffonder\xE0 rapidamente in tutto il modello, perch\xE9 quando quel valore "),kn=i(E,"CODE",{});var qu=t(kn);It=l(qu,"nan"),qu.forEach(e),Mt=l(E," si propagher\xE0 attraverso la rete, si otterranno gradienti "),Cn=i(E,"CODE",{});var wu=t(Cn);Ft=l(wu,"nan"),wu.forEach(e),Qt=l(E,", e quando gli aggiornamenti dei pesi saranno calcolati con quei gradienti, si otterranno pesi "),Pn=i(E,"CODE",{});var yu=t(Pn);Ut=l(yu,"nan"),yu.forEach(e),Gt=l(E,", e quei pesi calcoleranno ancora pi\xF9 output "),An=i(E,"CODE",{});var ku=t(An);xt=l(ku,"nan"),ku.forEach(e),Lt=l(E,"! Presto l\u2019intera rete sar\xE0 solo un grande blocco di "),Sn=i(E,"CODE",{});var Cu=t(Sn);Kt=l(Cu,"nan"),Cu.forEach(e),Bt=l(E,". Una volta che ci\xF2 accade, \xE8 piuttosto difficile capire dove sia iniziato il problema. Come possiamo isolare il punto in cui "),Tn=i(E,"CODE",{});var Pu=t(Tn);Ht=l(Pu,"nan"),Pu.forEach(e),Rt=l(E," si \xE8 insinuato per la prima volta?"),E.forEach(e),uo=m(s),Q=i(s,"P",{});var la=t(Q);Vt=l(la,"La risposta \xE8 provare a "),Dn=i(la,"EM",{});var Au=t(Dn);Jt=l(Au,"reinizializzare"),Au.forEach(e),Wt=l(la," il nostro modello. Una volta iniziato l\u2019addestramento, abbiamo avuto un "),On=i(la,"CODE",{});var Su=t(On);Xt=l(Su,"nan"),Su.forEach(e),Yt=l(la," da qualche parte e questo si \xE8 rapidamente propagato all\u2019intero modello. Quindi, carichiamo il modello da un checkpoint e non eseguiamo alcun aggiornamento dei pesi, e vediamo dove otteniamo un valore "),Nn=i(la,"CODE",{});var Tu=t(Nn);Zt=l(Tu,"nan"),Tu.forEach(e),sp=l(la,":"),la.forEach(e),ho=m(s),b(wa.$$.fragment,s),bo=m(s),de=i(s,"P",{});var Du=t(de);ap=l(Du,"Quando lo si esegue, si ottiene:"),Du.forEach(e),fo=m(s),b(ya.$$.fragment,s),jo=m(s),J=i(s,"P",{});var re=t(J);In=i(re,"EM",{});var Ou=t(In);ep=l(Ou,"Adesso"),Ou.forEach(e),np=l(re," s\xEC che ci capiamo! Non ci sono valori "),Mn=i(re,"CODE",{});var Nu=t(Mn);lp=l(Nu,"nan"),Nu.forEach(e),op=l(re," nei nostri logit, il che \xE8 rassicurante. Ma vediamo alcuni valori "),Fn=i(re,"CODE",{});var Iu=t(Fn);rp=l(Iu,"nan"),Iu.forEach(e),ip=l(re," nella nostra loss! C\u2019\xE8 qualcosa in quei campioni in particolare che sta causando questo problema? Vediamo quali sono (nota che se esegui questo codice da solo/a, potresti ottenere indici diversi perch\xE9 il dataset \xE8 stato rimescolato):"),re.forEach(e),vo=m(s),b(ka.$$.fragment,s),go=m(s),b(Ca.$$.fragment,s),$o=m(s),he=i(s,"P",{});var Mu=t(he);tp=l(Mu,"Visualizziamo i campioni associati a questi indici:"),Mu.forEach(e),_o=m(s),b(Pa.$$.fragment,s),Eo=m(s),b(Aa.$$.fragment,s),zo=m(s),be=i(s,"P",{});var Fu=t(be);pp=l(Fu,"Beh, ci sono tante cose qui dentro, ma non c\u2019\xE8 nulla che si distingua come insolito. Diamo un\u2019occhiata alle label:"),Fu.forEach(e),qo=m(s),b(Sa.$$.fragment,s),wo=m(s),b(Ta.$$.fragment,s),yo=m(s),ns=i(s,"P",{});var Ie=t(ns);cp=l(Ie,"I campioni "),Qn=i(Ie,"CODE",{});var Qu=t(Qn);mp=l(Qu,"nan"),Qu.forEach(e),up=l(Ie," hanno tutti la stessa label, ed \xE8 la classe 2. Questo \xE8 un indizio molto chiaro. Il fatto che si abbia una loss di "),Un=i(Ie,"CODE",{});var Uu=t(Un);dp=l(Uu,"nan"),Uu.forEach(e),hp=l(Ie," solo quando la label \xE8 2 suggerisce che questo \xE8 un ottimo momento per verificare il numero di label nel nostro modello:"),Ie.forEach(e),ko=m(s),b(Da.$$.fragment,s),Co=m(s),b(Oa.$$.fragment,s),Po=m(s),Qs=i(s,"P",{});var Nr=t(Qs);bp=l(Nr,"Ora vediamo il problema: il modello pensa che ci siano solo due classi, ma le label arrivano a 2, il che significa che in realt\xE0 ci sono tre classi (perch\xE9 anche lo 0 \xE8 una classe). Ecco come abbiamo ottenuto un "),Gn=i(Nr,"CODE",{});var Gu=t(Gn);fp=l(Gu,"nan"),Gu.forEach(e),jp=l(Nr,": cercando di calcolare la loss per una classe inesistente! Proviamo a cambiare il modello e ad adattarlo di nuovo:"),Nr.forEach(e),Ao=m(s),b(Na.$$.fragment,s),So=m(s),b(Ia.$$.fragment,s),To=m(s),Us=i(s,"P",{});var Ir=t(Us);vp=l(Ir,"Staimo addestrando! Non ci sono pi\xF9 "),xn=i(Ir,"CODE",{});var xu=t(xn);gp=l(xu,"nan"),xu.forEach(e),$p=l(Ir," e la nostra loss sta diminuendo\u2026 pi\xF9 o meno. Se la si osserva per un po\u2019, si potrebbe iniziare a spazientirsi, perch\xE9 il valore della loss rimane ostinatamente alto. Interrompiamo il training e cerchiamo di capire quale potrebbe essere la causa di questo problema. A questo punto, siamo abbastanza sicuri che sia i dati che il modello siano a posto, ma il nostro modello non sta imparando bene. Cos\u2019altro rimane? \xC8 ora di\u2026"),Ir.forEach(e),Do=m(s),bs=i(s,"H3",{class:!0});var Mr=t(bs);Gs=i(Mr,"A",{id:!0,class:!0,href:!0});var Lu=t(Gs);Ln=i(Lu,"SPAN",{});var Ku=t(Ln);b(Ma.$$.fragment,Ku),Ku.forEach(e),Lu.forEach(e),_p=m(Mr),Kn=i(Mr,"SPAN",{});var Bu=t(Kn);Ep=l(Bu,"Controllare gli iperparametri"),Bu.forEach(e),Mr.forEach(e),Oo=m(s),U=i(s,"P",{});var oa=t(U);zp=l(oa,"Se si guarda al codice precedente, \xE8 possibile che non si riesca a vedere alcun iperparametro, a parte forse il "),Bn=i(oa,"CODE",{});var Hu=t(Bn);qp=l(Hu,"batch_size"),Hu.forEach(e),wp=l(oa,", e questo non sembra un possibile problema. Non lasciarti ingannare, per\xF2: gli iperparametri ci sono sempre e se non li vedi significa che non conosci il valore a cui sono impostati. In particolare, ricorda una cosa fondamentale di Keras: se imposti una loss, un optimizer ("),Hn=i(oa,"EM",{});var Ru=t(Hn);yp=l(Ru,"ottimizzatore"),Ru.forEach(e),kp=l(oa,") o una funzione di attivazione con una stringa, "),Rn=i(oa,"EM",{});var Vu=t(Rn);Cp=l(Vu,"tutti i suoi argomenti saranno impostati ai loro valori predefiniti"),Vu.forEach(e),Pp=l(oa,". Ci\xF2 significa che, anche se usare le stringhe \xE8 molto comodo, bisogna fare molta attenzione, perch\xE9 questa cosa potrebbe facilmente nascondere alcuni aspetti importanti. (Chiunque si cimenti nella sfida opzionale qui sopra dovrebbe prendere nota di questo fatto)."),oa.forEach(e),No=m(s),xs=i(s,"P",{});var Fr=t(xs);Ap=l(Fr,"In questo caso, dove abbiamo impostato un argomento con una stringa? Inizialmente settavamo la loss con una stringa, ma ora non lo facciamo pi\xF9. Tuttavia, impostiamo l\u2019optimizer usando una stringa. Potrebbe nasconderci qualcosa? Diamo un\u2019occhiata ai "),Fa=i(Fr,"A",{href:!0,rel:!0});var Ju=t(Fa);Sp=l(Ju,"suoi argomenti"),Ju.forEach(e),Tp=l(Fr,"."),Fr.forEach(e),Io=m(s),G=i(s,"P",{});var ra=t(G);Dp=l(ra,"C\u2019\xE8 qualcosa che balza all\u2019occhio? Esatto: il learning rate ("),Vn=i(ra,"EM",{});var Wu=t(Vn);Op=l(Wu,"tasso di apprendimento"),Wu.forEach(e),Np=l(ra,")! Quando usiamo semplicemente la stringa "),Jn=i(ra,"CODE",{});var Xu=t(Jn);Ip=l(Xu,"'adam'"),Xu.forEach(e),Mp=l(ra,", otterremo il tasso di apprendimento predefinito, che \xE8 0,001, o 1e-3. Questo \xE8 decisamente troppo alto per un modello Transformer! In generale, si consiglia di provare learning rate tra 1e-5 e 1e-4 per i modelli; si tratta di un valore tra 10 e 100 volte inferiore a quello che stiamo usando qui. Questo sembra essere un problema importante, quindi proviamo a ridurlo. Per farlo, dobbiamo importare l\u2019oggetto "),Wn=i(ra,"CODE",{});var Yu=t(Wn);Fp=l(Yu,"optimizer"),Yu.forEach(e),Qp=l(ra,". Gi\xE0 che ci siamo, reinizializziamo il modello dal checkpoint, nel caso in cui il training con un learning rate elevato abbia compromesso i suoi pesi:"),ra.forEach(e),Mo=m(s),b(Qa.$$.fragment,s),Fo=m(s),b(Ls.$$.fragment,s),Qo=m(s),fe=i(s,"P",{});var Zu=t(fe);Up=l(Zu,"Adess, possiamo tentarde di fare training del modell con il nuovo learning rate migliorato:"),Zu.forEach(e),Uo=m(s),b(Ua.$$.fragment,s),Go=m(s),b(Ga.$$.fragment,s),xo=m(s),je=i(s,"P",{});var s0=t(je);Gp=l(s0,"Ora la nostra loss sta davvero andando da qualche parte! L\u2019addestramento sembra finalmente funzionare. C\u2019\xE8 una lezione da imparare: quando il modello funziona, ma la loss non diminuisce, e si \xE8 sicuri che i dati siano corretti, \xE8 una buona idea controllare gli iperparametri come il learning rate e il weight decay. Impostando uno di questi parametri troppo alto, \xE8 molto probabile che l\u2019addestramento si \u201Cblocchi\u201D a un valore di loss elevato."),s0.forEach(e),Lo=m(s),fs=i(s,"H2",{class:!0});var Qr=t(fs);Ks=i(Qr,"A",{id:!0,class:!0,href:!0});var a0=t(Ks);Xn=i(a0,"SPAN",{});var e0=t(Xn);b(xa.$$.fragment,e0),e0.forEach(e),a0.forEach(e),xp=m(Qr),Yn=i(Qr,"SPAN",{});var n0=t(Yn);Lp=l(n0,"Altri potenziali problemi"),n0.forEach(e),Qr.forEach(e),Ko=m(s),ve=i(s,"P",{});var l0=t(ve);Kp=l(l0,"Abbiamo trattato i problemi dello script di cui sopra, ma ci sono molti altri errori comuni che si possono incontrare. Vediamo un elenco (molto incompleto)."),l0.forEach(e),Bo=m(s),js=i(s,"H3",{class:!0});var Ur=t(js);Bs=i(Ur,"A",{id:!0,class:!0,href:!0});var o0=t(Bs);Zn=i(o0,"SPAN",{});var r0=t(Zn);b(La.$$.fragment,r0),r0.forEach(e),o0.forEach(e),Bp=m(Ur),sl=i(Ur,"SPAN",{});var i0=t(sl);Hp=l(i0,"Gestire gli errori out-of-memory"),i0.forEach(e),Ur.forEach(e),Ho=m(s),ls=i(s,"P",{});var Me=t(ls);Rp=l(Me,"Il segnale che indica che la memoria \xE8 esaurita \xE8 un errore del tipo \u201COOM when allocating tensor\u201D (OOM \xE8 l\u2019abbreviazione di \u201Cout of memory\u201D). Si tratta di un rischio molto comune quando si ha a che fare con modelli linguistici di grandi dimensioni. In questo caso, una buona strategia \xE8 quella di dimezzare le dimensioni del batch e riprovare. Tenete presente, per\xF2, che alcuni modelli sono "),al=i(Me,"EM",{});var t0=t(al);Vp=l(t0,"molto"),t0.forEach(e),Jp=l(Me," grandi. Ad esempio, il modello GPT-2 completo ha 1,5B parametri, il che significa che sono necessari 6 GB di memoria solo per memorizzare il modello e altri 6 GB per i suoi gradienti! L\u2019addestramento del modello GPT-2 completo richiede di solito oltre 20 GB di VRAM, indipendentemente dalla dimensione del batch utilizzato, che solo poche GPU hanno. Modelli pi\xF9 leggeri come "),el=i(Me,"CODE",{});var p0=t(el);Wp=l(p0,"distilbert-base-cased"),p0.forEach(e),Xp=l(Me," sono molto pi\xF9 facili da eseguire e si addestrano molto pi\xF9 rapidamente."),Me.forEach(e),Ro=m(s),b(Hs.$$.fragment,s),Vo=m(s),vs=i(s,"H3",{class:!0});var Gr=t(vs);Rs=i(Gr,"A",{id:!0,class:!0,href:!0});var c0=t(Rs);nl=i(c0,"SPAN",{});var m0=t(nl);b(Ka.$$.fragment,m0),m0.forEach(e),c0.forEach(e),Yp=m(Gr),ll=i(Gr,"SPAN",{});var u0=t(ll);Zp=l(u0,"TensorFlow \xE8 molto affamato \u{1F99B}"),u0.forEach(e),Gr.forEach(e),Jo=m(s),os=i(s,"P",{});var Fe=t(os);sc=l(Fe,"Una particolarit\xE0 di TensorFlow di cui bisogna essere consapevoli \xE8 che alloca "),ol=i(Fe,"EM",{});var d0=t(ol);ac=l(d0,"tutta"),d0.forEach(e),ec=l(Fe," la memoria della GPU su se stesso non appena si carica un modello o si esegue un addestramento, e poi divide la memoria in base alle esigenze. Questo comportamento \xE8 diverso da quello di altri framework, come PyTorch, che allocano la memoria come richiesto con CUDA invece di farlo internamente. Un vantaggio dell\u2019approccio di TensorFlow \xE8 che spesso pu\xF2 produrre errori utili quando esaurisci la memoria e pu\xF2 recuperare da questo stato senza mandare in crash l\u2019intero kernel CUDA. Ma c\u2019\xE8 anche un importante aspetto negativo: se si eseguono due processi TensorFlow contemporaneamente, allora "),rl=i(Fe,"STRONG",{});var h0=t(rl);nc=l(h0,"sar\xE0 un bel guaio"),h0.forEach(e),lc=l(Fe,"."),Fe.forEach(e),Wo=m(s),ge=i(s,"P",{});var b0=t(ge);oc=l(b0,"Se si esegue su Colab non ci si deve preoccupare di questo, ma se si lavora in locale \xE8 sicuramente qualcosa a cui si deve fare attenzione. In particolare, \xE8 bene ricordare che la chiusura della scheda di un notebook non comporta necessariamente la chiusura del notebook stesso! Potresti dover selezionare i notebook in esecuzione (quelli con l\u2019icona verde) e chiuderli manualmente nell\u2019elenco della directory. Qualsiasi notebook in esecuzione che utilizzava TensorFlow potrebbe ancora conservare una buona parte della memoria della GPU e ci\xF2 significa che qualsiasi nuovo notebook avviato potrebbe presentare problemi molto strani."),b0.forEach(e),Xo=m(s),Vs=i(s,"P",{});var xr=t(Vs);rc=l(xr,"Se inizi a ricevere errori relativi a CUDA, BLAS o cuBLAS in un codice che prima funzionava, questa \xE8 molto spesso la ragione. Si pu\xF2 usare un comando come "),il=i(xr,"CODE",{});var f0=t(il);ic=l(f0,"nvidia-smi"),f0.forEach(e),tc=l(xr," per controllare: quando si spegne o si riavvia il notebook usato, la maggior parte della memoria \xE8 libera o \xE8 ancora in uso? Se \xE8 ancora in uso, c\u2019\xE8 qualcos\u2019altro che la sta occupando!"),xr.forEach(e),Yo=m(s),gs=i(s,"H3",{class:!0});var Lr=t(gs);Js=i(Lr,"A",{id:!0,class:!0,href:!0});var j0=t(Js);tl=i(j0,"SPAN",{});var v0=t(tl);b(Ba.$$.fragment,v0),v0.forEach(e),j0.forEach(e),pc=m(Lr),pl=i(Lr,"SPAN",{});var g0=t(pl);cc=l(g0,"Check your data (again!)"),g0.forEach(e),Lr.forEach(e),Zo=m(s),N=i(s,"P",{});var ts=t(N);mc=l(ts,"Il tuo modello imparer\xE0 qualcosa solo se \xE8 effettivamente possibile imparare qualcosa dai tuoi dati. Se c\u2019\xE8 un bug che corrompe i dati o le label sono assegnate in modo casuale, \xE8 molto probabile che non si riesca ad addestrare il modello sul dataset. In questo caso uno strumento utile \xE8 "),cl=i(ts,"CODE",{});var $0=t(cl);uc=l($0,"tokenizer.decode()"),$0.forEach(e),dc=l(ts,". Questo trasformer\xE0 gli "),ml=i(ts,"CODE",{});var _0=t(ml);hc=l(_0,"input_ids"),_0.forEach(e),bc=l(ts," in stringhe, in modo da poter visualizzare i dati e vedere se i dati di training stanno addestrando ci\xF2 che si vuole. Per esempio, dopo aver ottenuto un "),ul=i(ts,"CODE",{});var E0=t(ul);fc=l(E0,"batch"),E0.forEach(e),jc=l(ts," dal proprio "),dl=i(ts,"CODE",{});var z0=t(dl);vc=l(z0,"tf.data.Dataset"),z0.forEach(e),gc=l(ts," come abbiamo fatto sopra, si pu\xF2 decodificare il primo elemento in questo modo:"),ts.forEach(e),sr=m(s),b(Ha.$$.fragment,s),ar=m(s),$e=i(s,"P",{});var q0=t($e);$c=l(q0,"Poi si pu\xF2 confrontare con la prima label, in questo modo:"),q0.forEach(e),er=m(s),b(Ra.$$.fragment,s),nr=m(s),_e=i(s,"P",{});var w0=t(_e);_c=l(w0,"Una volta visualizzati i dati in questo modo, puoi porti le seguenti domande:"),w0.forEach(e),lr=m(s),x=i(s,"UL",{});var ia=t(x);hl=i(ia,"LI",{});var y0=t(hl);Ec=l(y0,"I dati decodificati sono comprensibili?"),y0.forEach(e),zc=m(ia),bl=i(ia,"LI",{});var k0=t(bl);qc=l(k0,"Sei d\u2019accordo con le label?"),k0.forEach(e),wc=m(ia),fl=i(ia,"LI",{});var C0=t(fl);yc=l(C0,"C\u2019\xE8 una label pi\xF9 comune delle altre?"),C0.forEach(e),kc=m(ia),jl=i(ia,"LI",{});var P0=t(jl);Cc=l(P0,"Quale dovrebbe essere la funzione di perdita/metrica se il modello predicesse una risposta a caso/sempre la stessa risposta?"),P0.forEach(e),ia.forEach(e),or=m(s),Ws=i(s,"P",{});var Kr=t(Ws);Pc=l(Kr,"Dopo aver osservato i dati, esamina alcune previsioni del modello: se il modello produce dei token, prova a decodificare anche quelli! Se il modello prevede sempre la stessa cosa, potrebbe essere perch\xE9 il tuo set di dati \xE8 influenzato verso una categoria (per i problemi di classificazione); tecniche come fare oversampling ("),vl=i(Kr,"EM",{});var A0=t(vl);Ac=l(A0,"sovra-campionamento"),A0.forEach(e),Sc=l(Kr,") delle classi rare potrebbero aiutare. In alternativa, ci\xF2 pu\xF2 essere causato da problemi di addestramento, come ad esempio una scorretta impostazione degli iperparametri."),Kr.forEach(e),rr=m(s),Ee=i(s,"P",{});var S0=t(Ee);Tc=l(S0,"Se la funzione di perdita/metrica ottenuta con il tuo modello iniziale \xE8 molto diversa da quella che ci si aspetterebbe per le previsioni casuali, ricontrolla il modo in cui viene calcolata la funzione o la metrica, perch\xE9 probabilmente c\u2019\xE8 un bug. Se si utilizzano diverse funzioni che aggiungi alla fine, assicurati che siano della stessa grandezza."),S0.forEach(e),ir=m(s),ze=i(s,"P",{});var T0=t(ze);Dc=l(T0,"Quando sei sicuro/a che i dati sono perfetti, puoi verificare se il modello \xE8 in grado di addestrarsi su di essi con un semplice test."),T0.forEach(e),tr=m(s),$s=i(s,"H3",{class:!0});var Br=t($s);Xs=i(Br,"A",{id:!0,class:!0,href:!0});var D0=t(Xs);gl=i(D0,"SPAN",{});var O0=t(gl);b(Va.$$.fragment,O0),O0.forEach(e),D0.forEach(e),Oc=m(Br),$l=i(Br,"SPAN",{});var N0=t($l);Nc=l(N0,"Fare overfitting del modello su un batch"),N0.forEach(e),Br.forEach(e),pr=m(s),qe=i(s,"P",{});var I0=t(qe);Ic=l(I0,"L\u2019overfitting \xE8 di solito qualcosa che cerchiamo di evitare durante l\u2019addestramento, poich\xE9 significa che il modello non sta imparando a riconoscere le propriet\xE0 generali che vogliamo, ma sta invece memorizzando i campioni di addestramento. Tuttavia, provare ad addestrare il modello su un batch pi\xF9 e pi\xF9 volte \xE8 un buon test per verificare se il problema cos\xEC come \xE8 stato inquadrato pu\xF2 essere risolto dal modello che si sta cercando di addestrare. Inoltre, ti aiuter\xE0 a capire se il learning rate iniziale \xE8 troppo alta."),I0.forEach(e),cr=m(s),rs=i(s,"P",{});var Qe=t(rs);Mc=l(Qe,"Una volta definito il "),_l=i(Qe,"CODE",{});var M0=t(_l);Fc=l(M0,"Trainer"),M0.forEach(e),Qc=l(Qe,", \xE8 molto semplice: basta prendere un batch dal training set, ed eseguire un piccolo ciclo di addestramento manuale utilizzando solo quel "),El=i(Qe,"CODE",{});var F0=t(El);Uc=l(F0,"batch"),F0.forEach(e),Gc=l(Qe," per qualcosa come 20 step:"),Qe.forEach(e),mr=m(s),b(Ja.$$.fragment,s),ur=m(s),b(Ys.$$.fragment,s),dr=m(s),Zs=i(s,"P",{});var Hr=t(Zs);xc=l(Hr,"Il modello risultante dovrebbe avere risultati quasi perfetti sul "),zl=i(Hr,"CODE",{});var Q0=t(zl);Lc=l(Q0,"batch"),Q0.forEach(e),Kc=l(Hr,", con una loss che diminuisce rapidamente verso lo 0 (o il valore minimo per la loss che si sta utilizzando)."),Hr.forEach(e),hr=m(s),we=i(s,"P",{});var U0=t(we);Bc=l(U0,"Se non si riesci a far s\xEC che il modello ottenga risultati perfetti come questo, significa che c\u2019\xE8 qualcosa di sbagliato nel modo in cui si \xE8 impostato il problema o con i dati, e quindi dovresti risolvere questa cosa. Solo quando riesci a superare il test di overfitting puoi essere sicuro/a che il tuo modello possa effettivamente imparare qualcosa."),U0.forEach(e),br=m(s),b(sa.$$.fragment,s),fr=m(s),_s=i(s,"H3",{class:!0});var Rr=t(_s);aa=i(Rr,"A",{id:!0,class:!0,href:!0});var G0=t(aa);ql=i(G0,"SPAN",{});var x0=t(ql);b(Wa.$$.fragment,x0),x0.forEach(e),G0.forEach(e),Hc=m(Rr),wl=i(Rr,"SPAN",{});var L0=t(wl);Rc=l(L0,"Non calibrare niente prima di avere una prima baseline"),L0.forEach(e),Rr.forEach(e),jr=m(s),L=i(s,"P",{});var ta=t(L);Vc=l(ta,"Hyperparameter tuning ("),yl=i(ta,"EM",{});var K0=t(yl);Jc=l(K0,"calibrazione degli iperparametri"),K0.forEach(e),Wc=l(ta,") \xE8 sempre considerato come la parte pi\xF9 difficile di machine learning, ma \xE8 solo l\u2019ultimo passo per aiutarti a migliorare un po\u2019 la metrica. Valori "),kl=i(ta,"EM",{});var B0=t(kl);Xc=l(B0,"molto"),B0.forEach(e),Yc=l(ta," sbagliati di iperparametri, come l\u2019uso del learning rate predefinito di Adam di 1e-3 con un modello Transformer, faranno s\xEC che l\u2019apprendimento proceda molto lentamente o si blocchi completamente, naturalmente, ma la maggior parte delle volte iperparametri \u201Cragionevoli\u201D, come un learning rate da 1e-5 a 5e-5, funzioneranno bene per darti buoni risultati. Quindi, non ci si deve lanciare in una ricerca di iperparametri dispendiosa in termini di tempo e di costi, finch\xE9 non si \xE8 ottenuto qualcosa che batta la baseline ("),Cl=i(ta,"EM",{});var H0=t(Cl);Zc=l(H0,"base di partenza"),H0.forEach(e),sm=l(ta,") che si ha sul dataset."),ta.forEach(e),vr=m(s),ye=i(s,"P",{});var R0=t(ye);am=l(R0,"Una volta ottenuto un modello sufficientemente buono, si pu\xF2 iniziare a modificarlo un po\u2019. Non provare a eseguire l\u2019addestramento un migliaio di volte con iperparametri diversi, ma confronta un paio di esecuzioni che hanno valori diversi per un iperparametro cos\xEC da avere un\u2019idea di quale abbia il maggiore impatto."),R0.forEach(e),gr=m(s),ke=i(s,"P",{});var V0=t(ke);em=l(V0,"Se stai modificando il modello stesso, mantieni le cose semplici e non provare nulla che non possa essere ragionevolmente giustificato. Assicurati sempre di rifare il test di overfitting per verificare che la modifica non abbia avuto conseguenze indesiderate."),V0.forEach(e),$r=m(s),Es=i(s,"H3",{class:!0});var Vr=t(Es);ea=i(Vr,"A",{id:!0,class:!0,href:!0});var J0=t(ea);Pl=i(J0,"SPAN",{});var W0=t(Pl);b(Xa.$$.fragment,W0),W0.forEach(e),J0.forEach(e),nm=m(Vr),Al=i(Vr,"SPAN",{});var X0=t(Al);lm=l(X0,"Chiedere aiuto"),X0.forEach(e),Vr.forEach(e),_r=m(s),na=i(s,"P",{});var Jr=t(na);om=l(Jr,"Speriamo che in questa sezione tu abbia trovato qualche consiglio utile a risolvere il tuo problema, ma se cos\xEC non fosse, ricordati che puoi sempre chiedere aiuto alla community nei "),Ya=i(Jr,"A",{href:!0,rel:!0});var Y0=t(Ya);rm=l(Y0,"forum"),Y0.forEach(e),im=l(Jr,"."),Jr.forEach(e),Er=m(s),Ce=i(s,"P",{});var Z0=t(Ce);tm=l(Z0,"Qui di seguito sono riportate alcune risorse aggiuntive che potrebbero rivelarsi utili:"),Z0.forEach(e),zr=m(s),K=i(s,"UL",{});var pa=t(K);Pe=i(pa,"LI",{});var Em=t(Pe);Za=i(Em,"A",{href:!0,rel:!0});var sd=t(Za);pm=l(sd,"\u201CReproducibility as a vehicle for engineering best practices\u201D"),sd.forEach(e),cm=l(Em," di Joel Grus"),Em.forEach(e),mm=m(pa),Ae=i(pa,"LI",{});var zm=t(Ae);se=i(zm,"A",{href:!0,rel:!0});var ad=t(se);um=l(ad,"\u201CChecklist for debugging neural networks\u201D"),ad.forEach(e),dm=l(zm," di Cecelia Shao"),zm.forEach(e),hm=m(pa),Se=i(pa,"LI",{});var qm=t(Se);ae=i(qm,"A",{href:!0,rel:!0});var ed=t(ae);bm=l(ed,"\u201CHow to unit test machine learning code\u201D"),ed.forEach(e),fm=l(qm," di Chase Roberts"),qm.forEach(e),jm=m(pa),Te=i(pa,"LI",{});var wm=t(Te);ee=i(wm,"A",{href:!0,rel:!0});var nd=t(ee);vm=l(nd,"\u201CA Recipe for Training Neural Networks\u201D"),nd.forEach(e),gm=l(wm," di Andrej Karpathy"),wm.forEach(e),pa.forEach(e),qr=m(s),De=i(s,"P",{});var ld=t(De);$m=l(ld,"Naturalmente, non tutti i problemi che incontrerai durante l\u2019addestramento delle reti neurali sono colpa tua! Se si incontra qualcosa nella libreria \u{1F917} Transformers o \u{1F917} Datasets che non sembra corretto, \xE8 possibile che si sia trovato un bug. Dovresti assolutamente segnalarcelo e nella prossima sezione ti spiegheremo esattamente come fare."),ld.forEach(e),this.h()},h(){u(d,"name","hf:doc:metadata"),u(d,"content",JSON.stringify(gd)),u(C,"id","fare-il-debug-di-una-training-pipeline"),u(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(C,"href","#fare-il-debug-di-una-training-pipeline"),u(A,"class","relative group"),u(X,"href","/course/chapter7"),u(k,"id","debugging-the-training-pipeline"),u(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(k,"href","#debugging-the-training-pipeline"),u(q,"class","relative group"),u(da,"href","https://huggingface.co/datasets/glue"),u(da,"rel","nofollow"),u(Os,"id","controllare-i-dati"),u(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Os,"href","#controllare-i-dati"),u(ds,"class","relative group"),u(Fs,"id","controllare-il-modello"),u(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Fs,"href","#controllare-il-modello"),u(hs,"class","relative group"),u(Gs,"id","controllare-gli-iperparametri"),u(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Gs,"href","#controllare-gli-iperparametri"),u(bs,"class","relative group"),u(Fa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"),u(Fa,"rel","nofollow"),u(Ks,"id","altri-potenziali-problemi"),u(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ks,"href","#altri-potenziali-problemi"),u(fs,"class","relative group"),u(Bs,"id","gestire-gli-errori-outofmemory"),u(Bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Bs,"href","#gestire-gli-errori-outofmemory"),u(js,"class","relative group"),u(Rs,"id","tensorflow-molto-affamato"),u(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Rs,"href","#tensorflow-molto-affamato"),u(vs,"class","relative group"),u(Js,"id","check-your-data-again"),u(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Js,"href","#check-your-data-again"),u(gs,"class","relative group"),u(Xs,"id","fare-overfitting-del-modello-su-un-batch"),u(Xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Xs,"href","#fare-overfitting-del-modello-su-un-batch"),u($s,"class","relative group"),u(aa,"id","non-calibrare-niente-prima-di-avere-una-prima-baseline"),u(aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(aa,"href","#non-calibrare-niente-prima-di-avere-una-prima-baseline"),u(_s,"class","relative group"),u(ea,"id","chiedere-aiuto"),u(ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ea,"href","#chiedere-aiuto"),u(Es,"class","relative group"),u(Ya,"href","https://discuss.huggingface.co/"),u(Ya,"rel","nofollow"),u(Za,"href","https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p"),u(Za,"rel","nofollow"),u(se,"href","https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21"),u(se,"rel","nofollow"),u(ae,"href","https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765"),u(ae,"rel","nofollow"),u(ee,"href","http://karpathy.github.io/2019/04/25/recipe/"),u(ee,"rel","nofollow")},m(s,o){a(document.head,d),p(s,w,o),f($,s,o),p(s,z,o),p(s,A,o),a(A,C),a(C,S),f(D,S,null),a(A,ps),a(A,cs),a(cs,ca),p(s,W,o),f(H,s,o),p(s,qs,o),p(s,T,o),a(T,F),a(T,X),a(X,ms),a(T,ma),a(T,us),a(us,ws),a(T,Y),p(s,ys,o),p(s,q,o),a(q,k),a(k,ks),f(Z,ks,null),a(q,ie),a(q,Cs),a(Cs,Ps),p(s,ua,o),f(ss,s,o),p(s,Il,o),p(s,As,o),a(As,Wr),a(As,Ue),a(Ue,Xr),a(As,Yr),p(s,Ml,o),p(s,Ss,o),a(Ss,Zr),a(Ss,Ge),a(Ge,si),a(Ss,ai),p(s,Fl,o),p(s,Ts,o),a(Ts,ei),a(Ts,da),a(da,ni),a(Ts,li),p(s,Ql,o),f(ha,s,o),p(s,Ul,o),p(s,Ds,o),a(Ds,oi),a(Ds,xe),a(xe,ri),a(Ds,ii),p(s,Gl,o),p(s,te,o),a(te,ti),p(s,xl,o),f(ba,s,o),p(s,Ll,o),p(s,pe,o),a(pe,pi),p(s,Kl,o),p(s,ds,o),a(ds,Os),a(Os,Le),f(fa,Le,null),a(ds,ci),a(ds,Ke),a(Ke,mi),p(s,Bl,o),p(s,ce,o),a(ce,ui),p(s,Hl,o),p(s,P,o),a(P,di),a(P,Be),a(Be,hi),a(P,bi),a(P,He),a(He,fi),a(P,ji),a(P,Re),a(Re,vi),a(P,gi),a(P,Ve),a(Ve,$i),a(P,_i),a(P,Je),a(Je,Ei),a(P,zi),a(P,We),a(We,qi),a(P,wi),p(s,Rl,o),f(ja,s,o),p(s,Vl,o),p(s,R,o),a(R,Xe),a(Xe,yi),a(R,ki),a(R,Ye),a(Ye,Ci),a(R,Pi),a(R,Ze),a(Ze,Ai),a(R,Si),p(s,Jl,o),f(va,s,o),p(s,Wl,o),p(s,O,o),a(O,Ti),a(O,sn),a(sn,Di),a(O,Oi),a(O,an),a(an,Ni),a(O,Ii),a(O,en),a(en,Mi),a(O,Fi),a(O,nn),a(nn,Qi),a(O,Ui),p(s,Xl,o),p(s,Ns,o),a(Ns,Gi),a(Ns,ln),a(ln,xi),a(Ns,Li),p(s,Yl,o),p(s,as,o),a(as,Ki),a(as,on),a(on,Bi),a(as,Hi),a(as,rn),a(rn,Ri),a(as,Vi),p(s,Zl,o),f(ga,s,o),p(s,so,o),p(s,me,o),a(me,Ji),p(s,ao,o),f(Is,s,o),p(s,eo,o),p(s,Ms,o),a(Ms,Wi),a(Ms,tn),a(tn,Xi),a(Ms,Yi),p(s,no,o),f($a,s,o),p(s,lo,o),p(s,ue,o),a(ue,Zi),p(s,oo,o),p(s,_a,o),a(_a,pn),a(pn,st),a(_a,at),p(s,ro,o),p(s,hs,o),a(hs,Fs),a(Fs,cn),f(Ea,cn,null),a(hs,et),a(hs,mn),a(mn,nt),p(s,io,o),p(s,V,o),a(V,un),a(un,lt),a(V,ot),a(V,dn),a(dn,rt),a(V,it),a(V,hn),a(hn,tt),a(V,pt),p(s,to,o),p(s,es,o),a(es,ct),a(es,bn),a(bn,mt),a(es,ut),a(es,fn),a(fn,dt),a(es,ht),p(s,po,o),f(za,s,o),p(s,co,o),f(qa,s,o),p(s,mo,o),p(s,_,o),a(_,bt),a(_,jn),a(jn,ft),a(_,jt),a(_,vn),a(vn,vt),a(_,gt),a(_,gn),a(gn,$t),a(_,_t),a(_,$n),a($n,Et),a(_,zt),a(_,_n),a(_n,qt),a(_,wt),a(_,En),a(En,yt),a(_,kt),a(_,zn),a(zn,Ct),a(_,Pt),a(_,qn),a(qn,At),a(_,St),a(_,wn),a(wn,Tt),a(_,Dt),a(_,yn),a(yn,Ot),a(_,Nt),a(_,kn),a(kn,It),a(_,Mt),a(_,Cn),a(Cn,Ft),a(_,Qt),a(_,Pn),a(Pn,Ut),a(_,Gt),a(_,An),a(An,xt),a(_,Lt),a(_,Sn),a(Sn,Kt),a(_,Bt),a(_,Tn),a(Tn,Ht),a(_,Rt),p(s,uo,o),p(s,Q,o),a(Q,Vt),a(Q,Dn),a(Dn,Jt),a(Q,Wt),a(Q,On),a(On,Xt),a(Q,Yt),a(Q,Nn),a(Nn,Zt),a(Q,sp),p(s,ho,o),f(wa,s,o),p(s,bo,o),p(s,de,o),a(de,ap),p(s,fo,o),f(ya,s,o),p(s,jo,o),p(s,J,o),a(J,In),a(In,ep),a(J,np),a(J,Mn),a(Mn,lp),a(J,op),a(J,Fn),a(Fn,rp),a(J,ip),p(s,vo,o),f(ka,s,o),p(s,go,o),f(Ca,s,o),p(s,$o,o),p(s,he,o),a(he,tp),p(s,_o,o),f(Pa,s,o),p(s,Eo,o),f(Aa,s,o),p(s,zo,o),p(s,be,o),a(be,pp),p(s,qo,o),f(Sa,s,o),p(s,wo,o),f(Ta,s,o),p(s,yo,o),p(s,ns,o),a(ns,cp),a(ns,Qn),a(Qn,mp),a(ns,up),a(ns,Un),a(Un,dp),a(ns,hp),p(s,ko,o),f(Da,s,o),p(s,Co,o),f(Oa,s,o),p(s,Po,o),p(s,Qs,o),a(Qs,bp),a(Qs,Gn),a(Gn,fp),a(Qs,jp),p(s,Ao,o),f(Na,s,o),p(s,So,o),f(Ia,s,o),p(s,To,o),p(s,Us,o),a(Us,vp),a(Us,xn),a(xn,gp),a(Us,$p),p(s,Do,o),p(s,bs,o),a(bs,Gs),a(Gs,Ln),f(Ma,Ln,null),a(bs,_p),a(bs,Kn),a(Kn,Ep),p(s,Oo,o),p(s,U,o),a(U,zp),a(U,Bn),a(Bn,qp),a(U,wp),a(U,Hn),a(Hn,yp),a(U,kp),a(U,Rn),a(Rn,Cp),a(U,Pp),p(s,No,o),p(s,xs,o),a(xs,Ap),a(xs,Fa),a(Fa,Sp),a(xs,Tp),p(s,Io,o),p(s,G,o),a(G,Dp),a(G,Vn),a(Vn,Op),a(G,Np),a(G,Jn),a(Jn,Ip),a(G,Mp),a(G,Wn),a(Wn,Fp),a(G,Qp),p(s,Mo,o),f(Qa,s,o),p(s,Fo,o),f(Ls,s,o),p(s,Qo,o),p(s,fe,o),a(fe,Up),p(s,Uo,o),f(Ua,s,o),p(s,Go,o),f(Ga,s,o),p(s,xo,o),p(s,je,o),a(je,Gp),p(s,Lo,o),p(s,fs,o),a(fs,Ks),a(Ks,Xn),f(xa,Xn,null),a(fs,xp),a(fs,Yn),a(Yn,Lp),p(s,Ko,o),p(s,ve,o),a(ve,Kp),p(s,Bo,o),p(s,js,o),a(js,Bs),a(Bs,Zn),f(La,Zn,null),a(js,Bp),a(js,sl),a(sl,Hp),p(s,Ho,o),p(s,ls,o),a(ls,Rp),a(ls,al),a(al,Vp),a(ls,Jp),a(ls,el),a(el,Wp),a(ls,Xp),p(s,Ro,o),f(Hs,s,o),p(s,Vo,o),p(s,vs,o),a(vs,Rs),a(Rs,nl),f(Ka,nl,null),a(vs,Yp),a(vs,ll),a(ll,Zp),p(s,Jo,o),p(s,os,o),a(os,sc),a(os,ol),a(ol,ac),a(os,ec),a(os,rl),a(rl,nc),a(os,lc),p(s,Wo,o),p(s,ge,o),a(ge,oc),p(s,Xo,o),p(s,Vs,o),a(Vs,rc),a(Vs,il),a(il,ic),a(Vs,tc),p(s,Yo,o),p(s,gs,o),a(gs,Js),a(Js,tl),f(Ba,tl,null),a(gs,pc),a(gs,pl),a(pl,cc),p(s,Zo,o),p(s,N,o),a(N,mc),a(N,cl),a(cl,uc),a(N,dc),a(N,ml),a(ml,hc),a(N,bc),a(N,ul),a(ul,fc),a(N,jc),a(N,dl),a(dl,vc),a(N,gc),p(s,sr,o),f(Ha,s,o),p(s,ar,o),p(s,$e,o),a($e,$c),p(s,er,o),f(Ra,s,o),p(s,nr,o),p(s,_e,o),a(_e,_c),p(s,lr,o),p(s,x,o),a(x,hl),a(hl,Ec),a(x,zc),a(x,bl),a(bl,qc),a(x,wc),a(x,fl),a(fl,yc),a(x,kc),a(x,jl),a(jl,Cc),p(s,or,o),p(s,Ws,o),a(Ws,Pc),a(Ws,vl),a(vl,Ac),a(Ws,Sc),p(s,rr,o),p(s,Ee,o),a(Ee,Tc),p(s,ir,o),p(s,ze,o),a(ze,Dc),p(s,tr,o),p(s,$s,o),a($s,Xs),a(Xs,gl),f(Va,gl,null),a($s,Oc),a($s,$l),a($l,Nc),p(s,pr,o),p(s,qe,o),a(qe,Ic),p(s,cr,o),p(s,rs,o),a(rs,Mc),a(rs,_l),a(_l,Fc),a(rs,Qc),a(rs,El),a(El,Uc),a(rs,Gc),p(s,mr,o),f(Ja,s,o),p(s,ur,o),f(Ys,s,o),p(s,dr,o),p(s,Zs,o),a(Zs,xc),a(Zs,zl),a(zl,Lc),a(Zs,Kc),p(s,hr,o),p(s,we,o),a(we,Bc),p(s,br,o),f(sa,s,o),p(s,fr,o),p(s,_s,o),a(_s,aa),a(aa,ql),f(Wa,ql,null),a(_s,Hc),a(_s,wl),a(wl,Rc),p(s,jr,o),p(s,L,o),a(L,Vc),a(L,yl),a(yl,Jc),a(L,Wc),a(L,kl),a(kl,Xc),a(L,Yc),a(L,Cl),a(Cl,Zc),a(L,sm),p(s,vr,o),p(s,ye,o),a(ye,am),p(s,gr,o),p(s,ke,o),a(ke,em),p(s,$r,o),p(s,Es,o),a(Es,ea),a(ea,Pl),f(Xa,Pl,null),a(Es,nm),a(Es,Al),a(Al,lm),p(s,_r,o),p(s,na,o),a(na,om),a(na,Ya),a(Ya,rm),a(na,im),p(s,Er,o),p(s,Ce,o),a(Ce,tm),p(s,zr,o),p(s,K,o),a(K,Pe),a(Pe,Za),a(Za,pm),a(Pe,cm),a(K,mm),a(K,Ae),a(Ae,se),a(se,um),a(Ae,dm),a(K,hm),a(K,Se),a(Se,ae),a(ae,bm),a(Se,fm),a(K,jm),a(K,Te),a(Te,ee),a(ee,vm),a(Te,gm),p(s,qr,o),p(s,De,o),a(De,$m),wr=!0},p(s,[o]){const ne={};o&1&&(ne.fw=s[0]),$.$set(ne);const Sl={};o&2&&(Sl.$$scope={dirty:o,ctx:s}),Is.$set(Sl);const Tl={};o&2&&(Tl.$$scope={dirty:o,ctx:s}),Ls.$set(Tl);const Dl={};o&2&&(Dl.$$scope={dirty:o,ctx:s}),Hs.$set(Dl);const zs={};o&2&&(zs.$$scope={dirty:o,ctx:s}),Ys.$set(zs);const Ol={};o&2&&(Ol.$$scope={dirty:o,ctx:s}),sa.$set(Ol)},i(s){wr||(j($.$$.fragment,s),j(D.$$.fragment,s),j(H.$$.fragment,s),j(Z.$$.fragment,s),j(ss.$$.fragment,s),j(ha.$$.fragment,s),j(ba.$$.fragment,s),j(fa.$$.fragment,s),j(ja.$$.fragment,s),j(va.$$.fragment,s),j(ga.$$.fragment,s),j(Is.$$.fragment,s),j($a.$$.fragment,s),j(Ea.$$.fragment,s),j(za.$$.fragment,s),j(qa.$$.fragment,s),j(wa.$$.fragment,s),j(ya.$$.fragment,s),j(ka.$$.fragment,s),j(Ca.$$.fragment,s),j(Pa.$$.fragment,s),j(Aa.$$.fragment,s),j(Sa.$$.fragment,s),j(Ta.$$.fragment,s),j(Da.$$.fragment,s),j(Oa.$$.fragment,s),j(Na.$$.fragment,s),j(Ia.$$.fragment,s),j(Ma.$$.fragment,s),j(Qa.$$.fragment,s),j(Ls.$$.fragment,s),j(Ua.$$.fragment,s),j(Ga.$$.fragment,s),j(xa.$$.fragment,s),j(La.$$.fragment,s),j(Hs.$$.fragment,s),j(Ka.$$.fragment,s),j(Ba.$$.fragment,s),j(Ha.$$.fragment,s),j(Ra.$$.fragment,s),j(Va.$$.fragment,s),j(Ja.$$.fragment,s),j(Ys.$$.fragment,s),j(sa.$$.fragment,s),j(Wa.$$.fragment,s),j(Xa.$$.fragment,s),wr=!0)},o(s){v($.$$.fragment,s),v(D.$$.fragment,s),v(H.$$.fragment,s),v(Z.$$.fragment,s),v(ss.$$.fragment,s),v(ha.$$.fragment,s),v(ba.$$.fragment,s),v(fa.$$.fragment,s),v(ja.$$.fragment,s),v(va.$$.fragment,s),v(ga.$$.fragment,s),v(Is.$$.fragment,s),v($a.$$.fragment,s),v(Ea.$$.fragment,s),v(za.$$.fragment,s),v(qa.$$.fragment,s),v(wa.$$.fragment,s),v(ya.$$.fragment,s),v(ka.$$.fragment,s),v(Ca.$$.fragment,s),v(Pa.$$.fragment,s),v(Aa.$$.fragment,s),v(Sa.$$.fragment,s),v(Ta.$$.fragment,s),v(Da.$$.fragment,s),v(Oa.$$.fragment,s),v(Na.$$.fragment,s),v(Ia.$$.fragment,s),v(Ma.$$.fragment,s),v(Qa.$$.fragment,s),v(Ls.$$.fragment,s),v(Ua.$$.fragment,s),v(Ga.$$.fragment,s),v(xa.$$.fragment,s),v(La.$$.fragment,s),v(Hs.$$.fragment,s),v(Ka.$$.fragment,s),v(Ba.$$.fragment,s),v(Ha.$$.fragment,s),v(Ra.$$.fragment,s),v(Va.$$.fragment,s),v(Ja.$$.fragment,s),v(Ys.$$.fragment,s),v(sa.$$.fragment,s),v(Wa.$$.fragment,s),v(Xa.$$.fragment,s),wr=!1},d(s){e(d),s&&e(w),g($,s),s&&e(z),s&&e(A),g(D),s&&e(W),g(H,s),s&&e(qs),s&&e(T),s&&e(ys),s&&e(q),g(Z),s&&e(ua),g(ss,s),s&&e(Il),s&&e(As),s&&e(Ml),s&&e(Ss),s&&e(Fl),s&&e(Ts),s&&e(Ql),g(ha,s),s&&e(Ul),s&&e(Ds),s&&e(Gl),s&&e(te),s&&e(xl),g(ba,s),s&&e(Ll),s&&e(pe),s&&e(Kl),s&&e(ds),g(fa),s&&e(Bl),s&&e(ce),s&&e(Hl),s&&e(P),s&&e(Rl),g(ja,s),s&&e(Vl),s&&e(R),s&&e(Jl),g(va,s),s&&e(Wl),s&&e(O),s&&e(Xl),s&&e(Ns),s&&e(Yl),s&&e(as),s&&e(Zl),g(ga,s),s&&e(so),s&&e(me),s&&e(ao),g(Is,s),s&&e(eo),s&&e(Ms),s&&e(no),g($a,s),s&&e(lo),s&&e(ue),s&&e(oo),s&&e(_a),s&&e(ro),s&&e(hs),g(Ea),s&&e(io),s&&e(V),s&&e(to),s&&e(es),s&&e(po),g(za,s),s&&e(co),g(qa,s),s&&e(mo),s&&e(_),s&&e(uo),s&&e(Q),s&&e(ho),g(wa,s),s&&e(bo),s&&e(de),s&&e(fo),g(ya,s),s&&e(jo),s&&e(J),s&&e(vo),g(ka,s),s&&e(go),g(Ca,s),s&&e($o),s&&e(he),s&&e(_o),g(Pa,s),s&&e(Eo),g(Aa,s),s&&e(zo),s&&e(be),s&&e(qo),g(Sa,s),s&&e(wo),g(Ta,s),s&&e(yo),s&&e(ns),s&&e(ko),g(Da,s),s&&e(Co),g(Oa,s),s&&e(Po),s&&e(Qs),s&&e(Ao),g(Na,s),s&&e(So),g(Ia,s),s&&e(To),s&&e(Us),s&&e(Do),s&&e(bs),g(Ma),s&&e(Oo),s&&e(U),s&&e(No),s&&e(xs),s&&e(Io),s&&e(G),s&&e(Mo),g(Qa,s),s&&e(Fo),g(Ls,s),s&&e(Qo),s&&e(fe),s&&e(Uo),g(Ua,s),s&&e(Go),g(Ga,s),s&&e(xo),s&&e(je),s&&e(Lo),s&&e(fs),g(xa),s&&e(Ko),s&&e(ve),s&&e(Bo),s&&e(js),g(La),s&&e(Ho),s&&e(ls),s&&e(Ro),g(Hs,s),s&&e(Vo),s&&e(vs),g(Ka),s&&e(Jo),s&&e(os),s&&e(Wo),s&&e(ge),s&&e(Xo),s&&e(Vs),s&&e(Yo),s&&e(gs),g(Ba),s&&e(Zo),s&&e(N),s&&e(sr),g(Ha,s),s&&e(ar),s&&e($e),s&&e(er),g(Ra,s),s&&e(nr),s&&e(_e),s&&e(lr),s&&e(x),s&&e(or),s&&e(Ws),s&&e(rr),s&&e(Ee),s&&e(ir),s&&e(ze),s&&e(tr),s&&e($s),g(Va),s&&e(pr),s&&e(qe),s&&e(cr),s&&e(rs),s&&e(mr),g(Ja,s),s&&e(ur),g(Ys,s),s&&e(dr),s&&e(Zs),s&&e(hr),s&&e(we),s&&e(br),g(sa,s),s&&e(fr),s&&e(_s),g(Wa),s&&e(jr),s&&e(L),s&&e(vr),s&&e(ye),s&&e(gr),s&&e(ke),s&&e($r),s&&e(Es),g(Xa),s&&e(_r),s&&e(na),s&&e(Er),s&&e(Ce),s&&e(zr),s&&e(K),s&&e(qr),s&&e(De)}}}const gd={local:"fare-il-debug-di-una-training-pipeline",sections:[{local:"debugging-the-training-pipeline",sections:[{local:"controllare-i-dati",title:"Controllare i dati"},{local:"controllare-il-modello",title:"Controllare il modello"},{local:"controllare-gli-iperparametri",title:"Controllare gli iperparametri"}],title:"Debugging the training pipeline"},{local:"altri-potenziali-problemi",sections:[{local:"gestire-gli-errori-outofmemory",title:"Gestire gli errori out-of-memory"},{local:"tensorflow-molto-affamato",title:"TensorFlow \xE8 molto affamato \u{1F99B}"},{local:"check-your-data-again",title:"Check your data (again!)"},{local:"fare-overfitting-del-modello-su-un-batch",title:"Fare overfitting del modello su un batch"},{local:"non-calibrare-niente-prima-di-avere-una-prima-baseline",title:"Non calibrare niente prima di avere una prima baseline"},{local:"chiedere-aiuto",title:"Chiedere aiuto"}],title:"Altri potenziali problemi "}],title:"Fare il debug di una training pipeline"};function $d(M,d,w){let $="pt";return pd(()=>{const z=new URLSearchParams(window.location.search);w(0,$=z.get("fw")||"pt")}),[$]}class Cd extends od{constructor(d){super();rd(this,d,$d,vd,id,{})}}export{Cd as default,gd as metadata};
