import{S as sn,i as on,s as ln,e as l,k as d,w as g,t as s,M as nn,c as n,d as t,m as c,a as r,x as b,h as i,b as u,G as a,g as p,y as _,q as $,o as v,B as z,v as rn}from"../../chunks/vendor-hf-doc-builder.js";import{T as $t}from"../../chunks/Tip-hf-doc-builder.js";import{Y as pn}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Os}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as w}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as dn}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function cn(N){let m,I,f,E,A,h,P,D,j,x,q,y,C,R;return{c(){m=l("p"),I=s("\u270E Di base, \u{1F917} Datasets decomprimer\xE0 i file necessari a caricare un dataset. Se vuoi risparmiare sullo spazio dell\u2019hard disk, puoi passare "),f=l("code"),E=s("DownloadConfig(delete_extracted_True)"),A=s(" all\u2019argomento "),h=l("code"),P=s("download_config"),D=s(" di "),j=l("code"),x=s("load_dataset()"),q=s(". Per maggiori dettagli leggi la "),y=l("a"),C=s("documentazione"),R=s("."),this.h()},l(k){m=n(k,"P",{});var T=r(m);I=i(T,"\u270E Di base, \u{1F917} Datasets decomprimer\xE0 i file necessari a caricare un dataset. Se vuoi risparmiare sullo spazio dell\u2019hard disk, puoi passare "),f=n(T,"CODE",{});var ee=r(f);E=i(ee,"DownloadConfig(delete_extracted_True)"),ee.forEach(t),A=i(T," all\u2019argomento "),h=n(T,"CODE",{});var J=r(h);P=i(J,"download_config"),J.forEach(t),D=i(T," di "),j=n(T,"CODE",{});var S=r(j);x=i(S,"load_dataset()"),S.forEach(t),q=i(T,". Per maggiori dettagli leggi la "),y=n(T,"A",{href:!0,rel:!0});var ae=r(y);C=i(ae,"documentazione"),ae.forEach(t),R=i(T,"."),T.forEach(t),this.h()},h(){u(y,"href","https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig"),u(y,"rel","nofollow")},m(k,T){p(k,m,T),a(m,I),a(m,f),a(f,E),a(m,A),a(m,h),a(h,P),a(m,D),a(m,j),a(j,x),a(m,q),a(m,y),a(y,C),a(m,R)},d(k){k&&t(m)}}}function mn(N){let m,I,f,E,A,h,P,D,j,x;return{c(){m=l("p"),I=s("\u270F\uFE0F "),f=l("strong"),E=s("Provaci tu!"),A=s(" Scegli uno dei "),h=l("a"),P=s("subset"),D=s(" di Pile che \xE8 pi\xF9 grande della RAM del tuo PC o del tuo portatile, caricalo utilizzando \u{1F917} Datasets e calcola la quantit\xE0 di RAM utilizzata. Nota che per avere un valore preciso, dovrai creare un nuovo processo. Puoi trovare le grandezze decompresse di ogni subset nella Tavola 1 dell\u2019"),j=l("a"),x=s("articolo su Pile"),this.h()},l(q){m=n(q,"P",{});var y=r(m);I=i(y,"\u270F\uFE0F "),f=n(y,"STRONG",{});var C=r(f);E=i(C,"Provaci tu!"),C.forEach(t),A=i(y," Scegli uno dei "),h=n(y,"A",{href:!0,rel:!0});var R=r(h);P=i(R,"subset"),R.forEach(t),D=i(y," di Pile che \xE8 pi\xF9 grande della RAM del tuo PC o del tuo portatile, caricalo utilizzando \u{1F917} Datasets e calcola la quantit\xE0 di RAM utilizzata. Nota che per avere un valore preciso, dovrai creare un nuovo processo. Puoi trovare le grandezze decompresse di ogni subset nella Tavola 1 dell\u2019"),j=n(y,"A",{href:!0,rel:!0});var k=r(j);x=i(k,"articolo su Pile"),k.forEach(t),y.forEach(t),this.h()},h(){u(h,"href","https://mystic.the-eye.eu/public/AI/pile_preliminary_components/"),u(h,"rel","nofollow"),u(j,"href","https://arxiv.org/abs/2101.00027"),u(j,"rel","nofollow")},m(q,y){p(q,m,y),a(m,I),a(m,f),a(f,E),a(m,A),a(m,h),a(h,P),a(m,D),a(m,j),a(j,x)},d(q){q&&t(m)}}}function un(N){let m,I,f,E,A,h;return{c(){m=l("p"),I=s("\u{1F4A1} Nei notebook Jupyter, puoi cronometrare le celle utilizzando la "),f=l("a"),E=s("funzione magica "),A=l("code"),h=s("%%timeit"),this.h()},l(P){m=n(P,"P",{});var D=r(m);I=i(D,"\u{1F4A1} Nei notebook Jupyter, puoi cronometrare le celle utilizzando la "),f=n(D,"A",{href:!0,rel:!0});var j=r(f);E=i(j,"funzione magica "),A=n(j,"CODE",{});var x=r(A);h=i(x,"%%timeit"),x.forEach(t),j.forEach(t),D.forEach(t),this.h()},h(){u(f,"href","https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit"),u(f,"rel","nofollow")},m(P,D){p(P,m,D),a(m,I),a(m,f),a(f,E),a(f,A),a(A,h)},d(P){P&&t(m)}}}function fn(N){let m,I,f,E,A,h,P,D;return{c(){m=l("p"),I=s("\u{1F4A1} Per velocizzare la tokenizzazione con lo streaming puoi passare "),f=l("code"),E=s("batchet=True"),A=s(", come abbiamo visto nell\u2019ultima sezione. Questo processer\xE0 gli esempi per batch. Di default, la grandezza di un batch \xE8 1.000, e pu\xF2 essere specificata attraverso l\u2019argomento "),h=l("code"),P=s("batch_size"),D=s(".")},l(j){m=n(j,"P",{});var x=r(m);I=i(x,"\u{1F4A1} Per velocizzare la tokenizzazione con lo streaming puoi passare "),f=n(x,"CODE",{});var q=r(f);E=i(q,"batchet=True"),q.forEach(t),A=i(x,", come abbiamo visto nell\u2019ultima sezione. Questo processer\xE0 gli esempi per batch. Di default, la grandezza di un batch \xE8 1.000, e pu\xF2 essere specificata attraverso l\u2019argomento "),h=n(x,"CODE",{});var y=r(h);P=i(y,"batch_size"),y.forEach(t),D=i(x,"."),x.forEach(t)},m(j,x){p(j,m,x),a(m,I),a(m,f),a(f,E),a(m,A),a(m,h),a(h,P),a(m,D)},d(j){j&&t(m)}}}function hn(N){let m,I,f,E,A,h,P,D,j,x,q,y,C;return{c(){m=l("p"),I=s("\u270F\uFE0F "),f=l("strong"),E=s("Prova tu!"),A=s(" Usa uno dei corpora Common Crawl come "),h=l("a"),P=l("code"),D=s("mc4"),j=s(" oppure "),x=l("a"),q=l("code"),y=s("oscar"),C=s(" per crare un dataset multilingue in streaming, che rappresenta le proporzioni delle lingue parlate in un paese a tua scelta. Ad esempio, le quattro lingue ufficiali in Svizzera sono il tedesco, il francesce, l\u2019italiano e il romancio, per cui potresti creare un corpus della Svizzera raccogliendo i campioni da Oscar, secondo la percentuale di parlanti di ognuna."),this.h()},l(R){m=n(R,"P",{});var k=r(m);I=i(k,"\u270F\uFE0F "),f=n(k,"STRONG",{});var T=r(f);E=i(T,"Prova tu!"),T.forEach(t),A=i(k," Usa uno dei corpora Common Crawl come "),h=n(k,"A",{href:!0,rel:!0});var ee=r(h);P=n(ee,"CODE",{});var J=r(P);D=i(J,"mc4"),J.forEach(t),ee.forEach(t),j=i(k," oppure "),x=n(k,"A",{href:!0,rel:!0});var S=r(x);q=n(S,"CODE",{});var ae=r(q);y=i(ae,"oscar"),ae.forEach(t),S.forEach(t),C=i(k," per crare un dataset multilingue in streaming, che rappresenta le proporzioni delle lingue parlate in un paese a tua scelta. Ad esempio, le quattro lingue ufficiali in Svizzera sono il tedesco, il francesce, l\u2019italiano e il romancio, per cui potresti creare un corpus della Svizzera raccogliendo i campioni da Oscar, secondo la percentuale di parlanti di ognuna."),k.forEach(t),this.h()},h(){u(h,"href","https://huggingface.co/datasets/mc4"),u(h,"rel","nofollow"),u(x,"href","https://huggingface.co/datasets/oscar"),u(x,"rel","nofollow")},m(R,k){p(R,m,k),a(m,I),a(m,f),a(f,E),a(m,A),a(m,h),a(h,P),a(P,D),a(m,j),a(m,x),a(x,q),a(q,y),a(m,C)},d(R){R&&t(m)}}}function gn(N){let m,I,f,E,A,h,P,D,j,x,q,y,C,R,k,T,ee,J,S,ae,qa,Rs,Ss,ka,Ms,Gs,vt,je,zt,le,Ns,ye,Bs,Ls,xt,te,ne,Ia,Ee,Us,Ca,Hs,jt,O,Fs,we,Js,Ws,Ae,Qs,Ks,Pe,Ys,Zs,De,Vs,Xs,qe,ei,ai,Ta,ti,si,yt,ke,Et,re,ii,ha,oi,li,wt,Ie,At,Ce,Pt,ga,ni,Dt,pe,qt,ba,ri,kt,Te,It,Oe,Ct,_a,pi,Tt,se,de,Oa,Re,di,Ra,ci,Ot,W,mi,Se,Sa,ui,fi,Ma,hi,gi,Rt,Me,St,ie,Ga,bi,_i,Na,$i,vi,Mt,Ge,Gt,Ne,Nt,B,zi,Ba,xi,ji,La,yi,Ei,Ua,wi,Ai,Bt,Be,Lt,Le,Ut,$a,Pi,Ht,ce,Ft,Q,Di,Ue,qi,ki,He,Ii,Ci,Jt,M,Ti,Ha,Oi,Ri,Fe,Si,Mi,Je,Fa,Gi,Ni,We,Bi,Li,Wt,Qe,Qt,Ke,Kt,K,Ui,Ja,Hi,Fi,Wa,Ji,Wi,Yt,me,Zt,oe,ue,Qa,Ye,Qi,Ka,Ki,Vt,Y,Yi,Ya,Zi,Vi,Za,Xi,eo,Xt,Ze,es,L,ao,Va,to,so,Xa,io,oo,et,lo,no,as,Ve,ts,Xe,ss,Z,ro,at,po,co,va,mo,uo,is,ea,os,aa,ls,fe,ns,U,fo,tt,ho,go,st,bo,_o,it,$o,vo,rs,ta,ps,sa,ds,H,zo,ot,xo,jo,lt,yo,Eo,nt,wo,Ao,cs,ia,ms,oa,us,he,Po,rt,Do,qo,fs,la,hs,F,ko,pt,Io,Co,dt,To,Oo,ct,Ro,So,gs,na,bs,ra,_s,ge,Mo,mt,Go,No,$s,pa,vs,da,zs,V,Bo,ut,Lo,Uo,ft,Ho,Fo,xs,za,Jo,js,ca,ys,ma,Es,be,ws,xa,Wo,As;return h=new Os({}),q=new dn({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section4.ipynb"}]}}),je=new pn({props:{id:"JwISwTCPPWo"}}),Ee=new Os({}),ke=new w({props:{code:"!pip install zstandard",highlighted:"!pip install zstandard"}}),Ie=new w({props:{code:`from datasets import load_dataset

# Ci vuole qualche minuto per l'esecuzione, quindi preparati un t\xE8 o un caff\xE8 nell'attesa :)
data_files = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># Ci vuole qualche minuto per l&#x27;esecuzione, quindi preparati un t\xE8 o un caff\xE8 nell&#x27;attesa :)</span>
data_files = <span class="hljs-string">&quot;https://mystic.the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst&quot;</span>
pubmed_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)
pubmed_dataset`}}),Ce=new w({props:{code:`Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;meta&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>],
    num_rows: <span class="hljs-number">15518009</span>
})`}}),pe=new $t({props:{$$slots:{default:[cn]},$$scope:{ctx:N}}}),Te=new w({props:{code:"pubmed_dataset[0]",highlighted:'pubmed_dataset[<span class="hljs-number">0</span>]'}}),Oe=new w({props:{code:`{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...&#x27;</span>}`}}),Re=new Os({}),Me=new w({props:{code:"!pip install psutil",highlighted:"!pip install psutil"}}),Ge=new w({props:{code:`import psutil

# Process.memory_info mostra i dati in byte, quindi convertiamo in megabyte
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")`,highlighted:`<span class="hljs-keyword">import</span> psutil

<span class="hljs-comment"># Process.memory_info mostra i dati in byte, quindi convertiamo in megabyte</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;RAM used: <span class="hljs-subst">{psutil.Process().memory_info().rss / (<span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>):<span class="hljs-number">.2</span>f}</span> MB&quot;</span>)`}}),Ne=new w({props:{code:"RAM used: 5678.33 MB",highlighted:'RAM used: <span class="hljs-number">5678.33</span> MB'}}),Be=new w({props:{code:`print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")`,highlighted:`<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Number of files in dataset : <span class="hljs-subst">{pubmed_dataset.dataset_size}</span>&quot;</span>)
size_gb = pubmed_dataset.dataset_size / (<span class="hljs-number">1024</span>**<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Dataset size (cache file) : <span class="hljs-subst">{size_gb:<span class="hljs-number">.2</span>f}</span> GB&quot;</span>)`}}),Le=new w({props:{code:`Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB`,highlighted:`Number of files <span class="hljs-keyword">in</span> dataset : <span class="hljs-number">20979437051</span>
Dataset size (cache file) : <span class="hljs-number">19.54</span> GB`}}),ce=new $t({props:{$$slots:{default:[mn]},$$scope:{ctx:N}}}),Qe=new w({props:{code:`import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)`,highlighted:`<span class="hljs-keyword">import</span> timeit

code_snippet = <span class="hljs-string">&quot;&quot;&quot;batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
&quot;&quot;&quot;</span>

time = timeit.timeit(stmt=code_snippet, number=<span class="hljs-number">1</span>, <span class="hljs-built_in">globals</span>=<span class="hljs-built_in">globals</span>())
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">f&quot;Iterated over <span class="hljs-subst">{<span class="hljs-built_in">len</span>(pubmed_dataset)}</span> examples (about <span class="hljs-subst">{size_gb:<span class="hljs-number">.1</span>f}</span> GB) in &quot;</span>
    <span class="hljs-string">f&quot;<span class="hljs-subst">{time:<span class="hljs-number">.1</span>f}</span>s, i.e. <span class="hljs-subst">{size_gb/time:<span class="hljs-number">.3</span>f}</span> GB/s&quot;</span>
)`}}),Ke=new w({props:{code:"'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'",highlighted:'<span class="hljs-string">&#x27;Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s&#x27;</span>'}}),me=new $t({props:{$$slots:{default:[un]},$$scope:{ctx:N}}}),Ye=new Os({}),Ze=new w({props:{code:`pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)`,highlighted:`pubmed_dataset_streamed = load_dataset(
    <span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>, streaming=<span class="hljs-literal">True</span>
)`}}),Ve=new w({props:{code:"next(iter(pubmed_dataset_streamed))",highlighted:'<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(pubmed_dataset_streamed))'}}),Xe=new w({props:{code:`{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...&#x27;</span>}`}}),ea=new w({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
tokenized_dataset = pubmed_dataset_streamed.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: tokenizer(x[<span class="hljs-string">&quot;text&quot;</span>]))
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(tokenized_dataset))`}}),aa=new w({props:{code:"{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}",highlighted:'{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">4958</span>, <span class="hljs-number">5178</span>, <span class="hljs-number">4328</span>, <span class="hljs-number">6779</span>, ...], <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ...]}'}}),fe=new $t({props:{$$slots:{default:[fn]},$$scope:{ctx:N}}}),ta=new w({props:{code:`shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))`,highlighted:`shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=<span class="hljs-number">10_000</span>, seed=<span class="hljs-number">42</span>)
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(shuffled_dataset))`}}),sa=new w({props:{code:`{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11410799</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...&#x27;</span>}`}}),ia=new w({props:{code:`dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)`,highlighted:`dataset_head = pubmed_dataset_streamed.take(<span class="hljs-number">5</span>)
<span class="hljs-built_in">list</span>(dataset_head)`}}),oa=new w({props:{code:`[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]`,highlighted:`[{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409575</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409576</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;Hypoxaemia in children with severe pneumonia in Papua New Guinea ...&quot;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409577</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Oxygen concentrators and cylinders ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409578</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Oxygen supply in rural africa: a personal experience ...&#x27;</span>}]`}}),la=new w({props:{code:`# Salta i primi 1.000 esempi, il resto viene incluso nell'insieme di addestramento
train_dataset = shuffled_dataset.skip(1000)
# Includi i primi 1.000 esempi nell'insieme di validazione
validation_dataset = shuffled_dataset.take(1000)`,highlighted:`<span class="hljs-comment"># Salta i primi 1.000 esempi, il resto viene incluso nell&#x27;insieme di addestramento</span>
train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)
<span class="hljs-comment"># Includi i primi 1.000 esempi nell&#x27;insieme di validazione</span>
validation_dataset = shuffled_dataset.take(<span class="hljs-number">1000</span>)`}}),na=new w({props:{code:`law_dataset_streamed = load_dataset(
    "json",
    data_files="https://mystic.the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))`,highlighted:`law_dataset_streamed = load_dataset(
    <span class="hljs-string">&quot;json&quot;</span>,
    data_files=<span class="hljs-string">&quot;https://mystic.the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst&quot;</span>,
    split=<span class="hljs-string">&quot;train&quot;</span>,
    streaming=<span class="hljs-literal">True</span>,
)
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(law_dataset_streamed))`}}),ra=new w({props:{code:`{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;case_ID&#x27;</span>: <span class="hljs-string">&#x27;110921.json&#x27;</span>,
  <span class="hljs-string">&#x27;case_jurisdiction&#x27;</span>: <span class="hljs-string">&#x27;scotus.tar.gz&#x27;</span>,
  <span class="hljs-string">&#x27;date_created&#x27;</span>: <span class="hljs-string">&#x27;2010-04-28T17:12:49Z&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...&#x27;</span>}`}}),pa=new w({props:{code:`from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))`,highlighted:`<span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
<span class="hljs-built_in">list</span>(islice(combined_dataset, <span class="hljs-number">2</span>))`}}),da=new w({props:{code:`[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]`,highlighted:`[{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;case_ID&#x27;</span>: <span class="hljs-string">&#x27;110921.json&#x27;</span>,
   <span class="hljs-string">&#x27;case_jurisdiction&#x27;</span>: <span class="hljs-string">&#x27;scotus.tar.gz&#x27;</span>,
   <span class="hljs-string">&#x27;date_created&#x27;</span>: <span class="hljs-string">&#x27;2010-04-28T17:12:49Z&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...&#x27;</span>}]`}}),ca=new w({props:{code:`base_url = "https://mystic.the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))`,highlighted:`base_url = <span class="hljs-string">&quot;https://mystic.the-eye.eu/public/AI/pile/&quot;</span>
data_files = {
    <span class="hljs-string">&quot;train&quot;</span>: [base_url + <span class="hljs-string">&quot;train/&quot;</span> + <span class="hljs-string">f&quot;<span class="hljs-subst">{idx:02d}</span>.jsonl.zst&quot;</span> <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">30</span>)],
    <span class="hljs-string">&quot;validation&quot;</span>: base_url + <span class="hljs-string">&quot;val.jsonl.zst&quot;</span>,
    <span class="hljs-string">&quot;test&quot;</span>: base_url + <span class="hljs-string">&quot;test.jsonl.zst&quot;</span>,
}
pile_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(pile_dataset[<span class="hljs-string">&quot;train&quot;</span>]))`}}),ma=new w({props:{code:`{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play \u201CSurvival of the Tastiest\u201D on Android, and on the web...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pile_set_name&#x27;</span>: <span class="hljs-string">&#x27;Pile-CC&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;It is done, and submitted. You can play \u201CSurvival of the Tastiest\u201D on Android, and on the web...&#x27;</span>}`}}),be=new $t({props:{$$slots:{default:[hn]},$$scope:{ctx:N}}}),{c(){m=l("meta"),I=d(),f=l("h1"),E=l("a"),A=l("span"),g(h.$$.fragment),P=d(),D=l("span"),j=s("Big data? Ci pensa \u{1F917} Datasets!"),x=d(),g(q.$$.fragment),y=d(),C=l("p"),R=s("Al giorno d\u2019oggi non \xE8 raro trovarsi a lavorare con dataset grandi diversi gigabyte, soprattutto quando si vuole addestrare un transformer come BERT o GPT-2 da zero. In questi casi, persino "),k=l("em"),T=s("caricare"),ee=s(" i dati pu\xF2 essere un\u2019impresa difficile. Ad esempio, il corpus WebText utilizzato per preaddestrare GPT-2 contiente pi\xF9 di 8 milioni di documenti e 40gb di testo \u2014 caricare un dataset del genere sulla RAM del tuo portatile gli farebbe venire un colpo!"),J=d(),S=l("p"),ae=s("Per fortuna, \u{1F917} Datasets \xE8 stato sviluppato per superare queste limitazioni, e pu\xF2 risolvere i problemi relativi alla gestione della memoria trattando i dataset come file "),qa=l("em"),Rs=s("memory-mapped"),Ss=s(", e quelli relativi ai limiti del disco rigido attraverso lo "),ka=l("em"),Ms=s("stream processing"),Gs=s(" delle voci del corpus."),vt=d(),g(je.$$.fragment),zt=d(),le=l("p"),Ns=s("In questa sezione esploreremo queste funzionalit\xE0 di \u{1F917} Datasets con un enorme corpus di 825 GB conosciuto come "),ye=l("a"),Bs=s("Pile"),Ls=s(". Iniziamo!"),xt=d(),te=l("h2"),ne=l("a"),Ia=l("span"),g(Ee.$$.fragment),Us=d(),Ca=l("span"),Hs=s("Cos'\xE8 Pile?"),jt=d(),O=l("p"),Fs=s("The Pile \xE8 un corpus testuale creato da "),we=l("a"),Js=s("EleutherAI"),Ws=s(" per addestrare modelli di linguaggio su grande scala. Include un grande variet\xE0 di dataset, a partire da articoli scientifici, repository di codici da GitHub, e testi dal web filtrati. Il corpus di addestramento \xE8 disponibili in "),Ae=l("a"),Qs=s("frammenti da 14 GB"),Ks=s(", ed \xE8 possibile scaricare diverse delle "),Pe=l("a"),Ys=s("componenti singole"),Zs=s(". Iniziamo dando uno sguardo al dataset PubMed Abstracts, un corpus di abstract da 15 milioni di pubblicazioni in ambito biomedico da "),De=l("a"),Vs=s("PubMed"),Xs=s(". Il dataset \xE8 in "),qe=l("a"),ei=s("formato JSON Lines"),ai=s(" ed \xE8 stato compressato usando la libreria "),Ta=l("code"),ti=s("zstandard"),si=s(", per cui dobbiamo prima installarla:"),yt=d(),g(ke.$$.fragment),Et=d(),re=l("p"),ii=s("Ora, possiamo caricare il dataset utilizzando il meotodo per file remoti che abbiamo visto nella "),ha=l("a"),oi=s("sezione 2"),li=s(":"),wt=d(),g(Ie.$$.fragment),At=d(),g(Ce.$$.fragment),Pt=d(),ga=l("p"),ni=s("Possiamo vedere che ci sono 15.518.009 righe e 2 colonne nel nostro dataset \u2014 un bel po\u2019!"),Dt=d(),g(pe.$$.fragment),qt=d(),ba=l("p"),ri=s("Ispezioniamo i contenuti del primo esempio:"),kt=d(),g(Te.$$.fragment),It=d(),g(Oe.$$.fragment),Ct=d(),_a=l("p"),pi=s("Okay, questo sembra proprio l\u2019abstract di un articolo di medicina. Ora vediamo quanta RAM \xE8 stata usata per caricare il dataset!"),Tt=d(),se=l("h2"),de=l("a"),Oa=l("span"),g(Re.$$.fragment),di=d(),Ra=l("span"),ci=s("La magia del memory mapping"),Ot=d(),W=l("p"),mi=s("Un modo semplice per calcolare l\u2019uso di memoria su Python \xE8 utilizzando la libreria "),Se=l("a"),Sa=l("code"),ui=s("psutil"),fi=s(", che pu\xF2 essere installata con "),Ma=l("code"),hi=s("pip"),gi=s(" come segue:"),Rt=d(),g(Me.$$.fragment),St=d(),ie=l("p"),Ga=l("code"),bi=s("psutil"),_i=s(" offre una classe "),Na=l("code"),$i=s("Process"),vi=s(" che permette di controllare l\u2019utilizzo della memoria del processo attuale come segue::"),Mt=d(),g(Ge.$$.fragment),Gt=d(),g(Ne.$$.fragment),Nt=d(),B=l("p"),zi=s("L\u2019attributo "),Ba=l("code"),xi=s("rss"),ji=s(" qui fa riferimento alla "),La=l("em"),yi=s("grandezza del resident set"),Ei=s(", che equivale alla frazione di memoria che il processo occupa nella RAM. Questo valore include inoltre la memoria utilizzata dall\u2019interprete Python e dalle librerie caricate, per cui l\u2019ammontare effettivo utilizzato per caricare il dataset \xE8 un po\u2019 pi\xF9 piccolo. Per fare un confronto, vediamo quant\u2019\xE8 grande il dataset su disco utilizzando l\u2019attributo "),Ua=l("code"),wi=s("dataset_size"),Ai=s(". Come prima, il risultato \xE8 espresso in byte, e abbiamo bisogno di convertirlo in gigabyte:"),Bt=d(),g(Be.$$.fragment),Lt=d(),g(Le.$$.fragment),Ut=d(),$a=l("p"),Pi=s("Bene \u2014 nonostante sia grande quasi 30 GB, siamo in grado di caricare e accedere al dataset utilizzando molta meno RAM!"),Ht=d(),g(ce.$$.fragment),Ft=d(),Q=l("p"),Di=s("Se hai dimestichezza con Pandas, questo risultato potrebbe sorprenderti, vista la famosa "),Ue=l("a"),qi=s("regola di Wes Kinney"),ki=s(", ovvero che, in linea di massima, serve una RAM 5-10 volte pi\xF9 grande del dataset che vuoi caricare. Come fa \u{1F917} Datasets a risolvere questo problema di gestione della memoria? \u{1F917} Datasets tratta ogni dataset come un "),He=l("a"),Ii=s("file mappato in memoria"),Ci=s(", il che permette di avere un mapping tra la RAM e l\u2019archiviazione dei file di sistema, che permette alla librera di accedere e operare su elementi del dataset senza doverli caricare completamente in memoria."),Jt=d(),M=l("p"),Ti=s("I file mappati in memoria possono inoltre essre condivisi su pi\xF9 processi, il che permette a metodi come "),Ha=l("code"),Oi=s("Dataset.map()"),Ri=s(" di poter essere eseguiti in parallelo senza bisogno di spostare o copiare il dataset. Dietro le quinte, tutto ci\xF2 \xE8 realizzato dal formato di memoria "),Fe=l("a"),Si=s("Apache Arrow"),Mi=s(" e dalla libreria "),Je=l("a"),Fa=l("code"),Gi=s("pyarrow"),Ni=s(", che rendono pi\xF9 veloci il caricamento e il processamento dei dati. (per maggiori dettagli su Apache Arrow, e per un confronto con Pandas, dai un\u2019occhiata al "),We=l("a"),Bi=s("post di Dejan Simic"),Li=s(".) Per vederlo in azione, eseguiamo un piccolo test di velocit\xE0 con un loop su tutti gli elementi nel dataset PubMed Abstracts:"),Wt=d(),g(Qe.$$.fragment),Qt=d(),g(Ke.$$.fragment),Kt=d(),K=l("p"),Ui=s("Abbiamo usato il modulo di Python "),Ja=l("code"),Hi=s("timeit"),Fi=s(" per calcolare il tempo di esecuzione impiegato da "),Wa=l("code"),Ji=s("code_snippet"),Wi=s(". Tipicamente l\u2019iterazione su un dataset impiega un tempo che va da un decimo di GB al secondo, a diversi GB al secondo. Questo funziona perfettamente per la maggior parte delle applicazioni, ma a volte avrai bisogno di lavorare con un dataset che \xE8 troppo grande persino per essere salvato sul tuo portatile. Ad esempio, se cercassimo di scaricare Pile per intero, avremo bisogno di 825 GB di spazio libero su disko! In questi casi, \u{1F917} Datasets permette di utilizzare processi di streaming che ci permettono di scaricare e accedere al volo ai dati, senza bisogno di scaricare l\u2019intero dataset. Diamo un\u2019occhiata a come funziona."),Yt=d(),g(me.$$.fragment),Zt=d(),oe=l("h2"),ue=l("a"),Qa=l("span"),g(Ye.$$.fragment),Qi=d(),Ka=l("span"),Ki=s("Streaming di dataset"),Vt=d(),Y=l("p"),Yi=s("Per abilitare lo streaming dei dataset devi semplicemente passare l\u2019argomento "),Ya=l("code"),Zi=s("streaming=True"),Vi=s(" alla funzione "),Za=l("code"),Xi=s("load_dataset()"),eo=s(". Ad esempio, carichiamo un\u2019altra volta il dataset PubMed Abstract, ma in modalit\xE0 streaming:"),Xt=d(),g(Ze.$$.fragment),es=d(),L=l("p"),ao=s("Invece del solito "),Va=l("code"),to=s("Dataset"),so=s(" che abbiamo incontrato in precedenza in questo capitolo, l\u2019oggetto ritornato con "),Xa=l("code"),io=s("streaming=True' \xE8 un "),oo=s("IterableDataset"),et=l("code"),lo=s(". Come suggerito dal nome, per accedere agli elementi di un "),no=s("IterableDataset`, dobbiamo iterare di esso. Possiamo accedere al primo elemento del nostro dataset in streaming come segue:"),as=d(),g(Ve.$$.fragment),ts=d(),g(Xe.$$.fragment),ss=d(),Z=l("p"),ro=s("Gli elementi di un dataset in streaming possono essere processati al volo utilizzando "),at=l("code"),po=s("IterableDataset.map()"),co=s(", che \xE8 utile durante l\u2019addestramento se hai bisogno di tokenizzare gli input. Il processo \xE8 uguale a quello che abbiamo utilizzato per tokenizzare il nostro dataset nel "),va=l("a"),mo=s("Capitolo 3"),uo=s(", con l\u2019unica differenza che ora ritorneremo gli output uno alla volta:"),is=d(),g(ea.$$.fragment),os=d(),g(aa.$$.fragment),ls=d(),g(fe.$$.fragment),ns=d(),U=l("p"),fo=s("\xC8 anche possibile mescolare un dataset in streaming utilizzato "),tt=l("code"),ho=s("Iterabledataset.shuffle()"),go=s(", ma a differenza di "),st=l("code"),bo=s("Dataset.shuffle()"),_o=s(", questo metodo mescola solo gli elementi in un "),it=l("code"),$o=s("buffer_size"),vo=s(" predefinito:"),rs=d(),g(ta.$$.fragment),ps=d(),g(sa.$$.fragment),ds=d(),H=l("p"),zo=s("In questo esempio, abbiamo selezionato un esempio casuale dai primi 10.000 esempi nel buffer. Una volta che accediamo a un esempio, il suo posto nel buffer \xE8 subito occupato dall\u2019esempio successivo nel corpus (in questo caso l\u2019esempio 10.0001). Puoi inoltre selezionare gli elementi da un dataset in streaming utilizzando le funzioni "),ot=l("code"),xo=s("IterableDataset.take()"),jo=s(" a "),lt=l("code"),yo=s("IterableDataset.skip()"),Eo=s(", che funzionano un po\u2019 come "),nt=l("code"),wo=s("Dataset.select()"),Ao=s(". Ad esempio, per selezionare i primi 5 esempi nel dataset PubMed Abstract dovremmo fare come segue:"),cs=d(),g(ia.$$.fragment),ms=d(),g(oa.$$.fragment),us=d(),he=l("p"),Po=s("Allo stesso modo, \xE8 possibile utilizzare la funzione "),rt=l("code"),Do=s("IterableDataset.skip()"),qo=s(" per creare sezioni di addestramento e di validazione da un dataset mescolato, come segue:"),fs=d(),g(la.$$.fragment),hs=d(),F=l("p"),ko=s("Concludiamo la nostra ricognizione dello streaming di dataset con un\u2019applicazione comune: la combinazione di pi\xF9 dataset per creare un unico corpus. \u{1F917} Datasets fornisce una funzione "),pt=l("code"),Io=s("interleave_datasets()"),Co=s(", che converte una lista di oggetti "),dt=l("code"),To=s("IterableDataset"),Oo=s(" in un unico "),ct=l("code"),Ro=s("IterableDataset"),So=s(", dove gli elementi del nuovo dataset sono ottenuti alternando tra gli esempi forniti. Questa funzione \xE8 particolarmente utile quando cerchiamo di combinare dataset di grandi dimensioni, come esempio possiamo utilizzare in streaming la sezione FreeLaw del Pile, un dataset di 51 GB di pareri legali dai tribunali statunitensi:"),gs=d(),g(na.$$.fragment),bs=d(),g(ra.$$.fragment),_s=d(),ge=l("p"),Mo=s("Questo dataset \xE8 abbastanza grande da mettere sotto sforzo la RAM di molto portatili, ma siamo riusciti a caricarlo e accedervi senza alcun problema! Ora cominiamo gli esempi di FreeLaw e di PubMed Abstracts con la funzione "),mt=l("code"),Go=s("interleave_datasets()"),No=s(":"),$s=d(),g(pa.$$.fragment),vs=d(),g(da.$$.fragment),zs=d(),V=l("p"),Bo=s("Abbiamo utilizzato la funzione "),ut=l("code"),Lo=s("islice()"),Uo=s(" del modulo Python "),ft=l("code"),Ho=s("itertools"),Fo=s(" per selezionare i primi due esempi dai dataset combinati, e abbiamo visto che corrispondono ai primi esempi di ognuno dei due dataset originali."),xs=d(),za=l("p"),Jo=s("Infine, se vuoi processare il Pile in streaming, in tutti i suoi 825 GB, puoi recuperare tutti i file preparati, come segue:"),js=d(),g(ca.$$.fragment),ys=d(),g(ma.$$.fragment),Es=d(),g(be.$$.fragment),ws=d(),xa=l("p"),Wo=s("Ora hai a tua disposizione tutti gli strumenti per caricare e processare dataset di ogni tipo \u2014 ma a meno che tu non sia estremamente fortunato, arriver\xE0 un momento nel tuo cammino in cui dovrai effettivamente creare un dataset per risolvere i tuoi problemi. Questo sar\xE0 argomento della prossima sezione!"),this.h()},l(e){const o=nn('[data-svelte="svelte-1phssyn"]',document.head);m=n(o,"META",{name:!0,content:!0}),o.forEach(t),I=c(e),f=n(e,"H1",{class:!0});var ua=r(f);E=n(ua,"A",{id:!0,class:!0,href:!0});var ht=r(E);A=n(ht,"SPAN",{});var gt=r(A);b(h.$$.fragment,gt),gt.forEach(t),ht.forEach(t),P=c(ua),D=n(ua,"SPAN",{});var bt=r(D);j=i(bt,"Big data? Ci pensa \u{1F917} Datasets!"),bt.forEach(t),ua.forEach(t),x=c(e),b(q.$$.fragment,e),y=c(e),C=n(e,"P",{});var fa=r(C);R=i(fa,"Al giorno d\u2019oggi non \xE8 raro trovarsi a lavorare con dataset grandi diversi gigabyte, soprattutto quando si vuole addestrare un transformer come BERT o GPT-2 da zero. In questi casi, persino "),k=n(fa,"EM",{});var Qo=r(k);T=i(Qo,"caricare"),Qo.forEach(t),ee=i(fa," i dati pu\xF2 essere un\u2019impresa difficile. Ad esempio, il corpus WebText utilizzato per preaddestrare GPT-2 contiente pi\xF9 di 8 milioni di documenti e 40gb di testo \u2014 caricare un dataset del genere sulla RAM del tuo portatile gli farebbe venire un colpo!"),fa.forEach(t),J=c(e),S=n(e,"P",{});var ja=r(S);ae=i(ja,"Per fortuna, \u{1F917} Datasets \xE8 stato sviluppato per superare queste limitazioni, e pu\xF2 risolvere i problemi relativi alla gestione della memoria trattando i dataset come file "),qa=n(ja,"EM",{});var Ko=r(qa);Rs=i(Ko,"memory-mapped"),Ko.forEach(t),Ss=i(ja,", e quelli relativi ai limiti del disco rigido attraverso lo "),ka=n(ja,"EM",{});var Yo=r(ka);Ms=i(Yo,"stream processing"),Yo.forEach(t),Gs=i(ja," delle voci del corpus."),ja.forEach(t),vt=c(e),b(je.$$.fragment,e),zt=c(e),le=n(e,"P",{});var Ps=r(le);Ns=i(Ps,"In questa sezione esploreremo queste funzionalit\xE0 di \u{1F917} Datasets con un enorme corpus di 825 GB conosciuto come "),ye=n(Ps,"A",{href:!0,rel:!0});var Zo=r(ye);Bs=i(Zo,"Pile"),Zo.forEach(t),Ls=i(Ps,". Iniziamo!"),Ps.forEach(t),xt=c(e),te=n(e,"H2",{class:!0});var Ds=r(te);ne=n(Ds,"A",{id:!0,class:!0,href:!0});var Vo=r(ne);Ia=n(Vo,"SPAN",{});var Xo=r(Ia);b(Ee.$$.fragment,Xo),Xo.forEach(t),Vo.forEach(t),Us=c(Ds),Ca=n(Ds,"SPAN",{});var el=r(Ca);Hs=i(el,"Cos'\xE8 Pile?"),el.forEach(t),Ds.forEach(t),jt=c(e),O=n(e,"P",{});var G=r(O);Fs=i(G,"The Pile \xE8 un corpus testuale creato da "),we=n(G,"A",{href:!0,rel:!0});var al=r(we);Js=i(al,"EleutherAI"),al.forEach(t),Ws=i(G," per addestrare modelli di linguaggio su grande scala. Include un grande variet\xE0 di dataset, a partire da articoli scientifici, repository di codici da GitHub, e testi dal web filtrati. Il corpus di addestramento \xE8 disponibili in "),Ae=n(G,"A",{href:!0,rel:!0});var tl=r(Ae);Qs=i(tl,"frammenti da 14 GB"),tl.forEach(t),Ks=i(G,", ed \xE8 possibile scaricare diverse delle "),Pe=n(G,"A",{href:!0,rel:!0});var sl=r(Pe);Ys=i(sl,"componenti singole"),sl.forEach(t),Zs=i(G,". Iniziamo dando uno sguardo al dataset PubMed Abstracts, un corpus di abstract da 15 milioni di pubblicazioni in ambito biomedico da "),De=n(G,"A",{href:!0,rel:!0});var il=r(De);Vs=i(il,"PubMed"),il.forEach(t),Xs=i(G,". Il dataset \xE8 in "),qe=n(G,"A",{href:!0,rel:!0});var ol=r(qe);ei=i(ol,"formato JSON Lines"),ol.forEach(t),ai=i(G," ed \xE8 stato compressato usando la libreria "),Ta=n(G,"CODE",{});var ll=r(Ta);ti=i(ll,"zstandard"),ll.forEach(t),si=i(G,", per cui dobbiamo prima installarla:"),G.forEach(t),yt=c(e),b(ke.$$.fragment,e),Et=c(e),re=n(e,"P",{});var qs=r(re);ii=i(qs,"Ora, possiamo caricare il dataset utilizzando il meotodo per file remoti che abbiamo visto nella "),ha=n(qs,"A",{href:!0});var nl=r(ha);oi=i(nl,"sezione 2"),nl.forEach(t),li=i(qs,":"),qs.forEach(t),wt=c(e),b(Ie.$$.fragment,e),At=c(e),b(Ce.$$.fragment,e),Pt=c(e),ga=n(e,"P",{});var rl=r(ga);ni=i(rl,"Possiamo vedere che ci sono 15.518.009 righe e 2 colonne nel nostro dataset \u2014 un bel po\u2019!"),rl.forEach(t),Dt=c(e),b(pe.$$.fragment,e),qt=c(e),ba=n(e,"P",{});var pl=r(ba);ri=i(pl,"Ispezioniamo i contenuti del primo esempio:"),pl.forEach(t),kt=c(e),b(Te.$$.fragment,e),It=c(e),b(Oe.$$.fragment,e),Ct=c(e),_a=n(e,"P",{});var dl=r(_a);pi=i(dl,"Okay, questo sembra proprio l\u2019abstract di un articolo di medicina. Ora vediamo quanta RAM \xE8 stata usata per caricare il dataset!"),dl.forEach(t),Tt=c(e),se=n(e,"H2",{class:!0});var ks=r(se);de=n(ks,"A",{id:!0,class:!0,href:!0});var cl=r(de);Oa=n(cl,"SPAN",{});var ml=r(Oa);b(Re.$$.fragment,ml),ml.forEach(t),cl.forEach(t),di=c(ks),Ra=n(ks,"SPAN",{});var ul=r(Ra);ci=i(ul,"La magia del memory mapping"),ul.forEach(t),ks.forEach(t),Ot=c(e),W=n(e,"P",{});var ya=r(W);mi=i(ya,"Un modo semplice per calcolare l\u2019uso di memoria su Python \xE8 utilizzando la libreria "),Se=n(ya,"A",{href:!0,rel:!0});var fl=r(Se);Sa=n(fl,"CODE",{});var hl=r(Sa);ui=i(hl,"psutil"),hl.forEach(t),fl.forEach(t),fi=i(ya,", che pu\xF2 essere installata con "),Ma=n(ya,"CODE",{});var gl=r(Ma);hi=i(gl,"pip"),gl.forEach(t),gi=i(ya," come segue:"),ya.forEach(t),Rt=c(e),b(Me.$$.fragment,e),St=c(e),ie=n(e,"P",{});var _t=r(ie);Ga=n(_t,"CODE",{});var bl=r(Ga);bi=i(bl,"psutil"),bl.forEach(t),_i=i(_t," offre una classe "),Na=n(_t,"CODE",{});var _l=r(Na);$i=i(_l,"Process"),_l.forEach(t),vi=i(_t," che permette di controllare l\u2019utilizzo della memoria del processo attuale come segue::"),_t.forEach(t),Mt=c(e),b(Ge.$$.fragment,e),Gt=c(e),b(Ne.$$.fragment,e),Nt=c(e),B=n(e,"P",{});var _e=r(B);zi=i(_e,"L\u2019attributo "),Ba=n(_e,"CODE",{});var $l=r(Ba);xi=i($l,"rss"),$l.forEach(t),ji=i(_e," qui fa riferimento alla "),La=n(_e,"EM",{});var vl=r(La);yi=i(vl,"grandezza del resident set"),vl.forEach(t),Ei=i(_e,", che equivale alla frazione di memoria che il processo occupa nella RAM. Questo valore include inoltre la memoria utilizzata dall\u2019interprete Python e dalle librerie caricate, per cui l\u2019ammontare effettivo utilizzato per caricare il dataset \xE8 un po\u2019 pi\xF9 piccolo. Per fare un confronto, vediamo quant\u2019\xE8 grande il dataset su disco utilizzando l\u2019attributo "),Ua=n(_e,"CODE",{});var zl=r(Ua);wi=i(zl,"dataset_size"),zl.forEach(t),Ai=i(_e,". Come prima, il risultato \xE8 espresso in byte, e abbiamo bisogno di convertirlo in gigabyte:"),_e.forEach(t),Bt=c(e),b(Be.$$.fragment,e),Lt=c(e),b(Le.$$.fragment,e),Ut=c(e),$a=n(e,"P",{});var xl=r($a);Pi=i(xl,"Bene \u2014 nonostante sia grande quasi 30 GB, siamo in grado di caricare e accedere al dataset utilizzando molta meno RAM!"),xl.forEach(t),Ht=c(e),b(ce.$$.fragment,e),Ft=c(e),Q=n(e,"P",{});var Ea=r(Q);Di=i(Ea,"Se hai dimestichezza con Pandas, questo risultato potrebbe sorprenderti, vista la famosa "),Ue=n(Ea,"A",{href:!0,rel:!0});var jl=r(Ue);qi=i(jl,"regola di Wes Kinney"),jl.forEach(t),ki=i(Ea,", ovvero che, in linea di massima, serve una RAM 5-10 volte pi\xF9 grande del dataset che vuoi caricare. Come fa \u{1F917} Datasets a risolvere questo problema di gestione della memoria? \u{1F917} Datasets tratta ogni dataset come un "),He=n(Ea,"A",{href:!0,rel:!0});var yl=r(He);Ii=i(yl,"file mappato in memoria"),yl.forEach(t),Ci=i(Ea,", il che permette di avere un mapping tra la RAM e l\u2019archiviazione dei file di sistema, che permette alla librera di accedere e operare su elementi del dataset senza doverli caricare completamente in memoria."),Ea.forEach(t),Jt=c(e),M=n(e,"P",{});var X=r(M);Ti=i(X,"I file mappati in memoria possono inoltre essre condivisi su pi\xF9 processi, il che permette a metodi come "),Ha=n(X,"CODE",{});var El=r(Ha);Oi=i(El,"Dataset.map()"),El.forEach(t),Ri=i(X," di poter essere eseguiti in parallelo senza bisogno di spostare o copiare il dataset. Dietro le quinte, tutto ci\xF2 \xE8 realizzato dal formato di memoria "),Fe=n(X,"A",{href:!0,rel:!0});var wl=r(Fe);Si=i(wl,"Apache Arrow"),wl.forEach(t),Mi=i(X," e dalla libreria "),Je=n(X,"A",{href:!0,rel:!0});var Al=r(Je);Fa=n(Al,"CODE",{});var Pl=r(Fa);Gi=i(Pl,"pyarrow"),Pl.forEach(t),Al.forEach(t),Ni=i(X,", che rendono pi\xF9 veloci il caricamento e il processamento dei dati. (per maggiori dettagli su Apache Arrow, e per un confronto con Pandas, dai un\u2019occhiata al "),We=n(X,"A",{href:!0,rel:!0});var Dl=r(We);Bi=i(Dl,"post di Dejan Simic"),Dl.forEach(t),Li=i(X,".) Per vederlo in azione, eseguiamo un piccolo test di velocit\xE0 con un loop su tutti gli elementi nel dataset PubMed Abstracts:"),X.forEach(t),Wt=c(e),b(Qe.$$.fragment,e),Qt=c(e),b(Ke.$$.fragment,e),Kt=c(e),K=n(e,"P",{});var wa=r(K);Ui=i(wa,"Abbiamo usato il modulo di Python "),Ja=n(wa,"CODE",{});var ql=r(Ja);Hi=i(ql,"timeit"),ql.forEach(t),Fi=i(wa," per calcolare il tempo di esecuzione impiegato da "),Wa=n(wa,"CODE",{});var kl=r(Wa);Ji=i(kl,"code_snippet"),kl.forEach(t),Wi=i(wa,". Tipicamente l\u2019iterazione su un dataset impiega un tempo che va da un decimo di GB al secondo, a diversi GB al secondo. Questo funziona perfettamente per la maggior parte delle applicazioni, ma a volte avrai bisogno di lavorare con un dataset che \xE8 troppo grande persino per essere salvato sul tuo portatile. Ad esempio, se cercassimo di scaricare Pile per intero, avremo bisogno di 825 GB di spazio libero su disko! In questi casi, \u{1F917} Datasets permette di utilizzare processi di streaming che ci permettono di scaricare e accedere al volo ai dati, senza bisogno di scaricare l\u2019intero dataset. Diamo un\u2019occhiata a come funziona."),wa.forEach(t),Yt=c(e),b(me.$$.fragment,e),Zt=c(e),oe=n(e,"H2",{class:!0});var Is=r(oe);ue=n(Is,"A",{id:!0,class:!0,href:!0});var Il=r(ue);Qa=n(Il,"SPAN",{});var Cl=r(Qa);b(Ye.$$.fragment,Cl),Cl.forEach(t),Il.forEach(t),Qi=c(Is),Ka=n(Is,"SPAN",{});var Tl=r(Ka);Ki=i(Tl,"Streaming di dataset"),Tl.forEach(t),Is.forEach(t),Vt=c(e),Y=n(e,"P",{});var Aa=r(Y);Yi=i(Aa,"Per abilitare lo streaming dei dataset devi semplicemente passare l\u2019argomento "),Ya=n(Aa,"CODE",{});var Ol=r(Ya);Zi=i(Ol,"streaming=True"),Ol.forEach(t),Vi=i(Aa," alla funzione "),Za=n(Aa,"CODE",{});var Rl=r(Za);Xi=i(Rl,"load_dataset()"),Rl.forEach(t),eo=i(Aa,". Ad esempio, carichiamo un\u2019altra volta il dataset PubMed Abstract, ma in modalit\xE0 streaming:"),Aa.forEach(t),Xt=c(e),b(Ze.$$.fragment,e),es=c(e),L=n(e,"P",{});var $e=r(L);ao=i($e,"Invece del solito "),Va=n($e,"CODE",{});var Sl=r(Va);to=i(Sl,"Dataset"),Sl.forEach(t),so=i($e," che abbiamo incontrato in precedenza in questo capitolo, l\u2019oggetto ritornato con "),Xa=n($e,"CODE",{});var Ml=r(Xa);io=i(Ml,"streaming=True' \xE8 un "),Ml.forEach(t),oo=i($e,"IterableDataset"),et=n($e,"CODE",{});var Gl=r(et);lo=i(Gl,". Come suggerito dal nome, per accedere agli elementi di un "),Gl.forEach(t),no=i($e,"IterableDataset`, dobbiamo iterare di esso. Possiamo accedere al primo elemento del nostro dataset in streaming come segue:"),$e.forEach(t),as=c(e),b(Ve.$$.fragment,e),ts=c(e),b(Xe.$$.fragment,e),ss=c(e),Z=n(e,"P",{});var Pa=r(Z);ro=i(Pa,"Gli elementi di un dataset in streaming possono essere processati al volo utilizzando "),at=n(Pa,"CODE",{});var Nl=r(at);po=i(Nl,"IterableDataset.map()"),Nl.forEach(t),co=i(Pa,", che \xE8 utile durante l\u2019addestramento se hai bisogno di tokenizzare gli input. Il processo \xE8 uguale a quello che abbiamo utilizzato per tokenizzare il nostro dataset nel "),va=n(Pa,"A",{href:!0});var Bl=r(va);mo=i(Bl,"Capitolo 3"),Bl.forEach(t),uo=i(Pa,", con l\u2019unica differenza che ora ritorneremo gli output uno alla volta:"),Pa.forEach(t),is=c(e),b(ea.$$.fragment,e),os=c(e),b(aa.$$.fragment,e),ls=c(e),b(fe.$$.fragment,e),ns=c(e),U=n(e,"P",{});var ve=r(U);fo=i(ve,"\xC8 anche possibile mescolare un dataset in streaming utilizzato "),tt=n(ve,"CODE",{});var Ll=r(tt);ho=i(Ll,"Iterabledataset.shuffle()"),Ll.forEach(t),go=i(ve,", ma a differenza di "),st=n(ve,"CODE",{});var Ul=r(st);bo=i(Ul,"Dataset.shuffle()"),Ul.forEach(t),_o=i(ve,", questo metodo mescola solo gli elementi in un "),it=n(ve,"CODE",{});var Hl=r(it);$o=i(Hl,"buffer_size"),Hl.forEach(t),vo=i(ve," predefinito:"),ve.forEach(t),rs=c(e),b(ta.$$.fragment,e),ps=c(e),b(sa.$$.fragment,e),ds=c(e),H=n(e,"P",{});var ze=r(H);zo=i(ze,"In questo esempio, abbiamo selezionato un esempio casuale dai primi 10.000 esempi nel buffer. Una volta che accediamo a un esempio, il suo posto nel buffer \xE8 subito occupato dall\u2019esempio successivo nel corpus (in questo caso l\u2019esempio 10.0001). Puoi inoltre selezionare gli elementi da un dataset in streaming utilizzando le funzioni "),ot=n(ze,"CODE",{});var Fl=r(ot);xo=i(Fl,"IterableDataset.take()"),Fl.forEach(t),jo=i(ze," a "),lt=n(ze,"CODE",{});var Jl=r(lt);yo=i(Jl,"IterableDataset.skip()"),Jl.forEach(t),Eo=i(ze,", che funzionano un po\u2019 come "),nt=n(ze,"CODE",{});var Wl=r(nt);wo=i(Wl,"Dataset.select()"),Wl.forEach(t),Ao=i(ze,". Ad esempio, per selezionare i primi 5 esempi nel dataset PubMed Abstract dovremmo fare come segue:"),ze.forEach(t),cs=c(e),b(ia.$$.fragment,e),ms=c(e),b(oa.$$.fragment,e),us=c(e),he=n(e,"P",{});var Cs=r(he);Po=i(Cs,"Allo stesso modo, \xE8 possibile utilizzare la funzione "),rt=n(Cs,"CODE",{});var Ql=r(rt);Do=i(Ql,"IterableDataset.skip()"),Ql.forEach(t),qo=i(Cs," per creare sezioni di addestramento e di validazione da un dataset mescolato, come segue:"),Cs.forEach(t),fs=c(e),b(la.$$.fragment,e),hs=c(e),F=n(e,"P",{});var xe=r(F);ko=i(xe,"Concludiamo la nostra ricognizione dello streaming di dataset con un\u2019applicazione comune: la combinazione di pi\xF9 dataset per creare un unico corpus. \u{1F917} Datasets fornisce una funzione "),pt=n(xe,"CODE",{});var Kl=r(pt);Io=i(Kl,"interleave_datasets()"),Kl.forEach(t),Co=i(xe,", che converte una lista di oggetti "),dt=n(xe,"CODE",{});var Yl=r(dt);To=i(Yl,"IterableDataset"),Yl.forEach(t),Oo=i(xe," in un unico "),ct=n(xe,"CODE",{});var Zl=r(ct);Ro=i(Zl,"IterableDataset"),Zl.forEach(t),So=i(xe,", dove gli elementi del nuovo dataset sono ottenuti alternando tra gli esempi forniti. Questa funzione \xE8 particolarmente utile quando cerchiamo di combinare dataset di grandi dimensioni, come esempio possiamo utilizzare in streaming la sezione FreeLaw del Pile, un dataset di 51 GB di pareri legali dai tribunali statunitensi:"),xe.forEach(t),gs=c(e),b(na.$$.fragment,e),bs=c(e),b(ra.$$.fragment,e),_s=c(e),ge=n(e,"P",{});var Ts=r(ge);Mo=i(Ts,"Questo dataset \xE8 abbastanza grande da mettere sotto sforzo la RAM di molto portatili, ma siamo riusciti a caricarlo e accedervi senza alcun problema! Ora cominiamo gli esempi di FreeLaw e di PubMed Abstracts con la funzione "),mt=n(Ts,"CODE",{});var Vl=r(mt);Go=i(Vl,"interleave_datasets()"),Vl.forEach(t),No=i(Ts,":"),Ts.forEach(t),$s=c(e),b(pa.$$.fragment,e),vs=c(e),b(da.$$.fragment,e),zs=c(e),V=n(e,"P",{});var Da=r(V);Bo=i(Da,"Abbiamo utilizzato la funzione "),ut=n(Da,"CODE",{});var Xl=r(ut);Lo=i(Xl,"islice()"),Xl.forEach(t),Uo=i(Da," del modulo Python "),ft=n(Da,"CODE",{});var en=r(ft);Ho=i(en,"itertools"),en.forEach(t),Fo=i(Da," per selezionare i primi due esempi dai dataset combinati, e abbiamo visto che corrispondono ai primi esempi di ognuno dei due dataset originali."),Da.forEach(t),xs=c(e),za=n(e,"P",{});var an=r(za);Jo=i(an,"Infine, se vuoi processare il Pile in streaming, in tutti i suoi 825 GB, puoi recuperare tutti i file preparati, come segue:"),an.forEach(t),js=c(e),b(ca.$$.fragment,e),ys=c(e),b(ma.$$.fragment,e),Es=c(e),b(be.$$.fragment,e),ws=c(e),xa=n(e,"P",{});var tn=r(xa);Wo=i(tn,"Ora hai a tua disposizione tutti gli strumenti per caricare e processare dataset di ogni tipo \u2014 ma a meno che tu non sia estremamente fortunato, arriver\xE0 un momento nel tuo cammino in cui dovrai effettivamente creare un dataset per risolvere i tuoi problemi. Questo sar\xE0 argomento della prossima sezione!"),tn.forEach(t),this.h()},h(){u(m,"name","hf:doc:metadata"),u(m,"content",JSON.stringify(bn)),u(E,"id","big-data-ci-pensa-datasets"),u(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(E,"href","#big-data-ci-pensa-datasets"),u(f,"class","relative group"),u(ye,"href","https://pile.eleuther.ai"),u(ye,"rel","nofollow"),u(ne,"id","cos-pile"),u(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ne,"href","#cos-pile"),u(te,"class","relative group"),u(we,"href","https://www.eleuther.ai"),u(we,"rel","nofollow"),u(Ae,"href","https://mystic.the-eye.eu/public/AI/pile/"),u(Ae,"rel","nofollow"),u(Pe,"href","https://mystic.the-eye.eu/public/AI/pile_preliminary_components/"),u(Pe,"rel","nofollow"),u(De,"href","https://pubmed.ncbi.nlm.nih.gov/"),u(De,"rel","nofollow"),u(qe,"href","https://jsonlines.org"),u(qe,"rel","nofollow"),u(ha,"href","/course/chapter5/2"),u(de,"id","la-magia-del-memory-mapping"),u(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(de,"href","#la-magia-del-memory-mapping"),u(se,"class","relative group"),u(Se,"href","https://psutil.readthedocs.io/en/latest/"),u(Se,"rel","nofollow"),u(Ue,"href","https://wesmckinney.com/blog/apache-arrow-pandas-internals/"),u(Ue,"rel","nofollow"),u(He,"href","https://it.wikipedia.org/wiki/File_mappato_in_memoria"),u(He,"rel","nofollow"),u(Fe,"href","https://arrow.apache.org"),u(Fe,"rel","nofollow"),u(Je,"href","https://arrow.apache.org/docs/python/index.html"),u(Je,"rel","nofollow"),u(We,"href","https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a"),u(We,"rel","nofollow"),u(ue,"id","streaming-di-dataset"),u(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ue,"href","#streaming-di-dataset"),u(oe,"class","relative group"),u(va,"href","/course/chapter3")},m(e,o){a(document.head,m),p(e,I,o),p(e,f,o),a(f,E),a(E,A),_(h,A,null),a(f,P),a(f,D),a(D,j),p(e,x,o),_(q,e,o),p(e,y,o),p(e,C,o),a(C,R),a(C,k),a(k,T),a(C,ee),p(e,J,o),p(e,S,o),a(S,ae),a(S,qa),a(qa,Rs),a(S,Ss),a(S,ka),a(ka,Ms),a(S,Gs),p(e,vt,o),_(je,e,o),p(e,zt,o),p(e,le,o),a(le,Ns),a(le,ye),a(ye,Bs),a(le,Ls),p(e,xt,o),p(e,te,o),a(te,ne),a(ne,Ia),_(Ee,Ia,null),a(te,Us),a(te,Ca),a(Ca,Hs),p(e,jt,o),p(e,O,o),a(O,Fs),a(O,we),a(we,Js),a(O,Ws),a(O,Ae),a(Ae,Qs),a(O,Ks),a(O,Pe),a(Pe,Ys),a(O,Zs),a(O,De),a(De,Vs),a(O,Xs),a(O,qe),a(qe,ei),a(O,ai),a(O,Ta),a(Ta,ti),a(O,si),p(e,yt,o),_(ke,e,o),p(e,Et,o),p(e,re,o),a(re,ii),a(re,ha),a(ha,oi),a(re,li),p(e,wt,o),_(Ie,e,o),p(e,At,o),_(Ce,e,o),p(e,Pt,o),p(e,ga,o),a(ga,ni),p(e,Dt,o),_(pe,e,o),p(e,qt,o),p(e,ba,o),a(ba,ri),p(e,kt,o),_(Te,e,o),p(e,It,o),_(Oe,e,o),p(e,Ct,o),p(e,_a,o),a(_a,pi),p(e,Tt,o),p(e,se,o),a(se,de),a(de,Oa),_(Re,Oa,null),a(se,di),a(se,Ra),a(Ra,ci),p(e,Ot,o),p(e,W,o),a(W,mi),a(W,Se),a(Se,Sa),a(Sa,ui),a(W,fi),a(W,Ma),a(Ma,hi),a(W,gi),p(e,Rt,o),_(Me,e,o),p(e,St,o),p(e,ie,o),a(ie,Ga),a(Ga,bi),a(ie,_i),a(ie,Na),a(Na,$i),a(ie,vi),p(e,Mt,o),_(Ge,e,o),p(e,Gt,o),_(Ne,e,o),p(e,Nt,o),p(e,B,o),a(B,zi),a(B,Ba),a(Ba,xi),a(B,ji),a(B,La),a(La,yi),a(B,Ei),a(B,Ua),a(Ua,wi),a(B,Ai),p(e,Bt,o),_(Be,e,o),p(e,Lt,o),_(Le,e,o),p(e,Ut,o),p(e,$a,o),a($a,Pi),p(e,Ht,o),_(ce,e,o),p(e,Ft,o),p(e,Q,o),a(Q,Di),a(Q,Ue),a(Ue,qi),a(Q,ki),a(Q,He),a(He,Ii),a(Q,Ci),p(e,Jt,o),p(e,M,o),a(M,Ti),a(M,Ha),a(Ha,Oi),a(M,Ri),a(M,Fe),a(Fe,Si),a(M,Mi),a(M,Je),a(Je,Fa),a(Fa,Gi),a(M,Ni),a(M,We),a(We,Bi),a(M,Li),p(e,Wt,o),_(Qe,e,o),p(e,Qt,o),_(Ke,e,o),p(e,Kt,o),p(e,K,o),a(K,Ui),a(K,Ja),a(Ja,Hi),a(K,Fi),a(K,Wa),a(Wa,Ji),a(K,Wi),p(e,Yt,o),_(me,e,o),p(e,Zt,o),p(e,oe,o),a(oe,ue),a(ue,Qa),_(Ye,Qa,null),a(oe,Qi),a(oe,Ka),a(Ka,Ki),p(e,Vt,o),p(e,Y,o),a(Y,Yi),a(Y,Ya),a(Ya,Zi),a(Y,Vi),a(Y,Za),a(Za,Xi),a(Y,eo),p(e,Xt,o),_(Ze,e,o),p(e,es,o),p(e,L,o),a(L,ao),a(L,Va),a(Va,to),a(L,so),a(L,Xa),a(Xa,io),a(L,oo),a(L,et),a(et,lo),a(L,no),p(e,as,o),_(Ve,e,o),p(e,ts,o),_(Xe,e,o),p(e,ss,o),p(e,Z,o),a(Z,ro),a(Z,at),a(at,po),a(Z,co),a(Z,va),a(va,mo),a(Z,uo),p(e,is,o),_(ea,e,o),p(e,os,o),_(aa,e,o),p(e,ls,o),_(fe,e,o),p(e,ns,o),p(e,U,o),a(U,fo),a(U,tt),a(tt,ho),a(U,go),a(U,st),a(st,bo),a(U,_o),a(U,it),a(it,$o),a(U,vo),p(e,rs,o),_(ta,e,o),p(e,ps,o),_(sa,e,o),p(e,ds,o),p(e,H,o),a(H,zo),a(H,ot),a(ot,xo),a(H,jo),a(H,lt),a(lt,yo),a(H,Eo),a(H,nt),a(nt,wo),a(H,Ao),p(e,cs,o),_(ia,e,o),p(e,ms,o),_(oa,e,o),p(e,us,o),p(e,he,o),a(he,Po),a(he,rt),a(rt,Do),a(he,qo),p(e,fs,o),_(la,e,o),p(e,hs,o),p(e,F,o),a(F,ko),a(F,pt),a(pt,Io),a(F,Co),a(F,dt),a(dt,To),a(F,Oo),a(F,ct),a(ct,Ro),a(F,So),p(e,gs,o),_(na,e,o),p(e,bs,o),_(ra,e,o),p(e,_s,o),p(e,ge,o),a(ge,Mo),a(ge,mt),a(mt,Go),a(ge,No),p(e,$s,o),_(pa,e,o),p(e,vs,o),_(da,e,o),p(e,zs,o),p(e,V,o),a(V,Bo),a(V,ut),a(ut,Lo),a(V,Uo),a(V,ft),a(ft,Ho),a(V,Fo),p(e,xs,o),p(e,za,o),a(za,Jo),p(e,js,o),_(ca,e,o),p(e,ys,o),_(ma,e,o),p(e,Es,o),_(be,e,o),p(e,ws,o),p(e,xa,o),a(xa,Wo),As=!0},p(e,[o]){const ua={};o&2&&(ua.$$scope={dirty:o,ctx:e}),pe.$set(ua);const ht={};o&2&&(ht.$$scope={dirty:o,ctx:e}),ce.$set(ht);const gt={};o&2&&(gt.$$scope={dirty:o,ctx:e}),me.$set(gt);const bt={};o&2&&(bt.$$scope={dirty:o,ctx:e}),fe.$set(bt);const fa={};o&2&&(fa.$$scope={dirty:o,ctx:e}),be.$set(fa)},i(e){As||($(h.$$.fragment,e),$(q.$$.fragment,e),$(je.$$.fragment,e),$(Ee.$$.fragment,e),$(ke.$$.fragment,e),$(Ie.$$.fragment,e),$(Ce.$$.fragment,e),$(pe.$$.fragment,e),$(Te.$$.fragment,e),$(Oe.$$.fragment,e),$(Re.$$.fragment,e),$(Me.$$.fragment,e),$(Ge.$$.fragment,e),$(Ne.$$.fragment,e),$(Be.$$.fragment,e),$(Le.$$.fragment,e),$(ce.$$.fragment,e),$(Qe.$$.fragment,e),$(Ke.$$.fragment,e),$(me.$$.fragment,e),$(Ye.$$.fragment,e),$(Ze.$$.fragment,e),$(Ve.$$.fragment,e),$(Xe.$$.fragment,e),$(ea.$$.fragment,e),$(aa.$$.fragment,e),$(fe.$$.fragment,e),$(ta.$$.fragment,e),$(sa.$$.fragment,e),$(ia.$$.fragment,e),$(oa.$$.fragment,e),$(la.$$.fragment,e),$(na.$$.fragment,e),$(ra.$$.fragment,e),$(pa.$$.fragment,e),$(da.$$.fragment,e),$(ca.$$.fragment,e),$(ma.$$.fragment,e),$(be.$$.fragment,e),As=!0)},o(e){v(h.$$.fragment,e),v(q.$$.fragment,e),v(je.$$.fragment,e),v(Ee.$$.fragment,e),v(ke.$$.fragment,e),v(Ie.$$.fragment,e),v(Ce.$$.fragment,e),v(pe.$$.fragment,e),v(Te.$$.fragment,e),v(Oe.$$.fragment,e),v(Re.$$.fragment,e),v(Me.$$.fragment,e),v(Ge.$$.fragment,e),v(Ne.$$.fragment,e),v(Be.$$.fragment,e),v(Le.$$.fragment,e),v(ce.$$.fragment,e),v(Qe.$$.fragment,e),v(Ke.$$.fragment,e),v(me.$$.fragment,e),v(Ye.$$.fragment,e),v(Ze.$$.fragment,e),v(Ve.$$.fragment,e),v(Xe.$$.fragment,e),v(ea.$$.fragment,e),v(aa.$$.fragment,e),v(fe.$$.fragment,e),v(ta.$$.fragment,e),v(sa.$$.fragment,e),v(ia.$$.fragment,e),v(oa.$$.fragment,e),v(la.$$.fragment,e),v(na.$$.fragment,e),v(ra.$$.fragment,e),v(pa.$$.fragment,e),v(da.$$.fragment,e),v(ca.$$.fragment,e),v(ma.$$.fragment,e),v(be.$$.fragment,e),As=!1},d(e){t(m),e&&t(I),e&&t(f),z(h),e&&t(x),z(q,e),e&&t(y),e&&t(C),e&&t(J),e&&t(S),e&&t(vt),z(je,e),e&&t(zt),e&&t(le),e&&t(xt),e&&t(te),z(Ee),e&&t(jt),e&&t(O),e&&t(yt),z(ke,e),e&&t(Et),e&&t(re),e&&t(wt),z(Ie,e),e&&t(At),z(Ce,e),e&&t(Pt),e&&t(ga),e&&t(Dt),z(pe,e),e&&t(qt),e&&t(ba),e&&t(kt),z(Te,e),e&&t(It),z(Oe,e),e&&t(Ct),e&&t(_a),e&&t(Tt),e&&t(se),z(Re),e&&t(Ot),e&&t(W),e&&t(Rt),z(Me,e),e&&t(St),e&&t(ie),e&&t(Mt),z(Ge,e),e&&t(Gt),z(Ne,e),e&&t(Nt),e&&t(B),e&&t(Bt),z(Be,e),e&&t(Lt),z(Le,e),e&&t(Ut),e&&t($a),e&&t(Ht),z(ce,e),e&&t(Ft),e&&t(Q),e&&t(Jt),e&&t(M),e&&t(Wt),z(Qe,e),e&&t(Qt),z(Ke,e),e&&t(Kt),e&&t(K),e&&t(Yt),z(me,e),e&&t(Zt),e&&t(oe),z(Ye),e&&t(Vt),e&&t(Y),e&&t(Xt),z(Ze,e),e&&t(es),e&&t(L),e&&t(as),z(Ve,e),e&&t(ts),z(Xe,e),e&&t(ss),e&&t(Z),e&&t(is),z(ea,e),e&&t(os),z(aa,e),e&&t(ls),z(fe,e),e&&t(ns),e&&t(U),e&&t(rs),z(ta,e),e&&t(ps),z(sa,e),e&&t(ds),e&&t(H),e&&t(cs),z(ia,e),e&&t(ms),z(oa,e),e&&t(us),e&&t(he),e&&t(fs),z(la,e),e&&t(hs),e&&t(F),e&&t(gs),z(na,e),e&&t(bs),z(ra,e),e&&t(_s),e&&t(ge),e&&t($s),z(pa,e),e&&t(vs),z(da,e),e&&t(zs),e&&t(V),e&&t(xs),e&&t(za),e&&t(js),z(ca,e),e&&t(ys),z(ma,e),e&&t(Es),z(be,e),e&&t(ws),e&&t(xa)}}}const bn={local:"big-data-ci-pensa-datasets",sections:[{local:"cos-pile",title:"Cos'\xE8 Pile?"},{local:"la-magia-del-memory-mapping",title:"La magia del memory mapping "},{local:"streaming-di-dataset",title:"Streaming di dataset"}],title:"Big data? Ci pensa \u{1F917} Datasets!"};function _n(N){return rn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class En extends sn{constructor(m){super();on(this,m,_n,gn,ln,{})}}export{En as default,bn as metadata};
