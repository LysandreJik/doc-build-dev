import{S as li,i as ai,s as ri,e as r,k as h,w as Xe,t as l,M as ti,c as t,d as o,m as _,a as n,x as Ye,h as a,b as $,G as i,g as c,y as Ze,q as ei,o as ii,B as oi,v as ni}from"../../chunks/vendor-hf-doc-builder.js";import{T as si}from"../../chunks/Tip-hf-doc-builder.js";import{I as ci}from"../../chunks/IconCopyLink-hf-doc-builder.js";function di(J){let u,v,p,f;return{c(){u=l("\u26A0\uFE0F Per poter usufruire di tutte le funzioni disponibili con il Model Hub e i \u{1F917} Transformers, si consiglia di "),v=r("a"),p=l("creare un account"),f=l("."),this.h()},l(d){u=a(d,"\u26A0\uFE0F Per poter usufruire di tutte le funzioni disponibili con il Model Hub e i \u{1F917} Transformers, si consiglia di "),v=t(d,"A",{href:!0});var m=n(v);p=a(m,"creare un account"),m.forEach(o),f=a(d,"."),this.h()},h(){$(v,"href","https://huggingface.co/join")},m(d,m){c(d,u,m),c(d,v,m),i(v,p),c(d,f,m)},d(d){d&&o(u),d&&o(v),d&&o(f)}}}function ui(J){let u,v,p,f,d,m,oe,x,le,K,z,ae,w,re,te,F,ne,se,V,L,ce,W,b,k,D,de,ue,me,E,G,pe,fe,Q,ve,he,R,_e,ze,be,A,H,Ee,ge,X,C,Ie,Y,g,$e,U,qe,Pe,S,Te,we,Z,I,Le,j,ke,Ae,B,Ce,Se,ee,q,ie;return m=new ci({}),q=new si({props:{$$slots:{default:[di]},$$scope:{ctx:J}}}),{c(){u=r("meta"),v=h(),p=r("h1"),f=r("a"),d=r("span"),Xe(m.$$.fragment),oe=h(),x=r("span"),le=l("Introduzione"),K=h(),z=r("p"),ae=l("Come si \xE8 visto nel "),w=r("a"),re=l("Capitolo 1"),te=l(`,  I modelli Transformers sono solitamente molto grandi.
Con milioni o decine di `),F=r("em"),ne=l("miliardi"),se=l(` di parametri, l\u2019addestramento e la distribuzione di questi modelli \xE8 un\u2019impresa complicata.
Inoltre, con i nuovi modelli che vengono rilasciati quasi ogni giorno e ognuno dei quali ha una propria implementazione, provarli tutti non \xE8 un lavoro facile.`),V=h(),L=r("p"),ce=l("La libreria \u{1F917} Transformers \xE8 stata creata per risolvere questo problema. Il suo obiettivo \xE8 fornire un\u2019unica API attraverso la quale caricare, addestrare e salvare qualsiasi modello Transformer. Le caratteristiche principali della libreria sono:"),W=h(),b=r("ul"),k=r("li"),D=r("strong"),de=l("Facilit\xE0 d\u2019uso"),ue=l(": \xC8 possibile scaricare, caricare ed utilizzare un modello NLP all\u2019avanguardia per fare inferenza con appena due righe di codice."),me=h(),E=r("li"),G=r("strong"),pe=l("Flessibilit\xE0"),fe=l(": Al loro interno, tutti i modelli sono semplici classi PyTorch "),Q=r("code"),ve=l("nn.Module"),he=l(" o TensorFlow "),R=r("code"),_e=l("tf.keras.Model"),ze=l(" e possono essere gestiti come qualsiasi altro modello nei rispettivi framework di apprendimento automatico (ML)."),be=h(),A=r("li"),H=r("strong"),Ee=l("Semplicit\xE0"),ge=l(": La libreria non contiene quasi nessuna astrazione. Il concetto di \u201CAll in one file\u201D \xE8 fondamentale: il forward pass di un modello \xE8 interamente definito in un singolo file, in modo che il codice stesso sia comprensibile e violabile."),X=h(),C=r("p"),Ie=l("Quest\u2019ultima caratteristica rende \u{1F917} Transformers molto diversi da altre librerie ML. I modelli non sono costruiti su moduli condivisi tra i file, ma ogni modello ha i propri layers. Oltre a rendere i modelli pi\xF9 accessibili e comprensibili, questo permette di sperimentare facilmente su un modello senza influenzare gli altri."),Y=h(),g=r("p"),$e=l("Questo capitolo inizier\xE0 con un esempio in cui usiamo un modello e un tokenizer insieme per replicare la funzione "),U=r("code"),qe=l("pipeline()"),Pe=l(" introdotta nel "),S=r("a"),Te=l("Capitolo 1"),we=l(". Successivamente, parleremo dell\u2019API del modello: ci immergeremo nelle classi del modello e della configurazione e mostreremo come caricare un modello e come esso elabora gli input numerici per produrre previsioni."),Z=h(),I=r("p"),Le=l("Successivamente vedremo l\u2019API del tokenizer, che \xE8 l\u2019altro componente principale della funzione "),j=r("code"),ke=l("pipeline()"),Ae=l(". I tokenizer si occupano della prima e dell\u2019ultima fase di elaborazione, gestendo la conversione da testo a input numerici per la rete neurale e la conversione di nuovo in testo quando \xE8 necessario. Infine, mostreremo come gestire l\u2019invio di pi\xF9 frasi a un modello in un batch preparato, per poi concludere il tutto con un\u2019analisi pi\xF9 approfondita della funzione di alto livello "),B=r("code"),Ce=l("tokenizer()"),Se=l("."),ee=h(),Xe(q.$$.fragment),this.h()},l(e){const s=ti('[data-svelte="svelte-1phssyn"]',document.head);u=t(s,"META",{name:!0,content:!0}),s.forEach(o),v=_(e),p=t(e,"H1",{class:!0});var P=n(p);f=t(P,"A",{id:!0,class:!0,href:!0});var Oe=n(f);d=t(Oe,"SPAN",{});var Ne=n(d);Ye(m.$$.fragment,Ne),Ne.forEach(o),Oe.forEach(o),oe=_(P),x=t(P,"SPAN",{});var xe=n(x);le=a(xe,"Introduzione"),xe.forEach(o),P.forEach(o),K=_(e),z=t(e,"P",{});var y=n(z);ae=a(y,"Come si \xE8 visto nel "),w=t(y,"A",{href:!0});var Fe=n(w);re=a(Fe,"Capitolo 1"),Fe.forEach(o),te=a(y,`,  I modelli Transformers sono solitamente molto grandi.
Con milioni o decine di `),F=t(y,"EM",{});var De=n(F);ne=a(De,"miliardi"),De.forEach(o),se=a(y,` di parametri, l\u2019addestramento e la distribuzione di questi modelli \xE8 un\u2019impresa complicata.
Inoltre, con i nuovi modelli che vengono rilasciati quasi ogni giorno e ognuno dei quali ha una propria implementazione, provarli tutti non \xE8 un lavoro facile.`),y.forEach(o),V=_(e),L=t(e,"P",{});var Ge=n(L);ce=a(Ge,"La libreria \u{1F917} Transformers \xE8 stata creata per risolvere questo problema. Il suo obiettivo \xE8 fornire un\u2019unica API attraverso la quale caricare, addestrare e salvare qualsiasi modello Transformer. Le caratteristiche principali della libreria sono:"),Ge.forEach(o),W=_(e),b=t(e,"UL",{});var M=n(b);k=t(M,"LI",{});var ye=n(k);D=t(ye,"STRONG",{});var Qe=n(D);de=a(Qe,"Facilit\xE0 d\u2019uso"),Qe.forEach(o),ue=a(ye,": \xC8 possibile scaricare, caricare ed utilizzare un modello NLP all\u2019avanguardia per fare inferenza con appena due righe di codice."),ye.forEach(o),me=_(M),E=t(M,"LI",{});var T=n(E);G=t(T,"STRONG",{});var Re=n(G);pe=a(Re,"Flessibilit\xE0"),Re.forEach(o),fe=a(T,": Al loro interno, tutti i modelli sono semplici classi PyTorch "),Q=t(T,"CODE",{});var He=n(Q);ve=a(He,"nn.Module"),He.forEach(o),he=a(T," o TensorFlow "),R=t(T,"CODE",{});var Ue=n(R);_e=a(Ue,"tf.keras.Model"),Ue.forEach(o),ze=a(T," e possono essere gestiti come qualsiasi altro modello nei rispettivi framework di apprendimento automatico (ML)."),T.forEach(o),be=_(M),A=t(M,"LI",{});var Me=n(A);H=t(Me,"STRONG",{});var je=n(H);Ee=a(je,"Semplicit\xE0"),je.forEach(o),ge=a(Me,": La libreria non contiene quasi nessuna astrazione. Il concetto di \u201CAll in one file\u201D \xE8 fondamentale: il forward pass di un modello \xE8 interamente definito in un singolo file, in modo che il codice stesso sia comprensibile e violabile."),Me.forEach(o),M.forEach(o),X=_(e),C=t(e,"P",{});var Be=n(C);Ie=a(Be,"Quest\u2019ultima caratteristica rende \u{1F917} Transformers molto diversi da altre librerie ML. I modelli non sono costruiti su moduli condivisi tra i file, ma ogni modello ha i propri layers. Oltre a rendere i modelli pi\xF9 accessibili e comprensibili, questo permette di sperimentare facilmente su un modello senza influenzare gli altri."),Be.forEach(o),Y=_(e),g=t(e,"P",{});var O=n(g);$e=a(O,"Questo capitolo inizier\xE0 con un esempio in cui usiamo un modello e un tokenizer insieme per replicare la funzione "),U=t(O,"CODE",{});var Je=n(U);qe=a(Je,"pipeline()"),Je.forEach(o),Pe=a(O," introdotta nel "),S=t(O,"A",{href:!0});var Ke=n(S);Te=a(Ke,"Capitolo 1"),Ke.forEach(o),we=a(O,". Successivamente, parleremo dell\u2019API del modello: ci immergeremo nelle classi del modello e della configurazione e mostreremo come caricare un modello e come esso elabora gli input numerici per produrre previsioni."),O.forEach(o),Z=_(e),I=t(e,"P",{});var N=n(I);Le=a(N,"Successivamente vedremo l\u2019API del tokenizer, che \xE8 l\u2019altro componente principale della funzione "),j=t(N,"CODE",{});var Ve=n(j);ke=a(Ve,"pipeline()"),Ve.forEach(o),Ae=a(N,". I tokenizer si occupano della prima e dell\u2019ultima fase di elaborazione, gestendo la conversione da testo a input numerici per la rete neurale e la conversione di nuovo in testo quando \xE8 necessario. Infine, mostreremo come gestire l\u2019invio di pi\xF9 frasi a un modello in un batch preparato, per poi concludere il tutto con un\u2019analisi pi\xF9 approfondita della funzione di alto livello "),B=t(N,"CODE",{});var We=n(B);Ce=a(We,"tokenizer()"),We.forEach(o),Se=a(N,"."),N.forEach(o),ee=_(e),Ye(q.$$.fragment,e),this.h()},h(){$(u,"name","hf:doc:metadata"),$(u,"content",JSON.stringify(mi)),$(f,"id","introduzione"),$(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(f,"href","#introduzione"),$(p,"class","relative group"),$(w,"href","/course/chapter1"),$(S,"href","/course/chapter1")},m(e,s){i(document.head,u),c(e,v,s),c(e,p,s),i(p,f),i(f,d),Ze(m,d,null),i(p,oe),i(p,x),i(x,le),c(e,K,s),c(e,z,s),i(z,ae),i(z,w),i(w,re),i(z,te),i(z,F),i(F,ne),i(z,se),c(e,V,s),c(e,L,s),i(L,ce),c(e,W,s),c(e,b,s),i(b,k),i(k,D),i(D,de),i(k,ue),i(b,me),i(b,E),i(E,G),i(G,pe),i(E,fe),i(E,Q),i(Q,ve),i(E,he),i(E,R),i(R,_e),i(E,ze),i(b,be),i(b,A),i(A,H),i(H,Ee),i(A,ge),c(e,X,s),c(e,C,s),i(C,Ie),c(e,Y,s),c(e,g,s),i(g,$e),i(g,U),i(U,qe),i(g,Pe),i(g,S),i(S,Te),i(g,we),c(e,Z,s),c(e,I,s),i(I,Le),i(I,j),i(j,ke),i(I,Ae),i(I,B),i(B,Ce),i(I,Se),c(e,ee,s),Ze(q,e,s),ie=!0},p(e,[s]){const P={};s&2&&(P.$$scope={dirty:s,ctx:e}),q.$set(P)},i(e){ie||(ei(m.$$.fragment,e),ei(q.$$.fragment,e),ie=!0)},o(e){ii(m.$$.fragment,e),ii(q.$$.fragment,e),ie=!1},d(e){o(u),e&&o(v),e&&o(p),oi(m),e&&o(K),e&&o(z),e&&o(V),e&&o(L),e&&o(W),e&&o(b),e&&o(X),e&&o(C),e&&o(Y),e&&o(g),e&&o(Z),e&&o(I),e&&o(ee),oi(q,e)}}}const mi={local:"introduzione",title:"Introduzione"};function pi(J){return ni(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _i extends li{constructor(u){super();ai(this,u,pi,ui,ri,{})}}export{_i as default,mi as metadata};
