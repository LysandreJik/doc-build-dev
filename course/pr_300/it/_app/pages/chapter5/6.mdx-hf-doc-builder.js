import{S as Lc,i as Fc,s as Hc,e as n,k as d,w as E,t as a,M as Mc,c as i,d as s,m,x as k,a as r,h as o,b as y,N as Ic,f as Po,G as t,g as c,y as w,o as g,p as So,q as b,B as x,v as Uc,n as Ao}from"../../chunks/vendor-hf-doc-builder.js";import{T as Rc}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Gc}from"../../chunks/Youtube-hf-doc-builder.js";import{I as ga}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as S}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Nc}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Qc}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Bc(M){let p,$;return p=new Nc({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"}]}}),{c(){E(p.$$.fragment)},l(f){k(p.$$.fragment,f)},m(f,q){w(p,f,q),$=!0},i(f){$||(b(p.$$.fragment,f),$=!0)},o(f){g(p.$$.fragment,f),$=!1},d(f){x(p,f)}}}function Yc(M){let p,$;return p=new Nc({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"}]}}),{c(){E(p.$$.fragment)},l(f){k(p.$$.fragment,f)},m(f,q){w(p,f,q),$=!0},i(f){$||(b(p.$$.fragment,f),$=!0)},o(f){g(p.$$.fragment,f),$=!1},d(f){x(p,f)}}}function Wc(M){let p,$,f,q,h,v,A,_,T,j,H,O,P,I,R,L,G,z,N,Y;return{c(){p=n("p"),$=a("\u270F\uFE0F "),f=n("strong"),q=a("Prova tu!"),h=a(" Prova ad utilizzare "),v=n("code"),A=a("Dataset.map()"),_=a(" per far esplodere la colonna "),T=n("code"),j=a("commenti"),H=a(" di "),O=n("code"),P=a("issues_dataset"),I=d(),R=n("em"),L=a("senza"),G=a(" utilizzare Pandas. \xC8 un po\u2019 difficile: potrebbe tornarti utile la sezione "),z=n("a"),N=a("\u201CBatch mapping\u201D"),Y=a(" della documentazione di \u{1F917} Datasets."),this.h()},l(U){p=i(U,"P",{});var D=r(p);$=o(D,"\u270F\uFE0F "),f=i(D,"STRONG",{});var Q=r(f);q=o(Q,"Prova tu!"),Q.forEach(s),h=o(D," Prova ad utilizzare "),v=i(D,"CODE",{});var u=r(v);A=o(u,"Dataset.map()"),u.forEach(s),_=o(D," per far esplodere la colonna "),T=i(D,"CODE",{});var C=r(T);j=o(C,"commenti"),C.forEach(s),H=o(D," di "),O=i(D,"CODE",{});var F=r(O);P=o(F,"issues_dataset"),F.forEach(s),I=m(D),R=i(D,"EM",{});var W=r(R);L=o(W,"senza"),W.forEach(s),G=o(D," utilizzare Pandas. \xC8 un po\u2019 difficile: potrebbe tornarti utile la sezione "),z=i(D,"A",{href:!0,rel:!0});var ne=r(z);N=o(ne,"\u201CBatch mapping\u201D"),ne.forEach(s),Y=o(D," della documentazione di \u{1F917} Datasets."),D.forEach(s),this.h()},h(){y(z,"href","https://huggingface.co/docs/datasets/v1.12.1/about_map_batch.html?batch-mapping#batch-mapping"),y(z,"rel","nofollow")},m(U,D){c(U,p,D),t(p,$),t(p,f),t(f,q),t(p,h),t(p,v),t(v,A),t(p,_),t(p,T),t(T,j),t(p,H),t(p,O),t(O,P),t(p,I),t(p,R),t(R,L),t(p,G),t(p,z),t(z,N),t(p,Y)},d(U){U&&s(p)}}}function Vc(M){let p,$,f,q,h,v,A,_,T,j,H,O,P,I,R,L,G;return p=new S({props:{code:`from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModel

model_ckpt = <span class="hljs-string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){E(p.$$.fragment),$=d(),f=n("p"),q=a("Nota che abbiamo impostato "),h=n("code"),v=a("from_pt=True"),A=a(" come argomento del metodo "),_=n("code"),T=a("from_pretrained()"),j=a(". Questo perch\xE8 il checkpoint "),H=n("code"),O=a("multi-qa-mpnet-base-dot-v1"),P=a(" ha solo pesi PyTorch, quindi impostare "),I=n("code"),R=a("from_pt=True"),L=a(" li convertir\xE0 automaticamente in formato TensorFlow. Come puoi vedere, \xE8 molto facile passare dall\u2019uno all\u2019altro su \u{1F917} Transformers!")},l(z){k(p.$$.fragment,z),$=m(z),f=i(z,"P",{});var N=r(f);q=o(N,"Nota che abbiamo impostato "),h=i(N,"CODE",{});var Y=r(h);v=o(Y,"from_pt=True"),Y.forEach(s),A=o(N," come argomento del metodo "),_=i(N,"CODE",{});var U=r(_);T=o(U,"from_pretrained()"),U.forEach(s),j=o(N,". Questo perch\xE8 il checkpoint "),H=i(N,"CODE",{});var D=r(H);O=o(D,"multi-qa-mpnet-base-dot-v1"),D.forEach(s),P=o(N," ha solo pesi PyTorch, quindi impostare "),I=i(N,"CODE",{});var Q=r(I);R=o(Q,"from_pt=True"),Q.forEach(s),L=o(N," li convertir\xE0 automaticamente in formato TensorFlow. Come puoi vedere, \xE8 molto facile passare dall\u2019uno all\u2019altro su \u{1F917} Transformers!"),N.forEach(s)},m(z,N){w(p,z,N),c(z,$,N),c(z,f,N),t(f,q),t(f,h),t(h,v),t(f,A),t(f,_),t(_,T),t(f,j),t(f,H),t(H,O),t(f,P),t(f,I),t(I,R),t(f,L),G=!0},i(z){G||(b(p.$$.fragment,z),G=!0)},o(z){g(p.$$.fragment,z),G=!1},d(z){x(p,z),z&&s($),z&&s(f)}}}function Jc(M){let p,$,f,q,h,v,A;return p=new S({props:{code:`from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

model_ckpt = <span class="hljs-string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)`}}),v=new S({props:{code:`import torch

device = torch.device("cuda")
model.to(device)`,highlighted:`<span class="hljs-keyword">import</span> torch

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)
model.to(device)`}}),{c(){E(p.$$.fragment),$=d(),f=n("p"),q=a("Per accelerare il processo di embedding, \xE8 bene usare la GPU per il modello e gli input, quindi:"),h=d(),E(v.$$.fragment)},l(_){k(p.$$.fragment,_),$=m(_),f=i(_,"P",{});var T=r(f);q=o(T,"Per accelerare il processo di embedding, \xE8 bene usare la GPU per il modello e gli input, quindi:"),T.forEach(s),h=m(_),k(v.$$.fragment,_)},m(_,T){w(p,_,T),c(_,$,T),c(_,f,T),t(f,q),c(_,h,T),w(v,_,T),A=!0},i(_){A||(b(p.$$.fragment,_),b(v.$$.fragment,_),A=!0)},o(_){g(p.$$.fragment,_),g(v.$$.fragment,_),A=!1},d(_){x(p,_),_&&s($),_&&s(f),_&&s(h),x(v,_)}}}function Xc(M){let p,$,f,q,h,v,A,_,T,j,H,O,P,I,R,L,G,z,N,Y,U,D,Q;return p=new S({props:{code:`def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">text_list</span>):
    encoded_input = tokenizer(
        text_list, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
    )
    encoded_input = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()}
    model_output = model(**encoded_input)
    <span class="hljs-keyword">return</span> cls_pooling(model_output)`}}),v=new S({props:{code:`embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape`,highlighted:`embedding = get_embeddings(comments_dataset[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
embedding.shape`}}),_=new S({props:{code:"TensorShape([1, 768])",highlighted:'TensorShape([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),D=new S({props:{code:`embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)`,highlighted:`embeddings_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="hljs-string">&quot;text&quot;</span>]).numpy()[<span class="hljs-number">0</span>]}
)`}}),{c(){E(p.$$.fragment),$=d(),f=n("p"),q=a("Possiamo testare la funzione dandole in input la prima voce testuale del nostro corpus e studiando le dimensioni dell\u2019output:"),h=d(),E(v.$$.fragment),A=d(),E(_.$$.fragment),T=d(),j=n("p"),H=a("Bene, abbiamo convertito la prima voce del nostro corpus in un vettore a 768 dimensioni! Possiamo usare "),O=n("code"),P=a("Dataset.map()"),I=a(" per applicare la nostra funzione "),R=n("code"),L=a("get_embedding()"),G=a(" a ogni riga del nostro corpus, quindi creiamo una nuova colonna "),z=n("code"),N=a("embedding"),Y=a(" cos\xEC:"),U=d(),E(D.$$.fragment)},l(u){k(p.$$.fragment,u),$=m(u),f=i(u,"P",{});var C=r(f);q=o(C,"Possiamo testare la funzione dandole in input la prima voce testuale del nostro corpus e studiando le dimensioni dell\u2019output:"),C.forEach(s),h=m(u),k(v.$$.fragment,u),A=m(u),k(_.$$.fragment,u),T=m(u),j=i(u,"P",{});var F=r(j);H=o(F,"Bene, abbiamo convertito la prima voce del nostro corpus in un vettore a 768 dimensioni! Possiamo usare "),O=i(F,"CODE",{});var W=r(O);P=o(W,"Dataset.map()"),W.forEach(s),I=o(F," per applicare la nostra funzione "),R=i(F,"CODE",{});var ne=r(R);L=o(ne,"get_embedding()"),ne.forEach(s),G=o(F," a ogni riga del nostro corpus, quindi creiamo una nuova colonna "),z=i(F,"CODE",{});var de=r(z);N=o(de,"embedding"),de.forEach(s),Y=o(F," cos\xEC:"),F.forEach(s),U=m(u),k(D.$$.fragment,u)},m(u,C){w(p,u,C),c(u,$,C),c(u,f,C),t(f,q),c(u,h,C),w(v,u,C),c(u,A,C),w(_,u,C),c(u,T,C),c(u,j,C),t(j,H),t(j,O),t(O,P),t(j,I),t(j,R),t(R,L),t(j,G),t(j,z),t(z,N),t(j,Y),c(u,U,C),w(D,u,C),Q=!0},i(u){Q||(b(p.$$.fragment,u),b(v.$$.fragment,u),b(_.$$.fragment,u),b(D.$$.fragment,u),Q=!0)},o(u){g(p.$$.fragment,u),g(v.$$.fragment,u),g(_.$$.fragment,u),g(D.$$.fragment,u),Q=!1},d(u){x(p,u),u&&s($),u&&s(f),u&&s(h),x(v,u),u&&s(A),x(_,u),u&&s(T),u&&s(j),u&&s(U),x(D,u)}}}function Kc(M){let p,$,f,q,h,v,A,_,T,j,H,O,P,I,R,L,G,z,N,Y,U,D,Q;return p=new S({props:{code:`def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">text_list</span>):
    encoded_input = tokenizer(
        text_list, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
    )
    encoded_input = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()}
    model_output = model(**encoded_input)
    <span class="hljs-keyword">return</span> cls_pooling(model_output)`}}),v=new S({props:{code:`embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape`,highlighted:`embedding = get_embeddings(comments_dataset[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
embedding.shape`}}),_=new S({props:{code:"torch.Size([1, 768])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),D=new S({props:{code:`embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)`,highlighted:`embeddings_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="hljs-string">&quot;text&quot;</span>]).detach().cpu().numpy()[<span class="hljs-number">0</span>]}
)`}}),{c(){E(p.$$.fragment),$=d(),f=n("p"),q=a("Possiamo testare la funzione sul primo testo nel nostro corpus, e ispezionandone le dimensioni dell\u2019ouput:"),h=d(),E(v.$$.fragment),A=d(),E(_.$$.fragment),T=d(),j=n("p"),H=a("Bene, abbiamo convertito la prima voce del nostro corpus in un vettore a 768 dimensioni! Possiamo usare "),O=n("code"),P=a("Dataset.map()"),I=a(" per applicare la nostra funzione "),R=n("code"),L=a("get_embedding()"),G=a(" a ogni riga del nostro corpus, quindi creiamo una nuova colonna "),z=n("code"),N=a("embedding"),Y=a(" cos\xEC:"),U=d(),E(D.$$.fragment)},l(u){k(p.$$.fragment,u),$=m(u),f=i(u,"P",{});var C=r(f);q=o(C,"Possiamo testare la funzione sul primo testo nel nostro corpus, e ispezionandone le dimensioni dell\u2019ouput:"),C.forEach(s),h=m(u),k(v.$$.fragment,u),A=m(u),k(_.$$.fragment,u),T=m(u),j=i(u,"P",{});var F=r(j);H=o(F,"Bene, abbiamo convertito la prima voce del nostro corpus in un vettore a 768 dimensioni! Possiamo usare "),O=i(F,"CODE",{});var W=r(O);P=o(W,"Dataset.map()"),W.forEach(s),I=o(F," per applicare la nostra funzione "),R=i(F,"CODE",{});var ne=r(R);L=o(ne,"get_embedding()"),ne.forEach(s),G=o(F," a ogni riga del nostro corpus, quindi creiamo una nuova colonna "),z=i(F,"CODE",{});var de=r(z);N=o(de,"embedding"),de.forEach(s),Y=o(F," cos\xEC:"),F.forEach(s),U=m(u),k(D.$$.fragment,u)},m(u,C){w(p,u,C),c(u,$,C),c(u,f,C),t(f,q),c(u,h,C),w(v,u,C),c(u,A,C),w(_,u,C),c(u,T,C),c(u,j,C),t(j,H),t(j,O),t(O,P),t(j,I),t(j,R),t(R,L),t(j,G),t(j,z),t(z,N),t(j,Y),c(u,U,C),w(D,u,C),Q=!0},i(u){Q||(b(p.$$.fragment,u),b(v.$$.fragment,u),b(_.$$.fragment,u),b(D.$$.fragment,u),Q=!0)},o(u){g(p.$$.fragment,u),g(v.$$.fragment,u),g(_.$$.fragment,u),g(D.$$.fragment,u),Q=!1},d(u){x(p,u),u&&s($),u&&s(f),u&&s(h),x(v,u),u&&s(A),x(_,u),u&&s(T),u&&s(j),u&&s(U),x(D,u)}}}function Zc(M){let p,$,f,q;return p=new S({props:{code:`question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape`,highlighted:`question = <span class="hljs-string">&quot;How can I load a dataset offline?&quot;</span>
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape`}}),f=new S({props:{code:"(1, 768)",highlighted:'(<span class="hljs-number">1</span>, <span class="hljs-number">768</span>)'}}),{c(){E(p.$$.fragment),$=d(),E(f.$$.fragment)},l(h){k(p.$$.fragment,h),$=m(h),k(f.$$.fragment,h)},m(h,v){w(p,h,v),c(h,$,v),w(f,h,v),q=!0},i(h){q||(b(p.$$.fragment,h),b(f.$$.fragment,h),q=!0)},o(h){g(p.$$.fragment,h),g(f.$$.fragment,h),q=!1},d(h){x(p,h),h&&s($),x(f,h)}}}function ed(M){let p,$,f,q;return p=new S({props:{code:`question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape`,highlighted:`question = <span class="hljs-string">&quot;How can I load a dataset offline?&quot;</span>
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape`}}),f=new S({props:{code:"torch.Size([1, 768])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),{c(){E(p.$$.fragment),$=d(),E(f.$$.fragment)},l(h){k(p.$$.fragment,h),$=m(h),k(f.$$.fragment,h)},m(h,v){w(p,h,v),c(h,$,v),w(f,h,v),q=!0},i(h){q||(b(p.$$.fragment,h),b(f.$$.fragment,h),q=!0)},o(h){g(p.$$.fragment,h),g(f.$$.fragment,h),q=!1},d(h){x(p,h),h&&s($),x(f,h)}}}function td(M){let p,$,f,q,h,v,A,_,T,j,H;return{c(){p=n("p"),$=a("\u270F\uFE0F "),f=n("strong"),q=a("Prova tu!"),h=a(" Crea la tua query e prova a trovare una risposta tra i documenti raccolti. Potresti aver bisogno di aumentare il parametro "),v=n("code"),A=a("k"),_=a(" in "),T=n("code"),j=a("Dataset.get_nearest_examples()"),H=a(" per allargare la ricerca.")},l(O){p=i(O,"P",{});var P=r(p);$=o(P,"\u270F\uFE0F "),f=i(P,"STRONG",{});var I=r(f);q=o(I,"Prova tu!"),I.forEach(s),h=o(P," Crea la tua query e prova a trovare una risposta tra i documenti raccolti. Potresti aver bisogno di aumentare il parametro "),v=i(P,"CODE",{});var R=r(v);A=o(R,"k"),R.forEach(s),_=o(P," in "),T=i(P,"CODE",{});var L=r(T);j=o(L,"Dataset.get_nearest_examples()"),L.forEach(s),H=o(P," per allargare la ricerca."),P.forEach(s)},m(O,P){c(O,p,P),t(p,$),t(p,f),t(f,q),t(p,h),t(p,v),t(v,A),t(p,_),t(p,T),t(T,j),t(p,H)},d(O){O&&s(p)}}}function sd(M){let p,$,f,q,h,v,A,_,T,j,H,O,P,I,R,L,G,z,N,Y,U,D,Q,u,C,F,W,ne,de,Oo,ba,ie,Io,Tt,Ro,No,ss,Lo,Fo,as,Ho,Mo,va,Pt,Uo,$a,Ce,We,Gr,Go,Ve,Qr,Ea,Te,Ae,os,Je,Qo,ns,Bo,ka,St,Yo,wa,Xe,xa,be,Wo,is,Vo,Jo,At,Xo,Ko,ya,Ke,qa,Ze,ja,V,Zo,rs,en,tn,ls,sn,an,cs,on,nn,ds,rn,ln,ms,cn,dn,Da,et,za,tt,Ca,J,mn,ps,pn,un,us,fn,hn,fs,_n,gn,hs,bn,vn,_s,$n,En,Ta,st,Pa,at,Sa,K,kn,gs,wn,xn,bs,yn,qn,Oe,jn,vs,Dn,zn,$s,Cn,Tn,Aa,ot,Oa,Ie,Pn,Es,Sn,An,Ia,nt,Ra,it,Na,Re,On,ks,In,Rn,La,rt,Fa,ee,ws,Z,Ha,Nn,xs,Ln,Fn,ys,Hn,Mn,qs,Un,Gn,js,Qn,Bn,me,te,Ds,Yn,Wn,zs,Vn,Jn,Cs,Xn,Kn,Ts,Zn,ei,Ps,ti,si,se,Ss,ai,oi,As,ni,ii,Os,ri,li,Is,ci,di,Rs,mi,pi,ae,Ns,ui,fi,Ls,hi,_i,Fs,gi,bi,Hs,vi,$i,Ms,Ei,ki,oe,Us,wi,xi,Gs,yi,qi,Qs,ji,Di,Bs,zi,Ci,Ys,Ti,Ma,re,Pi,Ws,Si,Ai,Vs,Oi,Ii,Js,Ri,Ni,Ua,lt,Ga,ct,Qa,Ot,Li,Ba,Ne,Ya,Le,Fi,Xs,Hi,Mi,Wa,dt,Va,It,Ui,Ja,mt,Xa,pt,Ka,ve,Gi,Ks,Qi,Bi,Zs,Yi,Wi,Za,ut,eo,Rt,Vi,to,Pe,Fe,ea,ft,Ji,ta,Xi,so,B,Ki,Nt,Zi,er,sa,tr,sr,aa,ar,or,ht,nr,ir,oa,rr,lr,_t,cr,dr,na,mr,pr,ao,pe,ue,Lt,$e,ur,ia,fr,hr,ra,_r,gr,oo,gt,no,Ft,br,io,fe,he,Ht,Mt,vr,ro,Se,He,la,bt,$r,ca,Er,lo,Ee,kr,da,wr,xr,vt,yr,qr,co,ke,jr,ma,Dr,zr,pa,Cr,Tr,mo,$t,po,Me,Pr,ua,Sr,Ar,uo,_e,ge,Ut,Gt,Or,fo,Et,ho,we,Ir,fa,Rr,Nr,ha,Lr,Fr,_o,kt,go,Qt,Hr,bo,wt,vo,xt,$o,Bt,Mr,Eo,Ue,ko;f=new Qc({props:{fw:M[0]}}),_=new ga({});const Br=[Yc,Bc],yt=[];function Yr(e,l){return e[0]==="pt"?0:1}P=Yr(M),I=yt[P]=Br[P](M),D=new Gc({props:{id:"OATCgQtNX2o"}}),W=new ga({}),Je=new ga({}),Xe=new S({props:{code:`from huggingface_hub import hf_hub_url

data_files = hf_hub_url(
    repo_id="lewtun/github-issues",
    filename="datasets-issues-with-hf-doc-builder.jsonl",
    repo_type="dataset",
)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_url

data_files = hf_hub_url(
    repo_id=<span class="hljs-string">&quot;lewtun/github-issues&quot;</span>,
    filename=<span class="hljs-string">&quot;datasets-issues-with-hf-doc-builder.jsonl&quot;</span>,
    repo_type=<span class="hljs-string">&quot;dataset&quot;</span>,
)`}}),Ke=new S({props:{code:`from datasets import load_dataset

issues_dataset = load_dataset("json", data_files=data_files, split="train")
issues_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

issues_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)
issues_dataset`}}),Ze=new S({props:{code:`Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;url&#x27;</span>, <span class="hljs-string">&#x27;repository_url&#x27;</span>, <span class="hljs-string">&#x27;labels_url&#x27;</span>, <span class="hljs-string">&#x27;comments_url&#x27;</span>, <span class="hljs-string">&#x27;events_url&#x27;</span>, <span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;node_id&#x27;</span>, <span class="hljs-string">&#x27;number&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;locked&#x27;</span>, <span class="hljs-string">&#x27;assignee&#x27;</span>, <span class="hljs-string">&#x27;assignees&#x27;</span>, <span class="hljs-string">&#x27;milestone&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;created_at&#x27;</span>, <span class="hljs-string">&#x27;updated_at&#x27;</span>, <span class="hljs-string">&#x27;closed_at&#x27;</span>, <span class="hljs-string">&#x27;author_association&#x27;</span>, <span class="hljs-string">&#x27;active_lock_reason&#x27;</span>, <span class="hljs-string">&#x27;pull_request&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;performed_via_github_app&#x27;</span>, <span class="hljs-string">&#x27;is_pull_request&#x27;</span>],
    num_rows: <span class="hljs-number">2855</span>
})`}}),et=new S({props:{code:`issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset`,highlighted:`issues_dataset = issues_dataset.<span class="hljs-built_in">filter</span>(
    <span class="hljs-keyword">lambda</span> x: (x[<span class="hljs-string">&quot;is_pull_request&quot;</span>] == <span class="hljs-literal">False</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;comments&quot;</span>]) &gt; <span class="hljs-number">0</span>)
)
issues_dataset`}}),tt=new S({props:{code:`Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;url&#x27;</span>, <span class="hljs-string">&#x27;repository_url&#x27;</span>, <span class="hljs-string">&#x27;labels_url&#x27;</span>, <span class="hljs-string">&#x27;comments_url&#x27;</span>, <span class="hljs-string">&#x27;events_url&#x27;</span>, <span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;node_id&#x27;</span>, <span class="hljs-string">&#x27;number&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;locked&#x27;</span>, <span class="hljs-string">&#x27;assignee&#x27;</span>, <span class="hljs-string">&#x27;assignees&#x27;</span>, <span class="hljs-string">&#x27;milestone&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;created_at&#x27;</span>, <span class="hljs-string">&#x27;updated_at&#x27;</span>, <span class="hljs-string">&#x27;closed_at&#x27;</span>, <span class="hljs-string">&#x27;author_association&#x27;</span>, <span class="hljs-string">&#x27;active_lock_reason&#x27;</span>, <span class="hljs-string">&#x27;pull_request&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;performed_via_github_app&#x27;</span>, <span class="hljs-string">&#x27;is_pull_request&#x27;</span>],
    num_rows: <span class="hljs-number">771</span>
})`}}),st=new S({props:{code:`columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset`,highlighted:`columns = issues_dataset.column_names
columns_to_keep = [<span class="hljs-string">&quot;title&quot;</span>, <span class="hljs-string">&quot;body&quot;</span>, <span class="hljs-string">&quot;html_url&quot;</span>, <span class="hljs-string">&quot;comments&quot;</span>]
columns_to_remove = <span class="hljs-built_in">set</span>(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset`}}),at=new S({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>],
    num_rows: <span class="hljs-number">771</span>
})`}}),ot=new S({props:{code:`issues_dataset.set_format("pandas")
df = issues_dataset[:]`,highlighted:`issues_dataset.set_format(<span class="hljs-string">&quot;pandas&quot;</span>)
df = issues_dataset[:]`}}),nt=new S({props:{code:'df["comments"][0].tolist()',highlighted:'df[<span class="hljs-string">&quot;comments&quot;</span>][<span class="hljs-number">0</span>].tolist()'}}),it=new S({props:{code:`['the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of \`ConnectionError\` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?',
 'cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']`,highlighted:`[<span class="hljs-string">&#x27;the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset(&quot;glue&quot;, data_args.task_name, cache_dir=model_args.cache_dir)&#x27;</span>,
 <span class="hljs-string">&#x27;Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of \`ConnectionError\` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?&#x27;</span>,
 <span class="hljs-string">&#x27;cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002&#x27;</span>,
 <span class="hljs-string">&#x27;I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...&#x27;</span>]`}}),rt=new S({props:{code:`comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)`,highlighted:`comments_df = df.explode(<span class="hljs-string">&quot;comments&quot;</span>, ignore_index=<span class="hljs-literal">True</span>)
comments_df.head(<span class="hljs-number">4</span>)`}}),lt=new S({props:{code:`from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset`}}),ct=new S({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>],
    num_rows: <span class="hljs-number">2842</span>
})`}}),Ne=new Rc({props:{$$slots:{default:[Wc]},$$scope:{ctx:M}}}),dt=new S({props:{code:`comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)`,highlighted:`comments_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;comment_length&quot;</span>: <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;comments&quot;</span>].split())}
)`}}),mt=new S({props:{code:`comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset`,highlighted:`comments_dataset = comments_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&quot;comment_length&quot;</span>] &gt; <span class="hljs-number">15</span>)
comments_dataset`}}),pt=new S({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;comment_length&#x27;</span>],
    num_rows: <span class="hljs-number">2098</span>
})`}}),ut=new S({props:{code:`def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \\n "
        + examples["body"]
        + " \\n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">concatenate_text</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">&quot;text&quot;</span>: examples[<span class="hljs-string">&quot;title&quot;</span>]
        + <span class="hljs-string">&quot; \\n &quot;</span>
        + examples[<span class="hljs-string">&quot;body&quot;</span>]
        + <span class="hljs-string">&quot; \\n &quot;</span>
        + examples[<span class="hljs-string">&quot;comments&quot;</span>]
    }


comments_dataset = comments_dataset.<span class="hljs-built_in">map</span>(concatenate_text)`}}),ft=new ga({});const Wr=[Jc,Vc],qt=[];function Vr(e,l){return e[0]==="pt"?0:1}pe=Vr(M),ue=qt[pe]=Wr[pe](M),gt=new S({props:{code:`def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">cls_pooling</span>(<span class="hljs-params">model_output</span>):
    <span class="hljs-keyword">return</span> model_output.last_hidden_state[:, <span class="hljs-number">0</span>]`}});const Jr=[Kc,Xc],jt=[];function Xr(e,l){return e[0]==="pt"?0:1}fe=Xr(M),he=jt[fe]=Jr[fe](M),bt=new ga({}),$t=new S({props:{code:'embeddings_dataset.add_faiss_index(column="embeddings")',highlighted:'embeddings_dataset.add_faiss_index(column=<span class="hljs-string">&quot;embeddings&quot;</span>)'}});const Kr=[ed,Zc],Dt=[];function Zr(e,l){return e[0]==="pt"?0:1}return _e=Zr(M),ge=Dt[_e]=Kr[_e](M),Et=new S({props:{code:`scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)`,highlighted:`scores, samples = embeddings_dataset.get_nearest_examples(
    <span class="hljs-string">&quot;embeddings&quot;</span>, question_embedding, k=<span class="hljs-number">5</span>
)`}}),kt=new S({props:{code:`import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)`,highlighted:`<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df[<span class="hljs-string">&quot;scores&quot;</span>] = scores
samples_df.sort_values(<span class="hljs-string">&quot;scores&quot;</span>, ascending=<span class="hljs-literal">False</span>, inplace=<span class="hljs-literal">True</span>)`}}),wt=new S({props:{code:`for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()`,highlighted:`<span class="hljs-keyword">for</span> _, row <span class="hljs-keyword">in</span> samples_df.iterrows():
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;COMMENT: <span class="hljs-subst">{row.comments}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;SCORE: <span class="hljs-subst">{row.scores}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;TITLE: <span class="hljs-subst">{row.title}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;URL: <span class="hljs-subst">{row.html_url}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;=&quot;</span> * <span class="hljs-number">50</span>)
    <span class="hljs-built_in">print</span>()`}}),xt=new S({props:{code:`"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how \`transformers\` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the \`datasets\` package since #1726 :)
You can now use them offline
\\\`\\\`\\\`python
datasets = load_dataset("text", data_files=data_files)
\\\`\\\`\\\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?

Indeed \`load_dataset\` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at \`./my_dataset/my_dataset.py\` then you can do
\\\`\\\`\\\`python
load_dataset("./my_dataset")
\\\`\\\`\\\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having \`csv\`, \`json\`, \`text\`, \`pandas\` dataset builders already included in the \`datasets\` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> \`\`\`
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> \`\`\`
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> \`\`\`
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> \`\`\`
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\\\`\\\`\\\`
2. copy the dir from online to the offline machine
3. (offline machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\\\`\\\`\\\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it&#x27;d be great if offline mode is added similar to how \`transformers\` loads models offline fine.

@mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the \`datasets\` package since #1726 :)
You can now use them offline
\\\`\\\`\\\`python
datasets = load_dataset(&quot;text&quot;, data_files=data_files)
\\\`\\\`\\\`

We&#x27;ll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there&#x27;s no internet.

Let me know if you know other ways that can make the offline mode experience better. I&#x27;d be happy to add them :)

I already note the &quot;freeze&quot; modules option, to prevent local modules updates. It would be a cool feature.

----------

&gt; @mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?

Indeed \`load_dataset\` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at \`./my_dataset/my_dataset.py\` then you can do
\\\`\\\`\\\`python
load_dataset(&quot;./my_dataset&quot;)
\\\`\\\`\\\`
and the dataset script will generate your dataset once and for all.

----------

About I&#x27;m looking into having \`csv\`, \`json\`, \`text\`, \`pandas\` dataset builders already included in the \`datasets\` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine
&gt;
&gt; 1. (online machine)
&gt;
&gt; \`\`\`
&gt;
&gt; import datasets
&gt;
&gt; data = datasets.load_dataset(...)
&gt;
&gt; data.save_to_disk(/YOUR/DATASET/DIR)
&gt;
&gt; \`\`\`
&gt;
&gt; 2. copy the dir from online to the offline machine
&gt;
&gt; 3. (offline machine)
&gt;
&gt; \`\`\`
&gt;
&gt; import datasets
&gt;
&gt; data = datasets.load_from_disk(/SAVED/DATA/DIR)
&gt;
&gt; \`\`\`
&gt;
&gt;
&gt;
&gt; HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\\\`\\\`\\\`
2. copy the dir from online to the offline machine
3. (offline machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\\\`\\\`\\\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
&quot;&quot;&quot;</span>`}}),Ue=new Rc({props:{$$slots:{default:[td]},$$scope:{ctx:M}}}),{c(){p=n("meta"),$=d(),E(f.$$.fragment),q=d(),h=n("h1"),v=n("a"),A=n("span"),E(_.$$.fragment),T=d(),j=n("span"),H=a("Ricerca semantica con FAISS"),O=d(),I.c(),R=d(),L=n("p"),G=a("Nella "),z=n("a"),N=a("sezione 5"),Y=a(" abbiamo creato un dataset di issue e commenti dalla repository GitHub di \u{1F917} Datasets. In questa sezione useremo queste informazioni per costrure un motore di ricerca semantico che ci pu\xF2 aiutare a trovare risposte alle nostre domande urgenti sulla libreria!"),U=d(),E(D.$$.fragment),Q=d(),u=n("h2"),C=n("a"),F=n("span"),E(W.$$.fragment),ne=d(),de=n("span"),Oo=a("Usare gli embedding per la ricerca semantica"),ba=d(),ie=n("p"),Io=a("Come abbiamo visto nel "),Tt=n("a"),Ro=a("Capitolo 1"),No=a(", i language model basati su Transformer rappresentano ogni token in un testo come un "),ss=n("em"),Lo=a("vettore"),Fo=a(", detto "),as=n("em"),Ho=a("embedding"),Mo=a(". \xC8 possibile \u201Cmettere insieme\u201D i diversi embedding per creare una rappresentazione vettoriale di un\u2019intera frase, paragrafo o (in alcuni casi) documento. Questi embedding possono essere usati per trovare documenti simili in un corpus calcolandone la similarit\xE0, ad esempio usando il prodotto scalere (o altre misure di similarit\xE0) tra ogni embedding, e restituendo i documenti pi\xF9 simili."),va=d(),Pt=n("p"),Uo=a("In questa sezione useremo gli embedding per sviluppare un motore di ricerca semantico. Questi motori di ricerca offrono diversi vantagig rispetto ai metodo convenzionali, basati sulla ricerca, all\u2019interno dei documenti, delle parole chiavi presente in una query."),$a=d(),Ce=n("div"),We=n("img"),Go=d(),Ve=n("img"),Ea=d(),Te=n("h2"),Ae=n("a"),os=n("span"),E(Je.$$.fragment),Qo=d(),ns=n("span"),Bo=a("Caricare e preparare il dataset"),ka=d(),St=n("p"),Yo=a("La prima cosa che dobbiamo fare \xE8 scaricare il nostro dataset di issue, quindi utilizziamo la libreria \u{1F917} Hub per scaricare i file usando l\u2019URL dell\u2019Hub Hugging Face:"),wa=d(),E(Xe.$$.fragment),xa=d(),be=n("p"),Wo=a("Se conseriamo l\u2019URL iin "),is=n("code"),Vo=a("data_files"),Jo=a(", possiamo caricare il dataset utilizzando il metodo introdotto nella "),At=n("a"),Xo=a("sezione 2"),Ko=a(":"),ya=d(),E(Ke.$$.fragment),qa=d(),E(Ze.$$.fragment),ja=d(),V=n("p"),Zo=a("Qui abbiamo specificato la sezione di defaul "),rs=n("code"),en=a("train"),tn=a(" in "),ls=n("code"),sn=a("load_dataset()"),an=a(", cos\xEC che questa funzione resituisce un "),cs=n("code"),on=a("Dataset"),nn=a(" invece di un "),ds=n("code"),rn=a("DatasetDict"),ln=a(". La prima cosa da fare \xE8 filtrare le richieste di pull, poich\xE8 queste tendono a essere usate raramente come risposta alle domande degli utenti, e introdurrebbero rumore nel nostro motore di ricerca. Come dovrebbe esser enoto, possiamo usare la funzione "),ms=n("code"),cn=a("Dataset.filter()"),dn=a(" per escludere questi dati dal nostro dataset. Gi\xE0 che ci siamo, eliminiamo anche le righe senza commenti, poich\xE9 queste non presentano nessuna risposta alle domande degli utenti:"),Da=d(),E(et.$$.fragment),za=d(),E(tt.$$.fragment),Ca=d(),J=n("p"),mn=a("Possiamo vedere che ci sono molte colonne nel nostro dataset, molte delle quali non servono alla costruzione del nostro motore di ricerca. Da una prospettiva di ricerca, le colonne maggiormente informative sono "),ps=n("code"),pn=a("title"),un=a(", "),us=n("code"),fn=a("body"),hn=a(", e "),fs=n("code"),_n=a("comments"),gn=a(", mentre "),hs=n("code"),bn=a("html_url"),vn=a(" ci fornisce un link all\u2019issue originale. Usiamo la funzione "),_s=n("code"),$n=a("Dataset.remove_columns()"),En=a(" per eliminare le colonne rimanenti:"),Ta=d(),E(st.$$.fragment),Pa=d(),E(at.$$.fragment),Sa=d(),K=n("p"),kn=a("Per crare i nostri embedding arricchiremo ognu commento con il titolo e il corpo dell\u2019issue, visto che questi campi spesso includono informazioni utili sul contesto. Poich\xE9 la nostra colonna "),gs=n("code"),wn=a("comment"),xn=a(" \xE8 al momento una lista di commenti per ogni issue, dobbiamo \u201Cfarla esplodere\u201D cos\xEC che ogni riga consista in una tupla "),bs=n("code"),yn=a("(html_url, title, body, comment)"),qn=a(". In panda \xE8 possibile farlo utilizzando la "),Oe=n("a"),jn=a("funzione "),vs=n("code"),Dn=a("Dataframe.explode()"),zn=a(", che crea una nuova riga per ogni elemento in una colonna in formato di lista, ripetendo i valori di tutte le altre colonne. Per vederlo in azione, prima di tutto passiamo al formato "),$s=n("code"),Cn=a("DataFrame"),Tn=a(":"),Aa=d(),E(ot.$$.fragment),Oa=d(),Ie=n("p"),Pn=a("Se diamo un\u2019occhiata alla prima riga di questo "),Es=n("code"),Sn=a("DataFrame"),An=a(", possiamo vedere che ci sono quattro commenti associati con quest\u2019issue:"),Ia=d(),E(nt.$$.fragment),Ra=d(),E(it.$$.fragment),Na=d(),Re=n("p"),On=a("Quando \u201Cesplodiamo\u201D "),ks=n("code"),In=a("df"),Rn=a(", ci aspettiamo di avere una riga per ognuno di questi commenti. Controlliamo se \xE8 cos\xEC:"),La=d(),E(rt.$$.fragment),Fa=d(),ee=n("table"),ws=n("thead"),Z=n("tr"),Ha=n("th"),Nn=d(),xs=n("th"),Ln=a("html_url"),Fn=d(),ys=n("th"),Hn=a("title"),Mn=d(),qs=n("th"),Un=a("comments"),Gn=d(),js=n("th"),Qn=a("body"),Bn=d(),me=n("tbody"),te=n("tr"),Ds=n("th"),Yn=a("0"),Wn=d(),zs=n("td"),Vn=a("https://github.com/huggingface/datasets/issues/2787"),Jn=d(),Cs=n("td"),Xn=a("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Kn=d(),Ts=n("td"),Zn=a("the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None..."),ei=d(),Ps=n("td"),ti=a("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),si=d(),se=n("tr"),Ss=n("th"),ai=a("1"),oi=d(),As=n("td"),ni=a("https://github.com/huggingface/datasets/issues/2787"),ii=d(),Os=n("td"),ri=a("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),li=d(),Is=n("td"),ci=a("Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com..."),di=d(),Rs=n("td"),mi=a("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),pi=d(),ae=n("tr"),Ns=n("th"),ui=a("2"),fi=d(),Ls=n("td"),hi=a("https://github.com/huggingface/datasets/issues/2787"),_i=d(),Fs=n("td"),gi=a("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),bi=d(),Hs=n("td"),vi=a("cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002"),$i=d(),Ms=n("td"),Ei=a("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),ki=d(),oe=n("tr"),Us=n("th"),wi=a("3"),xi=d(),Gs=n("td"),yi=a("https://github.com/huggingface/datasets/issues/2787"),qi=d(),Qs=n("td"),ji=a("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Di=d(),Bs=n("td"),zi=a("I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem..."),Ci=d(),Ys=n("td"),Ti=a("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Ma=d(),re=n("p"),Pi=a("bene, possiamo vedere che le righe sono state duplicate, e che la colonna "),Ws=n("code"),Si=a("comment"),Ai=a(" contiene i diversi comment! Ora che abbiamo finito con Pandas, possiamo passare velocemente a "),Vs=n("code"),Oi=a("Dataset"),Ii=a(" caricando il "),Js=n("code"),Ri=a("DataFrame"),Ni=a(" in memoria:"),Ua=d(),E(lt.$$.fragment),Ga=d(),E(ct.$$.fragment),Qa=d(),Ot=n("p"),Li=a("Perfetto, ora abbiamo qualche migliaio di commenti con cui lavorare!"),Ba=d(),E(Ne.$$.fragment),Ya=d(),Le=n("p"),Fi=a("Ora che abbiamo un commento per riga, creiamo una nuova colonna "),Xs=n("code"),Hi=a("comments_length"),Mi=a(" che contiene il numero di parole per ogni commento:"),Wa=d(),E(dt.$$.fragment),Va=d(),It=n("p"),Ui=a("Possiamo usare questa nuova colonna per eliminare i commenti brevi, che solitamente includono cose del tipo \u201Ccc @lewtun\u201D o \u201CGrazie!\u201D, che non sono pertinenti per il nostro motore di ricerca. Non abbiamo un numero preciso da selezionare per questo filtro, ma 15 parole dovrebbero andare bene:"),Ja=d(),E(mt.$$.fragment),Xa=d(),E(pt.$$.fragment),Ka=d(),ve=n("p"),Gi=a("Una volta data una pulizia al nostro dataset, possiamo concatenare il titolo, la descrizione e i commenti delle issue in una nuova colonna "),Ks=n("code"),Qi=a("text"),Bi=a(". Come al solito , scriveremo una semplice funzione che possiamo passare a "),Zs=n("code"),Yi=a("Dataset.map()"),Wi=a(":"),Za=d(),E(ut.$$.fragment),eo=d(),Rt=n("p"),Vi=a("Siamo finalmente pronti a creare degli embedding! Diamo un\u2019occhiata."),to=d(),Pe=n("h2"),Fe=n("a"),ea=n("span"),E(ft.$$.fragment),Ji=d(),ta=n("span"),Xi=a("Creare i text embedding"),so=d(),B=n("p"),Ki=a("Abbiamo visto nel "),Nt=n("a"),Zi=a("Capitolo 2"),er=a(" che possiamo ottenere i token embedding utilizando la classe "),sa=n("code"),tr=a("AutoModel"),sr=a(". Dobbiamo solo scegliere un checkpoint valido da cui caricare il modell. Per fortuna, esiste una libreria chiamata "),aa=n("code"),ar=a("sentence-transformers"),or=a(", dedicata alla creazione di embedding. Seguendo la descrizione nella "),ht=n("a"),nr=a("documentazione"),ir=a("della libreria, il nostro caso d\u2019uso \xE8 un esempio di "),oa=n("em"),rr=a("asymmetric semantic search"),lr=a(" perch\xE9 abbiamo una breve query per cui vogliamo trovare risposte in un documento lungo, come ad esempio un commento a un issue. La "),_t=n("a"),cr=a("scheda di riepilogo dei modelli"),dr=a(" nella documentazione ci indica che il checkpoint "),na=n("code"),mr=a("multi-qa-mpnet-base-dot-v1"),pr=a(" ha mostrato la performance migliore per la ricerca semantica, quindi \xE8 quello che useremo per la nostra applicazione. Caricheremo anche il tokenizzatore usando lo stesso checkpoint:"),ao=d(),ue.c(),Lt=d(),$e=n("p"),ur=a("Come abbiamo gi\xE0 detto prima, vorremmo rappresentare ogni entrata nel nostro corpus di issue GitHub come un vettore singolo, per cui avremo bisogno di calcolare la media, o il \u201Cpool\u201D dei nostri token embedding. Un metodo comune \xE8 di effettuare un "),ia=n("em"),fr=a("CLS pooling"),hr=a(" sull\u2019output del nostro modello: questa tecnica su basa sul recuperare semplicemente l\u2019ultimo stato nascosto del token speciale "),ra=n("code"),_r=a("[CLS]"),gr=a(". La funzione seguente fa proprio questo:"),oo=d(),E(gt.$$.fragment),no=d(),Ft=n("p"),br=a("Poi, creeremo una funzione di supporto che: tokenizza una lista di documenti, inserire i tensori sulla GPU, li usa come input per il modello, e infine applica il CLS pooling agli output:"),io=d(),he.c(),Ht=d(),Mt=n("p"),vr=a("Node che abbiamo convertito gli embedding in array NumPy \u2014 questo perch\xE8 \u{1F917} Datasets ha bisogno di questo formato per indicizzare gli embedding con FAISS, che \xE8 ci\xF2 che faremo nella prossima sezione."),ro=d(),Se=n("h2"),He=n("a"),la=n("span"),E(bt.$$.fragment),$r=d(),ca=n("span"),Er=a("Usare FAISS per ricerca di similarit\xE0 efficiente"),lo=d(),Ee=n("p"),kr=a(`Ora che abbiamo un dataset di embedding, abbiamo bisogno di un modo per effettuare una ricerca. Per far ci\xF2, useremo una struttura specialie di \u{1F917} Datasets
chiamato `),da=n("em"),wr=a("indice FAISS"),xr=a(". "),vt=n("a"),yr=a("FAISS"),qr=a(" (Facebook AI Similarity Search) \xE8 una libreria che permette di utilizzare algoritmi efficient per ricercare e raggruppare gli embedding."),co=d(),ke=n("p"),jr=a("L\u2019idea di base dietro FAISS \xE8 di creare un formato speciale di dati chiamato "),ma=n("em"),Dr=a("indice"),zr=a(" che permette di trovare quali embedding sono simili a un embedding in input. Creare un indice FAISS su \u{1F917} Datasets \xE8 semplice \u2014 usiamo la funzione "),pa=n("code"),Cr=a("Dataset.add_faiss_index()"),Tr=a(" e specificare quale colonna nel nostro dataset vorremmo indicizzare:"),mo=d(),E($t.$$.fragment),po=d(),Me=n("p"),Pr=a("Ora possiamo eseguire dele query su questo indice effettuando una ricerca degli elementi pi\xF9 vicini usando la funzione "),ua=n("code"),Sr=a("Dataset.get_nearest_examples()"),Ar=a(". Testiamolo creando un embedding per una domanda."),uo=d(),ge.c(),Ut=d(),Gt=n("p"),Or=a("Proprio come con i documenti, ora abbiamo un vettore di 768 dimensioni che rappresenta la query, che possiamo confrontare con l\u2019intero corpus per trovare gli embedding pi\xF9 simili:"),fo=d(),E(Et.$$.fragment),ho=d(),we=n("p"),Ir=a("La funzione "),fa=n("code"),Rr=a("Dataset.get_nearest_examples()"),Nr=a(" restituisce una tupla di valori che valutano la sovrapposizione tra la query e il documento, e un set corrispondente di campioni (in questo caso, le 5 corrispondenze migliori). Salviamole in un "),ha=n("code"),Lr=a("pandas.DataFrame"),Fr=a(", cos\xEC che possiamo ordinarle facilmente:"),_o=d(),E(kt.$$.fragment),go=d(),Qt=n("p"),Hr=a("Ora possiamo iterare sulle prime righe per vedere quanto bene la nostra query corrisponde ai commenti disponibili:"),bo=d(),E(wt.$$.fragment),vo=d(),E(xt.$$.fragment),$o=d(),Bt=n("p"),Mr=a("Non male! Il nostro secondo risultato sembra soddisfare la nostra richiesta."),Eo=d(),E(Ue.$$.fragment),this.h()},l(e){const l=Mc('[data-svelte="svelte-1phssyn"]',document.head);p=i(l,"META",{name:!0,content:!0}),l.forEach(s),$=m(e),k(f.$$.fragment,e),q=m(e),h=i(e,"H1",{class:!0});var zt=r(h);v=i(zt,"A",{id:!0,class:!0,href:!0});var Yt=r(v);A=i(Yt,"SPAN",{});var _a=r(A);k(_.$$.fragment,_a),_a.forEach(s),Yt.forEach(s),T=m(zt),j=i(zt,"SPAN",{});var Wt=r(j);H=o(Wt,"Ricerca semantica con FAISS"),Wt.forEach(s),zt.forEach(s),O=m(e),I.l(e),R=m(e),L=i(e,"P",{});var Ge=r(L);G=o(Ge,"Nella "),z=i(Ge,"A",{href:!0});var Vt=r(z);N=o(Vt,"sezione 5"),Vt.forEach(s),Y=o(Ge," abbiamo creato un dataset di issue e commenti dalla repository GitHub di \u{1F917} Datasets. In questa sezione useremo queste informazioni per costrure un motore di ricerca semantico che ci pu\xF2 aiutare a trovare risposte alle nostre domande urgenti sulla libreria!"),Ge.forEach(s),U=m(e),k(D.$$.fragment,e),Q=m(e),u=i(e,"H2",{class:!0});var Ct=r(u);C=i(Ct,"A",{id:!0,class:!0,href:!0});var el=r(C);F=i(el,"SPAN",{});var tl=r(F);k(W.$$.fragment,tl),tl.forEach(s),el.forEach(s),ne=m(Ct),de=i(Ct,"SPAN",{});var sl=r(de);Oo=o(sl,"Usare gli embedding per la ricerca semantica"),sl.forEach(s),Ct.forEach(s),ba=m(e),ie=i(e,"P",{});var Qe=r(ie);Io=o(Qe,"Come abbiamo visto nel "),Tt=i(Qe,"A",{href:!0});var al=r(Tt);Ro=o(al,"Capitolo 1"),al.forEach(s),No=o(Qe,", i language model basati su Transformer rappresentano ogni token in un testo come un "),ss=i(Qe,"EM",{});var ol=r(ss);Lo=o(ol,"vettore"),ol.forEach(s),Fo=o(Qe,", detto "),as=i(Qe,"EM",{});var nl=r(as);Ho=o(nl,"embedding"),nl.forEach(s),Mo=o(Qe,". \xC8 possibile \u201Cmettere insieme\u201D i diversi embedding per creare una rappresentazione vettoriale di un\u2019intera frase, paragrafo o (in alcuni casi) documento. Questi embedding possono essere usati per trovare documenti simili in un corpus calcolandone la similarit\xE0, ad esempio usando il prodotto scalere (o altre misure di similarit\xE0) tra ogni embedding, e restituendo i documenti pi\xF9 simili."),Qe.forEach(s),va=m(e),Pt=i(e,"P",{});var il=r(Pt);Uo=o(il,"In questa sezione useremo gli embedding per sviluppare un motore di ricerca semantico. Questi motori di ricerca offrono diversi vantagig rispetto ai metodo convenzionali, basati sulla ricerca, all\u2019interno dei documenti, delle parole chiavi presente in una query."),il.forEach(s),$a=m(e),Ce=i(e,"DIV",{class:!0});var wo=r(Ce);We=i(wo,"IMG",{class:!0,src:!0,alt:!0}),Go=m(wo),Ve=i(wo,"IMG",{class:!0,src:!0,alt:!0}),wo.forEach(s),Ea=m(e),Te=i(e,"H2",{class:!0});var xo=r(Te);Ae=i(xo,"A",{id:!0,class:!0,href:!0});var rl=r(Ae);os=i(rl,"SPAN",{});var ll=r(os);k(Je.$$.fragment,ll),ll.forEach(s),rl.forEach(s),Qo=m(xo),ns=i(xo,"SPAN",{});var cl=r(ns);Bo=o(cl,"Caricare e preparare il dataset"),cl.forEach(s),xo.forEach(s),ka=m(e),St=i(e,"P",{});var dl=r(St);Yo=o(dl,"La prima cosa che dobbiamo fare \xE8 scaricare il nostro dataset di issue, quindi utilizziamo la libreria \u{1F917} Hub per scaricare i file usando l\u2019URL dell\u2019Hub Hugging Face:"),dl.forEach(s),wa=m(e),k(Xe.$$.fragment,e),xa=m(e),be=i(e,"P",{});var Jt=r(be);Wo=o(Jt,"Se conseriamo l\u2019URL iin "),is=i(Jt,"CODE",{});var ml=r(is);Vo=o(ml,"data_files"),ml.forEach(s),Jo=o(Jt,", possiamo caricare il dataset utilizzando il metodo introdotto nella "),At=i(Jt,"A",{href:!0});var pl=r(At);Xo=o(pl,"sezione 2"),pl.forEach(s),Ko=o(Jt,":"),Jt.forEach(s),ya=m(e),k(Ke.$$.fragment,e),qa=m(e),k(Ze.$$.fragment,e),ja=m(e),V=i(e,"P",{});var le=r(V);Zo=o(le,"Qui abbiamo specificato la sezione di defaul "),rs=i(le,"CODE",{});var ul=r(rs);en=o(ul,"train"),ul.forEach(s),tn=o(le," in "),ls=i(le,"CODE",{});var fl=r(ls);sn=o(fl,"load_dataset()"),fl.forEach(s),an=o(le,", cos\xEC che questa funzione resituisce un "),cs=i(le,"CODE",{});var hl=r(cs);on=o(hl,"Dataset"),hl.forEach(s),nn=o(le," invece di un "),ds=i(le,"CODE",{});var _l=r(ds);rn=o(_l,"DatasetDict"),_l.forEach(s),ln=o(le,". La prima cosa da fare \xE8 filtrare le richieste di pull, poich\xE8 queste tendono a essere usate raramente come risposta alle domande degli utenti, e introdurrebbero rumore nel nostro motore di ricerca. Come dovrebbe esser enoto, possiamo usare la funzione "),ms=i(le,"CODE",{});var gl=r(ms);cn=o(gl,"Dataset.filter()"),gl.forEach(s),dn=o(le," per escludere questi dati dal nostro dataset. Gi\xE0 che ci siamo, eliminiamo anche le righe senza commenti, poich\xE9 queste non presentano nessuna risposta alle domande degli utenti:"),le.forEach(s),Da=m(e),k(et.$$.fragment,e),za=m(e),k(tt.$$.fragment,e),Ca=m(e),J=i(e,"P",{});var ce=r(J);mn=o(ce,"Possiamo vedere che ci sono molte colonne nel nostro dataset, molte delle quali non servono alla costruzione del nostro motore di ricerca. Da una prospettiva di ricerca, le colonne maggiormente informative sono "),ps=i(ce,"CODE",{});var bl=r(ps);pn=o(bl,"title"),bl.forEach(s),un=o(ce,", "),us=i(ce,"CODE",{});var vl=r(us);fn=o(vl,"body"),vl.forEach(s),hn=o(ce,", e "),fs=i(ce,"CODE",{});var $l=r(fs);_n=o($l,"comments"),$l.forEach(s),gn=o(ce,", mentre "),hs=i(ce,"CODE",{});var El=r(hs);bn=o(El,"html_url"),El.forEach(s),vn=o(ce," ci fornisce un link all\u2019issue originale. Usiamo la funzione "),_s=i(ce,"CODE",{});var kl=r(_s);$n=o(kl,"Dataset.remove_columns()"),kl.forEach(s),En=o(ce," per eliminare le colonne rimanenti:"),ce.forEach(s),Ta=m(e),k(st.$$.fragment,e),Pa=m(e),k(at.$$.fragment,e),Sa=m(e),K=i(e,"P",{});var xe=r(K);kn=o(xe,"Per crare i nostri embedding arricchiremo ognu commento con il titolo e il corpo dell\u2019issue, visto che questi campi spesso includono informazioni utili sul contesto. Poich\xE9 la nostra colonna "),gs=i(xe,"CODE",{});var wl=r(gs);wn=o(wl,"comment"),wl.forEach(s),xn=o(xe," \xE8 al momento una lista di commenti per ogni issue, dobbiamo \u201Cfarla esplodere\u201D cos\xEC che ogni riga consista in una tupla "),bs=i(xe,"CODE",{});var xl=r(bs);yn=o(xl,"(html_url, title, body, comment)"),xl.forEach(s),qn=o(xe,". In panda \xE8 possibile farlo utilizzando la "),Oe=i(xe,"A",{href:!0,rel:!0});var Ur=r(Oe);jn=o(Ur,"funzione "),vs=i(Ur,"CODE",{});var yl=r(vs);Dn=o(yl,"Dataframe.explode()"),yl.forEach(s),Ur.forEach(s),zn=o(xe,", che crea una nuova riga per ogni elemento in una colonna in formato di lista, ripetendo i valori di tutte le altre colonne. Per vederlo in azione, prima di tutto passiamo al formato "),$s=i(xe,"CODE",{});var ql=r($s);Cn=o(ql,"DataFrame"),ql.forEach(s),Tn=o(xe,":"),xe.forEach(s),Aa=m(e),k(ot.$$.fragment,e),Oa=m(e),Ie=i(e,"P",{});var yo=r(Ie);Pn=o(yo,"Se diamo un\u2019occhiata alla prima riga di questo "),Es=i(yo,"CODE",{});var jl=r(Es);Sn=o(jl,"DataFrame"),jl.forEach(s),An=o(yo,", possiamo vedere che ci sono quattro commenti associati con quest\u2019issue:"),yo.forEach(s),Ia=m(e),k(nt.$$.fragment,e),Ra=m(e),k(it.$$.fragment,e),Na=m(e),Re=i(e,"P",{});var qo=r(Re);On=o(qo,"Quando \u201Cesplodiamo\u201D "),ks=i(qo,"CODE",{});var Dl=r(ks);In=o(Dl,"df"),Dl.forEach(s),Rn=o(qo,", ci aspettiamo di avere una riga per ognuno di questi commenti. Controlliamo se \xE8 cos\xEC:"),qo.forEach(s),La=m(e),k(rt.$$.fragment,e),Fa=m(e),ee=i(e,"TABLE",{border:!0,class:!0,style:!0});var jo=r(ee);ws=i(jo,"THEAD",{});var zl=r(ws);Z=i(zl,"TR",{style:!0});var ye=r(Z);Ha=i(ye,"TH",{}),r(Ha).forEach(s),Nn=m(ye),xs=i(ye,"TH",{});var Cl=r(xs);Ln=o(Cl,"html_url"),Cl.forEach(s),Fn=m(ye),ys=i(ye,"TH",{});var Tl=r(ys);Hn=o(Tl,"title"),Tl.forEach(s),Mn=m(ye),qs=i(ye,"TH",{});var Pl=r(qs);Un=o(Pl,"comments"),Pl.forEach(s),Gn=m(ye),js=i(ye,"TH",{});var Sl=r(js);Qn=o(Sl,"body"),Sl.forEach(s),ye.forEach(s),zl.forEach(s),Bn=m(jo),me=i(jo,"TBODY",{});var Be=r(me);te=i(Be,"TR",{});var qe=r(te);Ds=i(qe,"TH",{});var Al=r(Ds);Yn=o(Al,"0"),Al.forEach(s),Wn=m(qe),zs=i(qe,"TD",{});var Ol=r(zs);Vn=o(Ol,"https://github.com/huggingface/datasets/issues/2787"),Ol.forEach(s),Jn=m(qe),Cs=i(qe,"TD",{});var Il=r(Cs);Xn=o(Il,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Il.forEach(s),Kn=m(qe),Ts=i(qe,"TD",{});var Rl=r(Ts);Zn=o(Rl,"the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None..."),Rl.forEach(s),ei=m(qe),Ps=i(qe,"TD",{});var Nl=r(Ps);ti=o(Nl,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Nl.forEach(s),qe.forEach(s),si=m(Be),se=i(Be,"TR",{});var je=r(se);Ss=i(je,"TH",{});var Ll=r(Ss);ai=o(Ll,"1"),Ll.forEach(s),oi=m(je),As=i(je,"TD",{});var Fl=r(As);ni=o(Fl,"https://github.com/huggingface/datasets/issues/2787"),Fl.forEach(s),ii=m(je),Os=i(je,"TD",{});var Hl=r(Os);ri=o(Hl,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Hl.forEach(s),li=m(je),Is=i(je,"TD",{});var Ml=r(Is);ci=o(Ml,"Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com..."),Ml.forEach(s),di=m(je),Rs=i(je,"TD",{});var Ul=r(Rs);mi=o(Ul,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Ul.forEach(s),je.forEach(s),pi=m(Be),ae=i(Be,"TR",{});var De=r(ae);Ns=i(De,"TH",{});var Gl=r(Ns);ui=o(Gl,"2"),Gl.forEach(s),fi=m(De),Ls=i(De,"TD",{});var Ql=r(Ls);hi=o(Ql,"https://github.com/huggingface/datasets/issues/2787"),Ql.forEach(s),_i=m(De),Fs=i(De,"TD",{});var Bl=r(Fs);gi=o(Bl,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Bl.forEach(s),bi=m(De),Hs=i(De,"TD",{});var Yl=r(Hs);vi=o(Yl,"cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002"),Yl.forEach(s),$i=m(De),Ms=i(De,"TD",{});var Wl=r(Ms);Ei=o(Wl,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Wl.forEach(s),De.forEach(s),ki=m(Be),oe=i(Be,"TR",{});var ze=r(oe);Us=i(ze,"TH",{});var Vl=r(Us);wi=o(Vl,"3"),Vl.forEach(s),xi=m(ze),Gs=i(ze,"TD",{});var Jl=r(Gs);yi=o(Jl,"https://github.com/huggingface/datasets/issues/2787"),Jl.forEach(s),qi=m(ze),Qs=i(ze,"TD",{});var Xl=r(Qs);ji=o(Xl,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Xl.forEach(s),Di=m(ze),Bs=i(ze,"TD",{});var Kl=r(Bs);zi=o(Kl,"I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem..."),Kl.forEach(s),Ci=m(ze),Ys=i(ze,"TD",{});var Zl=r(Ys);Ti=o(Zl,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Zl.forEach(s),ze.forEach(s),Be.forEach(s),jo.forEach(s),Ma=m(e),re=i(e,"P",{});var Ye=r(re);Pi=o(Ye,"bene, possiamo vedere che le righe sono state duplicate, e che la colonna "),Ws=i(Ye,"CODE",{});var ec=r(Ws);Si=o(ec,"comment"),ec.forEach(s),Ai=o(Ye," contiene i diversi comment! Ora che abbiamo finito con Pandas, possiamo passare velocemente a "),Vs=i(Ye,"CODE",{});var tc=r(Vs);Oi=o(tc,"Dataset"),tc.forEach(s),Ii=o(Ye," caricando il "),Js=i(Ye,"CODE",{});var sc=r(Js);Ri=o(sc,"DataFrame"),sc.forEach(s),Ni=o(Ye," in memoria:"),Ye.forEach(s),Ua=m(e),k(lt.$$.fragment,e),Ga=m(e),k(ct.$$.fragment,e),Qa=m(e),Ot=i(e,"P",{});var ac=r(Ot);Li=o(ac,"Perfetto, ora abbiamo qualche migliaio di commenti con cui lavorare!"),ac.forEach(s),Ba=m(e),k(Ne.$$.fragment,e),Ya=m(e),Le=i(e,"P",{});var Do=r(Le);Fi=o(Do,"Ora che abbiamo un commento per riga, creiamo una nuova colonna "),Xs=i(Do,"CODE",{});var oc=r(Xs);Hi=o(oc,"comments_length"),oc.forEach(s),Mi=o(Do," che contiene il numero di parole per ogni commento:"),Do.forEach(s),Wa=m(e),k(dt.$$.fragment,e),Va=m(e),It=i(e,"P",{});var nc=r(It);Ui=o(nc,"Possiamo usare questa nuova colonna per eliminare i commenti brevi, che solitamente includono cose del tipo \u201Ccc @lewtun\u201D o \u201CGrazie!\u201D, che non sono pertinenti per il nostro motore di ricerca. Non abbiamo un numero preciso da selezionare per questo filtro, ma 15 parole dovrebbero andare bene:"),nc.forEach(s),Ja=m(e),k(mt.$$.fragment,e),Xa=m(e),k(pt.$$.fragment,e),Ka=m(e),ve=i(e,"P",{});var Xt=r(ve);Gi=o(Xt,"Una volta data una pulizia al nostro dataset, possiamo concatenare il titolo, la descrizione e i commenti delle issue in una nuova colonna "),Ks=i(Xt,"CODE",{});var ic=r(Ks);Qi=o(ic,"text"),ic.forEach(s),Bi=o(Xt,". Come al solito , scriveremo una semplice funzione che possiamo passare a "),Zs=i(Xt,"CODE",{});var rc=r(Zs);Yi=o(rc,"Dataset.map()"),rc.forEach(s),Wi=o(Xt,":"),Xt.forEach(s),Za=m(e),k(ut.$$.fragment,e),eo=m(e),Rt=i(e,"P",{});var lc=r(Rt);Vi=o(lc,"Siamo finalmente pronti a creare degli embedding! Diamo un\u2019occhiata."),lc.forEach(s),to=m(e),Pe=i(e,"H2",{class:!0});var zo=r(Pe);Fe=i(zo,"A",{id:!0,class:!0,href:!0});var cc=r(Fe);ea=i(cc,"SPAN",{});var dc=r(ea);k(ft.$$.fragment,dc),dc.forEach(s),cc.forEach(s),Ji=m(zo),ta=i(zo,"SPAN",{});var mc=r(ta);Xi=o(mc,"Creare i text embedding"),mc.forEach(s),zo.forEach(s),so=m(e),B=i(e,"P",{});var X=r(B);Ki=o(X,"Abbiamo visto nel "),Nt=i(X,"A",{href:!0});var pc=r(Nt);Zi=o(pc,"Capitolo 2"),pc.forEach(s),er=o(X," che possiamo ottenere i token embedding utilizando la classe "),sa=i(X,"CODE",{});var uc=r(sa);tr=o(uc,"AutoModel"),uc.forEach(s),sr=o(X,". Dobbiamo solo scegliere un checkpoint valido da cui caricare il modell. Per fortuna, esiste una libreria chiamata "),aa=i(X,"CODE",{});var fc=r(aa);ar=o(fc,"sentence-transformers"),fc.forEach(s),or=o(X,", dedicata alla creazione di embedding. Seguendo la descrizione nella "),ht=i(X,"A",{href:!0,rel:!0});var hc=r(ht);nr=o(hc,"documentazione"),hc.forEach(s),ir=o(X,"della libreria, il nostro caso d\u2019uso \xE8 un esempio di "),oa=i(X,"EM",{});var _c=r(oa);rr=o(_c,"asymmetric semantic search"),_c.forEach(s),lr=o(X," perch\xE9 abbiamo una breve query per cui vogliamo trovare risposte in un documento lungo, come ad esempio un commento a un issue. La "),_t=i(X,"A",{href:!0,rel:!0});var gc=r(_t);cr=o(gc,"scheda di riepilogo dei modelli"),gc.forEach(s),dr=o(X," nella documentazione ci indica che il checkpoint "),na=i(X,"CODE",{});var bc=r(na);mr=o(bc,"multi-qa-mpnet-base-dot-v1"),bc.forEach(s),pr=o(X," ha mostrato la performance migliore per la ricerca semantica, quindi \xE8 quello che useremo per la nostra applicazione. Caricheremo anche il tokenizzatore usando lo stesso checkpoint:"),X.forEach(s),ao=m(e),ue.l(e),Lt=m(e),$e=i(e,"P",{});var Kt=r($e);ur=o(Kt,"Come abbiamo gi\xE0 detto prima, vorremmo rappresentare ogni entrata nel nostro corpus di issue GitHub come un vettore singolo, per cui avremo bisogno di calcolare la media, o il \u201Cpool\u201D dei nostri token embedding. Un metodo comune \xE8 di effettuare un "),ia=i(Kt,"EM",{});var vc=r(ia);fr=o(vc,"CLS pooling"),vc.forEach(s),hr=o(Kt," sull\u2019output del nostro modello: questa tecnica su basa sul recuperare semplicemente l\u2019ultimo stato nascosto del token speciale "),ra=i(Kt,"CODE",{});var $c=r(ra);_r=o($c,"[CLS]"),$c.forEach(s),gr=o(Kt,". La funzione seguente fa proprio questo:"),Kt.forEach(s),oo=m(e),k(gt.$$.fragment,e),no=m(e),Ft=i(e,"P",{});var Ec=r(Ft);br=o(Ec,"Poi, creeremo una funzione di supporto che: tokenizza una lista di documenti, inserire i tensori sulla GPU, li usa come input per il modello, e infine applica il CLS pooling agli output:"),Ec.forEach(s),io=m(e),he.l(e),Ht=m(e),Mt=i(e,"P",{});var kc=r(Mt);vr=o(kc,"Node che abbiamo convertito gli embedding in array NumPy \u2014 questo perch\xE8 \u{1F917} Datasets ha bisogno di questo formato per indicizzare gli embedding con FAISS, che \xE8 ci\xF2 che faremo nella prossima sezione."),kc.forEach(s),ro=m(e),Se=i(e,"H2",{class:!0});var Co=r(Se);He=i(Co,"A",{id:!0,class:!0,href:!0});var wc=r(He);la=i(wc,"SPAN",{});var xc=r(la);k(bt.$$.fragment,xc),xc.forEach(s),wc.forEach(s),$r=m(Co),ca=i(Co,"SPAN",{});var yc=r(ca);Er=o(yc,"Usare FAISS per ricerca di similarit\xE0 efficiente"),yc.forEach(s),Co.forEach(s),lo=m(e),Ee=i(e,"P",{});var Zt=r(Ee);kr=o(Zt,`Ora che abbiamo un dataset di embedding, abbiamo bisogno di un modo per effettuare una ricerca. Per far ci\xF2, useremo una struttura specialie di \u{1F917} Datasets
chiamato `),da=i(Zt,"EM",{});var qc=r(da);wr=o(qc,"indice FAISS"),qc.forEach(s),xr=o(Zt,". "),vt=i(Zt,"A",{href:!0,rel:!0});var jc=r(vt);yr=o(jc,"FAISS"),jc.forEach(s),qr=o(Zt," (Facebook AI Similarity Search) \xE8 una libreria che permette di utilizzare algoritmi efficient per ricercare e raggruppare gli embedding."),Zt.forEach(s),co=m(e),ke=i(e,"P",{});var es=r(ke);jr=o(es,"L\u2019idea di base dietro FAISS \xE8 di creare un formato speciale di dati chiamato "),ma=i(es,"EM",{});var Dc=r(ma);Dr=o(Dc,"indice"),Dc.forEach(s),zr=o(es," che permette di trovare quali embedding sono simili a un embedding in input. Creare un indice FAISS su \u{1F917} Datasets \xE8 semplice \u2014 usiamo la funzione "),pa=i(es,"CODE",{});var zc=r(pa);Cr=o(zc,"Dataset.add_faiss_index()"),zc.forEach(s),Tr=o(es," e specificare quale colonna nel nostro dataset vorremmo indicizzare:"),es.forEach(s),mo=m(e),k($t.$$.fragment,e),po=m(e),Me=i(e,"P",{});var To=r(Me);Pr=o(To,"Ora possiamo eseguire dele query su questo indice effettuando una ricerca degli elementi pi\xF9 vicini usando la funzione "),ua=i(To,"CODE",{});var Cc=r(ua);Sr=o(Cc,"Dataset.get_nearest_examples()"),Cc.forEach(s),Ar=o(To,". Testiamolo creando un embedding per una domanda."),To.forEach(s),uo=m(e),ge.l(e),Ut=m(e),Gt=i(e,"P",{});var Tc=r(Gt);Or=o(Tc,"Proprio come con i documenti, ora abbiamo un vettore di 768 dimensioni che rappresenta la query, che possiamo confrontare con l\u2019intero corpus per trovare gli embedding pi\xF9 simili:"),Tc.forEach(s),fo=m(e),k(Et.$$.fragment,e),ho=m(e),we=i(e,"P",{});var ts=r(we);Ir=o(ts,"La funzione "),fa=i(ts,"CODE",{});var Pc=r(fa);Rr=o(Pc,"Dataset.get_nearest_examples()"),Pc.forEach(s),Nr=o(ts," restituisce una tupla di valori che valutano la sovrapposizione tra la query e il documento, e un set corrispondente di campioni (in questo caso, le 5 corrispondenze migliori). Salviamole in un "),ha=i(ts,"CODE",{});var Sc=r(ha);Lr=o(Sc,"pandas.DataFrame"),Sc.forEach(s),Fr=o(ts,", cos\xEC che possiamo ordinarle facilmente:"),ts.forEach(s),_o=m(e),k(kt.$$.fragment,e),go=m(e),Qt=i(e,"P",{});var Ac=r(Qt);Hr=o(Ac,"Ora possiamo iterare sulle prime righe per vedere quanto bene la nostra query corrisponde ai commenti disponibili:"),Ac.forEach(s),bo=m(e),k(wt.$$.fragment,e),vo=m(e),k(xt.$$.fragment,e),$o=m(e),Bt=i(e,"P",{});var Oc=r(Bt);Mr=o(Oc,"Non male! Il nostro secondo risultato sembra soddisfare la nostra richiesta."),Oc.forEach(s),Eo=m(e),k(Ue.$$.fragment,e),this.h()},h(){y(p,"name","hf:doc:metadata"),y(p,"content",JSON.stringify(ad)),y(v,"id","ricerca-semantica-con-faiss"),y(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(v,"href","#ricerca-semantica-con-faiss"),y(h,"class","relative group"),y(z,"href","/course/chapter5/5"),y(C,"id","usare-gli-embedding-per-la-ricerca-semantica"),y(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(C,"href","#usare-gli-embedding-per-la-ricerca-semantica"),y(u,"class","relative group"),y(Tt,"href","/course/chapter1"),y(We,"class","block dark:hidden"),Ic(We.src,Gr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg")||y(We,"src",Gr),y(We,"alt","Semantic search."),y(Ve,"class","hidden dark:block"),Ic(Ve.src,Qr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg")||y(Ve,"src",Qr),y(Ve,"alt","Semantic search."),y(Ce,"class","flex justify-center"),y(Ae,"id","caricare-e-preparare-il-dataset"),y(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Ae,"href","#caricare-e-preparare-il-dataset"),y(Te,"class","relative group"),y(At,"href","/course/chapter5/2"),y(Oe,"href","https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"),y(Oe,"rel","nofollow"),Po(Z,"text-align","right"),y(ee,"border","1"),y(ee,"class","dataframe"),Po(ee,"table-layout","fixed"),Po(ee,"word-wrap","break-word"),Po(ee,"width","100%"),y(Fe,"id","creare-i-text-embedding"),y(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Fe,"href","#creare-i-text-embedding"),y(Pe,"class","relative group"),y(Nt,"href","/course/chapter2"),y(ht,"href","https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"),y(ht,"rel","nofollow"),y(_t,"href","https://www.sbert.net/docs/pretrained_models.html#model-overview"),y(_t,"rel","nofollow"),y(He,"id","usare-faiss-per-ricerca-di-similarit-efficiente"),y(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(He,"href","#usare-faiss-per-ricerca-di-similarit-efficiente"),y(Se,"class","relative group"),y(vt,"href","https://faiss.ai/"),y(vt,"rel","nofollow")},m(e,l){t(document.head,p),c(e,$,l),w(f,e,l),c(e,q,l),c(e,h,l),t(h,v),t(v,A),w(_,A,null),t(h,T),t(h,j),t(j,H),c(e,O,l),yt[P].m(e,l),c(e,R,l),c(e,L,l),t(L,G),t(L,z),t(z,N),t(L,Y),c(e,U,l),w(D,e,l),c(e,Q,l),c(e,u,l),t(u,C),t(C,F),w(W,F,null),t(u,ne),t(u,de),t(de,Oo),c(e,ba,l),c(e,ie,l),t(ie,Io),t(ie,Tt),t(Tt,Ro),t(ie,No),t(ie,ss),t(ss,Lo),t(ie,Fo),t(ie,as),t(as,Ho),t(ie,Mo),c(e,va,l),c(e,Pt,l),t(Pt,Uo),c(e,$a,l),c(e,Ce,l),t(Ce,We),t(Ce,Go),t(Ce,Ve),c(e,Ea,l),c(e,Te,l),t(Te,Ae),t(Ae,os),w(Je,os,null),t(Te,Qo),t(Te,ns),t(ns,Bo),c(e,ka,l),c(e,St,l),t(St,Yo),c(e,wa,l),w(Xe,e,l),c(e,xa,l),c(e,be,l),t(be,Wo),t(be,is),t(is,Vo),t(be,Jo),t(be,At),t(At,Xo),t(be,Ko),c(e,ya,l),w(Ke,e,l),c(e,qa,l),w(Ze,e,l),c(e,ja,l),c(e,V,l),t(V,Zo),t(V,rs),t(rs,en),t(V,tn),t(V,ls),t(ls,sn),t(V,an),t(V,cs),t(cs,on),t(V,nn),t(V,ds),t(ds,rn),t(V,ln),t(V,ms),t(ms,cn),t(V,dn),c(e,Da,l),w(et,e,l),c(e,za,l),w(tt,e,l),c(e,Ca,l),c(e,J,l),t(J,mn),t(J,ps),t(ps,pn),t(J,un),t(J,us),t(us,fn),t(J,hn),t(J,fs),t(fs,_n),t(J,gn),t(J,hs),t(hs,bn),t(J,vn),t(J,_s),t(_s,$n),t(J,En),c(e,Ta,l),w(st,e,l),c(e,Pa,l),w(at,e,l),c(e,Sa,l),c(e,K,l),t(K,kn),t(K,gs),t(gs,wn),t(K,xn),t(K,bs),t(bs,yn),t(K,qn),t(K,Oe),t(Oe,jn),t(Oe,vs),t(vs,Dn),t(K,zn),t(K,$s),t($s,Cn),t(K,Tn),c(e,Aa,l),w(ot,e,l),c(e,Oa,l),c(e,Ie,l),t(Ie,Pn),t(Ie,Es),t(Es,Sn),t(Ie,An),c(e,Ia,l),w(nt,e,l),c(e,Ra,l),w(it,e,l),c(e,Na,l),c(e,Re,l),t(Re,On),t(Re,ks),t(ks,In),t(Re,Rn),c(e,La,l),w(rt,e,l),c(e,Fa,l),c(e,ee,l),t(ee,ws),t(ws,Z),t(Z,Ha),t(Z,Nn),t(Z,xs),t(xs,Ln),t(Z,Fn),t(Z,ys),t(ys,Hn),t(Z,Mn),t(Z,qs),t(qs,Un),t(Z,Gn),t(Z,js),t(js,Qn),t(ee,Bn),t(ee,me),t(me,te),t(te,Ds),t(Ds,Yn),t(te,Wn),t(te,zs),t(zs,Vn),t(te,Jn),t(te,Cs),t(Cs,Xn),t(te,Kn),t(te,Ts),t(Ts,Zn),t(te,ei),t(te,Ps),t(Ps,ti),t(me,si),t(me,se),t(se,Ss),t(Ss,ai),t(se,oi),t(se,As),t(As,ni),t(se,ii),t(se,Os),t(Os,ri),t(se,li),t(se,Is),t(Is,ci),t(se,di),t(se,Rs),t(Rs,mi),t(me,pi),t(me,ae),t(ae,Ns),t(Ns,ui),t(ae,fi),t(ae,Ls),t(Ls,hi),t(ae,_i),t(ae,Fs),t(Fs,gi),t(ae,bi),t(ae,Hs),t(Hs,vi),t(ae,$i),t(ae,Ms),t(Ms,Ei),t(me,ki),t(me,oe),t(oe,Us),t(Us,wi),t(oe,xi),t(oe,Gs),t(Gs,yi),t(oe,qi),t(oe,Qs),t(Qs,ji),t(oe,Di),t(oe,Bs),t(Bs,zi),t(oe,Ci),t(oe,Ys),t(Ys,Ti),c(e,Ma,l),c(e,re,l),t(re,Pi),t(re,Ws),t(Ws,Si),t(re,Ai),t(re,Vs),t(Vs,Oi),t(re,Ii),t(re,Js),t(Js,Ri),t(re,Ni),c(e,Ua,l),w(lt,e,l),c(e,Ga,l),w(ct,e,l),c(e,Qa,l),c(e,Ot,l),t(Ot,Li),c(e,Ba,l),w(Ne,e,l),c(e,Ya,l),c(e,Le,l),t(Le,Fi),t(Le,Xs),t(Xs,Hi),t(Le,Mi),c(e,Wa,l),w(dt,e,l),c(e,Va,l),c(e,It,l),t(It,Ui),c(e,Ja,l),w(mt,e,l),c(e,Xa,l),w(pt,e,l),c(e,Ka,l),c(e,ve,l),t(ve,Gi),t(ve,Ks),t(Ks,Qi),t(ve,Bi),t(ve,Zs),t(Zs,Yi),t(ve,Wi),c(e,Za,l),w(ut,e,l),c(e,eo,l),c(e,Rt,l),t(Rt,Vi),c(e,to,l),c(e,Pe,l),t(Pe,Fe),t(Fe,ea),w(ft,ea,null),t(Pe,Ji),t(Pe,ta),t(ta,Xi),c(e,so,l),c(e,B,l),t(B,Ki),t(B,Nt),t(Nt,Zi),t(B,er),t(B,sa),t(sa,tr),t(B,sr),t(B,aa),t(aa,ar),t(B,or),t(B,ht),t(ht,nr),t(B,ir),t(B,oa),t(oa,rr),t(B,lr),t(B,_t),t(_t,cr),t(B,dr),t(B,na),t(na,mr),t(B,pr),c(e,ao,l),qt[pe].m(e,l),c(e,Lt,l),c(e,$e,l),t($e,ur),t($e,ia),t(ia,fr),t($e,hr),t($e,ra),t(ra,_r),t($e,gr),c(e,oo,l),w(gt,e,l),c(e,no,l),c(e,Ft,l),t(Ft,br),c(e,io,l),jt[fe].m(e,l),c(e,Ht,l),c(e,Mt,l),t(Mt,vr),c(e,ro,l),c(e,Se,l),t(Se,He),t(He,la),w(bt,la,null),t(Se,$r),t(Se,ca),t(ca,Er),c(e,lo,l),c(e,Ee,l),t(Ee,kr),t(Ee,da),t(da,wr),t(Ee,xr),t(Ee,vt),t(vt,yr),t(Ee,qr),c(e,co,l),c(e,ke,l),t(ke,jr),t(ke,ma),t(ma,Dr),t(ke,zr),t(ke,pa),t(pa,Cr),t(ke,Tr),c(e,mo,l),w($t,e,l),c(e,po,l),c(e,Me,l),t(Me,Pr),t(Me,ua),t(ua,Sr),t(Me,Ar),c(e,uo,l),Dt[_e].m(e,l),c(e,Ut,l),c(e,Gt,l),t(Gt,Or),c(e,fo,l),w(Et,e,l),c(e,ho,l),c(e,we,l),t(we,Ir),t(we,fa),t(fa,Rr),t(we,Nr),t(we,ha),t(ha,Lr),t(we,Fr),c(e,_o,l),w(kt,e,l),c(e,go,l),c(e,Qt,l),t(Qt,Hr),c(e,bo,l),w(wt,e,l),c(e,vo,l),w(xt,e,l),c(e,$o,l),c(e,Bt,l),t(Bt,Mr),c(e,Eo,l),w(Ue,e,l),ko=!0},p(e,[l]){const zt={};l&1&&(zt.fw=e[0]),f.$set(zt);let Yt=P;P=Yr(e),P!==Yt&&(Ao(),g(yt[Yt],1,1,()=>{yt[Yt]=null}),So(),I=yt[P],I||(I=yt[P]=Br[P](e),I.c()),b(I,1),I.m(R.parentNode,R));const _a={};l&2&&(_a.$$scope={dirty:l,ctx:e}),Ne.$set(_a);let Wt=pe;pe=Vr(e),pe!==Wt&&(Ao(),g(qt[Wt],1,1,()=>{qt[Wt]=null}),So(),ue=qt[pe],ue||(ue=qt[pe]=Wr[pe](e),ue.c()),b(ue,1),ue.m(Lt.parentNode,Lt));let Ge=fe;fe=Xr(e),fe!==Ge&&(Ao(),g(jt[Ge],1,1,()=>{jt[Ge]=null}),So(),he=jt[fe],he||(he=jt[fe]=Jr[fe](e),he.c()),b(he,1),he.m(Ht.parentNode,Ht));let Vt=_e;_e=Zr(e),_e!==Vt&&(Ao(),g(Dt[Vt],1,1,()=>{Dt[Vt]=null}),So(),ge=Dt[_e],ge||(ge=Dt[_e]=Kr[_e](e),ge.c()),b(ge,1),ge.m(Ut.parentNode,Ut));const Ct={};l&2&&(Ct.$$scope={dirty:l,ctx:e}),Ue.$set(Ct)},i(e){ko||(b(f.$$.fragment,e),b(_.$$.fragment,e),b(I),b(D.$$.fragment,e),b(W.$$.fragment,e),b(Je.$$.fragment,e),b(Xe.$$.fragment,e),b(Ke.$$.fragment,e),b(Ze.$$.fragment,e),b(et.$$.fragment,e),b(tt.$$.fragment,e),b(st.$$.fragment,e),b(at.$$.fragment,e),b(ot.$$.fragment,e),b(nt.$$.fragment,e),b(it.$$.fragment,e),b(rt.$$.fragment,e),b(lt.$$.fragment,e),b(ct.$$.fragment,e),b(Ne.$$.fragment,e),b(dt.$$.fragment,e),b(mt.$$.fragment,e),b(pt.$$.fragment,e),b(ut.$$.fragment,e),b(ft.$$.fragment,e),b(ue),b(gt.$$.fragment,e),b(he),b(bt.$$.fragment,e),b($t.$$.fragment,e),b(ge),b(Et.$$.fragment,e),b(kt.$$.fragment,e),b(wt.$$.fragment,e),b(xt.$$.fragment,e),b(Ue.$$.fragment,e),ko=!0)},o(e){g(f.$$.fragment,e),g(_.$$.fragment,e),g(I),g(D.$$.fragment,e),g(W.$$.fragment,e),g(Je.$$.fragment,e),g(Xe.$$.fragment,e),g(Ke.$$.fragment,e),g(Ze.$$.fragment,e),g(et.$$.fragment,e),g(tt.$$.fragment,e),g(st.$$.fragment,e),g(at.$$.fragment,e),g(ot.$$.fragment,e),g(nt.$$.fragment,e),g(it.$$.fragment,e),g(rt.$$.fragment,e),g(lt.$$.fragment,e),g(ct.$$.fragment,e),g(Ne.$$.fragment,e),g(dt.$$.fragment,e),g(mt.$$.fragment,e),g(pt.$$.fragment,e),g(ut.$$.fragment,e),g(ft.$$.fragment,e),g(ue),g(gt.$$.fragment,e),g(he),g(bt.$$.fragment,e),g($t.$$.fragment,e),g(ge),g(Et.$$.fragment,e),g(kt.$$.fragment,e),g(wt.$$.fragment,e),g(xt.$$.fragment,e),g(Ue.$$.fragment,e),ko=!1},d(e){s(p),e&&s($),x(f,e),e&&s(q),e&&s(h),x(_),e&&s(O),yt[P].d(e),e&&s(R),e&&s(L),e&&s(U),x(D,e),e&&s(Q),e&&s(u),x(W),e&&s(ba),e&&s(ie),e&&s(va),e&&s(Pt),e&&s($a),e&&s(Ce),e&&s(Ea),e&&s(Te),x(Je),e&&s(ka),e&&s(St),e&&s(wa),x(Xe,e),e&&s(xa),e&&s(be),e&&s(ya),x(Ke,e),e&&s(qa),x(Ze,e),e&&s(ja),e&&s(V),e&&s(Da),x(et,e),e&&s(za),x(tt,e),e&&s(Ca),e&&s(J),e&&s(Ta),x(st,e),e&&s(Pa),x(at,e),e&&s(Sa),e&&s(K),e&&s(Aa),x(ot,e),e&&s(Oa),e&&s(Ie),e&&s(Ia),x(nt,e),e&&s(Ra),x(it,e),e&&s(Na),e&&s(Re),e&&s(La),x(rt,e),e&&s(Fa),e&&s(ee),e&&s(Ma),e&&s(re),e&&s(Ua),x(lt,e),e&&s(Ga),x(ct,e),e&&s(Qa),e&&s(Ot),e&&s(Ba),x(Ne,e),e&&s(Ya),e&&s(Le),e&&s(Wa),x(dt,e),e&&s(Va),e&&s(It),e&&s(Ja),x(mt,e),e&&s(Xa),x(pt,e),e&&s(Ka),e&&s(ve),e&&s(Za),x(ut,e),e&&s(eo),e&&s(Rt),e&&s(to),e&&s(Pe),x(ft),e&&s(so),e&&s(B),e&&s(ao),qt[pe].d(e),e&&s(Lt),e&&s($e),e&&s(oo),x(gt,e),e&&s(no),e&&s(Ft),e&&s(io),jt[fe].d(e),e&&s(Ht),e&&s(Mt),e&&s(ro),e&&s(Se),x(bt),e&&s(lo),e&&s(Ee),e&&s(co),e&&s(ke),e&&s(mo),x($t,e),e&&s(po),e&&s(Me),e&&s(uo),Dt[_e].d(e),e&&s(Ut),e&&s(Gt),e&&s(fo),x(Et,e),e&&s(ho),e&&s(we),e&&s(_o),x(kt,e),e&&s(go),e&&s(Qt),e&&s(bo),x(wt,e),e&&s(vo),x(xt,e),e&&s($o),e&&s(Bt),e&&s(Eo),x(Ue,e)}}}const ad={local:"ricerca-semantica-con-faiss",sections:[{local:"usare-gli-embedding-per-la-ricerca-semantica",title:"Usare gli embedding per la ricerca semantica "},{local:"caricare-e-preparare-il-dataset",title:"Caricare e preparare il dataset "},{local:"creare-i-text-embedding",title:"Creare i text embedding"},{local:"usare-faiss-per-ricerca-di-similarit-efficiente",title:"Usare FAISS per ricerca di similarit\xE0 efficiente "}],title:"Ricerca semantica con FAISS"};function od(M,p,$){let f="pt";return Uc(()=>{const q=new URLSearchParams(window.location.search);$(0,f=q.get("fw")||"pt")}),[f]}class pd extends Lc{constructor(p){super();Fc(this,p,od,sd,Hc,{})}}export{pd as default,ad as metadata};
