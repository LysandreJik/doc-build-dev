import{S as wr,i as vr,s as gr,e as o,k as p,w as u,t as a,M as br,c as n,d as s,m as c,a as i,x as m,h as r,b as f,N as mr,G as t,g as h,y as k,q as w,o as v,B as g,v as _r}from"../../chunks/vendor-hf-doc-builder.js";import{T as zr}from"../../chunks/Tip-hf-doc-builder.js";import{Y as kr}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Jt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as R}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as $r}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Er(Yt){let d,K,b,E,M,z,ue,U;return{c(){d=o("p"),K=a("\u270F\uFE0F "),b=o("strong"),E=a("Try it out!"),M=a(" Load a tokenizer from the "),z=o("code"),ue=a("bert-base-cased"),U=a(" checkpoint and pass the same example to it. What are the main differences you can see between the cased and uncased versions of the tokenizer?")},l(F){d=n(F,"P",{});var $=i(d);K=r($,"\u270F\uFE0F "),b=n($,"STRONG",{});var q=i(b);E=r(q,"Try it out!"),q.forEach(s),M=r($," Load a tokenizer from the "),z=n($,"CODE",{});var me=i(z);ue=r(me,"bert-base-cased"),me.forEach(s),U=r($," checkpoint and pass the same example to it. What are the main differences you can see between the cased and uncased versions of the tokenizer?"),$.forEach(s)},m(F,$){h(F,d,$),t(d,K),t(d,b),t(b,E),t(d,M),t(d,z),t(z,ue),t(d,U)},d(F){F&&s(d)}}}function yr(Yt){let d,K,b,E,M,z,ue,U,F,$,q,me,Oe,Fs,Kt,G,ke,Zn,Gs,we,ea,Vt,H,Ls,wt,Ws,Js,vt,Ys,Ks,Xt,L,V,gt,ve,Vs,bt,Xs,Qt,ge,Zt,X,Qs,be,Zs,eo,es,B,to,_t,so,oo,zt,no,ao,ts,_e,ss,ze,os,y,ro,$t,io,lo,Et,ho,po,yt,co,fo,ns,$e,as,Ee,rs,Q,uo,xt,mo,ko,is,Z,ls,W,ee,Tt,ye,wo,jt,vo,hs,xe,ps,te,go,Ie,bo,_o,cs,x,zo,Pt,$o,Eo,At,yo,xo,Dt,To,jo,fs,Te,ds,je,us,N,Po,Ct,Ao,Do,St,Co,So,ms,Re,qo,ks,Pe,ws,se,Ho,qt,Bo,No,vs,Ae,gs,Me,Oo,bs,Ue,Io,_s,De,zs,Ce,$s,_,Ro,Ht,Mo,Uo,Bt,Fo,Go,Nt,Lo,Wo,Ot,Jo,Yo,Es,Fe,Ko,ys,J,oe,It,Se,Vo,Rt,Xo,xs,T,qe,Qo,Zo,Mt,en,tn,Ge,sn,on,Ts,O,nn,Ut,an,rn,Ft,ln,hn,js,Y,ne,Gt,He,pn,Lt,cn,Ps,Le,fn,As,ae,Wt,j,We,dn,un,Je,mn,kn,Ye,wn,vn,Ke,gn,bn,P,A,Ve,_n,zn,Xe,$n,En,Qe,yn,xn,Ze,Tn,jn,D,et,Pn,An,tt,Dn,Cn,st,Sn,qn,ot,Hn,Bn,C,nt,Nn,On,at,In,Rn,rt,Mn,Un,it,Fn,Gn,S,lt,Ln,Wn,ht,Jn,Yn,pt,Kn,Vn,ct,Xn,Ds,ft,Qn,Cs;return z=new Jt({}),q=new $r({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section4.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section4.ipynb"}]}}),ve=new Jt({}),ge=new kr({props:{id:"4IIC2jI9CaU"}}),_e=new R({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(tokenizer.backend_tokenizer))`}}),ze=new R({props:{code:"<class 'tokenizers.Tokenizer'>",highlighted:'&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;tokenizers.Tokenizer&#x27;</span>&gt;'}}),$e=new R({props:{code:'print(tokenizer.backend_tokenizer.normalizer.normalize_str("H\xE9ll\xF2 h\xF4w are \xFC?"))',highlighted:'<span class="hljs-built_in">print</span>(tokenizer.backend_tokenizer.normalizer.normalize_str(<span class="hljs-string">&quot;H\xE9ll\xF2 h\xF4w are \xFC?&quot;</span>))'}}),Ee=new R({props:{code:"'hello how are u?'",highlighted:'<span class="hljs-string">&#x27;hello how are u?&#x27;</span>'}}),Z=new zr({props:{$$slots:{default:[Er]},$$scope:{ctx:Yt}}}),ye=new Jt({}),xe=new kr({props:{id:"grlLV8AIXug"}}),Te=new R({props:{code:'tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")',highlighted:'tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Hello, how are  you?&quot;</span>)'}}),je=new R({props:{code:"[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]",highlighted:'[(<span class="hljs-string">&#x27;Hello&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;,&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">6</span>)), (<span class="hljs-string">&#x27;how&#x27;</span>, (<span class="hljs-number">7</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;are&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;you&#x27;</span>, (<span class="hljs-number">16</span>, <span class="hljs-number">19</span>)), (<span class="hljs-string">&#x27;?&#x27;</span>, (<span class="hljs-number">19</span>, <span class="hljs-number">20</span>))]'}}),Pe=new R({props:{code:`tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Hello, how are  you?&quot;</span>)`}}),Ae=new R({props:{code:`[('Hello', (0, 5)), (',', (5, 6)), ('\u0120how', (6, 10)), ('\u0120are', (10, 14)), ('\u0120', (14, 15)), ('\u0120you', (15, 19)),
 ('?', (19, 20))]`,highlighted:`[(<span class="hljs-string">&#x27;Hello&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)), (<span class="hljs-string">&#x27;,&#x27;</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">6</span>)), (<span class="hljs-string">&#x27;\u0120how&#x27;</span>, (<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u0120are&#x27;</span>, (<span class="hljs-number">10</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u0120&#x27;</span>, (<span class="hljs-number">14</span>, <span class="hljs-number">15</span>)), (<span class="hljs-string">&#x27;\u0120you&#x27;</span>, (<span class="hljs-number">15</span>, <span class="hljs-number">19</span>)),
 (<span class="hljs-string">&#x27;?&#x27;</span>, (<span class="hljs-number">19</span>, <span class="hljs-number">20</span>))]`}}),De=new R({props:{code:`tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="hljs-string">&quot;Hello, how are  you?&quot;</span>)`}}),Ce=new R({props:{code:"[('\u2581Hello,', (0, 6)), ('\u2581how', (7, 10)), ('\u2581are', (11, 14)), ('\u2581you?', (16, 20))]",highlighted:'[(<span class="hljs-string">&#x27;\u2581Hello,&#x27;</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">6</span>)), (<span class="hljs-string">&#x27;\u2581how&#x27;</span>, (<span class="hljs-number">7</span>, <span class="hljs-number">10</span>)), (<span class="hljs-string">&#x27;\u2581are&#x27;</span>, (<span class="hljs-number">11</span>, <span class="hljs-number">14</span>)), (<span class="hljs-string">&#x27;\u2581you?&#x27;</span>, (<span class="hljs-number">16</span>, <span class="hljs-number">20</span>))]'}}),Se=new Jt({}),He=new Jt({}),{c(){d=o("meta"),K=p(),b=o("h1"),E=o("a"),M=o("span"),u(z.$$.fragment),ue=p(),U=o("span"),F=a("Chu\u1EA9n ho\xE1 v\xE0 ti\u1EC1n tokenize"),$=p(),u(q.$$.fragment),me=p(),Oe=o("p"),Fs=a("Before we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we\u2019ll first take a look at the preprocessing that each tokenizer applies to text. Here\u2019s a high-level overview of the steps in the tokenization pipeline:"),Kt=p(),G=o("div"),ke=o("img"),Gs=p(),we=o("img"),Vt=p(),H=o("p"),Ls=a("Before splitting a text into subtokens (according to its model), the tokenizer performs two steps: "),wt=o("em"),Ws=a("normalization"),Js=a(" and "),vt=o("em"),Ys=a("pre-tokenization"),Ks=a("."),Xt=p(),L=o("h2"),V=o("a"),gt=o("span"),u(ve.$$.fragment),Vs=p(),bt=o("span"),Xs=a("Normalization"),Qt=p(),u(ge.$$.fragment),Zt=p(),X=o("p"),Qs=a("The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you\u2019re familiar with "),be=o("a"),Zs=a("Unicode normalization"),eo=a(" (such as NFC or NFKC), this is also something the tokenizer may apply."),es=p(),B=o("p"),to=a("The \u{1F917} Transformers "),_t=o("code"),so=a("tokenizer"),oo=a(" has an attribute called "),zt=o("code"),no=a("backend_tokenizer"),ao=a(" that provides access to the underlying tokenizer from the \u{1F917} Tokenizers library:"),ts=p(),u(_e.$$.fragment),ss=p(),u(ze.$$.fragment),os=p(),y=o("p"),ro=a("The "),$t=o("code"),io=a("normalizer"),lo=a(" attribute of the "),Et=o("code"),ho=a("tokenizer"),po=a(" object has a "),yt=o("code"),co=a("normalize_str()"),fo=a(" method that we can use to see how the normalization is performed:"),ns=p(),u($e.$$.fragment),as=p(),u(Ee.$$.fragment),rs=p(),Q=o("p"),uo=a("In this example, since we picked the "),xt=o("code"),mo=a("bert-base-uncased"),ko=a(" checkpoint, the normalization applied lowercasing and removed the accents."),is=p(),u(Z.$$.fragment),ls=p(),W=o("h2"),ee=o("a"),Tt=o("span"),u(ye.$$.fragment),wo=p(),jt=o("span"),vo=a("Pre-tokenization"),hs=p(),u(xe.$$.fragment),ps=p(),te=o("p"),go=a("As we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That\u2019s where the pre-tokenization step comes in. As we saw in "),Ie=o("a"),bo=a("Chapter 2"),_o=a(", a word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training."),cs=p(),x=o("p"),zo=a("To see how a fast tokenizer performs pre-tokenization, we can use the "),Pt=o("code"),$o=a("pre_tokenize_str()"),Eo=a(" method of the "),At=o("code"),yo=a("pre_tokenizer"),xo=a(" attribute of the "),Dt=o("code"),To=a("tokenizer"),jo=a(" object:"),fs=p(),u(Te.$$.fragment),ds=p(),u(je.$$.fragment),us=p(),N=o("p"),Po=a("Notice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between "),Ct=o("code"),Ao=a("are"),Do=a(" and "),St=o("code"),Co=a("you"),So=a(" to account for that."),ms=p(),Re=o("p"),qo=a("Since we\u2019re using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer:"),ks=p(),u(Pe.$$.fragment),ws=p(),se=o("p"),Ho=a("it will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a "),qt=o("code"),Bo=a("\u0120"),No=a(" symbol, enabling it to recover the original spaces if we decode the tokens:"),vs=p(),u(Ae.$$.fragment),gs=p(),Me=o("p"),Oo=a("Also note that unlike the BERT tokenizer, this tokenizer does not ignore the double space."),bs=p(),Ue=o("p"),Io=a("For a last example, let\u2019s have a look at the T5 tokenizer, which is based on the SentencePiece algorithm:"),_s=p(),u(De.$$.fragment),zs=p(),u(Ce.$$.fragment),$s=p(),_=o("p"),Ro=a("Like the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token ("),Ht=o("code"),Mo=a("_"),Uo=a("), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before "),Bt=o("code"),Fo=a("Hello"),Go=a(") and ignored the double space between "),Nt=o("code"),Lo=a("are"),Wo=a(" and "),Ot=o("code"),Jo=a("you"),Yo=a("."),Es=p(),Fe=o("p"),Ko=a("Now that we\u2019ve seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We\u2019ll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we\u2019ll examine how the three main algorithms used for subword tokenization work."),ys=p(),J=o("h2"),oe=o("a"),It=o("span"),u(Se.$$.fragment),Vo=p(),Rt=o("span"),Xo=a("SentencePiece"),xs=p(),T=o("p"),qe=o("a"),Qo=a("SentencePiece"),Zo=a(" is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, "),Mt=o("code"),en=a("\u2581"),tn=a(". Used in conjunction with the Unigram algorithm (see "),Ge=o("a"),sn=a("section 7"),on=a("), it doesn\u2019t even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese)."),Ts=p(),O=o("p"),nn=a("The other main feature of SentencePiece is "),Ut=o("em"),an=a("reversible tokenization"),rn=a(": since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the "),Ft=o("code"),ln=a("_"),hn=a("s with spaces \u2014 this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible."),js=p(),Y=o("h2"),ne=o("a"),Gt=o("span"),u(He.$$.fragment),pn=p(),Lt=o("span"),cn=a("Algorithm overview"),Ps=p(),Le=o("p"),fn=a("In the following sections, we\u2019ll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here\u2019s a quick overview of how they each work. Don\u2019t hesitate to come back to this table after reading each of the next sections if it doesn\u2019t make sense to you yet."),As=p(),ae=o("table"),Wt=o("thead"),j=o("tr"),We=o("th"),dn=a("Model"),un=p(),Je=o("th"),mn=a("BPE"),kn=p(),Ye=o("th"),wn=a("WordPiece"),vn=p(),Ke=o("th"),gn=a("Unigram"),bn=p(),P=o("tbody"),A=o("tr"),Ve=o("td"),_n=a("Training"),zn=p(),Xe=o("td"),$n=a("Starts from a small vocabulary and learns rules to merge tokens"),En=p(),Qe=o("td"),yn=a("Starts from a small vocabulary and learns rules to merge tokens"),xn=p(),Ze=o("td"),Tn=a("Starts from a large vocabulary and learns rules to remove tokens"),jn=p(),D=o("tr"),et=o("td"),Pn=a("Training step"),An=p(),tt=o("td"),Dn=a("Merges the tokens corresponding to the most common pair"),Cn=p(),st=o("td"),Sn=a("Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent"),qn=p(),ot=o("td"),Hn=a("Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus"),Bn=p(),C=o("tr"),nt=o("td"),Nn=a("Learns"),On=p(),at=o("td"),In=a("Merge rules and a vocabulary"),Rn=p(),rt=o("td"),Mn=a("Just a vocabulary"),Un=p(),it=o("td"),Fn=a("A vocabulary with a score for each token"),Gn=p(),S=o("tr"),lt=o("td"),Ln=a("Encoding"),Wn=p(),ht=o("td"),Jn=a("Splits a word into characters and applies the merges learned during training"),Yn=p(),pt=o("td"),Kn=a("Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word"),Vn=p(),ct=o("td"),Xn=a("Finds the most likely split into tokens, using the scores learned during training"),Ds=p(),ft=o("p"),Qn=a("Now let\u2019s dive into BPE!"),this.h()},l(e){const l=br('[data-svelte="svelte-1phssyn"]',document.head);d=n(l,"META",{name:!0,content:!0}),l.forEach(s),K=c(e),b=n(e,"H1",{class:!0});var Be=i(b);E=n(Be,"A",{id:!0,class:!0,href:!0});var ta=i(E);M=n(ta,"SPAN",{});var sa=i(M);m(z.$$.fragment,sa),sa.forEach(s),ta.forEach(s),ue=c(Be),U=n(Be,"SPAN",{});var oa=i(U);F=r(oa,"Chu\u1EA9n ho\xE1 v\xE0 ti\u1EC1n tokenize"),oa.forEach(s),Be.forEach(s),$=c(e),m(q.$$.fragment,e),me=c(e),Oe=n(e,"P",{});var na=i(Oe);Fs=r(na,"Before we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we\u2019ll first take a look at the preprocessing that each tokenizer applies to text. Here\u2019s a high-level overview of the steps in the tokenization pipeline:"),na.forEach(s),Kt=c(e),G=n(e,"DIV",{class:!0});var Ss=i(G);ke=n(Ss,"IMG",{class:!0,src:!0,alt:!0}),Gs=c(Ss),we=n(Ss,"IMG",{class:!0,src:!0,alt:!0}),Ss.forEach(s),Vt=c(e),H=n(e,"P",{});var dt=i(H);Ls=r(dt,"Before splitting a text into subtokens (according to its model), the tokenizer performs two steps: "),wt=n(dt,"EM",{});var aa=i(wt);Ws=r(aa,"normalization"),aa.forEach(s),Js=r(dt," and "),vt=n(dt,"EM",{});var ra=i(vt);Ys=r(ra,"pre-tokenization"),ra.forEach(s),Ks=r(dt,"."),dt.forEach(s),Xt=c(e),L=n(e,"H2",{class:!0});var qs=i(L);V=n(qs,"A",{id:!0,class:!0,href:!0});var ia=i(V);gt=n(ia,"SPAN",{});var la=i(gt);m(ve.$$.fragment,la),la.forEach(s),ia.forEach(s),Vs=c(qs),bt=n(qs,"SPAN",{});var ha=i(bt);Xs=r(ha,"Normalization"),ha.forEach(s),qs.forEach(s),Qt=c(e),m(ge.$$.fragment,e),Zt=c(e),X=n(e,"P",{});var Hs=i(X);Qs=r(Hs,"The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you\u2019re familiar with "),be=n(Hs,"A",{href:!0,rel:!0});var pa=i(be);Zs=r(pa,"Unicode normalization"),pa.forEach(s),eo=r(Hs," (such as NFC or NFKC), this is also something the tokenizer may apply."),Hs.forEach(s),es=c(e),B=n(e,"P",{});var ut=i(B);to=r(ut,"The \u{1F917} Transformers "),_t=n(ut,"CODE",{});var ca=i(_t);so=r(ca,"tokenizer"),ca.forEach(s),oo=r(ut," has an attribute called "),zt=n(ut,"CODE",{});var fa=i(zt);no=r(fa,"backend_tokenizer"),fa.forEach(s),ao=r(ut," that provides access to the underlying tokenizer from the \u{1F917} Tokenizers library:"),ut.forEach(s),ts=c(e),m(_e.$$.fragment,e),ss=c(e),m(ze.$$.fragment,e),os=c(e),y=n(e,"P",{});var re=i(y);ro=r(re,"The "),$t=n(re,"CODE",{});var da=i($t);io=r(da,"normalizer"),da.forEach(s),lo=r(re," attribute of the "),Et=n(re,"CODE",{});var ua=i(Et);ho=r(ua,"tokenizer"),ua.forEach(s),po=r(re," object has a "),yt=n(re,"CODE",{});var ma=i(yt);co=r(ma,"normalize_str()"),ma.forEach(s),fo=r(re," method that we can use to see how the normalization is performed:"),re.forEach(s),ns=c(e),m($e.$$.fragment,e),as=c(e),m(Ee.$$.fragment,e),rs=c(e),Q=n(e,"P",{});var Bs=i(Q);uo=r(Bs,"In this example, since we picked the "),xt=n(Bs,"CODE",{});var ka=i(xt);mo=r(ka,"bert-base-uncased"),ka.forEach(s),ko=r(Bs," checkpoint, the normalization applied lowercasing and removed the accents."),Bs.forEach(s),is=c(e),m(Z.$$.fragment,e),ls=c(e),W=n(e,"H2",{class:!0});var Ns=i(W);ee=n(Ns,"A",{id:!0,class:!0,href:!0});var wa=i(ee);Tt=n(wa,"SPAN",{});var va=i(Tt);m(ye.$$.fragment,va),va.forEach(s),wa.forEach(s),wo=c(Ns),jt=n(Ns,"SPAN",{});var ga=i(jt);vo=r(ga,"Pre-tokenization"),ga.forEach(s),Ns.forEach(s),hs=c(e),m(xe.$$.fragment,e),ps=c(e),te=n(e,"P",{});var Os=i(te);go=r(Os,"As we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That\u2019s where the pre-tokenization step comes in. As we saw in "),Ie=n(Os,"A",{href:!0});var ba=i(Ie);bo=r(ba,"Chapter 2"),ba.forEach(s),_o=r(Os,", a word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training."),Os.forEach(s),cs=c(e),x=n(e,"P",{});var ie=i(x);zo=r(ie,"To see how a fast tokenizer performs pre-tokenization, we can use the "),Pt=n(ie,"CODE",{});var _a=i(Pt);$o=r(_a,"pre_tokenize_str()"),_a.forEach(s),Eo=r(ie," method of the "),At=n(ie,"CODE",{});var za=i(At);yo=r(za,"pre_tokenizer"),za.forEach(s),xo=r(ie," attribute of the "),Dt=n(ie,"CODE",{});var $a=i(Dt);To=r($a,"tokenizer"),$a.forEach(s),jo=r(ie," object:"),ie.forEach(s),fs=c(e),m(Te.$$.fragment,e),ds=c(e),m(je.$$.fragment,e),us=c(e),N=n(e,"P",{});var mt=i(N);Po=r(mt,"Notice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between "),Ct=n(mt,"CODE",{});var Ea=i(Ct);Ao=r(Ea,"are"),Ea.forEach(s),Do=r(mt," and "),St=n(mt,"CODE",{});var ya=i(St);Co=r(ya,"you"),ya.forEach(s),So=r(mt," to account for that."),mt.forEach(s),ms=c(e),Re=n(e,"P",{});var xa=i(Re);qo=r(xa,"Since we\u2019re using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer:"),xa.forEach(s),ks=c(e),m(Pe.$$.fragment,e),ws=c(e),se=n(e,"P",{});var Is=i(se);Ho=r(Is,"it will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a "),qt=n(Is,"CODE",{});var Ta=i(qt);Bo=r(Ta,"\u0120"),Ta.forEach(s),No=r(Is," symbol, enabling it to recover the original spaces if we decode the tokens:"),Is.forEach(s),vs=c(e),m(Ae.$$.fragment,e),gs=c(e),Me=n(e,"P",{});var ja=i(Me);Oo=r(ja,"Also note that unlike the BERT tokenizer, this tokenizer does not ignore the double space."),ja.forEach(s),bs=c(e),Ue=n(e,"P",{});var Pa=i(Ue);Io=r(Pa,"For a last example, let\u2019s have a look at the T5 tokenizer, which is based on the SentencePiece algorithm:"),Pa.forEach(s),_s=c(e),m(De.$$.fragment,e),zs=c(e),m(Ce.$$.fragment,e),$s=c(e),_=n(e,"P",{});var I=i(_);Ro=r(I,"Like the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token ("),Ht=n(I,"CODE",{});var Aa=i(Ht);Mo=r(Aa,"_"),Aa.forEach(s),Uo=r(I,"), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before "),Bt=n(I,"CODE",{});var Da=i(Bt);Fo=r(Da,"Hello"),Da.forEach(s),Go=r(I,") and ignored the double space between "),Nt=n(I,"CODE",{});var Ca=i(Nt);Lo=r(Ca,"are"),Ca.forEach(s),Wo=r(I," and "),Ot=n(I,"CODE",{});var Sa=i(Ot);Jo=r(Sa,"you"),Sa.forEach(s),Yo=r(I,"."),I.forEach(s),Es=c(e),Fe=n(e,"P",{});var qa=i(Fe);Ko=r(qa,"Now that we\u2019ve seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We\u2019ll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we\u2019ll examine how the three main algorithms used for subword tokenization work."),qa.forEach(s),ys=c(e),J=n(e,"H2",{class:!0});var Rs=i(J);oe=n(Rs,"A",{id:!0,class:!0,href:!0});var Ha=i(oe);It=n(Ha,"SPAN",{});var Ba=i(It);m(Se.$$.fragment,Ba),Ba.forEach(s),Ha.forEach(s),Vo=c(Rs),Rt=n(Rs,"SPAN",{});var Na=i(Rt);Xo=r(Na,"SentencePiece"),Na.forEach(s),Rs.forEach(s),xs=c(e),T=n(e,"P",{});var Ne=i(T);qe=n(Ne,"A",{href:!0,rel:!0});var Oa=i(qe);Qo=r(Oa,"SentencePiece"),Oa.forEach(s),Zo=r(Ne," is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, "),Mt=n(Ne,"CODE",{});var Ia=i(Mt);en=r(Ia,"\u2581"),Ia.forEach(s),tn=r(Ne,". Used in conjunction with the Unigram algorithm (see "),Ge=n(Ne,"A",{href:!0});var Ra=i(Ge);sn=r(Ra,"section 7"),Ra.forEach(s),on=r(Ne,"), it doesn\u2019t even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese)."),Ne.forEach(s),Ts=c(e),O=n(e,"P",{});var kt=i(O);nn=r(kt,"The other main feature of SentencePiece is "),Ut=n(kt,"EM",{});var Ma=i(Ut);an=r(Ma,"reversible tokenization"),Ma.forEach(s),rn=r(kt,": since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the "),Ft=n(kt,"CODE",{});var Ua=i(Ft);ln=r(Ua,"_"),Ua.forEach(s),hn=r(kt,"s with spaces \u2014 this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible."),kt.forEach(s),js=c(e),Y=n(e,"H2",{class:!0});var Ms=i(Y);ne=n(Ms,"A",{id:!0,class:!0,href:!0});var Fa=i(ne);Gt=n(Fa,"SPAN",{});var Ga=i(Gt);m(He.$$.fragment,Ga),Ga.forEach(s),Fa.forEach(s),pn=c(Ms),Lt=n(Ms,"SPAN",{});var La=i(Lt);cn=r(La,"Algorithm overview"),La.forEach(s),Ms.forEach(s),Ps=c(e),Le=n(e,"P",{});var Wa=i(Le);fn=r(Wa,"In the following sections, we\u2019ll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here\u2019s a quick overview of how they each work. Don\u2019t hesitate to come back to this table after reading each of the next sections if it doesn\u2019t make sense to you yet."),Wa.forEach(s),As=c(e),ae=n(e,"TABLE",{});var Us=i(ae);Wt=n(Us,"THEAD",{});var Ja=i(Wt);j=n(Ja,"TR",{});var le=i(j);We=n(le,"TH",{align:!0});var Ya=i(We);dn=r(Ya,"Model"),Ya.forEach(s),un=c(le),Je=n(le,"TH",{align:!0});var Ka=i(Je);mn=r(Ka,"BPE"),Ka.forEach(s),kn=c(le),Ye=n(le,"TH",{align:!0});var Va=i(Ye);wn=r(Va,"WordPiece"),Va.forEach(s),vn=c(le),Ke=n(le,"TH",{align:!0});var Xa=i(Ke);gn=r(Xa,"Unigram"),Xa.forEach(s),le.forEach(s),Ja.forEach(s),bn=c(Us),P=n(Us,"TBODY",{});var he=i(P);A=n(he,"TR",{});var pe=i(A);Ve=n(pe,"TD",{align:!0});var Qa=i(Ve);_n=r(Qa,"Training"),Qa.forEach(s),zn=c(pe),Xe=n(pe,"TD",{align:!0});var Za=i(Xe);$n=r(Za,"Starts from a small vocabulary and learns rules to merge tokens"),Za.forEach(s),En=c(pe),Qe=n(pe,"TD",{align:!0});var er=i(Qe);yn=r(er,"Starts from a small vocabulary and learns rules to merge tokens"),er.forEach(s),xn=c(pe),Ze=n(pe,"TD",{align:!0});var tr=i(Ze);Tn=r(tr,"Starts from a large vocabulary and learns rules to remove tokens"),tr.forEach(s),pe.forEach(s),jn=c(he),D=n(he,"TR",{});var ce=i(D);et=n(ce,"TD",{align:!0});var sr=i(et);Pn=r(sr,"Training step"),sr.forEach(s),An=c(ce),tt=n(ce,"TD",{align:!0});var or=i(tt);Dn=r(or,"Merges the tokens corresponding to the most common pair"),or.forEach(s),Cn=c(ce),st=n(ce,"TD",{align:!0});var nr=i(st);Sn=r(nr,"Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent"),nr.forEach(s),qn=c(ce),ot=n(ce,"TD",{align:!0});var ar=i(ot);Hn=r(ar,"Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus"),ar.forEach(s),ce.forEach(s),Bn=c(he),C=n(he,"TR",{});var fe=i(C);nt=n(fe,"TD",{align:!0});var rr=i(nt);Nn=r(rr,"Learns"),rr.forEach(s),On=c(fe),at=n(fe,"TD",{align:!0});var ir=i(at);In=r(ir,"Merge rules and a vocabulary"),ir.forEach(s),Rn=c(fe),rt=n(fe,"TD",{align:!0});var lr=i(rt);Mn=r(lr,"Just a vocabulary"),lr.forEach(s),Un=c(fe),it=n(fe,"TD",{align:!0});var hr=i(it);Fn=r(hr,"A vocabulary with a score for each token"),hr.forEach(s),fe.forEach(s),Gn=c(he),S=n(he,"TR",{});var de=i(S);lt=n(de,"TD",{align:!0});var pr=i(lt);Ln=r(pr,"Encoding"),pr.forEach(s),Wn=c(de),ht=n(de,"TD",{align:!0});var cr=i(ht);Jn=r(cr,"Splits a word into characters and applies the merges learned during training"),cr.forEach(s),Yn=c(de),pt=n(de,"TD",{align:!0});var fr=i(pt);Kn=r(fr,"Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word"),fr.forEach(s),Vn=c(de),ct=n(de,"TD",{align:!0});var dr=i(ct);Xn=r(dr,"Finds the most likely split into tokens, using the scores learned during training"),dr.forEach(s),de.forEach(s),he.forEach(s),Us.forEach(s),Ds=c(e),ft=n(e,"P",{});var ur=i(ft);Qn=r(ur,"Now let\u2019s dive into BPE!"),ur.forEach(s),this.h()},h(){f(d,"name","hf:doc:metadata"),f(d,"content",JSON.stringify(xr)),f(E,"id","chun-ho-v-tin-tokenize"),f(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(E,"href","#chun-ho-v-tin-tokenize"),f(b,"class","relative group"),f(ke,"class","block dark:hidden"),mr(ke.src,Zn="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg")||f(ke,"src",Zn),f(ke,"alt","The tokenization pipeline."),f(we,"class","hidden dark:block"),mr(we.src,ea="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg")||f(we,"src",ea),f(we,"alt","The tokenization pipeline."),f(G,"class","flex justify-center"),f(V,"id","normalization"),f(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(V,"href","#normalization"),f(L,"class","relative group"),f(be,"href","http://www.unicode.org/reports/tr15/"),f(be,"rel","nofollow"),f(ee,"id","pretokenization"),f(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ee,"href","#pretokenization"),f(W,"class","relative group"),f(Ie,"href","/course/chapter2"),f(oe,"id","sentencepiece"),f(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(oe,"href","#sentencepiece"),f(J,"class","relative group"),f(qe,"href","https://github.com/google/sentencepiece"),f(qe,"rel","nofollow"),f(Ge,"href","/course/chapter7/7"),f(ne,"id","algorithm-overview"),f(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ne,"href","#algorithm-overview"),f(Y,"class","relative group"),f(We,"align","center"),f(Je,"align","center"),f(Ye,"align","center"),f(Ke,"align","center"),f(Ve,"align","center"),f(Xe,"align","center"),f(Qe,"align","center"),f(Ze,"align","center"),f(et,"align","center"),f(tt,"align","center"),f(st,"align","center"),f(ot,"align","center"),f(nt,"align","center"),f(at,"align","center"),f(rt,"align","center"),f(it,"align","center"),f(lt,"align","center"),f(ht,"align","center"),f(pt,"align","center"),f(ct,"align","center")},m(e,l){t(document.head,d),h(e,K,l),h(e,b,l),t(b,E),t(E,M),k(z,M,null),t(b,ue),t(b,U),t(U,F),h(e,$,l),k(q,e,l),h(e,me,l),h(e,Oe,l),t(Oe,Fs),h(e,Kt,l),h(e,G,l),t(G,ke),t(G,Gs),t(G,we),h(e,Vt,l),h(e,H,l),t(H,Ls),t(H,wt),t(wt,Ws),t(H,Js),t(H,vt),t(vt,Ys),t(H,Ks),h(e,Xt,l),h(e,L,l),t(L,V),t(V,gt),k(ve,gt,null),t(L,Vs),t(L,bt),t(bt,Xs),h(e,Qt,l),k(ge,e,l),h(e,Zt,l),h(e,X,l),t(X,Qs),t(X,be),t(be,Zs),t(X,eo),h(e,es,l),h(e,B,l),t(B,to),t(B,_t),t(_t,so),t(B,oo),t(B,zt),t(zt,no),t(B,ao),h(e,ts,l),k(_e,e,l),h(e,ss,l),k(ze,e,l),h(e,os,l),h(e,y,l),t(y,ro),t(y,$t),t($t,io),t(y,lo),t(y,Et),t(Et,ho),t(y,po),t(y,yt),t(yt,co),t(y,fo),h(e,ns,l),k($e,e,l),h(e,as,l),k(Ee,e,l),h(e,rs,l),h(e,Q,l),t(Q,uo),t(Q,xt),t(xt,mo),t(Q,ko),h(e,is,l),k(Z,e,l),h(e,ls,l),h(e,W,l),t(W,ee),t(ee,Tt),k(ye,Tt,null),t(W,wo),t(W,jt),t(jt,vo),h(e,hs,l),k(xe,e,l),h(e,ps,l),h(e,te,l),t(te,go),t(te,Ie),t(Ie,bo),t(te,_o),h(e,cs,l),h(e,x,l),t(x,zo),t(x,Pt),t(Pt,$o),t(x,Eo),t(x,At),t(At,yo),t(x,xo),t(x,Dt),t(Dt,To),t(x,jo),h(e,fs,l),k(Te,e,l),h(e,ds,l),k(je,e,l),h(e,us,l),h(e,N,l),t(N,Po),t(N,Ct),t(Ct,Ao),t(N,Do),t(N,St),t(St,Co),t(N,So),h(e,ms,l),h(e,Re,l),t(Re,qo),h(e,ks,l),k(Pe,e,l),h(e,ws,l),h(e,se,l),t(se,Ho),t(se,qt),t(qt,Bo),t(se,No),h(e,vs,l),k(Ae,e,l),h(e,gs,l),h(e,Me,l),t(Me,Oo),h(e,bs,l),h(e,Ue,l),t(Ue,Io),h(e,_s,l),k(De,e,l),h(e,zs,l),k(Ce,e,l),h(e,$s,l),h(e,_,l),t(_,Ro),t(_,Ht),t(Ht,Mo),t(_,Uo),t(_,Bt),t(Bt,Fo),t(_,Go),t(_,Nt),t(Nt,Lo),t(_,Wo),t(_,Ot),t(Ot,Jo),t(_,Yo),h(e,Es,l),h(e,Fe,l),t(Fe,Ko),h(e,ys,l),h(e,J,l),t(J,oe),t(oe,It),k(Se,It,null),t(J,Vo),t(J,Rt),t(Rt,Xo),h(e,xs,l),h(e,T,l),t(T,qe),t(qe,Qo),t(T,Zo),t(T,Mt),t(Mt,en),t(T,tn),t(T,Ge),t(Ge,sn),t(T,on),h(e,Ts,l),h(e,O,l),t(O,nn),t(O,Ut),t(Ut,an),t(O,rn),t(O,Ft),t(Ft,ln),t(O,hn),h(e,js,l),h(e,Y,l),t(Y,ne),t(ne,Gt),k(He,Gt,null),t(Y,pn),t(Y,Lt),t(Lt,cn),h(e,Ps,l),h(e,Le,l),t(Le,fn),h(e,As,l),h(e,ae,l),t(ae,Wt),t(Wt,j),t(j,We),t(We,dn),t(j,un),t(j,Je),t(Je,mn),t(j,kn),t(j,Ye),t(Ye,wn),t(j,vn),t(j,Ke),t(Ke,gn),t(ae,bn),t(ae,P),t(P,A),t(A,Ve),t(Ve,_n),t(A,zn),t(A,Xe),t(Xe,$n),t(A,En),t(A,Qe),t(Qe,yn),t(A,xn),t(A,Ze),t(Ze,Tn),t(P,jn),t(P,D),t(D,et),t(et,Pn),t(D,An),t(D,tt),t(tt,Dn),t(D,Cn),t(D,st),t(st,Sn),t(D,qn),t(D,ot),t(ot,Hn),t(P,Bn),t(P,C),t(C,nt),t(nt,Nn),t(C,On),t(C,at),t(at,In),t(C,Rn),t(C,rt),t(rt,Mn),t(C,Un),t(C,it),t(it,Fn),t(P,Gn),t(P,S),t(S,lt),t(lt,Ln),t(S,Wn),t(S,ht),t(ht,Jn),t(S,Yn),t(S,pt),t(pt,Kn),t(S,Vn),t(S,ct),t(ct,Xn),h(e,Ds,l),h(e,ft,l),t(ft,Qn),Cs=!0},p(e,[l]){const Be={};l&2&&(Be.$$scope={dirty:l,ctx:e}),Z.$set(Be)},i(e){Cs||(w(z.$$.fragment,e),w(q.$$.fragment,e),w(ve.$$.fragment,e),w(ge.$$.fragment,e),w(_e.$$.fragment,e),w(ze.$$.fragment,e),w($e.$$.fragment,e),w(Ee.$$.fragment,e),w(Z.$$.fragment,e),w(ye.$$.fragment,e),w(xe.$$.fragment,e),w(Te.$$.fragment,e),w(je.$$.fragment,e),w(Pe.$$.fragment,e),w(Ae.$$.fragment,e),w(De.$$.fragment,e),w(Ce.$$.fragment,e),w(Se.$$.fragment,e),w(He.$$.fragment,e),Cs=!0)},o(e){v(z.$$.fragment,e),v(q.$$.fragment,e),v(ve.$$.fragment,e),v(ge.$$.fragment,e),v(_e.$$.fragment,e),v(ze.$$.fragment,e),v($e.$$.fragment,e),v(Ee.$$.fragment,e),v(Z.$$.fragment,e),v(ye.$$.fragment,e),v(xe.$$.fragment,e),v(Te.$$.fragment,e),v(je.$$.fragment,e),v(Pe.$$.fragment,e),v(Ae.$$.fragment,e),v(De.$$.fragment,e),v(Ce.$$.fragment,e),v(Se.$$.fragment,e),v(He.$$.fragment,e),Cs=!1},d(e){s(d),e&&s(K),e&&s(b),g(z),e&&s($),g(q,e),e&&s(me),e&&s(Oe),e&&s(Kt),e&&s(G),e&&s(Vt),e&&s(H),e&&s(Xt),e&&s(L),g(ve),e&&s(Qt),g(ge,e),e&&s(Zt),e&&s(X),e&&s(es),e&&s(B),e&&s(ts),g(_e,e),e&&s(ss),g(ze,e),e&&s(os),e&&s(y),e&&s(ns),g($e,e),e&&s(as),g(Ee,e),e&&s(rs),e&&s(Q),e&&s(is),g(Z,e),e&&s(ls),e&&s(W),g(ye),e&&s(hs),g(xe,e),e&&s(ps),e&&s(te),e&&s(cs),e&&s(x),e&&s(fs),g(Te,e),e&&s(ds),g(je,e),e&&s(us),e&&s(N),e&&s(ms),e&&s(Re),e&&s(ks),g(Pe,e),e&&s(ws),e&&s(se),e&&s(vs),g(Ae,e),e&&s(gs),e&&s(Me),e&&s(bs),e&&s(Ue),e&&s(_s),g(De,e),e&&s(zs),g(Ce,e),e&&s($s),e&&s(_),e&&s(Es),e&&s(Fe),e&&s(ys),e&&s(J),g(Se),e&&s(xs),e&&s(T),e&&s(Ts),e&&s(O),e&&s(js),e&&s(Y),g(He),e&&s(Ps),e&&s(Le),e&&s(As),e&&s(ae),e&&s(Ds),e&&s(ft)}}}const xr={local:"chun-ho-v-tin-tokenize",sections:[{local:"normalization",title:"Normalization"},{local:"pretokenization",title:"Pre-tokenization"},{local:"sentencepiece",title:"SentencePiece"},{local:"algorithm-overview",title:"Algorithm overview"}],title:"Chu\u1EA9n ho\xE1 v\xE0 ti\u1EC1n tokenize"};function Tr(Yt){return _r(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qr extends wr{constructor(d){super();vr(this,d,Tr,yr,gr,{})}}export{qr as default,xr as metadata};
