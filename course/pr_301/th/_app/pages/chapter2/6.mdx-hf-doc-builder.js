import{S as gn,i as _n,s as bn,e as p,k as f,w as g,t as l,l as fn,M as $n,c as u,d as s,m,x as _,a as c,h as i,b as I,G as t,g as r,y as b,o as k,p as mn,q as d,B as $,v as qn,n as hn}from"../../chunks/vendor-hf-doc-builder.js";import{I as kn}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as j}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as dn}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{F as wn}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function jn(v){let a,h;return a=new dn({props:{chapter:2,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section6_tf.ipynb"}]}}),{c(){g(a.$$.fragment)},l(o){_(a.$$.fragment,o)},m(o,q){b(a,o,q),h=!0},i(o){h||(d(a.$$.fragment,o),h=!0)},o(o){k(a.$$.fragment,o),h=!1},d(o){$(a,o)}}}function vn(v){let a,h;return a=new dn({props:{chapter:2,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section6_pt.ipynb"}]}}),{c(){g(a.$$.fragment)},l(o){_(a.$$.fragment,o)},m(o,q){b(a,o,q),h=!0},i(o){h||(d(a.$$.fragment,o),h=!0)},o(o){k(a.$$.fragment,o),h=!1},d(o){$(a,o)}}}function zn(v){let a,h;return a=new j({props:{code:`import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

tokens = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
output = model(**tokens)`}}),{c(){g(a.$$.fragment)},l(o){_(a.$$.fragment,o)},m(o,q){b(a,o,q),h=!0},i(o){h||(d(a.$$.fragment,o),h=!0)},o(o){k(a.$$.fragment,o),h=!1},d(o){$(a,o)}}}function yn(v){let a,h;return a=new j({props:{code:`import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

tokens = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
output = model(**tokens)`}}),{c(){g(a.$$.fragment)},l(o){_(a.$$.fragment,o)},m(o,q){b(a,o,q),h=!0},i(o){h||(d(a.$$.fragment,o),h=!0)},o(o){k(a.$$.fragment,o),h=!1},d(o){$(a,o)}}}function En(v){let a,h,o,q,N,ts,Te,z,y,se,ne,os,Ie,D,rs,me,as,ls,Se,M,Pe,S,is,he,ps,us,ke,cs,fs,Ae,te,ms,Fe,O,Ce,oe,hs,De,L,xe,re,ks,He,G,Re,ae,ds,Be,J,Ne,w,de,gs,_s,ge,bs,$s,_e,qs,ws,be,js,vs,Me,U,Oe,A,x,$e,K,zs,qe,ys,Le,le,Es,Ge,Q,Je,V,Ue,ie,Ts,Ke,W,Qe,X,Ve,P,Is,we,Ss,Ps,je,As,Fs,We,F,H,ve,Y,Cs,ze,Ds,Xe,R,xs,ye,Hs,Rs,Ye,E,T,pe,Ze;o=new wn({props:{fw:v[0]}});const Bs=[vn,jn],Z=[];function Ns(e,n){return e[0]==="pt"?0:1}z=Ns(v),y=Z[z]=Bs[z](v),M=new j({props:{code:`from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

model_inputs = tokenizer(sequence)`}}),O=new j({props:{code:`sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)`,highlighted:`sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

model_inputs = tokenizer(sequence)`}}),L=new j({props:{code:`sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)`,highlighted:`sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

model_inputs = tokenizer(sequences)`}}),G=new j({props:{code:`# \u0E08\u0E30\u0E40\u0E15\u0E34\u0E21\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E1B\u0E08\u0E19\u0E16\u0E36\u0E07\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E2A\u0E38\u0E14\u0E02\u0E2D\u0E07\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04
model_inputs = tokenizer(sequences, padding="longest")

# \u0E08\u0E30\u0E40\u0E15\u0E34\u0E21\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E1B\u0E08\u0E19\u0E16\u0E36\u0E07\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E2A\u0E38\u0E14\u0E17\u0E35\u0E48\u0E42\u0E21\u0E40\u0E14\u0E25\u0E23\u0E31\u0E1A\u0E44\u0E14\u0E49
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# \u0E08\u0E30\u0E40\u0E15\u0E34\u0E21\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E1B\u0E08\u0E19\u0E16\u0E36\u0E07\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E2A\u0E38\u0E14\u0E17\u0E35\u0E48\u0E23\u0E30\u0E1A\u0E38\u0E44\u0E27\u0E49
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)`,highlighted:`<span class="hljs-comment"># \u0E08\u0E30\u0E40\u0E15\u0E34\u0E21\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E1B\u0E08\u0E19\u0E16\u0E36\u0E07\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E2A\u0E38\u0E14\u0E02\u0E2D\u0E07\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-string">&quot;longest&quot;</span>)

<span class="hljs-comment"># \u0E08\u0E30\u0E40\u0E15\u0E34\u0E21\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E1B\u0E08\u0E19\u0E16\u0E36\u0E07\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E2A\u0E38\u0E14\u0E17\u0E35\u0E48\u0E42\u0E21\u0E40\u0E14\u0E25\u0E23\u0E31\u0E1A\u0E44\u0E14\u0E49</span>
<span class="hljs-comment"># (512 for BERT or DistilBERT)</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-string">&quot;max_length&quot;</span>)

<span class="hljs-comment"># \u0E08\u0E30\u0E40\u0E15\u0E34\u0E21\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E1B\u0E08\u0E19\u0E16\u0E36\u0E07\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E2A\u0E38\u0E14\u0E17\u0E35\u0E48\u0E23\u0E30\u0E1A\u0E38\u0E44\u0E27\u0E49</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">8</span>)`}}),J=new j({props:{code:`sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# \u0E08\u0E30\u0E15\u0E31\u0E14\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E17\u0E35\u0E48\u0E21\u0E35\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E40\u0E01\u0E34\u0E19\u0E01\u0E27\u0E48\u0E32\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E42\u0E21\u0E40\u0E14\u0E25\u0E23\u0E31\u0E1A\u0E44\u0E14\u0E49
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# \u0E08\u0E30\u0E15\u0E31\u0E14\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E17\u0E35\u0E48\u0E21\u0E35\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E40\u0E01\u0E34\u0E19\u0E01\u0E27\u0E48\u0E32\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E23\u0E30\u0E1A\u0E38\u0E44\u0E27\u0E49
model_inputs = tokenizer(sequences, max_length=8, truncation=True)`,highlighted:`sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

<span class="hljs-comment"># \u0E08\u0E30\u0E15\u0E31\u0E14\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E17\u0E35\u0E48\u0E21\u0E35\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E40\u0E01\u0E34\u0E19\u0E01\u0E27\u0E48\u0E32\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E42\u0E21\u0E40\u0E14\u0E25\u0E23\u0E31\u0E1A\u0E44\u0E14\u0E49</span>
<span class="hljs-comment"># (512 for BERT or DistilBERT)</span>
model_inputs = tokenizer(sequences, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># \u0E08\u0E30\u0E15\u0E31\u0E14\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E17\u0E35\u0E48\u0E21\u0E35\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E40\u0E01\u0E34\u0E19\u0E01\u0E27\u0E48\u0E32\u0E04\u0E27\u0E32\u0E21\u0E22\u0E32\u0E27\u0E17\u0E35\u0E48\u0E23\u0E30\u0E1A\u0E38\u0E44\u0E27\u0E49</span>
model_inputs = tokenizer(sequences, max_length=<span class="hljs-number">8</span>, truncation=<span class="hljs-literal">True</span>)`}}),U=new j({props:{code:`sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")`,highlighted:`sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

<span class="hljs-comment"># Returns PyTorch tensors</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># Returns TensorFlow tensors</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-comment"># Returns NumPy arrays</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)`}}),K=new kn({}),Q=new j({props:{code:`sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)`,highlighted:`sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

model_inputs = tokenizer(sequence)
<span class="hljs-built_in">print</span>(model_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
<span class="hljs-built_in">print</span>(ids)`}}),V=new j({props:{code:`[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]`,highlighted:`[<span class="hljs-number">101</span>, <span class="hljs-number">1045</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">2310</span>, <span class="hljs-number">2042</span>, <span class="hljs-number">3403</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>, <span class="hljs-number">2878</span>, <span class="hljs-number">2166</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>]
[<span class="hljs-number">1045</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">2310</span>, <span class="hljs-number">2042</span>, <span class="hljs-number">3403</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>, <span class="hljs-number">2878</span>, <span class="hljs-number">2166</span>, <span class="hljs-number">1012</span>]`}}),W=new j({props:{code:`print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))`,highlighted:`<span class="hljs-built_in">print</span>(tokenizer.decode(model_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))
<span class="hljs-built_in">print</span>(tokenizer.decode(ids))`}}),X=new j({props:{code:`"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."`,highlighted:`<span class="hljs-string">&quot;[CLS] i&#x27;ve been waiting for a huggingface course my whole life. [SEP]&quot;</span>
<span class="hljs-string">&quot;i&#x27;ve been waiting for a huggingface course my whole life.&quot;</span>`}}),Y=new kn({});const Ms=[yn,zn],ee=[];function Os(e,n){return e[0]==="pt"?0:1}return E=Os(v),T=ee[E]=Ms[E](v),{c(){a=p("meta"),h=f(),g(o.$$.fragment),q=f(),N=p("h1"),ts=l("\u0E1B\u0E23\u0E30\u0E01\u0E2D\u0E1A\u0E17\u0E38\u0E01\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E40\u0E02\u0E49\u0E32\u0E14\u0E49\u0E27\u0E22\u0E01\u0E31\u0E19"),Te=f(),y.c(),se=f(),ne=p("p"),os=l("\u0E43\u0E19\u0E2A\u0E2D\u0E07\u0E2A\u0E32\u0E21 sections \u0E17\u0E35\u0E48\u0E1C\u0E48\u0E32\u0E19\u0E21\u0E32 \u0E40\u0E23\u0E32\u0E44\u0E14\u0E49\u0E1E\u0E22\u0E32\u0E22\u0E32\u0E21\u0E17\u0E33\u0E17\u0E38\u0E01\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E14\u0E49\u0E27\u0E22\u0E21\u0E37\u0E2D\u0E02\u0E2D\u0E07\u0E40\u0E23\u0E32\u0E40\u0E2D\u0E07 \u0E40\u0E23\u0E32\u0E44\u0E14\u0E49\u0E25\u0E2D\u0E07\u0E28\u0E36\u0E01\u0E29\u0E32\u0E27\u0E48\u0E32 tokenizer \u0E19\u0E31\u0E49\u0E19\u0E17\u0E33\u0E07\u0E32\u0E19\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E44\u0E23\u0E41\u0E25\u0E30\u0E27\u0E34\u0E18\u0E35\u0E01\u0E32\u0E23 tokenization, \u0E41\u0E1B\u0E25\u0E07\u0E02\u0E49\u0E2D\u0E21\u0E39\u0E25\u0E44\u0E1B\u0E40\u0E1B\u0E47\u0E19 input IDs, \u0E01\u0E32\u0E23\u0E40\u0E15\u0E34\u0E21(padding), \u0E01\u0E32\u0E23\u0E15\u0E31\u0E14(truncation), \u0E41\u0E25\u0E30 attention masks"),Ie=f(),D=p("p"),rs=l("\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E44\u0E23\u0E01\u0E47\u0E15\u0E32\u0E21 \u0E40\u0E2B\u0E21\u0E37\u0E2D\u0E19\u0E17\u0E35\u0E48\u0E40\u0E23\u0E32\u0E40\u0E2B\u0E47\u0E19\u0E43\u0E19 section 2, \u{1F917} Transformers API \u0E19\u0E31\u0E49\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E01\u0E31\u0E1A\u0E2A\u0E34\u0E48\u0E07\u0E15\u0E48\u0E32\u0E07\u0E46\u0E40\u0E2B\u0E25\u0E48\u0E32\u0E19\u0E31\u0E49\u0E19\u0E43\u0E2B\u0E49\u0E40\u0E23\u0E32\u0E44\u0E14\u0E49\u0E14\u0E49\u0E27\u0E22 high-level \u0E1F\u0E31\u0E07\u0E01\u0E4C\u0E0A\u0E31\u0E19\u0E17\u0E35\u0E48\u0E40\u0E23\u0E32\u0E08\u0E30\u0E25\u0E07\u0E25\u0E36\u0E07\u0E43\u0E19\u0E23\u0E32\u0E22\u0E25\u0E30\u0E40\u0E2D\u0E35\u0E22\u0E14\u0E01\u0E31\u0E19\u0E43\u0E19\u0E17\u0E35\u0E48\u0E19\u0E35\u0E48 \u0E40\u0E21\u0E37\u0E48\u0E2D\u0E04\u0E38\u0E13\u0E40\u0E23\u0E35\u0E22\u0E01\u0E43\u0E0A\u0E49\u0E07\u0E32\u0E19 "),me=p("code"),as=l("tokenizer"),ls=l(" \u0E02\u0E2D\u0E07\u0E04\u0E38\u0E13\u0E15\u0E23\u0E07\u0E46\u0E01\u0E31\u0E1A\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E2B\u0E19\u0E36\u0E48\u0E07\u0E46, \u0E04\u0E38\u0E13\u0E44\u0E14\u0E49\u0E2D\u0E34\u0E19\u0E1E\u0E38\u0E15\u0E17\u0E35\u0E48\u0E1E\u0E23\u0E49\u0E2D\u0E21\u0E08\u0E30\u0E43\u0E2A\u0E48\u0E40\u0E02\u0E49\u0E32\u0E44\u0E1B\u0E22\u0E31\u0E07\u0E42\u0E21\u0E40\u0E14\u0E25\u0E01\u0E25\u0E31\u0E1A\u0E21\u0E32:"),Se=f(),g(M.$$.fragment),Pe=f(),S=p("p"),is=l("\u0E43\u0E19\u0E17\u0E35\u0E48\u0E19\u0E35\u0E49 \u0E15\u0E31\u0E27\u0E41\u0E1B\u0E23 "),he=p("code"),ps=l("model_inputs"),us=l(" \u0E19\u0E31\u0E49\u0E19\u0E1B\u0E23\u0E30\u0E01\u0E2D\u0E1A\u0E14\u0E49\u0E27\u0E22\u0E17\u0E38\u0E01\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E17\u0E35\u0E48\u0E08\u0E33\u0E40\u0E1B\u0E47\u0E19\u0E2A\u0E33\u0E2B\u0E23\u0E31\u0E1A\u0E42\u0E21\u0E40\u0E14\u0E25\u0E17\u0E35\u0E48\u0E08\u0E30\u0E17\u0E33\u0E07\u0E32\u0E19\u0E44\u0E14\u0E49\u0E40\u0E1B\u0E47\u0E19\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E14\u0E35 \u0E2A\u0E33\u0E2B\u0E23\u0E31\u0E1A DistilBERT \u0E19\u0E31\u0E49\u0E19\u0E23\u0E27\u0E21\u0E44\u0E1B\u0E16\u0E36\u0E07 input IDs \u0E41\u0E25\u0E30 attention mask \u0E14\u0E49\u0E27\u0E22 \u0E2A\u0E48\u0E27\u0E19\u0E42\u0E21\u0E40\u0E14\u0E25\u0E2D\u0E37\u0E48\u0E19\u0E46\u0E17\u0E35\u0E48\u0E23\u0E2D\u0E07\u0E23\u0E31\u0E1A\u0E2D\u0E34\u0E19\u0E1E\u0E38\u0E15\u0E15\u0E48\u0E32\u0E07\u0E46\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E40\u0E15\u0E34\u0E21\u0E01\u0E47\u0E08\u0E30\u0E44\u0E14\u0E49\u0E1C\u0E25\u0E25\u0E31\u0E1E\u0E17\u0E4C\u0E40\u0E2B\u0E25\u0E48\u0E32\u0E19\u0E31\u0E49\u0E19\u0E08\u0E32\u0E01 "),ke=p("code"),cs=l("tokenizer"),fs=l(" object \u0E14\u0E49\u0E27\u0E22"),Ae=f(),te=p("p"),ms=l("\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E17\u0E35\u0E48\u0E40\u0E23\u0E32\u0E08\u0E30\u0E44\u0E14\u0E49\u0E40\u0E2B\u0E47\u0E19\u0E43\u0E19\u0E1A\u0E32\u0E07\u0E15\u0E31\u0E27\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E14\u0E49\u0E32\u0E19\u0E25\u0E48\u0E32\u0E07\u0E19\u0E35\u0E49 \u0E27\u0E34\u0E18\u0E35\u0E19\u0E35\u0E49\u0E40\u0E1B\u0E47\u0E19\u0E27\u0E34\u0E18\u0E35\u0E17\u0E35\u0E48\u0E17\u0E23\u0E07\u0E1E\u0E25\u0E31\u0E07\u0E21\u0E32\u0E01 \u0E2D\u0E31\u0E19\u0E14\u0E31\u0E1A\u0E41\u0E23\u0E01 \u0E21\u0E31\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E17\u0E35\u0E48\u0E08\u0E30 tokenize \u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E40\u0E1E\u0E35\u0E22\u0E07\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E40\u0E14\u0E35\u0E22\u0E27\u0E44\u0E14\u0E49:"),Fe=f(),g(O.$$.fragment),Ce=f(),oe=p("p"),hs=l("\u0E21\u0E31\u0E19\u0E22\u0E31\u0E07\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E01\u0E31\u0E1A\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E2B\u0E25\u0E32\u0E22\u0E46\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E14\u0E49\u0E43\u0E19\u0E04\u0E23\u0E32\u0E27\u0E40\u0E14\u0E35\u0E22\u0E27\u0E01\u0E31\u0E19 \u0E42\u0E14\u0E22\u0E17\u0E35\u0E48\u0E44\u0E21\u0E48\u0E21\u0E35\u0E2D\u0E30\u0E44\u0E23\u0E40\u0E1B\u0E25\u0E35\u0E48\u0E22\u0E19\u0E43\u0E19  API \u0E40\u0E25\u0E22:"),De=f(),g(L.$$.fragment),xe=f(),re=p("p"),ks=l("\u0E21\u0E31\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E17\u0E35\u0E48\u0E08\u0E30\u0E40\u0E15\u0E34\u0E21(padding) \u0E43\u0E2B\u0E49\u0E2A\u0E2D\u0E14\u0E04\u0E25\u0E49\u0E2D\u0E07\u0E01\u0E31\u0E1A\u0E2B\u0E25\u0E32\u0E22\u0E46\u0E27\u0E31\u0E15\u0E16\u0E38\u0E1B\u0E23\u0E30\u0E2A\u0E07\u0E04\u0E4C:"),He=f(),g(G.$$.fragment),Re=f(),ae=p("p"),ds=l("\u0E21\u0E31\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E15\u0E31\u0E14\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E14\u0E49\u0E2D\u0E35\u0E01\u0E14\u0E49\u0E27\u0E22:"),Be=f(),g(J.$$.fragment),Ne=f(),w=p("p"),de=p("code"),gs=l("tokenizer"),_s=l(" object \u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E17\u0E35\u0E48\u0E08\u0E30\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E01\u0E31\u0E1A\u0E01\u0E32\u0E23\u0E41\u0E1B\u0E25\u0E07\u0E02\u0E49\u0E2D\u0E21\u0E39\u0E25\u0E44\u0E1B\u0E40\u0E1B\u0E47\u0E19 tensors \u0E2A\u0E33\u0E2B\u0E23\u0E31\u0E1A framework \u0E17\u0E35\u0E48\u0E40\u0E09\u0E1E\u0E32\u0E30\u0E40\u0E08\u0E32\u0E30\u0E08\u0E07\u0E44\u0E14\u0E49 \u0E0B\u0E36\u0E48\u0E07\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E17\u0E35\u0E48\u0E08\u0E30\u0E2A\u0E48\u0E07\u0E40\u0E02\u0E49\u0E32\u0E42\u0E21\u0E40\u0E14\u0E25\u0E44\u0E14\u0E49\u0E17\u0E31\u0E19\u0E17\u0E35 \u0E22\u0E01\u0E15\u0E31\u0E27\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E40\u0E0A\u0E48\u0E19 \u0E43\u0E19\u0E42\u0E04\u0E49\u0E14\u0E15\u0E31\u0E27\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E15\u0E48\u0E2D\u0E44\u0E1B\u0E19\u0E35\u0E49 \u0E40\u0E23\u0E32\u0E08\u0E30\u0E2A\u0E31\u0E48\u0E07\u0E43\u0E2B\u0E49 tokenizer \u0E2A\u0E48\u0E07 tensors \u0E08\u0E32\u0E01 frameworks \u0E15\u0E48\u0E32\u0E07\u0E46 \u0E01\u0E31\u0E19 \u2014 "),ge=p("code"),bs=l('"pt"'),$s=l(" \u0E43\u0E2B\u0E49 PyTorch tensors, "),_e=p("code"),qs=l('"tf"'),ws=l(" \u0E43\u0E2B\u0E49 TensorFlow tensors, and "),be=p("code"),js=l('"np"'),vs=l(" \u0E43\u0E2B\u0E49 NumPy arrays:"),Me=f(),g(U.$$.fragment),Oe=f(),A=p("h2"),x=p("a"),$e=p("span"),g(K.$$.fragment),zs=f(),qe=p("span"),ys=l("tokens \u0E1E\u0E34\u0E40\u0E28\u0E29"),Le=f(),le=p("p"),Es=l("\u0E16\u0E49\u0E32\u0E40\u0E23\u0E32\u0E14\u0E39\u0E17\u0E35\u0E48 input IDs \u0E17\u0E35\u0E48\u0E44\u0E14\u0E49\u0E08\u0E32\u0E01 tokenizer \u0E40\u0E23\u0E32\u0E08\u0E30\u0E40\u0E2B\u0E47\u0E19\u0E44\u0E14\u0E49\u0E27\u0E48\u0E32\u0E21\u0E31\u0E19\u0E04\u0E48\u0E2D\u0E19\u0E02\u0E49\u0E32\u0E07\u0E41\u0E15\u0E01\u0E15\u0E48\u0E32\u0E07\u0E44\u0E1B\u0E08\u0E32\u0E01\u0E2A\u0E34\u0E48\u0E07\u0E17\u0E35\u0E48\u0E40\u0E23\u0E32\u0E40\u0E04\u0E22\u0E44\u0E14\u0E49\u0E01\u0E48\u0E2D\u0E19\u0E2B\u0E19\u0E49\u0E32\u0E19\u0E35\u0E49:"),Ge=f(),g(Q.$$.fragment),Je=f(),g(V.$$.fragment),Ue=f(),ie=p("p"),Ts=l("\u0E21\u0E35\u0E2B\u0E19\u0E36\u0E48\u0E07 token ID \u0E44\u0E14\u0E49\u0E16\u0E39\u0E01\u0E43\u0E2A\u0E48\u0E40\u0E02\u0E49\u0E32\u0E21\u0E32\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E19\u0E49\u0E32\u0E2A\u0E38\u0E14 \u0E41\u0E25\u0E30\u0E2D\u0E35\u0E01\u0E2B\u0E19\u0E36\u0E48\u0E07 token ID \u0E43\u0E2A\u0E48\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E25\u0E31\u0E07\u0E2A\u0E38\u0E14 \u0E21\u0E32\u0E16\u0E2D\u0E14\u0E23\u0E2B\u0E31\u0E2A\u0E2A\u0E2D\u0E07\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E02\u0E2D\u0E07 IDs \u0E14\u0E49\u0E32\u0E19\u0E1A\u0E19\u0E14\u0E39\u0E27\u0E48\u0E32\u0E21\u0E31\u0E19\u0E40\u0E01\u0E35\u0E48\u0E22\u0E01\u0E31\u0E1A\u0E2D\u0E30\u0E44\u0E23:"),Ke=f(),g(W.$$.fragment),Qe=f(),g(X.$$.fragment),Ve=f(),P=p("p"),Is=l("tokenizer \u0E17\u0E33\u0E01\u0E32\u0E23\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E04\u0E33\u0E1E\u0E34\u0E40\u0E28\u0E29 "),we=p("code"),Ss=l("[CLS]"),Ps=l(" \u0E17\u0E35\u0E48\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E19\u0E49\u0E32\u0E2A\u0E38\u0E14 \u0E41\u0E25\u0E30\u0E04\u0E33\u0E1E\u0E34\u0E40\u0E28\u0E29 "),je=p("code"),As=l("[SEP]"),Fs=l(" \u0E17\u0E35\u0E48\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E25\u0E31\u0E07\u0E2A\u0E38\u0E14 \u0E19\u0E31\u0E49\u0E19\u0E01\u0E47\u0E40\u0E1E\u0E23\u0E32\u0E30\u0E27\u0E48\u0E32\u0E42\u0E21\u0E40\u0E14\u0E25\u0E19\u0E31\u0E49\u0E19\u0E44\u0E14\u0E49\u0E1C\u0E48\u0E32\u0E19\u0E01\u0E32\u0E23\u0E40\u0E17\u0E23\u0E19\u0E21\u0E32\u0E41\u0E1A\u0E1A\u0E19\u0E31\u0E49\u0E19 \u0E14\u0E31\u0E07\u0E19\u0E31\u0E49\u0E19\u0E40\u0E1E\u0E37\u0E48\u0E2D\u0E43\u0E2B\u0E49\u0E44\u0E14\u0E49\u0E1C\u0E25\u0E25\u0E31\u0E1E\u0E17\u0E4C\u0E40\u0E14\u0E35\u0E22\u0E27\u0E01\u0E31\u0E19\u0E2A\u0E33\u0E2B\u0E23\u0E31\u0E1A\u0E01\u0E32\u0E23\u0E2D\u0E19\u0E38\u0E21\u0E32\u0E19(inference) \u0E40\u0E23\u0E32\u0E08\u0E33\u0E40\u0E1B\u0E47\u0E19\u0E15\u0E49\u0E2D\u0E07\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E21\u0E31\u0E19\u0E40\u0E02\u0E49\u0E32\u0E44\u0E1B\u0E40\u0E0A\u0E48\u0E19\u0E40\u0E14\u0E35\u0E22\u0E27\u0E01\u0E31\u0E19 \u0E41\u0E15\u0E48\u0E01\u0E47\u0E15\u0E49\u0E2D\u0E07\u0E15\u0E23\u0E30\u0E2B\u0E19\u0E31\u0E01\u0E27\u0E48\u0E32\u0E1A\u0E32\u0E07\u0E42\u0E21\u0E40\u0E14\u0E25\u0E19\u0E31\u0E49\u0E19\u0E44\u0E21\u0E48\u0E44\u0E14\u0E49\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E04\u0E33\u0E1E\u0E34\u0E40\u0E28\u0E29 \u0E2B\u0E23\u0E37\u0E2D \u0E43\u0E2A\u0E48\u0E04\u0E33\u0E17\u0E35\u0E48\u0E15\u0E48\u0E32\u0E07\u0E2D\u0E2D\u0E01\u0E44\u0E1B; \u0E42\u0E21\u0E40\u0E14\u0E25\u0E2D\u0E32\u0E08\u0E08\u0E30\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E04\u0E33\u0E1E\u0E34\u0E40\u0E28\u0E29\u0E40\u0E2B\u0E25\u0E48\u0E32\u0E19\u0E35\u0E49\u0E41\u0E04\u0E48\u0E40\u0E09\u0E1E\u0E32\u0E30\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E19\u0E49\u0E32\u0E2A\u0E38\u0E14 \u0E2B\u0E23\u0E37\u0E2D \u0E14\u0E49\u0E32\u0E19\u0E2B\u0E25\u0E31\u0E07\u0E2A\u0E38\u0E14\u0E40\u0E17\u0E48\u0E32\u0E19\u0E31\u0E49\u0E19 \u0E44\u0E21\u0E48\u0E27\u0E48\u0E32\u0E08\u0E30\u0E43\u0E19\u0E01\u0E23\u0E13\u0E35\u0E43\u0E14\u0E46 tokenizer \u0E23\u0E39\u0E49\u0E27\u0E48\u0E32\u0E2D\u0E31\u0E19\u0E44\u0E2B\u0E19\u0E40\u0E1B\u0E47\u0E19\u0E2D\u0E31\u0E19\u0E17\u0E35\u0E48\u0E15\u0E49\u0E2D\u0E07\u0E01\u0E32\u0E23\u0E41\u0E25\u0E30\u0E21\u0E31\u0E19\u0E08\u0E30\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E43\u0E2B\u0E49\u0E04\u0E38\u0E13\u0E40\u0E2D\u0E07:"),We=f(),F=p("h2"),H=p("a"),ve=p("span"),g(Y.$$.fragment),Cs=f(),ze=p("span"),Ds=l("\u0E2A\u0E23\u0E38\u0E1B: \u0E08\u0E32\u0E01 tokenizer \u0E44\u0E1B\u0E22\u0E31\u0E07\u0E42\u0E21\u0E40\u0E14\u0E25"),Xe=f(),R=p("p"),xs=l("\u0E16\u0E36\u0E07\u0E15\u0E23\u0E07\u0E19\u0E35\u0E49\u0E40\u0E23\u0E32\u0E44\u0E14\u0E49\u0E40\u0E2B\u0E47\u0E19\u0E02\u0E31\u0E49\u0E19\u0E15\u0E2D\u0E19\u0E41\u0E15\u0E48\u0E25\u0E30\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E17\u0E31\u0E49\u0E07\u0E2B\u0E21\u0E14\u0E17\u0E35\u0E48 "),ye=p("code"),Hs=l("tokenizer"),Rs=l(" \u0E43\u0E0A\u0E49\u0E40\u0E1E\u0E37\u0E48\u0E2D\u0E1B\u0E23\u0E30\u0E21\u0E27\u0E25\u0E1C\u0E25\u0E02\u0E49\u0E2D\u0E04\u0E27\u0E32\u0E21 \u0E40\u0E23\u0E32\u0E21\u0E32\u0E14\u0E39\u0E01\u0E31\u0E19\u0E04\u0E23\u0E31\u0E49\u0E07\u0E2A\u0E38\u0E14\u0E17\u0E49\u0E32\u0E22\u0E27\u0E48\u0E32\u0E21\u0E31\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E2B\u0E25\u0E32\u0E22\u0E46\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04 (padding!), \u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E22\u0E32\u0E27\u0E46, \u0E41\u0E25\u0E30 tensors \u0E2B\u0E25\u0E32\u0E22\u0E46 \u0E1B\u0E23\u0E30\u0E40\u0E20\u0E17\u0E44\u0E14\u0E49\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E44\u0E23\u0E14\u0E49\u0E27\u0E22 API \u0E2B\u0E25\u0E31\u0E01\u0E02\u0E2D\u0E07\u0E21\u0E31\u0E19:"),Ye=f(),T.c(),pe=fn(),this.h()},l(e){const n=$n('[data-svelte="svelte-1phssyn"]',document.head);a=u(n,"META",{name:!0,content:!0}),n.forEach(s),h=m(e),_(o.$$.fragment,e),q=m(e),N=u(e,"H1",{id:!0});var Ee=c(N);ts=i(Ee,"\u0E1B\u0E23\u0E30\u0E01\u0E2D\u0E1A\u0E17\u0E38\u0E01\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E40\u0E02\u0E49\u0E32\u0E14\u0E49\u0E27\u0E22\u0E01\u0E31\u0E19"),Ee.forEach(s),Te=m(e),y.l(e),se=m(e),ne=u(e,"P",{});var ue=c(ne);os=i(ue,"\u0E43\u0E19\u0E2A\u0E2D\u0E07\u0E2A\u0E32\u0E21 sections \u0E17\u0E35\u0E48\u0E1C\u0E48\u0E32\u0E19\u0E21\u0E32 \u0E40\u0E23\u0E32\u0E44\u0E14\u0E49\u0E1E\u0E22\u0E32\u0E22\u0E32\u0E21\u0E17\u0E33\u0E17\u0E38\u0E01\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E14\u0E49\u0E27\u0E22\u0E21\u0E37\u0E2D\u0E02\u0E2D\u0E07\u0E40\u0E23\u0E32\u0E40\u0E2D\u0E07 \u0E40\u0E23\u0E32\u0E44\u0E14\u0E49\u0E25\u0E2D\u0E07\u0E28\u0E36\u0E01\u0E29\u0E32\u0E27\u0E48\u0E32 tokenizer \u0E19\u0E31\u0E49\u0E19\u0E17\u0E33\u0E07\u0E32\u0E19\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E44\u0E23\u0E41\u0E25\u0E30\u0E27\u0E34\u0E18\u0E35\u0E01\u0E32\u0E23 tokenization, \u0E41\u0E1B\u0E25\u0E07\u0E02\u0E49\u0E2D\u0E21\u0E39\u0E25\u0E44\u0E1B\u0E40\u0E1B\u0E47\u0E19 input IDs, \u0E01\u0E32\u0E23\u0E40\u0E15\u0E34\u0E21(padding), \u0E01\u0E32\u0E23\u0E15\u0E31\u0E14(truncation), \u0E41\u0E25\u0E30 attention masks"),ue.forEach(s),Ie=m(e),D=u(e,"P",{});var B=c(D);rs=i(B,"\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E44\u0E23\u0E01\u0E47\u0E15\u0E32\u0E21 \u0E40\u0E2B\u0E21\u0E37\u0E2D\u0E19\u0E17\u0E35\u0E48\u0E40\u0E23\u0E32\u0E40\u0E2B\u0E47\u0E19\u0E43\u0E19 section 2, \u{1F917} Transformers API \u0E19\u0E31\u0E49\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E01\u0E31\u0E1A\u0E2A\u0E34\u0E48\u0E07\u0E15\u0E48\u0E32\u0E07\u0E46\u0E40\u0E2B\u0E25\u0E48\u0E32\u0E19\u0E31\u0E49\u0E19\u0E43\u0E2B\u0E49\u0E40\u0E23\u0E32\u0E44\u0E14\u0E49\u0E14\u0E49\u0E27\u0E22 high-level \u0E1F\u0E31\u0E07\u0E01\u0E4C\u0E0A\u0E31\u0E19\u0E17\u0E35\u0E48\u0E40\u0E23\u0E32\u0E08\u0E30\u0E25\u0E07\u0E25\u0E36\u0E07\u0E43\u0E19\u0E23\u0E32\u0E22\u0E25\u0E30\u0E40\u0E2D\u0E35\u0E22\u0E14\u0E01\u0E31\u0E19\u0E43\u0E19\u0E17\u0E35\u0E48\u0E19\u0E35\u0E48 \u0E40\u0E21\u0E37\u0E48\u0E2D\u0E04\u0E38\u0E13\u0E40\u0E23\u0E35\u0E22\u0E01\u0E43\u0E0A\u0E49\u0E07\u0E32\u0E19 "),me=u(B,"CODE",{});var Ls=c(me);as=i(Ls,"tokenizer"),Ls.forEach(s),ls=i(B," \u0E02\u0E2D\u0E07\u0E04\u0E38\u0E13\u0E15\u0E23\u0E07\u0E46\u0E01\u0E31\u0E1A\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E2B\u0E19\u0E36\u0E48\u0E07\u0E46, \u0E04\u0E38\u0E13\u0E44\u0E14\u0E49\u0E2D\u0E34\u0E19\u0E1E\u0E38\u0E15\u0E17\u0E35\u0E48\u0E1E\u0E23\u0E49\u0E2D\u0E21\u0E08\u0E30\u0E43\u0E2A\u0E48\u0E40\u0E02\u0E49\u0E32\u0E44\u0E1B\u0E22\u0E31\u0E07\u0E42\u0E21\u0E40\u0E14\u0E25\u0E01\u0E25\u0E31\u0E1A\u0E21\u0E32:"),B.forEach(s),Se=m(e),_(M.$$.fragment,e),Pe=m(e),S=u(e,"P",{});var ce=c(S);is=i(ce,"\u0E43\u0E19\u0E17\u0E35\u0E48\u0E19\u0E35\u0E49 \u0E15\u0E31\u0E27\u0E41\u0E1B\u0E23 "),he=u(ce,"CODE",{});var Gs=c(he);ps=i(Gs,"model_inputs"),Gs.forEach(s),us=i(ce," \u0E19\u0E31\u0E49\u0E19\u0E1B\u0E23\u0E30\u0E01\u0E2D\u0E1A\u0E14\u0E49\u0E27\u0E22\u0E17\u0E38\u0E01\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E17\u0E35\u0E48\u0E08\u0E33\u0E40\u0E1B\u0E47\u0E19\u0E2A\u0E33\u0E2B\u0E23\u0E31\u0E1A\u0E42\u0E21\u0E40\u0E14\u0E25\u0E17\u0E35\u0E48\u0E08\u0E30\u0E17\u0E33\u0E07\u0E32\u0E19\u0E44\u0E14\u0E49\u0E40\u0E1B\u0E47\u0E19\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E14\u0E35 \u0E2A\u0E33\u0E2B\u0E23\u0E31\u0E1A DistilBERT \u0E19\u0E31\u0E49\u0E19\u0E23\u0E27\u0E21\u0E44\u0E1B\u0E16\u0E36\u0E07 input IDs \u0E41\u0E25\u0E30 attention mask \u0E14\u0E49\u0E27\u0E22 \u0E2A\u0E48\u0E27\u0E19\u0E42\u0E21\u0E40\u0E14\u0E25\u0E2D\u0E37\u0E48\u0E19\u0E46\u0E17\u0E35\u0E48\u0E23\u0E2D\u0E07\u0E23\u0E31\u0E1A\u0E2D\u0E34\u0E19\u0E1E\u0E38\u0E15\u0E15\u0E48\u0E32\u0E07\u0E46\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E40\u0E15\u0E34\u0E21\u0E01\u0E47\u0E08\u0E30\u0E44\u0E14\u0E49\u0E1C\u0E25\u0E25\u0E31\u0E1E\u0E17\u0E4C\u0E40\u0E2B\u0E25\u0E48\u0E32\u0E19\u0E31\u0E49\u0E19\u0E08\u0E32\u0E01 "),ke=u(ce,"CODE",{});var Js=c(ke);cs=i(Js,"tokenizer"),Js.forEach(s),fs=i(ce," object \u0E14\u0E49\u0E27\u0E22"),ce.forEach(s),Ae=m(e),te=u(e,"P",{});var Us=c(te);ms=i(Us,"\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E17\u0E35\u0E48\u0E40\u0E23\u0E32\u0E08\u0E30\u0E44\u0E14\u0E49\u0E40\u0E2B\u0E47\u0E19\u0E43\u0E19\u0E1A\u0E32\u0E07\u0E15\u0E31\u0E27\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E14\u0E49\u0E32\u0E19\u0E25\u0E48\u0E32\u0E07\u0E19\u0E35\u0E49 \u0E27\u0E34\u0E18\u0E35\u0E19\u0E35\u0E49\u0E40\u0E1B\u0E47\u0E19\u0E27\u0E34\u0E18\u0E35\u0E17\u0E35\u0E48\u0E17\u0E23\u0E07\u0E1E\u0E25\u0E31\u0E07\u0E21\u0E32\u0E01 \u0E2D\u0E31\u0E19\u0E14\u0E31\u0E1A\u0E41\u0E23\u0E01 \u0E21\u0E31\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E17\u0E35\u0E48\u0E08\u0E30 tokenize \u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E40\u0E1E\u0E35\u0E22\u0E07\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E40\u0E14\u0E35\u0E22\u0E27\u0E44\u0E14\u0E49:"),Us.forEach(s),Fe=m(e),_(O.$$.fragment,e),Ce=m(e),oe=u(e,"P",{});var Ks=c(oe);hs=i(Ks,"\u0E21\u0E31\u0E19\u0E22\u0E31\u0E07\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E01\u0E31\u0E1A\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E2B\u0E25\u0E32\u0E22\u0E46\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E14\u0E49\u0E43\u0E19\u0E04\u0E23\u0E32\u0E27\u0E40\u0E14\u0E35\u0E22\u0E27\u0E01\u0E31\u0E19 \u0E42\u0E14\u0E22\u0E17\u0E35\u0E48\u0E44\u0E21\u0E48\u0E21\u0E35\u0E2D\u0E30\u0E44\u0E23\u0E40\u0E1B\u0E25\u0E35\u0E48\u0E22\u0E19\u0E43\u0E19  API \u0E40\u0E25\u0E22:"),Ks.forEach(s),De=m(e),_(L.$$.fragment,e),xe=m(e),re=u(e,"P",{});var Qs=c(re);ks=i(Qs,"\u0E21\u0E31\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E17\u0E35\u0E48\u0E08\u0E30\u0E40\u0E15\u0E34\u0E21(padding) \u0E43\u0E2B\u0E49\u0E2A\u0E2D\u0E14\u0E04\u0E25\u0E49\u0E2D\u0E07\u0E01\u0E31\u0E1A\u0E2B\u0E25\u0E32\u0E22\u0E46\u0E27\u0E31\u0E15\u0E16\u0E38\u0E1B\u0E23\u0E30\u0E2A\u0E07\u0E04\u0E4C:"),Qs.forEach(s),He=m(e),_(G.$$.fragment,e),Re=m(e),ae=u(e,"P",{});var Vs=c(ae);ds=i(Vs,"\u0E21\u0E31\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E15\u0E31\u0E14\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E44\u0E14\u0E49\u0E2D\u0E35\u0E01\u0E14\u0E49\u0E27\u0E22:"),Vs.forEach(s),Be=m(e),_(J.$$.fragment,e),Ne=m(e),w=u(e,"P",{});var C=c(w);de=u(C,"CODE",{});var Ws=c(de);gs=i(Ws,"tokenizer"),Ws.forEach(s),_s=i(C," object \u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E17\u0E35\u0E48\u0E08\u0E30\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E01\u0E31\u0E1A\u0E01\u0E32\u0E23\u0E41\u0E1B\u0E25\u0E07\u0E02\u0E49\u0E2D\u0E21\u0E39\u0E25\u0E44\u0E1B\u0E40\u0E1B\u0E47\u0E19 tensors \u0E2A\u0E33\u0E2B\u0E23\u0E31\u0E1A framework \u0E17\u0E35\u0E48\u0E40\u0E09\u0E1E\u0E32\u0E30\u0E40\u0E08\u0E32\u0E30\u0E08\u0E07\u0E44\u0E14\u0E49 \u0E0B\u0E36\u0E48\u0E07\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E17\u0E35\u0E48\u0E08\u0E30\u0E2A\u0E48\u0E07\u0E40\u0E02\u0E49\u0E32\u0E42\u0E21\u0E40\u0E14\u0E25\u0E44\u0E14\u0E49\u0E17\u0E31\u0E19\u0E17\u0E35 \u0E22\u0E01\u0E15\u0E31\u0E27\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E40\u0E0A\u0E48\u0E19 \u0E43\u0E19\u0E42\u0E04\u0E49\u0E14\u0E15\u0E31\u0E27\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E15\u0E48\u0E2D\u0E44\u0E1B\u0E19\u0E35\u0E49 \u0E40\u0E23\u0E32\u0E08\u0E30\u0E2A\u0E31\u0E48\u0E07\u0E43\u0E2B\u0E49 tokenizer \u0E2A\u0E48\u0E07 tensors \u0E08\u0E32\u0E01 frameworks \u0E15\u0E48\u0E32\u0E07\u0E46 \u0E01\u0E31\u0E19 \u2014 "),ge=u(C,"CODE",{});var Xs=c(ge);bs=i(Xs,'"pt"'),Xs.forEach(s),$s=i(C," \u0E43\u0E2B\u0E49 PyTorch tensors, "),_e=u(C,"CODE",{});var Ys=c(_e);qs=i(Ys,'"tf"'),Ys.forEach(s),ws=i(C," \u0E43\u0E2B\u0E49 TensorFlow tensors, and "),be=u(C,"CODE",{});var Zs=c(be);js=i(Zs,'"np"'),Zs.forEach(s),vs=i(C," \u0E43\u0E2B\u0E49 NumPy arrays:"),C.forEach(s),Me=m(e),_(U.$$.fragment,e),Oe=m(e),A=u(e,"H2",{class:!0});var es=c(A);x=u(es,"A",{id:!0,class:!0,href:!0});var en=c(x);$e=u(en,"SPAN",{});var sn=c($e);_(K.$$.fragment,sn),sn.forEach(s),en.forEach(s),zs=m(es),qe=u(es,"SPAN",{});var nn=c(qe);ys=i(nn,"tokens \u0E1E\u0E34\u0E40\u0E28\u0E29"),nn.forEach(s),es.forEach(s),Le=m(e),le=u(e,"P",{});var tn=c(le);Es=i(tn,"\u0E16\u0E49\u0E32\u0E40\u0E23\u0E32\u0E14\u0E39\u0E17\u0E35\u0E48 input IDs \u0E17\u0E35\u0E48\u0E44\u0E14\u0E49\u0E08\u0E32\u0E01 tokenizer \u0E40\u0E23\u0E32\u0E08\u0E30\u0E40\u0E2B\u0E47\u0E19\u0E44\u0E14\u0E49\u0E27\u0E48\u0E32\u0E21\u0E31\u0E19\u0E04\u0E48\u0E2D\u0E19\u0E02\u0E49\u0E32\u0E07\u0E41\u0E15\u0E01\u0E15\u0E48\u0E32\u0E07\u0E44\u0E1B\u0E08\u0E32\u0E01\u0E2A\u0E34\u0E48\u0E07\u0E17\u0E35\u0E48\u0E40\u0E23\u0E32\u0E40\u0E04\u0E22\u0E44\u0E14\u0E49\u0E01\u0E48\u0E2D\u0E19\u0E2B\u0E19\u0E49\u0E32\u0E19\u0E35\u0E49:"),tn.forEach(s),Ge=m(e),_(Q.$$.fragment,e),Je=m(e),_(V.$$.fragment,e),Ue=m(e),ie=u(e,"P",{});var on=c(ie);Ts=i(on,"\u0E21\u0E35\u0E2B\u0E19\u0E36\u0E48\u0E07 token ID \u0E44\u0E14\u0E49\u0E16\u0E39\u0E01\u0E43\u0E2A\u0E48\u0E40\u0E02\u0E49\u0E32\u0E21\u0E32\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E19\u0E49\u0E32\u0E2A\u0E38\u0E14 \u0E41\u0E25\u0E30\u0E2D\u0E35\u0E01\u0E2B\u0E19\u0E36\u0E48\u0E07 token ID \u0E43\u0E2A\u0E48\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E25\u0E31\u0E07\u0E2A\u0E38\u0E14 \u0E21\u0E32\u0E16\u0E2D\u0E14\u0E23\u0E2B\u0E31\u0E2A\u0E2A\u0E2D\u0E07\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E02\u0E2D\u0E07 IDs \u0E14\u0E49\u0E32\u0E19\u0E1A\u0E19\u0E14\u0E39\u0E27\u0E48\u0E32\u0E21\u0E31\u0E19\u0E40\u0E01\u0E35\u0E48\u0E22\u0E01\u0E31\u0E1A\u0E2D\u0E30\u0E44\u0E23:"),on.forEach(s),Ke=m(e),_(W.$$.fragment,e),Qe=m(e),_(X.$$.fragment,e),Ve=m(e),P=u(e,"P",{});var fe=c(P);Is=i(fe,"tokenizer \u0E17\u0E33\u0E01\u0E32\u0E23\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E04\u0E33\u0E1E\u0E34\u0E40\u0E28\u0E29 "),we=u(fe,"CODE",{});var rn=c(we);Ss=i(rn,"[CLS]"),rn.forEach(s),Ps=i(fe," \u0E17\u0E35\u0E48\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E19\u0E49\u0E32\u0E2A\u0E38\u0E14 \u0E41\u0E25\u0E30\u0E04\u0E33\u0E1E\u0E34\u0E40\u0E28\u0E29 "),je=u(fe,"CODE",{});var an=c(je);As=i(an,"[SEP]"),an.forEach(s),Fs=i(fe," \u0E17\u0E35\u0E48\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E25\u0E31\u0E07\u0E2A\u0E38\u0E14 \u0E19\u0E31\u0E49\u0E19\u0E01\u0E47\u0E40\u0E1E\u0E23\u0E32\u0E30\u0E27\u0E48\u0E32\u0E42\u0E21\u0E40\u0E14\u0E25\u0E19\u0E31\u0E49\u0E19\u0E44\u0E14\u0E49\u0E1C\u0E48\u0E32\u0E19\u0E01\u0E32\u0E23\u0E40\u0E17\u0E23\u0E19\u0E21\u0E32\u0E41\u0E1A\u0E1A\u0E19\u0E31\u0E49\u0E19 \u0E14\u0E31\u0E07\u0E19\u0E31\u0E49\u0E19\u0E40\u0E1E\u0E37\u0E48\u0E2D\u0E43\u0E2B\u0E49\u0E44\u0E14\u0E49\u0E1C\u0E25\u0E25\u0E31\u0E1E\u0E17\u0E4C\u0E40\u0E14\u0E35\u0E22\u0E27\u0E01\u0E31\u0E19\u0E2A\u0E33\u0E2B\u0E23\u0E31\u0E1A\u0E01\u0E32\u0E23\u0E2D\u0E19\u0E38\u0E21\u0E32\u0E19(inference) \u0E40\u0E23\u0E32\u0E08\u0E33\u0E40\u0E1B\u0E47\u0E19\u0E15\u0E49\u0E2D\u0E07\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E21\u0E31\u0E19\u0E40\u0E02\u0E49\u0E32\u0E44\u0E1B\u0E40\u0E0A\u0E48\u0E19\u0E40\u0E14\u0E35\u0E22\u0E27\u0E01\u0E31\u0E19 \u0E41\u0E15\u0E48\u0E01\u0E47\u0E15\u0E49\u0E2D\u0E07\u0E15\u0E23\u0E30\u0E2B\u0E19\u0E31\u0E01\u0E27\u0E48\u0E32\u0E1A\u0E32\u0E07\u0E42\u0E21\u0E40\u0E14\u0E25\u0E19\u0E31\u0E49\u0E19\u0E44\u0E21\u0E48\u0E44\u0E14\u0E49\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E04\u0E33\u0E1E\u0E34\u0E40\u0E28\u0E29 \u0E2B\u0E23\u0E37\u0E2D \u0E43\u0E2A\u0E48\u0E04\u0E33\u0E17\u0E35\u0E48\u0E15\u0E48\u0E32\u0E07\u0E2D\u0E2D\u0E01\u0E44\u0E1B; \u0E42\u0E21\u0E40\u0E14\u0E25\u0E2D\u0E32\u0E08\u0E08\u0E30\u0E40\u0E1E\u0E34\u0E48\u0E21\u0E04\u0E33\u0E1E\u0E34\u0E40\u0E28\u0E29\u0E40\u0E2B\u0E25\u0E48\u0E32\u0E19\u0E35\u0E49\u0E41\u0E04\u0E48\u0E40\u0E09\u0E1E\u0E32\u0E30\u0E14\u0E49\u0E32\u0E19\u0E2B\u0E19\u0E49\u0E32\u0E2A\u0E38\u0E14 \u0E2B\u0E23\u0E37\u0E2D \u0E14\u0E49\u0E32\u0E19\u0E2B\u0E25\u0E31\u0E07\u0E2A\u0E38\u0E14\u0E40\u0E17\u0E48\u0E32\u0E19\u0E31\u0E49\u0E19 \u0E44\u0E21\u0E48\u0E27\u0E48\u0E32\u0E08\u0E30\u0E43\u0E19\u0E01\u0E23\u0E13\u0E35\u0E43\u0E14\u0E46 tokenizer \u0E23\u0E39\u0E49\u0E27\u0E48\u0E32\u0E2D\u0E31\u0E19\u0E44\u0E2B\u0E19\u0E40\u0E1B\u0E47\u0E19\u0E2D\u0E31\u0E19\u0E17\u0E35\u0E48\u0E15\u0E49\u0E2D\u0E07\u0E01\u0E32\u0E23\u0E41\u0E25\u0E30\u0E21\u0E31\u0E19\u0E08\u0E30\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E43\u0E2B\u0E49\u0E04\u0E38\u0E13\u0E40\u0E2D\u0E07:"),fe.forEach(s),We=m(e),F=u(e,"H2",{class:!0});var ss=c(F);H=u(ss,"A",{id:!0,class:!0,href:!0});var ln=c(H);ve=u(ln,"SPAN",{});var pn=c(ve);_(Y.$$.fragment,pn),pn.forEach(s),ln.forEach(s),Cs=m(ss),ze=u(ss,"SPAN",{});var un=c(ze);Ds=i(un,"\u0E2A\u0E23\u0E38\u0E1B: \u0E08\u0E32\u0E01 tokenizer \u0E44\u0E1B\u0E22\u0E31\u0E07\u0E42\u0E21\u0E40\u0E14\u0E25"),un.forEach(s),ss.forEach(s),Xe=m(e),R=u(e,"P",{});var ns=c(R);xs=i(ns,"\u0E16\u0E36\u0E07\u0E15\u0E23\u0E07\u0E19\u0E35\u0E49\u0E40\u0E23\u0E32\u0E44\u0E14\u0E49\u0E40\u0E2B\u0E47\u0E19\u0E02\u0E31\u0E49\u0E19\u0E15\u0E2D\u0E19\u0E41\u0E15\u0E48\u0E25\u0E30\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E17\u0E31\u0E49\u0E07\u0E2B\u0E21\u0E14\u0E17\u0E35\u0E48 "),ye=u(ns,"CODE",{});var cn=c(ye);Hs=i(cn,"tokenizer"),cn.forEach(s),Rs=i(ns," \u0E43\u0E0A\u0E49\u0E40\u0E1E\u0E37\u0E48\u0E2D\u0E1B\u0E23\u0E30\u0E21\u0E27\u0E25\u0E1C\u0E25\u0E02\u0E49\u0E2D\u0E04\u0E27\u0E32\u0E21 \u0E40\u0E23\u0E32\u0E21\u0E32\u0E14\u0E39\u0E01\u0E31\u0E19\u0E04\u0E23\u0E31\u0E49\u0E07\u0E2A\u0E38\u0E14\u0E17\u0E49\u0E32\u0E22\u0E27\u0E48\u0E32\u0E21\u0E31\u0E19\u0E2A\u0E32\u0E21\u0E32\u0E23\u0E16\u0E08\u0E31\u0E14\u0E01\u0E32\u0E23\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E2B\u0E25\u0E32\u0E22\u0E46\u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04 (padding!), \u0E1B\u0E23\u0E30\u0E42\u0E22\u0E04\u0E22\u0E32\u0E27\u0E46, \u0E41\u0E25\u0E30 tensors \u0E2B\u0E25\u0E32\u0E22\u0E46 \u0E1B\u0E23\u0E30\u0E40\u0E20\u0E17\u0E44\u0E14\u0E49\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E44\u0E23\u0E14\u0E49\u0E27\u0E22 API \u0E2B\u0E25\u0E31\u0E01\u0E02\u0E2D\u0E07\u0E21\u0E31\u0E19:"),ns.forEach(s),Ye=m(e),T.l(e),pe=fn(),this.h()},h(){I(a,"name","hf:doc:metadata"),I(a,"content",JSON.stringify(Tn)),I(N,"id",""),I(x,"id","tokens"),I(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),I(x,"href","#tokens"),I(A,"class","relative group"),I(H,"id","tokenizer"),I(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),I(H,"href","#tokenizer"),I(F,"class","relative group")},m(e,n){t(document.head,a),r(e,h,n),b(o,e,n),r(e,q,n),r(e,N,n),t(N,ts),r(e,Te,n),Z[z].m(e,n),r(e,se,n),r(e,ne,n),t(ne,os),r(e,Ie,n),r(e,D,n),t(D,rs),t(D,me),t(me,as),t(D,ls),r(e,Se,n),b(M,e,n),r(e,Pe,n),r(e,S,n),t(S,is),t(S,he),t(he,ps),t(S,us),t(S,ke),t(ke,cs),t(S,fs),r(e,Ae,n),r(e,te,n),t(te,ms),r(e,Fe,n),b(O,e,n),r(e,Ce,n),r(e,oe,n),t(oe,hs),r(e,De,n),b(L,e,n),r(e,xe,n),r(e,re,n),t(re,ks),r(e,He,n),b(G,e,n),r(e,Re,n),r(e,ae,n),t(ae,ds),r(e,Be,n),b(J,e,n),r(e,Ne,n),r(e,w,n),t(w,de),t(de,gs),t(w,_s),t(w,ge),t(ge,bs),t(w,$s),t(w,_e),t(_e,qs),t(w,ws),t(w,be),t(be,js),t(w,vs),r(e,Me,n),b(U,e,n),r(e,Oe,n),r(e,A,n),t(A,x),t(x,$e),b(K,$e,null),t(A,zs),t(A,qe),t(qe,ys),r(e,Le,n),r(e,le,n),t(le,Es),r(e,Ge,n),b(Q,e,n),r(e,Je,n),b(V,e,n),r(e,Ue,n),r(e,ie,n),t(ie,Ts),r(e,Ke,n),b(W,e,n),r(e,Qe,n),b(X,e,n),r(e,Ve,n),r(e,P,n),t(P,Is),t(P,we),t(we,Ss),t(P,Ps),t(P,je),t(je,As),t(P,Fs),r(e,We,n),r(e,F,n),t(F,H),t(H,ve),b(Y,ve,null),t(F,Cs),t(F,ze),t(ze,Ds),r(e,Xe,n),r(e,R,n),t(R,xs),t(R,ye),t(ye,Hs),t(R,Rs),r(e,Ye,n),ee[E].m(e,n),r(e,pe,n),Ze=!0},p(e,[n]){const Ee={};n&1&&(Ee.fw=e[0]),o.$set(Ee);let ue=z;z=Ns(e),z!==ue&&(hn(),k(Z[ue],1,1,()=>{Z[ue]=null}),mn(),y=Z[z],y||(y=Z[z]=Bs[z](e),y.c()),d(y,1),y.m(se.parentNode,se));let B=E;E=Os(e),E!==B&&(hn(),k(ee[B],1,1,()=>{ee[B]=null}),mn(),T=ee[E],T||(T=ee[E]=Ms[E](e),T.c()),d(T,1),T.m(pe.parentNode,pe))},i(e){Ze||(d(o.$$.fragment,e),d(y),d(M.$$.fragment,e),d(O.$$.fragment,e),d(L.$$.fragment,e),d(G.$$.fragment,e),d(J.$$.fragment,e),d(U.$$.fragment,e),d(K.$$.fragment,e),d(Q.$$.fragment,e),d(V.$$.fragment,e),d(W.$$.fragment,e),d(X.$$.fragment,e),d(Y.$$.fragment,e),d(T),Ze=!0)},o(e){k(o.$$.fragment,e),k(y),k(M.$$.fragment,e),k(O.$$.fragment,e),k(L.$$.fragment,e),k(G.$$.fragment,e),k(J.$$.fragment,e),k(U.$$.fragment,e),k(K.$$.fragment,e),k(Q.$$.fragment,e),k(V.$$.fragment,e),k(W.$$.fragment,e),k(X.$$.fragment,e),k(Y.$$.fragment,e),k(T),Ze=!1},d(e){s(a),e&&s(h),$(o,e),e&&s(q),e&&s(N),e&&s(Te),Z[z].d(e),e&&s(se),e&&s(ne),e&&s(Ie),e&&s(D),e&&s(Se),$(M,e),e&&s(Pe),e&&s(S),e&&s(Ae),e&&s(te),e&&s(Fe),$(O,e),e&&s(Ce),e&&s(oe),e&&s(De),$(L,e),e&&s(xe),e&&s(re),e&&s(He),$(G,e),e&&s(Re),e&&s(ae),e&&s(Be),$(J,e),e&&s(Ne),e&&s(w),e&&s(Me),$(U,e),e&&s(Oe),e&&s(A),$(K),e&&s(Le),e&&s(le),e&&s(Ge),$(Q,e),e&&s(Je),$(V,e),e&&s(Ue),e&&s(ie),e&&s(Ke),$(W,e),e&&s(Qe),$(X,e),e&&s(Ve),e&&s(P),e&&s(We),e&&s(F),$(Y),e&&s(Xe),e&&s(R),e&&s(Ye),ee[E].d(e),e&&s(pe)}}}const Tn={local:"",sections:[{local:"tokens",title:"tokens \u0E1E\u0E34\u0E40\u0E28\u0E29"},{local:"tokenizer",title:"\u0E2A\u0E23\u0E38\u0E1B: \u0E08\u0E32\u0E01 tokenizer \u0E44\u0E1B\u0E22\u0E31\u0E07\u0E42\u0E21\u0E40\u0E14\u0E25"}],title:"\u0E1B\u0E23\u0E30\u0E01\u0E2D\u0E1A\u0E17\u0E38\u0E01\u0E2D\u0E22\u0E48\u0E32\u0E07\u0E40\u0E02\u0E49\u0E32\u0E14\u0E49\u0E27\u0E22\u0E01\u0E31\u0E19"};function In(v,a,h){let o="pt";return qn(()=>{const q=new URLSearchParams(window.location.search);h(0,o=q.get("fw")||"pt")}),[o]}class Dn extends gn{constructor(a){super();_n(this,a,In,En,bn,{})}}export{Dn as default,Tn as metadata};
