import{S as Fr,i as Mr,s as Lr,e as o,k as h,w as $,t as s,M as zr,c as l,d as n,m,x as k,a as i,h as a,b as w,N as Pr,f as Aa,G as e,g as r,y as x,o as _,p as Oa,q as b,B as E,v as Ur,n as Ia}from"../../chunks/vendor-hf-doc-builder.js";import{T as Nr}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Br}from"../../chunks/Youtube-hf-doc-builder.js";import{I as bs}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as O}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as Rr}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{F as Gr}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Vr(z){let u,y;return u=new Rr({props:{chapter:5,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_tf.ipynb"}]}}),{c(){$(u.$$.fragment)},l(d){k(u.$$.fragment,d)},m(d,j){x(u,d,j),y=!0},i(d){y||(b(u.$$.fragment,d),y=!0)},o(d){_(u.$$.fragment,d),y=!1},d(d){E(u,d)}}}function Yr(z){let u,y;return u=new Rr({props:{chapter:5,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section6_pt.ipynb"}]}}),{c(){$(u.$$.fragment)},l(d){k(u.$$.fragment,d)},m(d,j){x(u,d,j),y=!0},i(d){y||(b(u.$$.fragment,d),y=!0)},o(d){_(u.$$.fragment,d),y=!1},d(d){E(u,d)}}}function Kr(z){let u,y,d,j,g,v,I,f,S,q,L,H,A,P,N,F,B,D,R,Y;return{c(){u=o("p"),y=s("\u270F\uFE0F "),d=o("strong"),j=s("Th\u1EED nghi\u1EC7m th\xF4i!"),g=s(" C\xF9ng xem li\u1EC7u b\u1EA1n c\xF3 th\u1EC3 s\u1EED d\u1EE5ng "),v=o("code"),I=s("Dataset.map()"),f=s(" \u0111\u1EC3 kh\xE1m ph\xE1 c\u1ED9t "),S=o("code"),q=s("comments"),L=s(" c\u1EE7a "),H=o("code"),A=s("issues_dataset"),P=h(),N=o("em"),F=s("m\xE0 kh\xF4ng c\u1EA7n"),B=s(" s\u1EED d\u1EE5ng Pandas hay kh\xF4ng. N\xF3 s\u1EBD h\u01A1i kh\xF3 kh\u0103n m\u1ED9t ch\xFAt; b\u1EA1n c\xF3 th\u1EC3 xem ph\u1EA7n "),D=o("a"),R=s("\u201CBatch mapping\u201D"),Y=s(" c\u1EE7a t\xE0i li\u1EC7u \u{1F917} Datasets, m\u1ED9t t\xE0i li\u1EC7u h\u1EEFu \xEDch cho t\xE1c v\u1EE5 n\xE0y."),this.h()},l(U){u=l(U,"P",{});var T=i(u);y=a(T,"\u270F\uFE0F "),d=l(T,"STRONG",{});var G=i(d);j=a(G,"Th\u1EED nghi\u1EC7m th\xF4i!"),G.forEach(n),g=a(T," C\xF9ng xem li\u1EC7u b\u1EA1n c\xF3 th\u1EC3 s\u1EED d\u1EE5ng "),v=l(T,"CODE",{});var p=i(v);I=a(p,"Dataset.map()"),p.forEach(n),f=a(T," \u0111\u1EC3 kh\xE1m ph\xE1 c\u1ED9t "),S=l(T,"CODE",{});var C=i(S);q=a(C,"comments"),C.forEach(n),L=a(T," c\u1EE7a "),H=l(T,"CODE",{});var M=i(H);A=a(M,"issues_dataset"),M.forEach(n),P=m(T),N=l(T,"EM",{});var K=i(N);F=a(K,"m\xE0 kh\xF4ng c\u1EA7n"),K.forEach(n),B=a(T," s\u1EED d\u1EE5ng Pandas hay kh\xF4ng. N\xF3 s\u1EBD h\u01A1i kh\xF3 kh\u0103n m\u1ED9t ch\xFAt; b\u1EA1n c\xF3 th\u1EC3 xem ph\u1EA7n "),D=l(T,"A",{href:!0,rel:!0});var ot=i(D);R=a(ot,"\u201CBatch mapping\u201D"),ot.forEach(n),Y=a(T," c\u1EE7a t\xE0i li\u1EC7u \u{1F917} Datasets, m\u1ED9t t\xE0i li\u1EC7u h\u1EEFu \xEDch cho t\xE1c v\u1EE5 n\xE0y."),T.forEach(n),this.h()},h(){w(D,"href","https://huggingface.co/docs/datasets/v1.12.1/about_map_batch.html?batch-mapping#batch-mapping"),w(D,"rel","nofollow")},m(U,T){r(U,u,T),e(u,y),e(u,d),e(d,j),e(u,g),e(u,v),e(v,I),e(u,f),e(u,S),e(S,q),e(u,L),e(u,H),e(H,A),e(u,P),e(u,N),e(N,F),e(u,B),e(u,D),e(D,R),e(u,Y)},d(U){U&&n(u)}}}function Wr(z){let u,y,d,j,g,v,I,f,S,q,L,H,A,P,N,F,B;return u=new O({props:{code:`from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModel

model_ckpt = <span class="hljs-string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){$(u.$$.fragment),y=h(),d=o("p"),j=s("L\u01B0u \xFD r\u1EB1ng ch\xFAng ta \u0111\u1EB7t "),g=o("code"),v=s("from_pt=True"),I=s(" nh\u01B0 m\u1ED9t tham s\u1ED1 c\u1EE7a ph\u01B0\u01A1ng th\u1EE9c "),f=o("code"),S=s("from_pretrained()"),q=s(". \u0110i\u1EC1u n\xE0y l\xE0 b\u1EDFi checkpoint "),L=o("code"),H=s("multi-qa-mpnet-base-dot-v1"),A=s(" ch\u1EC9 c\xF3 tr\u1ECDng s\u1ED1 Pytorch, v\xEC v\u1EADy thi\u1EBFt l\u1EADp "),P=o("code"),N=s("from_pt=True"),F=s(" s\u1EBD t\u1EF1 \u0111\u1ED9ng chuy\u1EC3n ch\xFAng v\u1EC1 \u0111\u1ECBnh d\u1EA1ng TensorFlow cho ch\xFAng ta. Nh\u01B0 c\xF3 th\u1EC3 th\u1EA5y, n\xF3 r\u1EA5t \u0111\u01A1n gi\u1EA3n \u0111\u1EC3 chuy\u1EC3n gi\u1EEFa hai khung n\xE0y trong \u{1F917} Transformers!")},l(D){k(u.$$.fragment,D),y=m(D),d=l(D,"P",{});var R=i(d);j=a(R,"L\u01B0u \xFD r\u1EB1ng ch\xFAng ta \u0111\u1EB7t "),g=l(R,"CODE",{});var Y=i(g);v=a(Y,"from_pt=True"),Y.forEach(n),I=a(R," nh\u01B0 m\u1ED9t tham s\u1ED1 c\u1EE7a ph\u01B0\u01A1ng th\u1EE9c "),f=l(R,"CODE",{});var U=i(f);S=a(U,"from_pretrained()"),U.forEach(n),q=a(R,". \u0110i\u1EC1u n\xE0y l\xE0 b\u1EDFi checkpoint "),L=l(R,"CODE",{});var T=i(L);H=a(T,"multi-qa-mpnet-base-dot-v1"),T.forEach(n),A=a(R," ch\u1EC9 c\xF3 tr\u1ECDng s\u1ED1 Pytorch, v\xEC v\u1EADy thi\u1EBFt l\u1EADp "),P=l(R,"CODE",{});var G=i(P);N=a(G,"from_pt=True"),G.forEach(n),F=a(R," s\u1EBD t\u1EF1 \u0111\u1ED9ng chuy\u1EC3n ch\xFAng v\u1EC1 \u0111\u1ECBnh d\u1EA1ng TensorFlow cho ch\xFAng ta. Nh\u01B0 c\xF3 th\u1EC3 th\u1EA5y, n\xF3 r\u1EA5t \u0111\u01A1n gi\u1EA3n \u0111\u1EC3 chuy\u1EC3n gi\u1EEFa hai khung n\xE0y trong \u{1F917} Transformers!"),R.forEach(n)},m(D,R){x(u,D,R),r(D,y,R),r(D,d,R),e(d,j),e(d,g),e(g,v),e(d,I),e(d,f),e(f,S),e(d,q),e(d,L),e(L,H),e(d,A),e(d,P),e(P,N),e(d,F),B=!0},i(D){B||(b(u.$$.fragment,D),B=!0)},o(D){_(u.$$.fragment,D),B=!1},d(D){E(u,D),D&&n(y),D&&n(d)}}}function Jr(z){let u,y,d,j,g,v,I;return u=new O({props:{code:`from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

model_ckpt = <span class="hljs-string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)`}}),v=new O({props:{code:`import torch

device = torch.device("cuda")
model.to(device)`,highlighted:`<span class="hljs-keyword">import</span> torch

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)
model.to(device)`}}),{c(){$(u.$$.fragment),y=h(),d=o("p"),j=s("\u0110\u1EC3 t\u0103ng t\u1ED1c qu\xE1 tr\xECnh bi\u1EC3u di\u1EC5n, ta s\u1EBD gi\xFAp \u0111\u1EB7t m\xF4 h\xECnh v\xE0 \u0111\u1EA7u v\xE0o tr\xEAn thi\u1EBFt b\u1ECB GPU, v\xEC v\u1EADy h\xE3y l\xE0m \u0111i\u1EC1u \u0111\xF3 ngay b\xE2y gi\u1EDD th\xF4i:"),g=h(),$(v.$$.fragment)},l(f){k(u.$$.fragment,f),y=m(f),d=l(f,"P",{});var S=i(d);j=a(S,"\u0110\u1EC3 t\u0103ng t\u1ED1c qu\xE1 tr\xECnh bi\u1EC3u di\u1EC5n, ta s\u1EBD gi\xFAp \u0111\u1EB7t m\xF4 h\xECnh v\xE0 \u0111\u1EA7u v\xE0o tr\xEAn thi\u1EBFt b\u1ECB GPU, v\xEC v\u1EADy h\xE3y l\xE0m \u0111i\u1EC1u \u0111\xF3 ngay b\xE2y gi\u1EDD th\xF4i:"),S.forEach(n),g=m(f),k(v.$$.fragment,f)},m(f,S){x(u,f,S),r(f,y,S),r(f,d,S),e(d,j),r(f,g,S),x(v,f,S),I=!0},i(f){I||(b(u.$$.fragment,f),b(v.$$.fragment,f),I=!0)},o(f){_(u.$$.fragment,f),_(v.$$.fragment,f),I=!1},d(f){E(u,f),f&&n(y),f&&n(d),f&&n(g),E(v,f)}}}function Qr(z){let u,y,d,j,g,v,I,f,S,q,L,H,A,P,N,F,B,D,R,Y,U,T,G;return u=new O({props:{code:`def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">text_list</span>):
    encoded_input = tokenizer(
        text_list, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
    )
    encoded_input = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()}
    model_output = model(**encoded_input)
    <span class="hljs-keyword">return</span> cls_pooling(model_output)`}}),v=new O({props:{code:`embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape`,highlighted:`embedding = get_embeddings(comments_dataset[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
embedding.shape`}}),f=new O({props:{code:"TensorShape([1, 768])",highlighted:'TensorShape([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),T=new O({props:{code:`embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)`,highlighted:`embeddings_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="hljs-string">&quot;text&quot;</span>]).numpy()[<span class="hljs-number">0</span>]}
)`}}),{c(){$(u.$$.fragment),y=h(),d=o("p"),j=s("Ch\xFAng t\xF4i c\xF3 th\u1EC3 ki\u1EC3m tra h\xE0m c\xF3 ho\u1EA1t \u0111\u1ED9ng kh\xF4ng b\u1EB1ng c\xE1ch cung c\u1EA5p cho n\xF3 v\u0103n b\u1EA3n \u0111\u1EA7u ti\xEAn trong kho t\xE0i li\u1EC7u v\xE0 ki\u1EC3m tra h\xECnh d\u1EA1ng \u0111\u1EA7u ra:"),g=h(),$(v.$$.fragment),I=h(),$(f.$$.fragment),S=h(),q=o("p"),L=s("Tuy\u1EC7t v\u1EDDi, ch\xFAng ta \u0111\xE3 chuy\u1EC3n \u0111\u1ED5i m\u1EE5c nh\u1EADp \u0111\u1EA7u ti\xEAn trong kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh th\xE0nh m\u1ED9t vect\u01A1 768 chi\u1EC1u! Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng "),H=o("code"),A=s("Dataset.map()"),P=s(" \u0111\u1EC3 \xE1p d\u1EE5ng h\xE0m "),N=o("code"),F=s("get_embeddings()"),B=s(" cho m\u1ED7i h\xE0ng trong kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh, v\xEC v\u1EADy h\xE3y t\u1EA1o m\u1ED9t c\u1ED9t "),D=o("code"),R=s("embeddings"),Y=s(" m\u1EDBi nh\u01B0 sau:"),U=h(),$(T.$$.fragment)},l(p){k(u.$$.fragment,p),y=m(p),d=l(p,"P",{});var C=i(d);j=a(C,"Ch\xFAng t\xF4i c\xF3 th\u1EC3 ki\u1EC3m tra h\xE0m c\xF3 ho\u1EA1t \u0111\u1ED9ng kh\xF4ng b\u1EB1ng c\xE1ch cung c\u1EA5p cho n\xF3 v\u0103n b\u1EA3n \u0111\u1EA7u ti\xEAn trong kho t\xE0i li\u1EC7u v\xE0 ki\u1EC3m tra h\xECnh d\u1EA1ng \u0111\u1EA7u ra:"),C.forEach(n),g=m(p),k(v.$$.fragment,p),I=m(p),k(f.$$.fragment,p),S=m(p),q=l(p,"P",{});var M=i(q);L=a(M,"Tuy\u1EC7t v\u1EDDi, ch\xFAng ta \u0111\xE3 chuy\u1EC3n \u0111\u1ED5i m\u1EE5c nh\u1EADp \u0111\u1EA7u ti\xEAn trong kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh th\xE0nh m\u1ED9t vect\u01A1 768 chi\u1EC1u! Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng "),H=l(M,"CODE",{});var K=i(H);A=a(K,"Dataset.map()"),K.forEach(n),P=a(M," \u0111\u1EC3 \xE1p d\u1EE5ng h\xE0m "),N=l(M,"CODE",{});var ot=i(N);F=a(ot,"get_embeddings()"),ot.forEach(n),B=a(M," cho m\u1ED7i h\xE0ng trong kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh, v\xEC v\u1EADy h\xE3y t\u1EA1o m\u1ED9t c\u1ED9t "),D=l(M,"CODE",{});var ht=i(D);R=a(ht,"embeddings"),ht.forEach(n),Y=a(M," m\u1EDBi nh\u01B0 sau:"),M.forEach(n),U=m(p),k(T.$$.fragment,p)},m(p,C){x(u,p,C),r(p,y,C),r(p,d,C),e(d,j),r(p,g,C),x(v,p,C),r(p,I,C),x(f,p,C),r(p,S,C),r(p,q,C),e(q,L),e(q,H),e(H,A),e(q,P),e(q,N),e(N,F),e(q,B),e(q,D),e(D,R),e(q,Y),r(p,U,C),x(T,p,C),G=!0},i(p){G||(b(u.$$.fragment,p),b(v.$$.fragment,p),b(f.$$.fragment,p),b(T.$$.fragment,p),G=!0)},o(p){_(u.$$.fragment,p),_(v.$$.fragment,p),_(f.$$.fragment,p),_(T.$$.fragment,p),G=!1},d(p){E(u,p),p&&n(y),p&&n(d),p&&n(g),E(v,p),p&&n(I),E(f,p),p&&n(S),p&&n(q),p&&n(U),E(T,p)}}}function Xr(z){let u,y,d,j,g,v,I,f,S,q,L,H,A,P,N,F,B,D,R,Y,U,T,G;return u=new O({props:{code:`def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_embeddings</span>(<span class="hljs-params">text_list</span>):
    encoded_input = tokenizer(
        text_list, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
    )
    encoded_input = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoded_input.items()}
    model_output = model(**encoded_input)
    <span class="hljs-keyword">return</span> cls_pooling(model_output)`}}),v=new O({props:{code:`embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape`,highlighted:`embedding = get_embeddings(comments_dataset[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
embedding.shape`}}),f=new O({props:{code:"torch.Size([1, 768])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),T=new O({props:{code:`embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)`,highlighted:`embeddings_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="hljs-string">&quot;text&quot;</span>]).detach().cpu().numpy()[<span class="hljs-number">0</span>]}
)`}}),{c(){$(u.$$.fragment),y=h(),d=o("p"),j=s("Ch\xFAng ta c\xF3 th\u1EC3 ki\u1EC3m tra ch\u1EE9c n\u0103ng ho\u1EA1t \u0111\u1ED9ng b\u1EB1ng c\xE1ch cung c\u1EA5p cho n\xF3 \u0111o\u1EA1n v\u0103n b\u1EA3n \u0111\u1EA7u ti\xEAn trong kho t\xE0i li\u1EC7u v\xE0 ki\u1EC3m tra h\xECnh d\u1EA1ng \u0111\u1EA7u ra:"),g=h(),$(v.$$.fragment),I=h(),$(f.$$.fragment),S=h(),q=o("p"),L=s("Tuy\u1EC7t v\u1EDDi, ch\xFAng ta \u0111\xE3 chuy\u1EC3n \u0111\u1ED5i m\u1EE5c nh\u1EADp \u0111\u1EA7u ti\xEAn trong kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh th\xE0nh m\u1ED9t vect\u01A1 768 chi\u1EC1u! Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng "),H=o("code"),A=s("Dataset.map()"),P=s(" \u0111\u1EC3 \xE1p d\u1EE5ng h\xE0m "),N=o("code"),F=s("get_embeddings()"),B=s(" cho m\u1ED7i h\xE0ng trong kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh, v\xEC v\u1EADy h\xE3y t\u1EA1o m\u1ED9t c\u1ED9t "),D=o("code"),R=s("embeddings"),Y=s(" m\u1EDBi nh\u01B0 sau:"),U=h(),$(T.$$.fragment)},l(p){k(u.$$.fragment,p),y=m(p),d=l(p,"P",{});var C=i(d);j=a(C,"Ch\xFAng ta c\xF3 th\u1EC3 ki\u1EC3m tra ch\u1EE9c n\u0103ng ho\u1EA1t \u0111\u1ED9ng b\u1EB1ng c\xE1ch cung c\u1EA5p cho n\xF3 \u0111o\u1EA1n v\u0103n b\u1EA3n \u0111\u1EA7u ti\xEAn trong kho t\xE0i li\u1EC7u v\xE0 ki\u1EC3m tra h\xECnh d\u1EA1ng \u0111\u1EA7u ra:"),C.forEach(n),g=m(p),k(v.$$.fragment,p),I=m(p),k(f.$$.fragment,p),S=m(p),q=l(p,"P",{});var M=i(q);L=a(M,"Tuy\u1EC7t v\u1EDDi, ch\xFAng ta \u0111\xE3 chuy\u1EC3n \u0111\u1ED5i m\u1EE5c nh\u1EADp \u0111\u1EA7u ti\xEAn trong kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh th\xE0nh m\u1ED9t vect\u01A1 768 chi\u1EC1u! Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng "),H=l(M,"CODE",{});var K=i(H);A=a(K,"Dataset.map()"),K.forEach(n),P=a(M," \u0111\u1EC3 \xE1p d\u1EE5ng h\xE0m "),N=l(M,"CODE",{});var ot=i(N);F=a(ot,"get_embeddings()"),ot.forEach(n),B=a(M," cho m\u1ED7i h\xE0ng trong kho t\xE0i li\u1EC7u c\u1EE7a m\xECnh, v\xEC v\u1EADy h\xE3y t\u1EA1o m\u1ED9t c\u1ED9t "),D=l(M,"CODE",{});var ht=i(D);R=a(ht,"embeddings"),ht.forEach(n),Y=a(M," m\u1EDBi nh\u01B0 sau:"),M.forEach(n),U=m(p),k(T.$$.fragment,p)},m(p,C){x(u,p,C),r(p,y,C),r(p,d,C),e(d,j),r(p,g,C),x(v,p,C),r(p,I,C),x(f,p,C),r(p,S,C),r(p,q,C),e(q,L),e(q,H),e(H,A),e(q,P),e(q,N),e(N,F),e(q,B),e(q,D),e(D,R),e(q,Y),r(p,U,C),x(T,p,C),G=!0},i(p){G||(b(u.$$.fragment,p),b(v.$$.fragment,p),b(f.$$.fragment,p),b(T.$$.fragment,p),G=!0)},o(p){_(u.$$.fragment,p),_(v.$$.fragment,p),_(f.$$.fragment,p),_(T.$$.fragment,p),G=!1},d(p){E(u,p),p&&n(y),p&&n(d),p&&n(g),E(v,p),p&&n(I),E(f,p),p&&n(S),p&&n(q),p&&n(U),E(T,p)}}}function Zr(z){let u,y,d,j;return u=new O({props:{code:`question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape`,highlighted:`question = <span class="hljs-string">&quot;How can I load a dataset offline?&quot;</span>
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape`}}),d=new O({props:{code:"(1, 768)",highlighted:'(<span class="hljs-number">1</span>, <span class="hljs-number">768</span>)'}}),{c(){$(u.$$.fragment),y=h(),$(d.$$.fragment)},l(g){k(u.$$.fragment,g),y=m(g),k(d.$$.fragment,g)},m(g,v){x(u,g,v),r(g,y,v),x(d,g,v),j=!0},i(g){j||(b(u.$$.fragment,g),b(d.$$.fragment,g),j=!0)},o(g){_(u.$$.fragment,g),_(d.$$.fragment,g),j=!1},d(g){E(u,g),g&&n(y),E(d,g)}}}function th(z){let u,y,d,j;return u=new O({props:{code:`question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape`,highlighted:`question = <span class="hljs-string">&quot;How can I load a dataset offline?&quot;</span>
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape`}}),d=new O({props:{code:"torch.Size([1, 768])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">768</span>])'}}),{c(){$(u.$$.fragment),y=h(),$(d.$$.fragment)},l(g){k(u.$$.fragment,g),y=m(g),k(d.$$.fragment,g)},m(g,v){x(u,g,v),r(g,y,v),x(d,g,v),j=!0},i(g){j||(b(u.$$.fragment,g),b(d.$$.fragment,g),j=!0)},o(g){_(u.$$.fragment,g),_(d.$$.fragment,g),j=!1},d(g){E(u,g),g&&n(y),E(d,g)}}}function eh(z){let u,y,d,j,g,v,I,f,S,q,L;return{c(){u=o("p"),y=s("\u270F\uFE0F "),d=o("strong"),j=s("Th\u1EED nghi\u1EC7m th\xF4i!"),g=s(" T\u1EA1o truy v\u1EA5n c\u1EE7a ri\xEAng b\u1EA1n v\xE0 xem li\u1EC7u b\u1EA1n c\xF3 th\u1EC3 t\xECm th\u1EA5y c\xE2u tr\u1EA3 l\u1EDDi trong c\xE1c t\xE0i li\u1EC7u \u0111\xE3 truy xu\u1EA5t hay kh\xF4ng. B\u1EA1n c\xF3 th\u1EC3 ph\u1EA3i t\u0103ng tham s\u1ED1 "),v=o("code"),I=s("k"),f=s(" trong "),S=o("code"),q=s("Dataset.get_nearest_examples()"),L=s(" \u0111\u1EC3 m\u1EDF r\u1ED9ng t\xECm ki\u1EBFm.")},l(H){u=l(H,"P",{});var A=i(u);y=a(A,"\u270F\uFE0F "),d=l(A,"STRONG",{});var P=i(d);j=a(P,"Th\u1EED nghi\u1EC7m th\xF4i!"),P.forEach(n),g=a(A," T\u1EA1o truy v\u1EA5n c\u1EE7a ri\xEAng b\u1EA1n v\xE0 xem li\u1EC7u b\u1EA1n c\xF3 th\u1EC3 t\xECm th\u1EA5y c\xE2u tr\u1EA3 l\u1EDDi trong c\xE1c t\xE0i li\u1EC7u \u0111\xE3 truy xu\u1EA5t hay kh\xF4ng. B\u1EA1n c\xF3 th\u1EC3 ph\u1EA3i t\u0103ng tham s\u1ED1 "),v=l(A,"CODE",{});var N=i(v);I=a(N,"k"),N.forEach(n),f=a(A," trong "),S=l(A,"CODE",{});var F=i(S);q=a(F,"Dataset.get_nearest_examples()"),F.forEach(n),L=a(A," \u0111\u1EC3 m\u1EDF r\u1ED9ng t\xECm ki\u1EBFm."),A.forEach(n)},m(H,A){r(H,u,A),e(u,y),e(u,d),e(d,j),e(u,g),e(u,v),e(v,I),e(u,f),e(u,S),e(S,q),e(u,L)},d(H){H&&n(u)}}}function nh(z){let u,y,d,j,g,v,I,f,S,q,L,H,A,P,N,F,B,D,R,Y,U,T,G,p,C,M,K,ot,ht,Ha,vs,bt,Pa,Se,Na,Ra,nn,Fa,Ma,ys,Ae,La,$s,Ct,Kt,Bi,za,Wt,Gi,ks,St,It,sn,Jt,Ua,an,Ba,xs,Oe,Ga,Es,Qt,ws,vt,Va,on,Ya,Ka,Ie,Wa,Ja,js,Xt,qs,Zt,Ts,W,Qa,ln,Xa,Za,cn,to,eo,rn,no,so,hn,ao,oo,mn,lo,io,Ds,te,Cs,ee,Ss,J,co,un,ro,ho,pn,mo,uo,dn,po,go,gn,fo,_o,fn,bo,vo,As,ne,Os,se,Is,X,yo,_n,$o,ko,bn,xo,Eo,Ht,wo,vn,jo,qo,yn,To,Do,Hs,ae,Ps,Pt,Co,$n,So,Ao,Ns,oe,Rs,le,Fs,Nt,Oo,kn,Io,Ho,Ms,ie,Ls,tt,xn,Z,zs,Po,En,No,Ro,wn,Fo,Mo,jn,Lo,zo,qn,Uo,Bo,mt,et,Tn,Go,Vo,Dn,Yo,Ko,Cn,Wo,Jo,Sn,Qo,Xo,An,Zo,tl,nt,On,el,nl,In,sl,al,Hn,ol,ll,Pn,il,cl,Nn,rl,hl,st,Rn,ml,ul,Fn,pl,dl,Mn,gl,fl,Ln,_l,bl,zn,vl,yl,at,Un,$l,kl,Bn,xl,El,Gn,wl,jl,Vn,ql,Tl,Yn,Dl,Us,lt,Cl,Kn,Sl,Al,Wn,Ol,Il,Jn,Hl,Pl,Bs,ce,Gs,re,Vs,He,Nl,Ys,Rt,Ks,Ft,Rl,Qn,Fl,Ml,Ws,he,Js,Pe,Ll,Qs,me,Xs,ue,Zs,yt,zl,Xn,Ul,Bl,Zn,Gl,Vl,ta,pe,ea,Ne,Yl,na,At,Mt,ts,de,Kl,es,Wl,sa,V,Jl,Re,Ql,Xl,ns,Zl,ti,ss,ei,ni,ge,si,ai,as,oi,li,fe,ii,ci,os,ri,hi,aa,ut,pt,Fe,$t,mi,ls,ui,pi,is,di,gi,oa,_e,la,Me,fi,ia,dt,gt,Le,ze,_i,ca,Ot,Lt,cs,be,bi,rs,vi,ra,kt,yi,hs,$i,ki,ve,xi,Ei,ha,it,wi,ms,ji,qi,us,Ti,Di,ps,Ci,Si,ma,ye,ua,zt,Ai,ds,Oi,Ii,pa,ft,_t,Ue,Be,Hi,da,$e,ga,xt,Pi,gs,Ni,Ri,fs,Fi,Mi,fa,ke,_a,Ge,Li,ba,xe,va,Ee,ya,Ve,zi,$a,Ut,ka;d=new Gr({props:{fw:z[0]}}),f=new bs({});const Vi=[Yr,Vr],we=[];function Yi(t,c){return t[0]==="pt"?0:1}A=Yi(z),P=we[A]=Vi[A](z),T=new Br({props:{id:"OATCgQtNX2o"}}),K=new bs({}),Jt=new bs({}),Qt=new O({props:{code:`from huggingface_hub import hf_hub_url

data_files = hf_hub_url(
    repo_id="lewtun/github-issues",
    filename="datasets-issues-with-hf-doc-builder.jsonl",
    repo_type="dataset",
)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_url

data_files = hf_hub_url(
    repo_id=<span class="hljs-string">&quot;lewtun/github-issues&quot;</span>,
    filename=<span class="hljs-string">&quot;datasets-issues-with-hf-doc-builder.jsonl&quot;</span>,
    repo_type=<span class="hljs-string">&quot;dataset&quot;</span>,
)`}}),Xt=new O({props:{code:`from datasets import load_dataset

issues_dataset = load_dataset("json", data_files=data_files, split="train")
issues_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

issues_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)
issues_dataset`}}),Zt=new O({props:{code:`Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;url&#x27;</span>, <span class="hljs-string">&#x27;repository_url&#x27;</span>, <span class="hljs-string">&#x27;labels_url&#x27;</span>, <span class="hljs-string">&#x27;comments_url&#x27;</span>, <span class="hljs-string">&#x27;events_url&#x27;</span>, <span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;node_id&#x27;</span>, <span class="hljs-string">&#x27;number&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;locked&#x27;</span>, <span class="hljs-string">&#x27;assignee&#x27;</span>, <span class="hljs-string">&#x27;assignees&#x27;</span>, <span class="hljs-string">&#x27;milestone&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;created_at&#x27;</span>, <span class="hljs-string">&#x27;updated_at&#x27;</span>, <span class="hljs-string">&#x27;closed_at&#x27;</span>, <span class="hljs-string">&#x27;author_association&#x27;</span>, <span class="hljs-string">&#x27;active_lock_reason&#x27;</span>, <span class="hljs-string">&#x27;pull_request&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;performed_via_github_app&#x27;</span>, <span class="hljs-string">&#x27;is_pull_request&#x27;</span>],
    num_rows: <span class="hljs-number">2855</span>
})`}}),te=new O({props:{code:`issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset`,highlighted:`issues_dataset = issues_dataset.<span class="hljs-built_in">filter</span>(
    <span class="hljs-keyword">lambda</span> x: (x[<span class="hljs-string">&quot;is_pull_request&quot;</span>] == <span class="hljs-literal">False</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;comments&quot;</span>]) &gt; <span class="hljs-number">0</span>)
)
issues_dataset`}}),ee=new O({props:{code:`Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;url&#x27;</span>, <span class="hljs-string">&#x27;repository_url&#x27;</span>, <span class="hljs-string">&#x27;labels_url&#x27;</span>, <span class="hljs-string">&#x27;comments_url&#x27;</span>, <span class="hljs-string">&#x27;events_url&#x27;</span>, <span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;node_id&#x27;</span>, <span class="hljs-string">&#x27;number&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;locked&#x27;</span>, <span class="hljs-string">&#x27;assignee&#x27;</span>, <span class="hljs-string">&#x27;assignees&#x27;</span>, <span class="hljs-string">&#x27;milestone&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;created_at&#x27;</span>, <span class="hljs-string">&#x27;updated_at&#x27;</span>, <span class="hljs-string">&#x27;closed_at&#x27;</span>, <span class="hljs-string">&#x27;author_association&#x27;</span>, <span class="hljs-string">&#x27;active_lock_reason&#x27;</span>, <span class="hljs-string">&#x27;pull_request&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;performed_via_github_app&#x27;</span>, <span class="hljs-string">&#x27;is_pull_request&#x27;</span>],
    num_rows: <span class="hljs-number">771</span>
})`}}),ne=new O({props:{code:`columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset`,highlighted:`columns = issues_dataset.column_names
columns_to_keep = [<span class="hljs-string">&quot;title&quot;</span>, <span class="hljs-string">&quot;body&quot;</span>, <span class="hljs-string">&quot;html_url&quot;</span>, <span class="hljs-string">&quot;comments&quot;</span>]
columns_to_remove = <span class="hljs-built_in">set</span>(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset`}}),se=new O({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>],
    num_rows: <span class="hljs-number">771</span>
})`}}),ae=new O({props:{code:`issues_dataset.set_format("pandas")
df = issues_dataset[:]`,highlighted:`issues_dataset.set_format(<span class="hljs-string">&quot;pandas&quot;</span>)
df = issues_dataset[:]`}}),oe=new O({props:{code:'df["comments"][0].tolist()',highlighted:'df[<span class="hljs-string">&quot;comments&quot;</span>][<span class="hljs-number">0</span>].tolist()'}}),le=new O({props:{code:`['the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of \`ConnectionError\` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?',
 'cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']`,highlighted:`[<span class="hljs-string">&#x27;the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset(&quot;glue&quot;, data_args.task_name, cache_dir=model_args.cache_dir)&#x27;</span>,
 <span class="hljs-string">&#x27;Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of \`ConnectionError\` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?&#x27;</span>,
 <span class="hljs-string">&#x27;cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002&#x27;</span>,
 <span class="hljs-string">&#x27;I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...&#x27;</span>]`}}),ie=new O({props:{code:`comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)`,highlighted:`comments_df = df.explode(<span class="hljs-string">&quot;comments&quot;</span>, ignore_index=<span class="hljs-literal">True</span>)
comments_df.head(<span class="hljs-number">4</span>)`}}),ce=new O({props:{code:`from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset`}}),re=new O({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>],
    num_rows: <span class="hljs-number">2842</span>
})`}}),Rt=new Nr({props:{$$slots:{default:[Kr]},$$scope:{ctx:z}}}),he=new O({props:{code:`comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)`,highlighted:`comments_dataset = comments_dataset.<span class="hljs-built_in">map</span>(
    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;comment_length&quot;</span>: <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;comments&quot;</span>].split())}
)`}}),me=new O({props:{code:`comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset`,highlighted:`comments_dataset = comments_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&quot;comment_length&quot;</span>] &gt; <span class="hljs-number">15</span>)
comments_dataset`}}),ue=new O({props:{code:`Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;html_url&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;comments&#x27;</span>, <span class="hljs-string">&#x27;body&#x27;</span>, <span class="hljs-string">&#x27;comment_length&#x27;</span>],
    num_rows: <span class="hljs-number">2098</span>
})`}}),pe=new O({props:{code:`def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \\n "
        + examples["body"]
        + " \\n "
        + examples["comments"]
    }


comments_dataset = comments_dataset.map(concatenate_text)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">concatenate_text</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">&quot;text&quot;</span>: examples[<span class="hljs-string">&quot;title&quot;</span>]
        + <span class="hljs-string">&quot; \\n &quot;</span>
        + examples[<span class="hljs-string">&quot;body&quot;</span>]
        + <span class="hljs-string">&quot; \\n &quot;</span>
        + examples[<span class="hljs-string">&quot;comments&quot;</span>]
    }


comments_dataset = comments_dataset.<span class="hljs-built_in">map</span>(concatenate_text)`}}),de=new bs({});const Ki=[Jr,Wr],je=[];function Wi(t,c){return t[0]==="pt"?0:1}ut=Wi(z),pt=je[ut]=Ki[ut](z),_e=new O({props:{code:`def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">cls_pooling</span>(<span class="hljs-params">model_output</span>):
    <span class="hljs-keyword">return</span> model_output.last_hidden_state[:, <span class="hljs-number">0</span>]`}});const Ji=[Xr,Qr],qe=[];function Qi(t,c){return t[0]==="pt"?0:1}dt=Qi(z),gt=qe[dt]=Ji[dt](z),be=new bs({}),ye=new O({props:{code:'embeddings_dataset.add_faiss_index(column="embeddings")',highlighted:'embeddings_dataset.add_faiss_index(column=<span class="hljs-string">&quot;embeddings&quot;</span>)'}});const Xi=[th,Zr],Te=[];function Zi(t,c){return t[0]==="pt"?0:1}return ft=Zi(z),_t=Te[ft]=Xi[ft](z),$e=new O({props:{code:`scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)`,highlighted:`scores, samples = embeddings_dataset.get_nearest_examples(
    <span class="hljs-string">&quot;embeddings&quot;</span>, question_embedding, k=<span class="hljs-number">5</span>
)`}}),ke=new O({props:{code:`import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)`,highlighted:`<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df[<span class="hljs-string">&quot;scores&quot;</span>] = scores
samples_df.sort_values(<span class="hljs-string">&quot;scores&quot;</span>, ascending=<span class="hljs-literal">False</span>, inplace=<span class="hljs-literal">True</span>)`}}),xe=new O({props:{code:`for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()`,highlighted:`<span class="hljs-keyword">for</span> _, row <span class="hljs-keyword">in</span> samples_df.iterrows():
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;COMMENT: <span class="hljs-subst">{row.comments}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;SCORE: <span class="hljs-subst">{row.scores}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;TITLE: <span class="hljs-subst">{row.title}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;URL: <span class="hljs-subst">{row.html_url}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;=&quot;</span> * <span class="hljs-number">50</span>)
    <span class="hljs-built_in">print</span>()`}}),Ee=new O({props:{code:`"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how \`transformers\` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the \`datasets\` package since #1726 :)
You can now use them offline
\\\`\\\`\\\`python
datasets = load_dataset("text", data_files=data_files)
\\\`\\\`\\\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?

Indeed \`load_dataset\` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at \`./my_dataset/my_dataset.py\` then you can do
\\\`\\\`\\\`python
load_dataset("./my_dataset")
\\\`\\\`\\\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having \`csv\`, \`json\`, \`text\`, \`pandas\` dataset builders already included in the \`datasets\` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> \`\`\`
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> \`\`\`
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> \`\`\`
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> \`\`\`
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\\\`\\\`\\\`
2. copy the dir from online to the offline machine
3. (offline machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\\\`\\\`\\\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it&#x27;d be great if offline mode is added similar to how \`transformers\` loads models offline fine.

@mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the \`datasets\` package since #1726 :)
You can now use them offline
\\\`\\\`\\\`python
datasets = load_dataset(&quot;text&quot;, data_files=data_files)
\\\`\\\`\\\`

We&#x27;ll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there&#x27;s no internet.

Let me know if you know other ways that can make the offline mode experience better. I&#x27;d be happy to add them :)

I already note the &quot;freeze&quot; modules option, to prevent local modules updates. It would be a cool feature.

----------

&gt; @mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with \`datasets\`. Could you please elaborate on how that should look like?

Indeed \`load_dataset\` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at \`./my_dataset/my_dataset.py\` then you can do
\\\`\\\`\\\`python
load_dataset(&quot;./my_dataset&quot;)
\\\`\\\`\\\`
and the dataset script will generate your dataset once and for all.

----------

About I&#x27;m looking into having \`csv\`, \`json\`, \`text\`, \`pandas\` dataset builders already included in the \`datasets\` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine
&gt;
&gt; 1. (online machine)
&gt;
&gt; \`\`\`
&gt;
&gt; import datasets
&gt;
&gt; data = datasets.load_dataset(...)
&gt;
&gt; data.save_to_disk(/YOUR/DATASET/DIR)
&gt;
&gt; \`\`\`
&gt;
&gt; 2. copy the dir from online to the offline machine
&gt;
&gt; 3. (offline machine)
&gt;
&gt; \`\`\`
&gt;
&gt; import datasets
&gt;
&gt; data = datasets.load_from_disk(/SAVED/DATA/DIR)
&gt;
&gt; \`\`\`
&gt;
&gt;
&gt;
&gt; HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\\\`\\\`\\\`
2. copy the dir from online to the offline machine
3. (offline machine)
\\\`\\\`\\\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\\\`\\\`\\\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
&quot;&quot;&quot;</span>`}}),Ut=new Nr({props:{$$slots:{default:[eh]},$$scope:{ctx:z}}}),{c(){u=o("meta"),y=h(),$(d.$$.fragment),j=h(),g=o("h1"),v=o("a"),I=o("span"),$(f.$$.fragment),S=h(),q=o("span"),L=s("T\xECm ki\u1EBFm ng\u1EEF ngh\u0129a v\u1EDBi FAISS"),H=h(),P.c(),N=h(),F=o("p"),B=s("Trong "),D=o("a"),R=s("ph\u1EA7n 5"),Y=s(", ch\xFAng ta \u0111\xE3 t\u1EA1o t\u1EADp d\u1EEF li\u1EC7u v\u1EC1 c\xE1c v\u1EA5n \u0111\u1EC1 GitHub v\xE0 nh\u1EADn x\xE9t t\u1EEB kho l\u01B0u tr\u1EEF \u{1F917} Datasets. Trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng th\xF4ng tin n\xE0y \u0111\u1EC3 x\xE2y d\u1EF1ng m\u1ED9t c\xF4ng c\u1EE5 t\xECm ki\u1EBFm c\xF3 th\u1EC3 gi\xFAp ta t\xECm c\xE2u tr\u1EA3 l\u1EDDi cho nh\u1EEFng c\xE2u h\u1ECFi c\u1EA5p b\xE1ch nh\u1EA5t v\u1EC1 th\u01B0 vi\u1EC7n!"),U=h(),$(T.$$.fragment),G=h(),p=o("h2"),C=o("a"),M=o("span"),$(K.$$.fragment),ot=h(),ht=o("span"),Ha=s("S\u1EED d\u1EE5ng nh\xFAng bi\u1EC3u di\u1EC5n t\u1EEB cho t\xECm ki\u1EBFm ng\u1EEF ngh\u0129a"),vs=h(),bt=o("p"),Pa=s("Nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),Se=o("a"),Na=s("Ch\u01B0\u01A1ng 1"),Ra=s(", c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF d\u1EF1a tr\xEAn Transformer \u0111\u1EA1i di\u1EC7n cho m\u1ED7i token trong m\u1ED9t kho\u1EA3ng v\u0103n b\u1EA3n d\u01B0\u1EDBi d\u1EA1ng m\u1ED9t "),nn=o("em"),Fa=s("vector nhugns bi\u1EC3u di\u1EC5n t\u1EEB"),Ma=s(". H\xF3a ra ng\u01B0\u1EDDi ta c\xF3 th\u1EC3 \u201Cg\u1ED9p\u201D c\xE1c bi\u1EC3u di\u1EC5n ri\xEAng l\u1EBB \u0111\u1EC3 t\u1EA1o bi\u1EC3u di\u1EC5n vect\u01A1 cho to\xE0n b\u1ED9 c\xE2u, \u0111o\u1EA1n v\u0103n ho\u1EB7c to\xE0n b\u1ED9 (trong m\u1ED9t s\u1ED1 tr\u01B0\u1EDDng h\u1EE3p) t\xE0i li\u1EC7u. Sau \u0111\xF3, c\xE1c ph\xE9p nh\xFAng n\xE0y c\xF3 th\u1EC3 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 t\xECm c\xE1c t\xE0i li\u1EC7u t\u01B0\u01A1ng t\u1EF1 trong kho t\xE0i li\u1EC7u b\u1EB1ng c\xE1ch t\xEDnh to\xE1n \u0111\u1ED9 t\u01B0\u01A1ng t\u1EF1 c\u1EE7a s\u1EA3n ph\u1EA9m (ho\u1EB7c m\u1ED9t s\u1ED1 ch\u1EC9 s\u1ED1 t\u01B0\u01A1ng t\u1EF1 kh\xE1c) gi\u1EEFa m\u1ED7i bi\u1EC5u di\u1EC5n v\xE0 tr\u1EA3 v\u1EC1 c\xE1c t\xE0i li\u1EC7u c\xF3 \u0111\u1ED9 t\u01B0\u01A1ng \u0111\u1ED3ng l\u1EDBn nh\u1EA5t."),ys=h(),Ae=o("p"),La=s("Trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng c\xE1c bi\u1EC3u di\u1EC5n t\u1EEB \u0111\u1EC3 ph\xE1t tri\u1EC3n m\u1ED9t c\xF4ng c\u1EE5 t\xECm ki\u1EBFm ng\u1EEF ngh\u0129a. C\xE1c c\xF4ng c\u1EE5 t\xECm ki\u1EBFm n\xE0y cung c\u1EA5p m\u1ED9t s\u1ED1 l\u1EE3i th\u1EBF so v\u1EDBi c\xE1c ph\u01B0\u01A1ng ph\xE1p ti\u1EBFp c\u1EADn th\xF4ng th\u01B0\u1EDDng d\u1EF1a tr\xEAn vi\u1EC7c k\u1EBFt h\u1EE3p c\xE1c t\u1EEB kh\xF3a trong m\u1ED9t truy v\u1EA5n v\u1EDBi c\xE1c t\xE0i li\u1EC7u."),$s=h(),Ct=o("div"),Kt=o("img"),za=h(),Wt=o("img"),ks=h(),St=o("h2"),It=o("a"),sn=o("span"),$(Jt.$$.fragment),Ua=h(),an=o("span"),Ba=s("T\u1EA3i v\xE0 chu\u1EA9n b\u1ECB t\u1EADp d\u1EEF li\u1EC7u"),xs=h(),Oe=o("p"),Ga=s("\u0110i\u1EC1u \u0111\u1EA7u ti\xEAn ch\xFAng ta c\u1EA7n l\xE0m l\xE0 t\u1EA3i xu\u1ED1ng t\u1EADp d\u1EEF li\u1EC7u v\u1EC1 c\xE1c s\u1EF1 c\u1ED1 GitHub, v\xEC v\u1EADy h\xE3y s\u1EED d\u1EE5ng th\u01B0 vi\u1EC7n \u{1F917} Hub \u0111\u1EC3 gi\u1EA3i quy\u1EBFt URL n\u01A1i t\u1EC7p c\u1EE7a ch\xFAng ta \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF tr\xEAn Hugging Face Hub:"),Es=h(),$(Qt.$$.fragment),ws=h(),vt=o("p"),Va=s("V\u1EDBi URL \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF trong "),on=o("code"),Ya=s("data_files"),Ka=s(", sau \u0111\xF3 ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i t\u1EADp d\u1EEF li\u1EC7u t\u1EEB xa b\u1EB1ng ph\u01B0\u01A1ng ph\xE1p \u0111\xE3 \u0111\u01B0\u1EE3c gi\u1EDBi thi\u1EC7u trong "),Ie=o("a"),Wa=s("ph\u1EA7n 2"),Ja=s(":"),js=h(),$(Xt.$$.fragment),qs=h(),$(Zt.$$.fragment),Ts=h(),W=o("p"),Qa=s("\u1EDE \u0111\xE2y ch\xFAng ta \u0111\xE3 ch\u1EC9 \u0111\u1ECBnh t\xE1ch "),ln=o("code"),Xa=s("train"),Za=s(" m\u1EB7c \u0111\u1ECBnh trong "),cn=o("code"),to=s("load_dataset()"),eo=s(", v\xEC v\u1EADy n\xF3 tr\u1EA3 v\u1EC1 m\u1ED9t "),rn=o("code"),no=s("Dataset"),so=s(" thay v\xEC "),hn=o("code"),ao=s("DatasetDict"),oo=s(". Tr\xECnh t\u1EF1 \u0111\u1EA7u ti\xEAn c\u1EE7a doanh nghi\u1EC7p l\xE0 l\u1ECDc ra c\xE1c y\xEAu c\u1EA7u k\xE9o, v\xEC nh\u1EEFng y\xEAu c\u1EA7u n\xE0y c\xF3 xu h\u01B0\u1EDBng hi\u1EBFm khi \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 tr\u1EA3 l\u1EDDi c\xE1c truy v\u1EA5n c\u1EE7a ng\u01B0\u1EDDi d\xF9ng v\xE0 s\u1EBD t\u1EA1o ra nhi\u1EC5u trong c\xF4ng c\u1EE5 t\xECm ki\u1EBFm m\xECnh. Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng h\xE0m "),mn=o("code"),lo=s("Dataset.filter()"),io=s(" \u0111\xE3 quen thu\u1ED9c v\u1EDBi b\u1EA1n \u0111\u1EC3 lo\u1EA1i tr\u1EEB c\xE1c h\xE0ng n\xE0y trong t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh. C\xF9ng l\xFAc \u0111\xF3, h\xE3y c\xF9ng l\u1ECDc ra c\xE1c h\xE0ng kh\xF4ng c\xF3 nh\u1EADn x\xE9t, v\xEC nh\u1EEFng h\xE0ng n\xE0y kh\xF4ng cung c\u1EA5p c\xE2u tr\u1EA3 l\u1EDDi cho c\xE1c truy v\u1EA5n c\u1EE7a ng\u01B0\u1EDDi d\xF9ng:"),Ds=h(),$(te.$$.fragment),Cs=h(),$(ee.$$.fragment),Ss=h(),J=o("p"),co=s("Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng c\xF3 r\u1EA5t nhi\u1EC1u c\u1ED9t trong t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng ta, h\u1EA7u h\u1EBFt trong s\u1ED1 \u0111\xF3 ch\xFAng ta kh\xF4ng c\u1EA7n ph\u1EA3i x\xE2y d\u1EF1ng c\xF4ng c\u1EE5 t\xECm ki\u1EBFm c\u1EE7a m\xECnh. T\u1EEB g\xF3c \u0111\u1ED9 t\xECm ki\u1EBFm, c\xE1c c\u1ED9t ch\u1EE9a nhi\u1EC1u th\xF4ng tin nh\u1EA5t l\xE0 "),un=o("code"),ro=s("title"),ho=s(", "),pn=o("code"),mo=s("body"),uo=s(", v\xE0 "),dn=o("code"),po=s("comments"),go=s(",  trong khi "),gn=o("code"),fo=s("html_url"),_o=s(" cung c\u1EA5p cho ch\xFAng ta m\u1ED9t li\xEAn k\u1EBFt tr\u1ECF v\u1EC1 ngu\u1ED3n. H\xE3y s\u1EED d\u1EE5ng h\xE0m "),fn=o("code"),bo=s("Dataset.remove_columns()"),vo=s(" \u0111\u1EC3 x\xF3a ph\u1EA7n c\xF2n l\u1EA1i:"),As=h(),$(ne.$$.fragment),Os=h(),$(se.$$.fragment),Is=h(),X=o("p"),yo=s("\u0110\u1EC3 t\u1EA1o c\xE1c bi\u1EC3u di\u1EC5n, ch\xFAng ta s\u1EBD b\u1ED5 sung m\u1ED7i nh\u1EADn x\xE9t v\u1EDBi ti\xEAu \u0111\u1EC1 v\xE0 n\u1ED9i dung c\u1EE7a v\u1EA5n \u0111\u1EC1, v\xEC c\xE1c tr\u01B0\u1EDDng n\xE0y th\u01B0\u1EDDng bao g\u1ED3m th\xF4ng tin ng\u1EEF c\u1EA3nh h\u1EEFu \xEDch. V\xEC c\u1ED9t "),_n=o("code"),$o=s("comments"),ko=s(" c\u1EE7a hi\u1EC7n l\xE0 danh s\xE1ch c\xE1c nh\u1EADn x\xE9t cho t\u1EEBng v\u1EA5n \u0111\u1EC1, ch\xFAng t\xF4i c\u1EA7n kh\xE1m ph\xE1 c\xE1c c\u1ED9t \u0111\u1EC3 m\u1ED7i h\xE0ng bao g\u1ED3m m\u1ED9t tuple "),bn=o("code"),xo=s("(html_url, title, body, comment)"),Eo=s(". Trong Pandas, ch\xFAng ta c\xF3 th\u1EC3 th\u1EF1c hi\u1EC7n vi\u1EC7c n\xE0y b\u1EB1ng h\xE0m "),Ht=o("a"),wo=s("h\xE0m "),vn=o("code"),jo=s("DataFrame.explode()"),qo=s(", t\u1EA1o m\u1ED9t h\xE0ng m\u1EDBi cho m\u1ED7i ph\u1EA7n t\u1EED trong c\u1ED9t gi\u1ED1ng nh\u01B0 danh s\xE1ch, trong khi sao ch\xE9p t\u1EA5t c\u1EA3 c\xE1c gi\xE1 tr\u1ECB c\u1ED9t kh\xE1c. \u0110\u1EC3 xem \u0111i\u1EC1u n\xE0y ho\u1EA1t \u0111\u1ED9ng, tr\u01B0\u1EDBc ti\xEAn ch\xFAng ta h\xE3y ch\xFAng chuy\u1EC3n th\xE0nh \u0111\u1ECBnh d\u1EA1ng Pandas "),yn=o("code"),To=s("DataFrame"),Do=s(":"),Hs=h(),$(ae.$$.fragment),Ps=h(),Pt=o("p"),Co=s("N\u1EBFu ta ki\u1EC3m tra h\xE0ng \u0111\u1EA7u ti\xEAn trong "),$n=o("code"),So=s("DataFrame"),Ao=s(" n\xE0y, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y c\xF3 b\u1ED1n nh\u1EADn x\xE9t li\xEAn quan \u0111\u1EBFn v\u1EA5n \u0111\u1EC1 n\xE0y:"),Ns=h(),$(oe.$$.fragment),Rs=h(),$(le.$$.fragment),Fs=h(),Nt=o("p"),Oo=s("Khi ch\xFAng ta kh\xE1m ph\xE1 "),kn=o("code"),Io=s("df"),Ho=s(", ch\xFAng t\xF4i mong \u0111\u1EE3i nh\u1EADn \u0111\u01B0\u1EE3c m\u1ED9t h\xE0ng cho m\u1ED7i nh\u1EADn x\xE9t n\xE0y. H\xE3y ki\u1EC3m tra xem n\xF3 \u0111\xE3 \u0111\xFAng ch\u01B0a:"),Ms=h(),$(ie.$$.fragment),Ls=h(),tt=o("table"),xn=o("thead"),Z=o("tr"),zs=o("th"),Po=h(),En=o("th"),No=s("html_url"),Ro=h(),wn=o("th"),Fo=s("title"),Mo=h(),jn=o("th"),Lo=s("comments"),zo=h(),qn=o("th"),Uo=s("body"),Bo=h(),mt=o("tbody"),et=o("tr"),Tn=o("th"),Go=s("0"),Vo=h(),Dn=o("td"),Yo=s("https://github.com/huggingface/datasets/issues/2787"),Ko=h(),Cn=o("td"),Wo=s("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Jo=h(),Sn=o("td"),Qo=s("the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None..."),Xo=h(),An=o("td"),Zo=s("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),tl=h(),nt=o("tr"),On=o("th"),el=s("1"),nl=h(),In=o("td"),sl=s("https://github.com/huggingface/datasets/issues/2787"),al=h(),Hn=o("td"),ol=s("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),ll=h(),Pn=o("td"),il=s("Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com..."),cl=h(),Nn=o("td"),rl=s("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),hl=h(),st=o("tr"),Rn=o("th"),ml=s("2"),ul=h(),Fn=o("td"),pl=s("https://github.com/huggingface/datasets/issues/2787"),dl=h(),Mn=o("td"),gl=s("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),fl=h(),Ln=o("td"),_l=s("cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002"),bl=h(),zn=o("td"),vl=s("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),yl=h(),at=o("tr"),Un=o("th"),$l=s("3"),kl=h(),Bn=o("td"),xl=s("https://github.com/huggingface/datasets/issues/2787"),El=h(),Gn=o("td"),wl=s("ConnectionError: Couldn't reach https://raw.githubusercontent.com"),jl=h(),Vn=o("td"),ql=s("I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem..."),Tl=h(),Yn=o("td"),Dl=s("Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Us=h(),lt=o("p"),Cl=s("Tuy\u1EC7t v\u1EDDi, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y c\xE1c h\xE0ng \u0111\xE3 \u0111\u01B0\u1EE3c nh\xE2n r\u1ED9ng, v\u1EDBi c\u1ED9t "),Kn=o("code"),Sl=s("comments"),Al=s(" ch\u1EE9a c\xE1c nh\u1EADn x\xE9t ri\xEAng l\u1EBB! B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 ho\xE0n th\xE0nh v\u1EDBi Pandas, ch\xFAng ta c\xF3 th\u1EC3 nhanh ch\xF3ng chuy\u1EC3n tr\u1EDF l\u1EA1i "),Wn=o("code"),Ol=s("Dataset"),Il=s(" b\u1EB1ng c\xE1ch t\u1EA3i "),Jn=o("code"),Hl=s("DataFrame"),Pl=s(" v\xE0o b\u1ED9 nh\u1EDB:"),Bs=h(),$(ce.$$.fragment),Gs=h(),$(re.$$.fragment),Vs=h(),He=o("p"),Nl=s("\u0110\u01B0\u1EE3c r\u1ED3i, \u0111i\u1EC1u n\xE0y \u0111\xE3 cho ch\xFAng ta v\xE0i ngh\xECn nh\u1EADn x\xE9t \u0111\u1EC3 l\xE0m vi\u1EC7c c\xF9ng!"),Ys=h(),$(Rt.$$.fragment),Ks=h(),Ft=o("p"),Rl=s("B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 m\u1ED9t nh\u1EADn x\xE9t tr\xEAn m\u1ED7i h\xE0ng, h\xE3y t\u1EA1o m\u1ED9t c\u1ED9t "),Qn=o("code"),Fl=s("comments_length"),Ml=s(" m\u1EDBi ch\u1EE9a s\u1ED1 t\u1EEB tr\xEAn m\u1ED7i nh\u1EADn x\xE9t:"),Ws=h(),$(he.$$.fragment),Js=h(),Pe=o("p"),Ll=s("Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng c\u1ED9t m\u1EDBi n\xE0y \u0111\u1EC3 l\u1ECDc ra c\xE1c nh\u1EADn x\xE9t ng\u1EAFn, th\u01B0\u1EDDng bao g\u1ED3m nh\u1EEFng th\u1EE9 nh\u01B0 \u201Ccc @lewtun\u201D ho\u1EB7c \u201CThanks!\u201D kh\xF4ng li\xEAn quan \u0111\u1EBFn c\xF4ng c\u1EE5 t\xECm ki\u1EBFm c\u1EE7a m\xECnh. Kh\xF4ng c\xF3 con s\u1ED1 ch\xEDnh x\xE1c \u0111\u1EC3 ch\u1ECDn cho b\u1ED9 l\u1ECDc, nh\u01B0ng kho\u1EA3ng 15 t\u1EEB c\xF3 v\u1EBB nh\u01B0 l\xE0 m\u1ED9t kh\u1EDFi \u0111\u1EA7u t\u1ED1t:"),Qs=h(),$(me.$$.fragment),Xs=h(),$(ue.$$.fragment),Zs=h(),yt=o("p"),zl=s("Sau khi d\u1ECDn d\u1EB9p t\u1EADp d\u1EEF li\u1EC7u m\u1ED9t ch\xFAt, h\xE3y gh\xE9p ti\xEAu \u0111\u1EC1, m\xF4 t\u1EA3 v\xE0 nh\u1EADn x\xE9t c\u1EE7a v\u1EA5n \u0111\u1EC1 v\u1EDBi nhau trong m\u1ED9t c\u1ED9t "),Xn=o("code"),Ul=s("text"),Bl=s(" m\u1EDBi. Nh\u01B0 th\u01B0\u1EDDng l\u1EC7, ch\xFAng ta s\u1EBD vi\u1EBFt m\u1ED9t h\xE0m \u0111\u01A1n gi\u1EA3n m\xE0 ch\xFAng ta c\xF3 th\u1EC3 truy\u1EC1n v\xE0o "),Zn=o("code"),Gl=s("Dataset.map()"),Vl=s(":"),ta=h(),$(pe.$$.fragment),ea=h(),Ne=o("p"),Yl=s("Cu\u1ED1i c\xF9ng th\xEC ch\xFAng ta c\u0169ng \u0111\xE3 s\u1EB5n s\xE0ng \u0111\u1EC3 t\u1EA1o m\u1ED9t s\u1ED1 bi\u1EC3u \u0111i\u1EC5n! Ch\xFAng ta h\xE3y xem n\xE0o."),na=h(),At=o("h2"),Mt=o("a"),ts=o("span"),$(de.$$.fragment),Kl=h(),es=o("span"),Wl=s("T\u1EA1o ra bi\u1EC3u di\u1EC5n v\u0103n b\u1EA3n"),sa=h(),V=o("p"),Jl=s("Ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),Re=o("a"),Ql=s("Ch\u01B0\u01A1ng 2"),Xl=s(" r\u1EB1ng ta c\xF3 th\u1EC3 nh\u1EADn \u0111\u01B0\u1EE3c token bi\u1EC5u di\u1EC5n nh\xFAng b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EDBp "),ns=o("code"),Zl=s("AutoModel"),ti=s(". T\u1EA5t c\u1EA3 nh\u1EEFng g\xEC ch\xFAng ta c\u1EA7n l\xE0m l\xE0 ch\u1ECDn m\u1ED9t checkpoint ph\xF9 h\u1EE3p \u0111\u1EC3 t\u1EA3i m\xF4 h\xECnh t\u1EEB \u0111\xF3. May m\u1EAFn thay, c\xF3 m\u1ED9t th\u01B0 vi\u1EC7n t\xEAn l\xE0  "),ss=o("code"),ei=s("sentence-transformers"),ni=s(" d\xE0nh ri\xEAng cho vi\u1EC7c t\u1EA1o c\xE1c bi\u1EC3u di\u1EC5n n\xE0y. Nh\u01B0 \u0111\u01B0\u1EE3c m\xF4 t\u1EA3 trong "),ge=o("a"),si=s("t\xE0i li\u1EC7u"),ai=s(" c\u1EE7a th\u01B0 vi\u1EC7n, tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng c\u1EE7a ta l\xE0 m\u1ED9t v\xED d\u1EE5 v\u1EC1 "),as=o("em"),oi=s("t\xECm ki\u1EBFm ng\u1EEF ngh\u0129a phi \u0111\u1ED1i x\u1EE9ng"),li=s(" b\u1EDFi v\xEC ch\xFAng ta c\xF3 m\u1ED9t truy v\u1EA5n ng\u1EAFn c\xF3 c\xE2u tr\u1EA3 l\u1EDDi ta mu\u1ED1n t\xECm th\u1EA5y trong m\u1ED9t t\xE0i li\u1EC7u l\u1EA1i d\xE0i h\u01A1n nhi\u1EC1u, ch\u1EB3ng h\u1EA1n nh\u01B0 m\u1ED9t nh\u1EADn x\xE9t v\u1EC1 v\u1EA5n \u0111\u1EC1. "),fe=o("a"),ii=s("B\u1EA3ng t\u1ED5ng quan v\u1EC1 m\xF4 h\xECnh"),ci=s(" trong ph\u1EA7n t\xE0i li\u1EC7u ch\u1EC9 ra r\u1EB1ng checkpoint "),os=o("code"),ri=s("multi-qa-mpnet-base-dot-v1"),hi=s(" c\xF3 hi\u1EC7u su\u1EA5t t\u1ED1t nh\u1EA5t cho t\xECm ki\u1EBFm ng\u1EEF ngh\u0129a, v\xEC v\u1EADy ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng n\xF3 cho \u1EE9ng d\u1EE5ng c\u1EE7a m\xECnh. Ch\xFAng ta c\u0169ng s\u1EBD t\u1EA3i tokenizer b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng c\xF9ng m\u1ED9t checkpoint:"),aa=h(),pt.c(),Fe=h(),$t=o("p"),mi=s("Nh\u01B0 \u0111\xE3 \u0111\u1EC1 c\u1EADp tr\u01B0\u1EDBc \u0111\xF3, ch\xFAng ta mu\u1ED1n bi\u1EC3u di\u1EC5n m\u1ED7i m\u1EE5c trong kho d\u1EEF li\u1EC7u c\xE1c v\u1EA5n \u0111\u1EC1 GitHub c\u1EE7a m\xECnh d\u01B0\u1EDBi d\u1EA1ng m\u1ED9t vect\u01A1 duy nh\u1EA5t, v\xEC v\u1EADy ch\xFAng ta c\u1EA7n \u201Cg\u1ED9p\u201D ho\u1EB7c t\xEDnh trung b\xECnh c\xE1c l\u1EA7n bi\u1EC5u di\u1EC5n token theo m\u1ED9t c\xE1ch n\xE0o \u0111\xF3. M\u1ED9t c\xE1ch ti\u1EBFp c\u1EADn ph\u1ED5 bi\u1EBFn l\xE0 th\u1EF1c hi\u1EC7n "),ls=o("em"),ui=s("CLS pooling"),pi=s(" tr\xEAn \u0111\u1EA7u ra c\u1EE7a m\xF4 h\xECnh, n\u01A1i ta ch\u1EC9 c\u1EA7n thu th\u1EADp tr\u1EA1ng th\xE1i \u1EA9n cu\u1ED1i c\xF9ng cho token \u0111\u1EB7c bi\u1EC7t "),is=o("code"),di=s("[CLS]"),gi=s(". H\xE0m sau th\u1EF1c hi\u1EC7n th\u1EE7 thu\u1EADt n\xE0y cho ch\xFAng ta:"),oa=h(),$(_e.$$.fragment),la=h(),Me=o("p"),fi=s("Ti\u1EBFp theo, ch\xFAng t\xF4i s\u1EBD t\u1EA1o m\u1ED9t ch\u1EE9c n\u0103ng tr\u1EE3 gi\xFAp s\u1EBD tokanize danh s\xE1ch c\xE1c t\xE0i li\u1EC7u, \u0111\u1EB7t c\xE1c tensor tr\xEAn GPU, \u0111\u01B0a ch\xFAng v\xE0o m\xF4 h\xECnh v\xE0 cu\u1ED1i c\xF9ng \xE1p d\u1EE5ng CLS g\u1ED9p cho c\xE1c \u0111\u1EA7u ra:"),ia=h(),gt.c(),Le=h(),ze=o("p"),_i=s("L\u01B0u \xFD r\u1EB1ng ch\xFAng ta \u0111\xE3 chuy\u1EC3n \u0111\u1ED5i c\xE1c bi\u1EC3u di\u1EC5n sang th\xE0nh m\u1EA3ng NumPy - \u0111\xF3 l\xE0 v\xEC \u{1F917} Datasets y\xEAu c\u1EA7u \u0111\u1ECBnh d\u1EA1ng n\xE0y khi ta c\u1ED1 g\u1EAFng l\u1EADp ch\u1EC9 m\u1EE5c ch\xFAng b\u1EB1ng FAISS, \u0111i\u1EC1u m\xE0 ta s\u1EBD th\u1EF1c hi\u1EC7n ti\u1EBFp theo."),ca=h(),Ot=o("h2"),Lt=o("a"),cs=o("span"),$(be.$$.fragment),bi=h(),rs=o("span"),vi=s("S\u1EED d\u1EE5ng FAISS \u0111\u1EC3 t\xECm ki\u1EBFm \u0111i\u1EC3m t\u01B0\u01A1ng \u0111\u1ED3ng hi\u1EC7u qu\u1EA3"),ra=h(),kt=o("p"),yi=s("B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u v\u1EC1 c\xE1c bi\u1EC3u di\u1EC5n, ch\xFAng ta c\u1EA7n m\u1ED9t s\u1ED1 c\xE1ch \u0111\u1EC3 t\xECm ki\u1EBFm ch\xFAng. \u0110\u1EC3 l\xE0m \u0111i\u1EC1u n\xE0y, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng m\u1ED9t c\u1EA5u tr\xFAc d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t trong \u{1F917} Datasets \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),hs=o("em"),$i=s("FAISS index"),ki=s(". "),ve=o("a"),xi=s("FAISS"),Ei=s(" (vi\u1EBFt t\u1EAFt c\u1EE7a Facebook AI Similarity Search) l\xE0 m\u1ED9t th\u01B0 vi\u1EC7n cung c\u1EA5p c\xE1c thu\u1EADt to\xE1n hi\u1EC7u qu\u1EA3 \u0111\u1EC3 nhanh ch\xF3ng t\xECm ki\u1EBFm v\xE0 ph\xE2n c\u1EE5m c\xE1c vect\u01A1 nh\xFAng bi\u1EC3u di\u1EC5n."),ha=h(),it=o("p"),wi=s("\xDD t\u01B0\u1EDFng c\u01A1 b\u1EA3n \u0111\u1EB1ng sau FAISS l\xE0 t\u1EA1o ra m\u1ED9t c\u1EA5u tr\xFAc d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),ms=o("em"),ji=s("index"),qi=s(" hay "),us=o("em"),Ti=s("ch\u1EC9 m\u1EE5c"),Di=s(" cho ph\xE9p ng\u01B0\u1EDDi ta t\xECm th\u1EA5y c\xE1c bi\u1EC3u di\u1EC5n nh\xFAng n\xE0o t\u01B0\u01A1ng t\u1EF1 nh\u01B0 bi\u1EC3u di\u1EC5n nh\xFAng \u0111\u1EA7u v\xE0o. T\u1EA1o ch\u1EC9 m\u1EE5c FAISS trong \u{1F917} Datasets r\u1EA5t \u0111\u01A1n gi\u1EA3n - ta s\u1EED d\u1EE5ng h\xE0m "),ps=o("code"),Ci=s("Dataset.add_faiss_index()"),Si=s(" v\xE0 ch\u1EC9 \u0111\u1ECBnh c\u1ED9t n\xE0o trong t\u1EADp d\u1EEF li\u1EC7u m\xE0 ta mu\u1ED1n l\u1EADp ch\u1EC9 m\u1EE5c:"),ma=h(),$(ye.$$.fragment),ua=h(),zt=o("p"),Ai=s("B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 th\u1EF1c hi\u1EC7n c\xE1c truy v\u1EA5n tr\xEAn ch\u1EC9 m\u1EE5c n\xE0y b\u1EB1ng c\xE1ch th\u1EF1c hi\u1EC7n tra c\u1EE9u nh\u1EEFng m\u1EABu l\xE2n c\u1EADn nh\u1EA5t th\xF4ng qua h\xE0m "),ds=o("code"),Oi=s("Dataset.get_nearest_examples()"),Ii=s(". H\xE3y ki\u1EC3m tra \u0111i\u1EC1u n\xE0y b\u1EB1ng c\xE1ch bi\u1EC3u di\u1EC5n m\u1ED9t c\xE2u h\u1ECFi nh\u01B0 sau:"),pa=h(),_t.c(),Ue=h(),Be=o("p"),Hi=s("C\u0169ng gi\u1ED1ng nh\u01B0 c\xE1c t\xE0i li\u1EC7u, gi\u1EDD \u0111\xE2y ch\xFAng ta c\xF3 m\u1ED9t vect\u01A1 768 chi\u1EC1u \u0111\u1EA1i di\u1EC7n cho truy v\u1EA5n, m\xE0 ch\xFAng ta c\xF3 th\u1EC3 so s\xE1nh v\u1EDBi to\xE0n b\u1ED9 kho d\u1EEF li\u1EC7u \u0111\u1EC3 t\xECm ra c\xE1c c\xE1ch bi\u1EC3u di\u1EC5n t\u01B0\u01A1ng t\u1EF1 nh\u1EA5t:"),da=h(),$($e.$$.fragment),ga=h(),xt=o("p"),Pi=s("H\xE0m "),gs=o("code"),Ni=s("Dataset.get_nearest_examples()"),Ri=s(" tr\u1EA3 v\u1EC1 m\u1ED9t lo\u1EA1t \u0111i\u1EC3m x\u1EBFp h\u1EA1ng s\u1EF1 t\u01B0\u01A1ng \u0111\u1ED3ng gi\u1EEFa truy v\u1EA5n v\xE0 t\xE0i li\u1EC7u v\xE0 m\u1ED9t t\u1EADp h\u1EE3p c\xE1c m\u1EABu t\u01B0\u01A1ng \u1EE9ng (\u1EDF \u0111\xE2y, l\xE0 5 k\u1EBFt qu\u1EA3 ph\xF9 h\u1EE3p nh\u1EA5t). H\xE3y thu th\u1EADp nh\u1EEFng th\u1EE9 n\xE0y v\xE0o m\u1ED9t "),fs=o("code"),Fi=s("pandas.DataFrame"),Mi=s(" \u0111\u1EC3 ch\xFAng ta c\xF3 th\u1EC3 d\u1EC5 d\xE0ng s\u1EAFp x\u1EBFp ch\xFAng:"),fa=h(),$(ke.$$.fragment),_a=h(),Ge=o("p"),Li=s("B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 l\u1EB7p l\u1EA1i m\u1ED9t v\xE0i h\xE0ng \u0111\u1EA7u ti\xEAn \u0111\u1EC3 xem truy v\u1EA5n c\u1EE7a ch\xFAng ta kh\u1EDBp v\u1EDBi c\xE1c nh\u1EADn x\xE9t c\xF3 s\u1EB5n nh\u01B0 th\u1EBF n\xE0o:"),ba=h(),$(xe.$$.fragment),va=h(),$(Ee.$$.fragment),ya=h(),Ve=o("p"),zi=s("Kh\xF4ng t\u1EC7! L\u1EA7n truy c\u1EADp th\u1EE9 hai c\u1EE7a ch\xFAng ta d\u01B0\u1EDDng nh\u01B0 ph\xF9 h\u1EE3p v\u1EDBi truy v\u1EA5n."),$a=h(),$(Ut.$$.fragment),this.h()},l(t){const c=zr('[data-svelte="svelte-1phssyn"]',document.head);u=l(c,"META",{name:!0,content:!0}),c.forEach(n),y=m(t),k(d.$$.fragment,t),j=m(t),g=l(t,"H1",{class:!0});var De=i(g);v=l(De,"A",{id:!0,class:!0,href:!0});var Ye=i(v);I=l(Ye,"SPAN",{});var _s=i(I);k(f.$$.fragment,_s),_s.forEach(n),Ye.forEach(n),S=m(De),q=l(De,"SPAN",{});var Ke=i(q);L=a(Ke,"T\xECm ki\u1EBFm ng\u1EEF ngh\u0129a v\u1EDBi FAISS"),Ke.forEach(n),De.forEach(n),H=m(t),P.l(t),N=m(t),F=l(t,"P",{});var Bt=i(F);B=a(Bt,"Trong "),D=l(Bt,"A",{href:!0});var We=i(D);R=a(We,"ph\u1EA7n 5"),We.forEach(n),Y=a(Bt,", ch\xFAng ta \u0111\xE3 t\u1EA1o t\u1EADp d\u1EEF li\u1EC7u v\u1EC1 c\xE1c v\u1EA5n \u0111\u1EC1 GitHub v\xE0 nh\u1EADn x\xE9t t\u1EEB kho l\u01B0u tr\u1EEF \u{1F917} Datasets. Trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng th\xF4ng tin n\xE0y \u0111\u1EC3 x\xE2y d\u1EF1ng m\u1ED9t c\xF4ng c\u1EE5 t\xECm ki\u1EBFm c\xF3 th\u1EC3 gi\xFAp ta t\xECm c\xE2u tr\u1EA3 l\u1EDDi cho nh\u1EEFng c\xE2u h\u1ECFi c\u1EA5p b\xE1ch nh\u1EA5t v\u1EC1 th\u01B0 vi\u1EC7n!"),Bt.forEach(n),U=m(t),k(T.$$.fragment,t),G=m(t),p=l(t,"H2",{class:!0});var Ce=i(p);C=l(Ce,"A",{id:!0,class:!0,href:!0});var tc=i(C);M=l(tc,"SPAN",{});var ec=i(M);k(K.$$.fragment,ec),ec.forEach(n),tc.forEach(n),ot=m(Ce),ht=l(Ce,"SPAN",{});var nc=i(ht);Ha=a(nc,"S\u1EED d\u1EE5ng nh\xFAng bi\u1EC3u di\u1EC5n t\u1EEB cho t\xECm ki\u1EBFm ng\u1EEF ngh\u0129a"),nc.forEach(n),Ce.forEach(n),vs=m(t),bt=l(t,"P",{});var Je=i(bt);Pa=a(Je,"Nh\u01B0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),Se=l(Je,"A",{href:!0});var sc=i(Se);Na=a(sc,"Ch\u01B0\u01A1ng 1"),sc.forEach(n),Ra=a(Je,", c\xE1c m\xF4 h\xECnh ng\xF4n ng\u1EEF d\u1EF1a tr\xEAn Transformer \u0111\u1EA1i di\u1EC7n cho m\u1ED7i token trong m\u1ED9t kho\u1EA3ng v\u0103n b\u1EA3n d\u01B0\u1EDBi d\u1EA1ng m\u1ED9t "),nn=l(Je,"EM",{});var ac=i(nn);Fa=a(ac,"vector nhugns bi\u1EC3u di\u1EC5n t\u1EEB"),ac.forEach(n),Ma=a(Je,". H\xF3a ra ng\u01B0\u1EDDi ta c\xF3 th\u1EC3 \u201Cg\u1ED9p\u201D c\xE1c bi\u1EC3u di\u1EC5n ri\xEAng l\u1EBB \u0111\u1EC3 t\u1EA1o bi\u1EC3u di\u1EC5n vect\u01A1 cho to\xE0n b\u1ED9 c\xE2u, \u0111o\u1EA1n v\u0103n ho\u1EB7c to\xE0n b\u1ED9 (trong m\u1ED9t s\u1ED1 tr\u01B0\u1EDDng h\u1EE3p) t\xE0i li\u1EC7u. Sau \u0111\xF3, c\xE1c ph\xE9p nh\xFAng n\xE0y c\xF3 th\u1EC3 \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 t\xECm c\xE1c t\xE0i li\u1EC7u t\u01B0\u01A1ng t\u1EF1 trong kho t\xE0i li\u1EC7u b\u1EB1ng c\xE1ch t\xEDnh to\xE1n \u0111\u1ED9 t\u01B0\u01A1ng t\u1EF1 c\u1EE7a s\u1EA3n ph\u1EA9m (ho\u1EB7c m\u1ED9t s\u1ED1 ch\u1EC9 s\u1ED1 t\u01B0\u01A1ng t\u1EF1 kh\xE1c) gi\u1EEFa m\u1ED7i bi\u1EC5u di\u1EC5n v\xE0 tr\u1EA3 v\u1EC1 c\xE1c t\xE0i li\u1EC7u c\xF3 \u0111\u1ED9 t\u01B0\u01A1ng \u0111\u1ED3ng l\u1EDBn nh\u1EA5t."),Je.forEach(n),ys=m(t),Ae=l(t,"P",{});var oc=i(Ae);La=a(oc,"Trong ph\u1EA7n n\xE0y, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng c\xE1c bi\u1EC3u di\u1EC5n t\u1EEB \u0111\u1EC3 ph\xE1t tri\u1EC3n m\u1ED9t c\xF4ng c\u1EE5 t\xECm ki\u1EBFm ng\u1EEF ngh\u0129a. C\xE1c c\xF4ng c\u1EE5 t\xECm ki\u1EBFm n\xE0y cung c\u1EA5p m\u1ED9t s\u1ED1 l\u1EE3i th\u1EBF so v\u1EDBi c\xE1c ph\u01B0\u01A1ng ph\xE1p ti\u1EBFp c\u1EADn th\xF4ng th\u01B0\u1EDDng d\u1EF1a tr\xEAn vi\u1EC7c k\u1EBFt h\u1EE3p c\xE1c t\u1EEB kh\xF3a trong m\u1ED9t truy v\u1EA5n v\u1EDBi c\xE1c t\xE0i li\u1EC7u."),oc.forEach(n),$s=m(t),Ct=l(t,"DIV",{class:!0});var xa=i(Ct);Kt=l(xa,"IMG",{class:!0,src:!0,alt:!0}),za=m(xa),Wt=l(xa,"IMG",{class:!0,src:!0,alt:!0}),xa.forEach(n),ks=m(t),St=l(t,"H2",{class:!0});var Ea=i(St);It=l(Ea,"A",{id:!0,class:!0,href:!0});var lc=i(It);sn=l(lc,"SPAN",{});var ic=i(sn);k(Jt.$$.fragment,ic),ic.forEach(n),lc.forEach(n),Ua=m(Ea),an=l(Ea,"SPAN",{});var cc=i(an);Ba=a(cc,"T\u1EA3i v\xE0 chu\u1EA9n b\u1ECB t\u1EADp d\u1EEF li\u1EC7u"),cc.forEach(n),Ea.forEach(n),xs=m(t),Oe=l(t,"P",{});var rc=i(Oe);Ga=a(rc,"\u0110i\u1EC1u \u0111\u1EA7u ti\xEAn ch\xFAng ta c\u1EA7n l\xE0m l\xE0 t\u1EA3i xu\u1ED1ng t\u1EADp d\u1EEF li\u1EC7u v\u1EC1 c\xE1c s\u1EF1 c\u1ED1 GitHub, v\xEC v\u1EADy h\xE3y s\u1EED d\u1EE5ng th\u01B0 vi\u1EC7n \u{1F917} Hub \u0111\u1EC3 gi\u1EA3i quy\u1EBFt URL n\u01A1i t\u1EC7p c\u1EE7a ch\xFAng ta \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF tr\xEAn Hugging Face Hub:"),rc.forEach(n),Es=m(t),k(Qt.$$.fragment,t),ws=m(t),vt=l(t,"P",{});var Qe=i(vt);Va=a(Qe,"V\u1EDBi URL \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF trong "),on=l(Qe,"CODE",{});var hc=i(on);Ya=a(hc,"data_files"),hc.forEach(n),Ka=a(Qe,", sau \u0111\xF3 ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i t\u1EADp d\u1EEF li\u1EC7u t\u1EEB xa b\u1EB1ng ph\u01B0\u01A1ng ph\xE1p \u0111\xE3 \u0111\u01B0\u1EE3c gi\u1EDBi thi\u1EC7u trong "),Ie=l(Qe,"A",{href:!0});var mc=i(Ie);Wa=a(mc,"ph\u1EA7n 2"),mc.forEach(n),Ja=a(Qe,":"),Qe.forEach(n),js=m(t),k(Xt.$$.fragment,t),qs=m(t),k(Zt.$$.fragment,t),Ts=m(t),W=l(t,"P",{});var ct=i(W);Qa=a(ct,"\u1EDE \u0111\xE2y ch\xFAng ta \u0111\xE3 ch\u1EC9 \u0111\u1ECBnh t\xE1ch "),ln=l(ct,"CODE",{});var uc=i(ln);Xa=a(uc,"train"),uc.forEach(n),Za=a(ct," m\u1EB7c \u0111\u1ECBnh trong "),cn=l(ct,"CODE",{});var pc=i(cn);to=a(pc,"load_dataset()"),pc.forEach(n),eo=a(ct,", v\xEC v\u1EADy n\xF3 tr\u1EA3 v\u1EC1 m\u1ED9t "),rn=l(ct,"CODE",{});var dc=i(rn);no=a(dc,"Dataset"),dc.forEach(n),so=a(ct," thay v\xEC "),hn=l(ct,"CODE",{});var gc=i(hn);ao=a(gc,"DatasetDict"),gc.forEach(n),oo=a(ct,". Tr\xECnh t\u1EF1 \u0111\u1EA7u ti\xEAn c\u1EE7a doanh nghi\u1EC7p l\xE0 l\u1ECDc ra c\xE1c y\xEAu c\u1EA7u k\xE9o, v\xEC nh\u1EEFng y\xEAu c\u1EA7u n\xE0y c\xF3 xu h\u01B0\u1EDBng hi\u1EBFm khi \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 tr\u1EA3 l\u1EDDi c\xE1c truy v\u1EA5n c\u1EE7a ng\u01B0\u1EDDi d\xF9ng v\xE0 s\u1EBD t\u1EA1o ra nhi\u1EC5u trong c\xF4ng c\u1EE5 t\xECm ki\u1EBFm m\xECnh. Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng h\xE0m "),mn=l(ct,"CODE",{});var fc=i(mn);lo=a(fc,"Dataset.filter()"),fc.forEach(n),io=a(ct," \u0111\xE3 quen thu\u1ED9c v\u1EDBi b\u1EA1n \u0111\u1EC3 lo\u1EA1i tr\u1EEB c\xE1c h\xE0ng n\xE0y trong t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh. C\xF9ng l\xFAc \u0111\xF3, h\xE3y c\xF9ng l\u1ECDc ra c\xE1c h\xE0ng kh\xF4ng c\xF3 nh\u1EADn x\xE9t, v\xEC nh\u1EEFng h\xE0ng n\xE0y kh\xF4ng cung c\u1EA5p c\xE2u tr\u1EA3 l\u1EDDi cho c\xE1c truy v\u1EA5n c\u1EE7a ng\u01B0\u1EDDi d\xF9ng:"),ct.forEach(n),Ds=m(t),k(te.$$.fragment,t),Cs=m(t),k(ee.$$.fragment,t),Ss=m(t),J=l(t,"P",{});var rt=i(J);co=a(rt,"Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y r\u1EB1ng c\xF3 r\u1EA5t nhi\u1EC1u c\u1ED9t trong t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng ta, h\u1EA7u h\u1EBFt trong s\u1ED1 \u0111\xF3 ch\xFAng ta kh\xF4ng c\u1EA7n ph\u1EA3i x\xE2y d\u1EF1ng c\xF4ng c\u1EE5 t\xECm ki\u1EBFm c\u1EE7a m\xECnh. T\u1EEB g\xF3c \u0111\u1ED9 t\xECm ki\u1EBFm, c\xE1c c\u1ED9t ch\u1EE9a nhi\u1EC1u th\xF4ng tin nh\u1EA5t l\xE0 "),un=l(rt,"CODE",{});var _c=i(un);ro=a(_c,"title"),_c.forEach(n),ho=a(rt,", "),pn=l(rt,"CODE",{});var bc=i(pn);mo=a(bc,"body"),bc.forEach(n),uo=a(rt,", v\xE0 "),dn=l(rt,"CODE",{});var vc=i(dn);po=a(vc,"comments"),vc.forEach(n),go=a(rt,",  trong khi "),gn=l(rt,"CODE",{});var yc=i(gn);fo=a(yc,"html_url"),yc.forEach(n),_o=a(rt," cung c\u1EA5p cho ch\xFAng ta m\u1ED9t li\xEAn k\u1EBFt tr\u1ECF v\u1EC1 ngu\u1ED3n. H\xE3y s\u1EED d\u1EE5ng h\xE0m "),fn=l(rt,"CODE",{});var $c=i(fn);bo=a($c,"Dataset.remove_columns()"),$c.forEach(n),vo=a(rt," \u0111\u1EC3 x\xF3a ph\u1EA7n c\xF2n l\u1EA1i:"),rt.forEach(n),As=m(t),k(ne.$$.fragment,t),Os=m(t),k(se.$$.fragment,t),Is=m(t),X=l(t,"P",{});var Et=i(X);yo=a(Et,"\u0110\u1EC3 t\u1EA1o c\xE1c bi\u1EC3u di\u1EC5n, ch\xFAng ta s\u1EBD b\u1ED5 sung m\u1ED7i nh\u1EADn x\xE9t v\u1EDBi ti\xEAu \u0111\u1EC1 v\xE0 n\u1ED9i dung c\u1EE7a v\u1EA5n \u0111\u1EC1, v\xEC c\xE1c tr\u01B0\u1EDDng n\xE0y th\u01B0\u1EDDng bao g\u1ED3m th\xF4ng tin ng\u1EEF c\u1EA3nh h\u1EEFu \xEDch. V\xEC c\u1ED9t "),_n=l(Et,"CODE",{});var kc=i(_n);$o=a(kc,"comments"),kc.forEach(n),ko=a(Et," c\u1EE7a hi\u1EC7n l\xE0 danh s\xE1ch c\xE1c nh\u1EADn x\xE9t cho t\u1EEBng v\u1EA5n \u0111\u1EC1, ch\xFAng t\xF4i c\u1EA7n kh\xE1m ph\xE1 c\xE1c c\u1ED9t \u0111\u1EC3 m\u1ED7i h\xE0ng bao g\u1ED3m m\u1ED9t tuple "),bn=l(Et,"CODE",{});var xc=i(bn);xo=a(xc,"(html_url, title, body, comment)"),xc.forEach(n),Eo=a(Et,". Trong Pandas, ch\xFAng ta c\xF3 th\u1EC3 th\u1EF1c hi\u1EC7n vi\u1EC7c n\xE0y b\u1EB1ng h\xE0m "),Ht=l(Et,"A",{href:!0,rel:!0});var Ui=i(Ht);wo=a(Ui,"h\xE0m "),vn=l(Ui,"CODE",{});var Ec=i(vn);jo=a(Ec,"DataFrame.explode()"),Ec.forEach(n),Ui.forEach(n),qo=a(Et,", t\u1EA1o m\u1ED9t h\xE0ng m\u1EDBi cho m\u1ED7i ph\u1EA7n t\u1EED trong c\u1ED9t gi\u1ED1ng nh\u01B0 danh s\xE1ch, trong khi sao ch\xE9p t\u1EA5t c\u1EA3 c\xE1c gi\xE1 tr\u1ECB c\u1ED9t kh\xE1c. \u0110\u1EC3 xem \u0111i\u1EC1u n\xE0y ho\u1EA1t \u0111\u1ED9ng, tr\u01B0\u1EDBc ti\xEAn ch\xFAng ta h\xE3y ch\xFAng chuy\u1EC3n th\xE0nh \u0111\u1ECBnh d\u1EA1ng Pandas "),yn=l(Et,"CODE",{});var wc=i(yn);To=a(wc,"DataFrame"),wc.forEach(n),Do=a(Et,":"),Et.forEach(n),Hs=m(t),k(ae.$$.fragment,t),Ps=m(t),Pt=l(t,"P",{});var wa=i(Pt);Co=a(wa,"N\u1EBFu ta ki\u1EC3m tra h\xE0ng \u0111\u1EA7u ti\xEAn trong "),$n=l(wa,"CODE",{});var jc=i($n);So=a(jc,"DataFrame"),jc.forEach(n),Ao=a(wa," n\xE0y, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y c\xF3 b\u1ED1n nh\u1EADn x\xE9t li\xEAn quan \u0111\u1EBFn v\u1EA5n \u0111\u1EC1 n\xE0y:"),wa.forEach(n),Ns=m(t),k(oe.$$.fragment,t),Rs=m(t),k(le.$$.fragment,t),Fs=m(t),Nt=l(t,"P",{});var ja=i(Nt);Oo=a(ja,"Khi ch\xFAng ta kh\xE1m ph\xE1 "),kn=l(ja,"CODE",{});var qc=i(kn);Io=a(qc,"df"),qc.forEach(n),Ho=a(ja,", ch\xFAng t\xF4i mong \u0111\u1EE3i nh\u1EADn \u0111\u01B0\u1EE3c m\u1ED9t h\xE0ng cho m\u1ED7i nh\u1EADn x\xE9t n\xE0y. H\xE3y ki\u1EC3m tra xem n\xF3 \u0111\xE3 \u0111\xFAng ch\u01B0a:"),ja.forEach(n),Ms=m(t),k(ie.$$.fragment,t),Ls=m(t),tt=l(t,"TABLE",{border:!0,class:!0,style:!0});var qa=i(tt);xn=l(qa,"THEAD",{});var Tc=i(xn);Z=l(Tc,"TR",{style:!0});var wt=i(Z);zs=l(wt,"TH",{}),i(zs).forEach(n),Po=m(wt),En=l(wt,"TH",{});var Dc=i(En);No=a(Dc,"html_url"),Dc.forEach(n),Ro=m(wt),wn=l(wt,"TH",{});var Cc=i(wn);Fo=a(Cc,"title"),Cc.forEach(n),Mo=m(wt),jn=l(wt,"TH",{});var Sc=i(jn);Lo=a(Sc,"comments"),Sc.forEach(n),zo=m(wt),qn=l(wt,"TH",{});var Ac=i(qn);Uo=a(Ac,"body"),Ac.forEach(n),wt.forEach(n),Tc.forEach(n),Bo=m(qa),mt=l(qa,"TBODY",{});var Gt=i(mt);et=l(Gt,"TR",{});var jt=i(et);Tn=l(jt,"TH",{});var Oc=i(Tn);Go=a(Oc,"0"),Oc.forEach(n),Vo=m(jt),Dn=l(jt,"TD",{});var Ic=i(Dn);Yo=a(Ic,"https://github.com/huggingface/datasets/issues/2787"),Ic.forEach(n),Ko=m(jt),Cn=l(jt,"TD",{});var Hc=i(Cn);Wo=a(Hc,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Hc.forEach(n),Jo=m(jt),Sn=l(jt,"TD",{});var Pc=i(Sn);Qo=a(Pc,"the bug code locate in \uFF1A\\r\\n    if data_args.task_name is not None..."),Pc.forEach(n),Xo=m(jt),An=l(jt,"TD",{});var Nc=i(An);Zo=a(Nc,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Nc.forEach(n),jt.forEach(n),tl=m(Gt),nt=l(Gt,"TR",{});var qt=i(nt);On=l(qt,"TH",{});var Rc=i(On);el=a(Rc,"1"),Rc.forEach(n),nl=m(qt),In=l(qt,"TD",{});var Fc=i(In);sl=a(Fc,"https://github.com/huggingface/datasets/issues/2787"),Fc.forEach(n),al=m(qt),Hn=l(qt,"TD",{});var Mc=i(Hn);ol=a(Mc,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Mc.forEach(n),ll=m(qt),Pn=l(qt,"TD",{});var Lc=i(Pn);il=a(Lc,"Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com..."),Lc.forEach(n),cl=m(qt),Nn=l(qt,"TD",{});var zc=i(Nn);rl=a(zc,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),zc.forEach(n),qt.forEach(n),hl=m(Gt),st=l(Gt,"TR",{});var Tt=i(st);Rn=l(Tt,"TH",{});var Uc=i(Rn);ml=a(Uc,"2"),Uc.forEach(n),ul=m(Tt),Fn=l(Tt,"TD",{});var Bc=i(Fn);pl=a(Bc,"https://github.com/huggingface/datasets/issues/2787"),Bc.forEach(n),dl=m(Tt),Mn=l(Tt,"TD",{});var Gc=i(Mn);gl=a(Gc,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Gc.forEach(n),fl=m(Tt),Ln=l(Tt,"TD",{});var Vc=i(Ln);_l=a(Vc,"cannot connect\uFF0Ceven by Web browser\uFF0Cplease check that  there is some  problems\u3002"),Vc.forEach(n),bl=m(Tt),zn=l(Tt,"TD",{});var Yc=i(zn);vl=a(Yc,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Yc.forEach(n),Tt.forEach(n),yl=m(Gt),at=l(Gt,"TR",{});var Dt=i(at);Un=l(Dt,"TH",{});var Kc=i(Un);$l=a(Kc,"3"),Kc.forEach(n),kl=m(Dt),Bn=l(Dt,"TD",{});var Wc=i(Bn);xl=a(Wc,"https://github.com/huggingface/datasets/issues/2787"),Wc.forEach(n),El=m(Dt),Gn=l(Dt,"TD",{});var Jc=i(Gn);wl=a(Jc,"ConnectionError: Couldn't reach https://raw.githubusercontent.com"),Jc.forEach(n),jl=m(Dt),Vn=l(Dt,"TD",{});var Qc=i(Vn);ql=a(Qc,"I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem..."),Qc.forEach(n),Tl=m(Dt),Yn=l(Dt,"TD",{});var Xc=i(Yn);Dl=a(Xc,"Hello,\\r\\nI am trying to run run_glue.py and it gives me this error..."),Xc.forEach(n),Dt.forEach(n),Gt.forEach(n),qa.forEach(n),Us=m(t),lt=l(t,"P",{});var Vt=i(lt);Cl=a(Vt,"Tuy\u1EC7t v\u1EDDi, ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y c\xE1c h\xE0ng \u0111\xE3 \u0111\u01B0\u1EE3c nh\xE2n r\u1ED9ng, v\u1EDBi c\u1ED9t "),Kn=l(Vt,"CODE",{});var Zc=i(Kn);Sl=a(Zc,"comments"),Zc.forEach(n),Al=a(Vt," ch\u1EE9a c\xE1c nh\u1EADn x\xE9t ri\xEAng l\u1EBB! B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 ho\xE0n th\xE0nh v\u1EDBi Pandas, ch\xFAng ta c\xF3 th\u1EC3 nhanh ch\xF3ng chuy\u1EC3n tr\u1EDF l\u1EA1i "),Wn=l(Vt,"CODE",{});var tr=i(Wn);Ol=a(tr,"Dataset"),tr.forEach(n),Il=a(Vt," b\u1EB1ng c\xE1ch t\u1EA3i "),Jn=l(Vt,"CODE",{});var er=i(Jn);Hl=a(er,"DataFrame"),er.forEach(n),Pl=a(Vt," v\xE0o b\u1ED9 nh\u1EDB:"),Vt.forEach(n),Bs=m(t),k(ce.$$.fragment,t),Gs=m(t),k(re.$$.fragment,t),Vs=m(t),He=l(t,"P",{});var nr=i(He);Nl=a(nr,"\u0110\u01B0\u1EE3c r\u1ED3i, \u0111i\u1EC1u n\xE0y \u0111\xE3 cho ch\xFAng ta v\xE0i ngh\xECn nh\u1EADn x\xE9t \u0111\u1EC3 l\xE0m vi\u1EC7c c\xF9ng!"),nr.forEach(n),Ys=m(t),k(Rt.$$.fragment,t),Ks=m(t),Ft=l(t,"P",{});var Ta=i(Ft);Rl=a(Ta,"B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 m\u1ED9t nh\u1EADn x\xE9t tr\xEAn m\u1ED7i h\xE0ng, h\xE3y t\u1EA1o m\u1ED9t c\u1ED9t "),Qn=l(Ta,"CODE",{});var sr=i(Qn);Fl=a(sr,"comments_length"),sr.forEach(n),Ml=a(Ta," m\u1EDBi ch\u1EE9a s\u1ED1 t\u1EEB tr\xEAn m\u1ED7i nh\u1EADn x\xE9t:"),Ta.forEach(n),Ws=m(t),k(he.$$.fragment,t),Js=m(t),Pe=l(t,"P",{});var ar=i(Pe);Ll=a(ar,"Ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng c\u1ED9t m\u1EDBi n\xE0y \u0111\u1EC3 l\u1ECDc ra c\xE1c nh\u1EADn x\xE9t ng\u1EAFn, th\u01B0\u1EDDng bao g\u1ED3m nh\u1EEFng th\u1EE9 nh\u01B0 \u201Ccc @lewtun\u201D ho\u1EB7c \u201CThanks!\u201D kh\xF4ng li\xEAn quan \u0111\u1EBFn c\xF4ng c\u1EE5 t\xECm ki\u1EBFm c\u1EE7a m\xECnh. Kh\xF4ng c\xF3 con s\u1ED1 ch\xEDnh x\xE1c \u0111\u1EC3 ch\u1ECDn cho b\u1ED9 l\u1ECDc, nh\u01B0ng kho\u1EA3ng 15 t\u1EEB c\xF3 v\u1EBB nh\u01B0 l\xE0 m\u1ED9t kh\u1EDFi \u0111\u1EA7u t\u1ED1t:"),ar.forEach(n),Qs=m(t),k(me.$$.fragment,t),Xs=m(t),k(ue.$$.fragment,t),Zs=m(t),yt=l(t,"P",{});var Xe=i(yt);zl=a(Xe,"Sau khi d\u1ECDn d\u1EB9p t\u1EADp d\u1EEF li\u1EC7u m\u1ED9t ch\xFAt, h\xE3y gh\xE9p ti\xEAu \u0111\u1EC1, m\xF4 t\u1EA3 v\xE0 nh\u1EADn x\xE9t c\u1EE7a v\u1EA5n \u0111\u1EC1 v\u1EDBi nhau trong m\u1ED9t c\u1ED9t "),Xn=l(Xe,"CODE",{});var or=i(Xn);Ul=a(or,"text"),or.forEach(n),Bl=a(Xe," m\u1EDBi. Nh\u01B0 th\u01B0\u1EDDng l\u1EC7, ch\xFAng ta s\u1EBD vi\u1EBFt m\u1ED9t h\xE0m \u0111\u01A1n gi\u1EA3n m\xE0 ch\xFAng ta c\xF3 th\u1EC3 truy\u1EC1n v\xE0o "),Zn=l(Xe,"CODE",{});var lr=i(Zn);Gl=a(lr,"Dataset.map()"),lr.forEach(n),Vl=a(Xe,":"),Xe.forEach(n),ta=m(t),k(pe.$$.fragment,t),ea=m(t),Ne=l(t,"P",{});var ir=i(Ne);Yl=a(ir,"Cu\u1ED1i c\xF9ng th\xEC ch\xFAng ta c\u0169ng \u0111\xE3 s\u1EB5n s\xE0ng \u0111\u1EC3 t\u1EA1o m\u1ED9t s\u1ED1 bi\u1EC3u \u0111i\u1EC5n! Ch\xFAng ta h\xE3y xem n\xE0o."),ir.forEach(n),na=m(t),At=l(t,"H2",{class:!0});var Da=i(At);Mt=l(Da,"A",{id:!0,class:!0,href:!0});var cr=i(Mt);ts=l(cr,"SPAN",{});var rr=i(ts);k(de.$$.fragment,rr),rr.forEach(n),cr.forEach(n),Kl=m(Da),es=l(Da,"SPAN",{});var hr=i(es);Wl=a(hr,"T\u1EA1o ra bi\u1EC3u di\u1EC5n v\u0103n b\u1EA3n"),hr.forEach(n),Da.forEach(n),sa=m(t),V=l(t,"P",{});var Q=i(V);Jl=a(Q,"Ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),Re=l(Q,"A",{href:!0});var mr=i(Re);Ql=a(mr,"Ch\u01B0\u01A1ng 2"),mr.forEach(n),Xl=a(Q," r\u1EB1ng ta c\xF3 th\u1EC3 nh\u1EADn \u0111\u01B0\u1EE3c token bi\u1EC5u di\u1EC5n nh\xFAng b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng l\u1EDBp "),ns=l(Q,"CODE",{});var ur=i(ns);Zl=a(ur,"AutoModel"),ur.forEach(n),ti=a(Q,". T\u1EA5t c\u1EA3 nh\u1EEFng g\xEC ch\xFAng ta c\u1EA7n l\xE0m l\xE0 ch\u1ECDn m\u1ED9t checkpoint ph\xF9 h\u1EE3p \u0111\u1EC3 t\u1EA3i m\xF4 h\xECnh t\u1EEB \u0111\xF3. May m\u1EAFn thay, c\xF3 m\u1ED9t th\u01B0 vi\u1EC7n t\xEAn l\xE0  "),ss=l(Q,"CODE",{});var pr=i(ss);ei=a(pr,"sentence-transformers"),pr.forEach(n),ni=a(Q," d\xE0nh ri\xEAng cho vi\u1EC7c t\u1EA1o c\xE1c bi\u1EC3u di\u1EC5n n\xE0y. Nh\u01B0 \u0111\u01B0\u1EE3c m\xF4 t\u1EA3 trong "),ge=l(Q,"A",{href:!0,rel:!0});var dr=i(ge);si=a(dr,"t\xE0i li\u1EC7u"),dr.forEach(n),ai=a(Q," c\u1EE7a th\u01B0 vi\u1EC7n, tr\u01B0\u1EDDng h\u1EE3p s\u1EED d\u1EE5ng c\u1EE7a ta l\xE0 m\u1ED9t v\xED d\u1EE5 v\u1EC1 "),as=l(Q,"EM",{});var gr=i(as);oi=a(gr,"t\xECm ki\u1EBFm ng\u1EEF ngh\u0129a phi \u0111\u1ED1i x\u1EE9ng"),gr.forEach(n),li=a(Q," b\u1EDFi v\xEC ch\xFAng ta c\xF3 m\u1ED9t truy v\u1EA5n ng\u1EAFn c\xF3 c\xE2u tr\u1EA3 l\u1EDDi ta mu\u1ED1n t\xECm th\u1EA5y trong m\u1ED9t t\xE0i li\u1EC7u l\u1EA1i d\xE0i h\u01A1n nhi\u1EC1u, ch\u1EB3ng h\u1EA1n nh\u01B0 m\u1ED9t nh\u1EADn x\xE9t v\u1EC1 v\u1EA5n \u0111\u1EC1. "),fe=l(Q,"A",{href:!0,rel:!0});var fr=i(fe);ii=a(fr,"B\u1EA3ng t\u1ED5ng quan v\u1EC1 m\xF4 h\xECnh"),fr.forEach(n),ci=a(Q," trong ph\u1EA7n t\xE0i li\u1EC7u ch\u1EC9 ra r\u1EB1ng checkpoint "),os=l(Q,"CODE",{});var _r=i(os);ri=a(_r,"multi-qa-mpnet-base-dot-v1"),_r.forEach(n),hi=a(Q," c\xF3 hi\u1EC7u su\u1EA5t t\u1ED1t nh\u1EA5t cho t\xECm ki\u1EBFm ng\u1EEF ngh\u0129a, v\xEC v\u1EADy ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng n\xF3 cho \u1EE9ng d\u1EE5ng c\u1EE7a m\xECnh. Ch\xFAng ta c\u0169ng s\u1EBD t\u1EA3i tokenizer b\u1EB1ng c\xE1ch s\u1EED d\u1EE5ng c\xF9ng m\u1ED9t checkpoint:"),Q.forEach(n),aa=m(t),pt.l(t),Fe=m(t),$t=l(t,"P",{});var Ze=i($t);mi=a(Ze,"Nh\u01B0 \u0111\xE3 \u0111\u1EC1 c\u1EADp tr\u01B0\u1EDBc \u0111\xF3, ch\xFAng ta mu\u1ED1n bi\u1EC3u di\u1EC5n m\u1ED7i m\u1EE5c trong kho d\u1EEF li\u1EC7u c\xE1c v\u1EA5n \u0111\u1EC1 GitHub c\u1EE7a m\xECnh d\u01B0\u1EDBi d\u1EA1ng m\u1ED9t vect\u01A1 duy nh\u1EA5t, v\xEC v\u1EADy ch\xFAng ta c\u1EA7n \u201Cg\u1ED9p\u201D ho\u1EB7c t\xEDnh trung b\xECnh c\xE1c l\u1EA7n bi\u1EC5u di\u1EC5n token theo m\u1ED9t c\xE1ch n\xE0o \u0111\xF3. M\u1ED9t c\xE1ch ti\u1EBFp c\u1EADn ph\u1ED5 bi\u1EBFn l\xE0 th\u1EF1c hi\u1EC7n "),ls=l(Ze,"EM",{});var br=i(ls);ui=a(br,"CLS pooling"),br.forEach(n),pi=a(Ze," tr\xEAn \u0111\u1EA7u ra c\u1EE7a m\xF4 h\xECnh, n\u01A1i ta ch\u1EC9 c\u1EA7n thu th\u1EADp tr\u1EA1ng th\xE1i \u1EA9n cu\u1ED1i c\xF9ng cho token \u0111\u1EB7c bi\u1EC7t "),is=l(Ze,"CODE",{});var vr=i(is);di=a(vr,"[CLS]"),vr.forEach(n),gi=a(Ze,". H\xE0m sau th\u1EF1c hi\u1EC7n th\u1EE7 thu\u1EADt n\xE0y cho ch\xFAng ta:"),Ze.forEach(n),oa=m(t),k(_e.$$.fragment,t),la=m(t),Me=l(t,"P",{});var yr=i(Me);fi=a(yr,"Ti\u1EBFp theo, ch\xFAng t\xF4i s\u1EBD t\u1EA1o m\u1ED9t ch\u1EE9c n\u0103ng tr\u1EE3 gi\xFAp s\u1EBD tokanize danh s\xE1ch c\xE1c t\xE0i li\u1EC7u, \u0111\u1EB7t c\xE1c tensor tr\xEAn GPU, \u0111\u01B0a ch\xFAng v\xE0o m\xF4 h\xECnh v\xE0 cu\u1ED1i c\xF9ng \xE1p d\u1EE5ng CLS g\u1ED9p cho c\xE1c \u0111\u1EA7u ra:"),yr.forEach(n),ia=m(t),gt.l(t),Le=m(t),ze=l(t,"P",{});var $r=i(ze);_i=a($r,"L\u01B0u \xFD r\u1EB1ng ch\xFAng ta \u0111\xE3 chuy\u1EC3n \u0111\u1ED5i c\xE1c bi\u1EC3u di\u1EC5n sang th\xE0nh m\u1EA3ng NumPy - \u0111\xF3 l\xE0 v\xEC \u{1F917} Datasets y\xEAu c\u1EA7u \u0111\u1ECBnh d\u1EA1ng n\xE0y khi ta c\u1ED1 g\u1EAFng l\u1EADp ch\u1EC9 m\u1EE5c ch\xFAng b\u1EB1ng FAISS, \u0111i\u1EC1u m\xE0 ta s\u1EBD th\u1EF1c hi\u1EC7n ti\u1EBFp theo."),$r.forEach(n),ca=m(t),Ot=l(t,"H2",{class:!0});var Ca=i(Ot);Lt=l(Ca,"A",{id:!0,class:!0,href:!0});var kr=i(Lt);cs=l(kr,"SPAN",{});var xr=i(cs);k(be.$$.fragment,xr),xr.forEach(n),kr.forEach(n),bi=m(Ca),rs=l(Ca,"SPAN",{});var Er=i(rs);vi=a(Er,"S\u1EED d\u1EE5ng FAISS \u0111\u1EC3 t\xECm ki\u1EBFm \u0111i\u1EC3m t\u01B0\u01A1ng \u0111\u1ED3ng hi\u1EC7u qu\u1EA3"),Er.forEach(n),Ca.forEach(n),ra=m(t),kt=l(t,"P",{});var tn=i(kt);yi=a(tn,"B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u v\u1EC1 c\xE1c bi\u1EC3u di\u1EC5n, ch\xFAng ta c\u1EA7n m\u1ED9t s\u1ED1 c\xE1ch \u0111\u1EC3 t\xECm ki\u1EBFm ch\xFAng. \u0110\u1EC3 l\xE0m \u0111i\u1EC1u n\xE0y, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng m\u1ED9t c\u1EA5u tr\xFAc d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t trong \u{1F917} Datasets \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),hs=l(tn,"EM",{});var wr=i(hs);$i=a(wr,"FAISS index"),wr.forEach(n),ki=a(tn,". "),ve=l(tn,"A",{href:!0,rel:!0});var jr=i(ve);xi=a(jr,"FAISS"),jr.forEach(n),Ei=a(tn," (vi\u1EBFt t\u1EAFt c\u1EE7a Facebook AI Similarity Search) l\xE0 m\u1ED9t th\u01B0 vi\u1EC7n cung c\u1EA5p c\xE1c thu\u1EADt to\xE1n hi\u1EC7u qu\u1EA3 \u0111\u1EC3 nhanh ch\xF3ng t\xECm ki\u1EBFm v\xE0 ph\xE2n c\u1EE5m c\xE1c vect\u01A1 nh\xFAng bi\u1EC3u di\u1EC5n."),tn.forEach(n),ha=m(t),it=l(t,"P",{});var Yt=i(it);wi=a(Yt,"\xDD t\u01B0\u1EDFng c\u01A1 b\u1EA3n \u0111\u1EB1ng sau FAISS l\xE0 t\u1EA1o ra m\u1ED9t c\u1EA5u tr\xFAc d\u1EEF li\u1EC7u \u0111\u1EB7c bi\u1EC7t \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),ms=l(Yt,"EM",{});var qr=i(ms);ji=a(qr,"index"),qr.forEach(n),qi=a(Yt," hay "),us=l(Yt,"EM",{});var Tr=i(us);Ti=a(Tr,"ch\u1EC9 m\u1EE5c"),Tr.forEach(n),Di=a(Yt," cho ph\xE9p ng\u01B0\u1EDDi ta t\xECm th\u1EA5y c\xE1c bi\u1EC3u di\u1EC5n nh\xFAng n\xE0o t\u01B0\u01A1ng t\u1EF1 nh\u01B0 bi\u1EC3u di\u1EC5n nh\xFAng \u0111\u1EA7u v\xE0o. T\u1EA1o ch\u1EC9 m\u1EE5c FAISS trong \u{1F917} Datasets r\u1EA5t \u0111\u01A1n gi\u1EA3n - ta s\u1EED d\u1EE5ng h\xE0m "),ps=l(Yt,"CODE",{});var Dr=i(ps);Ci=a(Dr,"Dataset.add_faiss_index()"),Dr.forEach(n),Si=a(Yt," v\xE0 ch\u1EC9 \u0111\u1ECBnh c\u1ED9t n\xE0o trong t\u1EADp d\u1EEF li\u1EC7u m\xE0 ta mu\u1ED1n l\u1EADp ch\u1EC9 m\u1EE5c:"),Yt.forEach(n),ma=m(t),k(ye.$$.fragment,t),ua=m(t),zt=l(t,"P",{});var Sa=i(zt);Ai=a(Sa,"B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 th\u1EF1c hi\u1EC7n c\xE1c truy v\u1EA5n tr\xEAn ch\u1EC9 m\u1EE5c n\xE0y b\u1EB1ng c\xE1ch th\u1EF1c hi\u1EC7n tra c\u1EE9u nh\u1EEFng m\u1EABu l\xE2n c\u1EADn nh\u1EA5t th\xF4ng qua h\xE0m "),ds=l(Sa,"CODE",{});var Cr=i(ds);Oi=a(Cr,"Dataset.get_nearest_examples()"),Cr.forEach(n),Ii=a(Sa,". H\xE3y ki\u1EC3m tra \u0111i\u1EC1u n\xE0y b\u1EB1ng c\xE1ch bi\u1EC3u di\u1EC5n m\u1ED9t c\xE2u h\u1ECFi nh\u01B0 sau:"),Sa.forEach(n),pa=m(t),_t.l(t),Ue=m(t),Be=l(t,"P",{});var Sr=i(Be);Hi=a(Sr,"C\u0169ng gi\u1ED1ng nh\u01B0 c\xE1c t\xE0i li\u1EC7u, gi\u1EDD \u0111\xE2y ch\xFAng ta c\xF3 m\u1ED9t vect\u01A1 768 chi\u1EC1u \u0111\u1EA1i di\u1EC7n cho truy v\u1EA5n, m\xE0 ch\xFAng ta c\xF3 th\u1EC3 so s\xE1nh v\u1EDBi to\xE0n b\u1ED9 kho d\u1EEF li\u1EC7u \u0111\u1EC3 t\xECm ra c\xE1c c\xE1ch bi\u1EC3u di\u1EC5n t\u01B0\u01A1ng t\u1EF1 nh\u1EA5t:"),Sr.forEach(n),da=m(t),k($e.$$.fragment,t),ga=m(t),xt=l(t,"P",{});var en=i(xt);Pi=a(en,"H\xE0m "),gs=l(en,"CODE",{});var Ar=i(gs);Ni=a(Ar,"Dataset.get_nearest_examples()"),Ar.forEach(n),Ri=a(en," tr\u1EA3 v\u1EC1 m\u1ED9t lo\u1EA1t \u0111i\u1EC3m x\u1EBFp h\u1EA1ng s\u1EF1 t\u01B0\u01A1ng \u0111\u1ED3ng gi\u1EEFa truy v\u1EA5n v\xE0 t\xE0i li\u1EC7u v\xE0 m\u1ED9t t\u1EADp h\u1EE3p c\xE1c m\u1EABu t\u01B0\u01A1ng \u1EE9ng (\u1EDF \u0111\xE2y, l\xE0 5 k\u1EBFt qu\u1EA3 ph\xF9 h\u1EE3p nh\u1EA5t). H\xE3y thu th\u1EADp nh\u1EEFng th\u1EE9 n\xE0y v\xE0o m\u1ED9t "),fs=l(en,"CODE",{});var Or=i(fs);Fi=a(Or,"pandas.DataFrame"),Or.forEach(n),Mi=a(en," \u0111\u1EC3 ch\xFAng ta c\xF3 th\u1EC3 d\u1EC5 d\xE0ng s\u1EAFp x\u1EBFp ch\xFAng:"),en.forEach(n),fa=m(t),k(ke.$$.fragment,t),_a=m(t),Ge=l(t,"P",{});var Ir=i(Ge);Li=a(Ir,"B\xE2y gi\u1EDD ch\xFAng ta c\xF3 th\u1EC3 l\u1EB7p l\u1EA1i m\u1ED9t v\xE0i h\xE0ng \u0111\u1EA7u ti\xEAn \u0111\u1EC3 xem truy v\u1EA5n c\u1EE7a ch\xFAng ta kh\u1EDBp v\u1EDBi c\xE1c nh\u1EADn x\xE9t c\xF3 s\u1EB5n nh\u01B0 th\u1EBF n\xE0o:"),Ir.forEach(n),ba=m(t),k(xe.$$.fragment,t),va=m(t),k(Ee.$$.fragment,t),ya=m(t),Ve=l(t,"P",{});var Hr=i(Ve);zi=a(Hr,"Kh\xF4ng t\u1EC7! L\u1EA7n truy c\u1EADp th\u1EE9 hai c\u1EE7a ch\xFAng ta d\u01B0\u1EDDng nh\u01B0 ph\xF9 h\u1EE3p v\u1EDBi truy v\u1EA5n."),Hr.forEach(n),$a=m(t),k(Ut.$$.fragment,t),this.h()},h(){w(u,"name","hf:doc:metadata"),w(u,"content",JSON.stringify(sh)),w(v,"id","tm-kim-ng-ngha-vi-faiss"),w(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(v,"href","#tm-kim-ng-ngha-vi-faiss"),w(g,"class","relative group"),w(D,"href","/course/chapter5/5"),w(C,"id","s-dng-nhng-biu-din-t-cho-tm-kim-ng-ngha"),w(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(C,"href","#s-dng-nhng-biu-din-t-cho-tm-kim-ng-ngha"),w(p,"class","relative group"),w(Se,"href","/course/chapter1"),w(Kt,"class","block dark:hidden"),Pr(Kt.src,Bi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg")||w(Kt,"src",Bi),w(Kt,"alt","Semantic search."),w(Wt,"class","hidden dark:block"),Pr(Wt.src,Gi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg")||w(Wt,"src",Gi),w(Wt,"alt","Semantic search."),w(Ct,"class","flex justify-center"),w(It,"id","ti-v-chun-b-tp-d-liu"),w(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(It,"href","#ti-v-chun-b-tp-d-liu"),w(St,"class","relative group"),w(Ie,"href","/course/chapter5/2"),w(Ht,"href","https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html"),w(Ht,"rel","nofollow"),Aa(Z,"text-align","right"),w(tt,"border","1"),w(tt,"class","dataframe"),Aa(tt,"table-layout","fixed"),Aa(tt,"word-wrap","break-word"),Aa(tt,"width","100%"),w(Mt,"id","to-ra-biu-din-vn-bn"),w(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Mt,"href","#to-ra-biu-din-vn-bn"),w(At,"class","relative group"),w(Re,"href","/course/chapter2"),w(ge,"href","https://www.sbert.net/examples/appices/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search"),w(ge,"rel","nofollow"),w(fe,"href","https://www.sbert.net/docs/pretrained_models.html#model-overview"),w(fe,"rel","nofollow"),w(Lt,"id","s-dng-faiss-tm-kim-im-tng-ng-hiu-qu"),w(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Lt,"href","#s-dng-faiss-tm-kim-im-tng-ng-hiu-qu"),w(Ot,"class","relative group"),w(ve,"href","https://faiss.ai/"),w(ve,"rel","nofollow")},m(t,c){e(document.head,u),r(t,y,c),x(d,t,c),r(t,j,c),r(t,g,c),e(g,v),e(v,I),x(f,I,null),e(g,S),e(g,q),e(q,L),r(t,H,c),we[A].m(t,c),r(t,N,c),r(t,F,c),e(F,B),e(F,D),e(D,R),e(F,Y),r(t,U,c),x(T,t,c),r(t,G,c),r(t,p,c),e(p,C),e(C,M),x(K,M,null),e(p,ot),e(p,ht),e(ht,Ha),r(t,vs,c),r(t,bt,c),e(bt,Pa),e(bt,Se),e(Se,Na),e(bt,Ra),e(bt,nn),e(nn,Fa),e(bt,Ma),r(t,ys,c),r(t,Ae,c),e(Ae,La),r(t,$s,c),r(t,Ct,c),e(Ct,Kt),e(Ct,za),e(Ct,Wt),r(t,ks,c),r(t,St,c),e(St,It),e(It,sn),x(Jt,sn,null),e(St,Ua),e(St,an),e(an,Ba),r(t,xs,c),r(t,Oe,c),e(Oe,Ga),r(t,Es,c),x(Qt,t,c),r(t,ws,c),r(t,vt,c),e(vt,Va),e(vt,on),e(on,Ya),e(vt,Ka),e(vt,Ie),e(Ie,Wa),e(vt,Ja),r(t,js,c),x(Xt,t,c),r(t,qs,c),x(Zt,t,c),r(t,Ts,c),r(t,W,c),e(W,Qa),e(W,ln),e(ln,Xa),e(W,Za),e(W,cn),e(cn,to),e(W,eo),e(W,rn),e(rn,no),e(W,so),e(W,hn),e(hn,ao),e(W,oo),e(W,mn),e(mn,lo),e(W,io),r(t,Ds,c),x(te,t,c),r(t,Cs,c),x(ee,t,c),r(t,Ss,c),r(t,J,c),e(J,co),e(J,un),e(un,ro),e(J,ho),e(J,pn),e(pn,mo),e(J,uo),e(J,dn),e(dn,po),e(J,go),e(J,gn),e(gn,fo),e(J,_o),e(J,fn),e(fn,bo),e(J,vo),r(t,As,c),x(ne,t,c),r(t,Os,c),x(se,t,c),r(t,Is,c),r(t,X,c),e(X,yo),e(X,_n),e(_n,$o),e(X,ko),e(X,bn),e(bn,xo),e(X,Eo),e(X,Ht),e(Ht,wo),e(Ht,vn),e(vn,jo),e(X,qo),e(X,yn),e(yn,To),e(X,Do),r(t,Hs,c),x(ae,t,c),r(t,Ps,c),r(t,Pt,c),e(Pt,Co),e(Pt,$n),e($n,So),e(Pt,Ao),r(t,Ns,c),x(oe,t,c),r(t,Rs,c),x(le,t,c),r(t,Fs,c),r(t,Nt,c),e(Nt,Oo),e(Nt,kn),e(kn,Io),e(Nt,Ho),r(t,Ms,c),x(ie,t,c),r(t,Ls,c),r(t,tt,c),e(tt,xn),e(xn,Z),e(Z,zs),e(Z,Po),e(Z,En),e(En,No),e(Z,Ro),e(Z,wn),e(wn,Fo),e(Z,Mo),e(Z,jn),e(jn,Lo),e(Z,zo),e(Z,qn),e(qn,Uo),e(tt,Bo),e(tt,mt),e(mt,et),e(et,Tn),e(Tn,Go),e(et,Vo),e(et,Dn),e(Dn,Yo),e(et,Ko),e(et,Cn),e(Cn,Wo),e(et,Jo),e(et,Sn),e(Sn,Qo),e(et,Xo),e(et,An),e(An,Zo),e(mt,tl),e(mt,nt),e(nt,On),e(On,el),e(nt,nl),e(nt,In),e(In,sl),e(nt,al),e(nt,Hn),e(Hn,ol),e(nt,ll),e(nt,Pn),e(Pn,il),e(nt,cl),e(nt,Nn),e(Nn,rl),e(mt,hl),e(mt,st),e(st,Rn),e(Rn,ml),e(st,ul),e(st,Fn),e(Fn,pl),e(st,dl),e(st,Mn),e(Mn,gl),e(st,fl),e(st,Ln),e(Ln,_l),e(st,bl),e(st,zn),e(zn,vl),e(mt,yl),e(mt,at),e(at,Un),e(Un,$l),e(at,kl),e(at,Bn),e(Bn,xl),e(at,El),e(at,Gn),e(Gn,wl),e(at,jl),e(at,Vn),e(Vn,ql),e(at,Tl),e(at,Yn),e(Yn,Dl),r(t,Us,c),r(t,lt,c),e(lt,Cl),e(lt,Kn),e(Kn,Sl),e(lt,Al),e(lt,Wn),e(Wn,Ol),e(lt,Il),e(lt,Jn),e(Jn,Hl),e(lt,Pl),r(t,Bs,c),x(ce,t,c),r(t,Gs,c),x(re,t,c),r(t,Vs,c),r(t,He,c),e(He,Nl),r(t,Ys,c),x(Rt,t,c),r(t,Ks,c),r(t,Ft,c),e(Ft,Rl),e(Ft,Qn),e(Qn,Fl),e(Ft,Ml),r(t,Ws,c),x(he,t,c),r(t,Js,c),r(t,Pe,c),e(Pe,Ll),r(t,Qs,c),x(me,t,c),r(t,Xs,c),x(ue,t,c),r(t,Zs,c),r(t,yt,c),e(yt,zl),e(yt,Xn),e(Xn,Ul),e(yt,Bl),e(yt,Zn),e(Zn,Gl),e(yt,Vl),r(t,ta,c),x(pe,t,c),r(t,ea,c),r(t,Ne,c),e(Ne,Yl),r(t,na,c),r(t,At,c),e(At,Mt),e(Mt,ts),x(de,ts,null),e(At,Kl),e(At,es),e(es,Wl),r(t,sa,c),r(t,V,c),e(V,Jl),e(V,Re),e(Re,Ql),e(V,Xl),e(V,ns),e(ns,Zl),e(V,ti),e(V,ss),e(ss,ei),e(V,ni),e(V,ge),e(ge,si),e(V,ai),e(V,as),e(as,oi),e(V,li),e(V,fe),e(fe,ii),e(V,ci),e(V,os),e(os,ri),e(V,hi),r(t,aa,c),je[ut].m(t,c),r(t,Fe,c),r(t,$t,c),e($t,mi),e($t,ls),e(ls,ui),e($t,pi),e($t,is),e(is,di),e($t,gi),r(t,oa,c),x(_e,t,c),r(t,la,c),r(t,Me,c),e(Me,fi),r(t,ia,c),qe[dt].m(t,c),r(t,Le,c),r(t,ze,c),e(ze,_i),r(t,ca,c),r(t,Ot,c),e(Ot,Lt),e(Lt,cs),x(be,cs,null),e(Ot,bi),e(Ot,rs),e(rs,vi),r(t,ra,c),r(t,kt,c),e(kt,yi),e(kt,hs),e(hs,$i),e(kt,ki),e(kt,ve),e(ve,xi),e(kt,Ei),r(t,ha,c),r(t,it,c),e(it,wi),e(it,ms),e(ms,ji),e(it,qi),e(it,us),e(us,Ti),e(it,Di),e(it,ps),e(ps,Ci),e(it,Si),r(t,ma,c),x(ye,t,c),r(t,ua,c),r(t,zt,c),e(zt,Ai),e(zt,ds),e(ds,Oi),e(zt,Ii),r(t,pa,c),Te[ft].m(t,c),r(t,Ue,c),r(t,Be,c),e(Be,Hi),r(t,da,c),x($e,t,c),r(t,ga,c),r(t,xt,c),e(xt,Pi),e(xt,gs),e(gs,Ni),e(xt,Ri),e(xt,fs),e(fs,Fi),e(xt,Mi),r(t,fa,c),x(ke,t,c),r(t,_a,c),r(t,Ge,c),e(Ge,Li),r(t,ba,c),x(xe,t,c),r(t,va,c),x(Ee,t,c),r(t,ya,c),r(t,Ve,c),e(Ve,zi),r(t,$a,c),x(Ut,t,c),ka=!0},p(t,[c]){const De={};c&1&&(De.fw=t[0]),d.$set(De);let Ye=A;A=Yi(t),A!==Ye&&(Ia(),_(we[Ye],1,1,()=>{we[Ye]=null}),Oa(),P=we[A],P||(P=we[A]=Vi[A](t),P.c()),b(P,1),P.m(N.parentNode,N));const _s={};c&2&&(_s.$$scope={dirty:c,ctx:t}),Rt.$set(_s);let Ke=ut;ut=Wi(t),ut!==Ke&&(Ia(),_(je[Ke],1,1,()=>{je[Ke]=null}),Oa(),pt=je[ut],pt||(pt=je[ut]=Ki[ut](t),pt.c()),b(pt,1),pt.m(Fe.parentNode,Fe));let Bt=dt;dt=Qi(t),dt!==Bt&&(Ia(),_(qe[Bt],1,1,()=>{qe[Bt]=null}),Oa(),gt=qe[dt],gt||(gt=qe[dt]=Ji[dt](t),gt.c()),b(gt,1),gt.m(Le.parentNode,Le));let We=ft;ft=Zi(t),ft!==We&&(Ia(),_(Te[We],1,1,()=>{Te[We]=null}),Oa(),_t=Te[ft],_t||(_t=Te[ft]=Xi[ft](t),_t.c()),b(_t,1),_t.m(Ue.parentNode,Ue));const Ce={};c&2&&(Ce.$$scope={dirty:c,ctx:t}),Ut.$set(Ce)},i(t){ka||(b(d.$$.fragment,t),b(f.$$.fragment,t),b(P),b(T.$$.fragment,t),b(K.$$.fragment,t),b(Jt.$$.fragment,t),b(Qt.$$.fragment,t),b(Xt.$$.fragment,t),b(Zt.$$.fragment,t),b(te.$$.fragment,t),b(ee.$$.fragment,t),b(ne.$$.fragment,t),b(se.$$.fragment,t),b(ae.$$.fragment,t),b(oe.$$.fragment,t),b(le.$$.fragment,t),b(ie.$$.fragment,t),b(ce.$$.fragment,t),b(re.$$.fragment,t),b(Rt.$$.fragment,t),b(he.$$.fragment,t),b(me.$$.fragment,t),b(ue.$$.fragment,t),b(pe.$$.fragment,t),b(de.$$.fragment,t),b(pt),b(_e.$$.fragment,t),b(gt),b(be.$$.fragment,t),b(ye.$$.fragment,t),b(_t),b($e.$$.fragment,t),b(ke.$$.fragment,t),b(xe.$$.fragment,t),b(Ee.$$.fragment,t),b(Ut.$$.fragment,t),ka=!0)},o(t){_(d.$$.fragment,t),_(f.$$.fragment,t),_(P),_(T.$$.fragment,t),_(K.$$.fragment,t),_(Jt.$$.fragment,t),_(Qt.$$.fragment,t),_(Xt.$$.fragment,t),_(Zt.$$.fragment,t),_(te.$$.fragment,t),_(ee.$$.fragment,t),_(ne.$$.fragment,t),_(se.$$.fragment,t),_(ae.$$.fragment,t),_(oe.$$.fragment,t),_(le.$$.fragment,t),_(ie.$$.fragment,t),_(ce.$$.fragment,t),_(re.$$.fragment,t),_(Rt.$$.fragment,t),_(he.$$.fragment,t),_(me.$$.fragment,t),_(ue.$$.fragment,t),_(pe.$$.fragment,t),_(de.$$.fragment,t),_(pt),_(_e.$$.fragment,t),_(gt),_(be.$$.fragment,t),_(ye.$$.fragment,t),_(_t),_($e.$$.fragment,t),_(ke.$$.fragment,t),_(xe.$$.fragment,t),_(Ee.$$.fragment,t),_(Ut.$$.fragment,t),ka=!1},d(t){n(u),t&&n(y),E(d,t),t&&n(j),t&&n(g),E(f),t&&n(H),we[A].d(t),t&&n(N),t&&n(F),t&&n(U),E(T,t),t&&n(G),t&&n(p),E(K),t&&n(vs),t&&n(bt),t&&n(ys),t&&n(Ae),t&&n($s),t&&n(Ct),t&&n(ks),t&&n(St),E(Jt),t&&n(xs),t&&n(Oe),t&&n(Es),E(Qt,t),t&&n(ws),t&&n(vt),t&&n(js),E(Xt,t),t&&n(qs),E(Zt,t),t&&n(Ts),t&&n(W),t&&n(Ds),E(te,t),t&&n(Cs),E(ee,t),t&&n(Ss),t&&n(J),t&&n(As),E(ne,t),t&&n(Os),E(se,t),t&&n(Is),t&&n(X),t&&n(Hs),E(ae,t),t&&n(Ps),t&&n(Pt),t&&n(Ns),E(oe,t),t&&n(Rs),E(le,t),t&&n(Fs),t&&n(Nt),t&&n(Ms),E(ie,t),t&&n(Ls),t&&n(tt),t&&n(Us),t&&n(lt),t&&n(Bs),E(ce,t),t&&n(Gs),E(re,t),t&&n(Vs),t&&n(He),t&&n(Ys),E(Rt,t),t&&n(Ks),t&&n(Ft),t&&n(Ws),E(he,t),t&&n(Js),t&&n(Pe),t&&n(Qs),E(me,t),t&&n(Xs),E(ue,t),t&&n(Zs),t&&n(yt),t&&n(ta),E(pe,t),t&&n(ea),t&&n(Ne),t&&n(na),t&&n(At),E(de),t&&n(sa),t&&n(V),t&&n(aa),je[ut].d(t),t&&n(Fe),t&&n($t),t&&n(oa),E(_e,t),t&&n(la),t&&n(Me),t&&n(ia),qe[dt].d(t),t&&n(Le),t&&n(ze),t&&n(ca),t&&n(Ot),E(be),t&&n(ra),t&&n(kt),t&&n(ha),t&&n(it),t&&n(ma),E(ye,t),t&&n(ua),t&&n(zt),t&&n(pa),Te[ft].d(t),t&&n(Ue),t&&n(Be),t&&n(da),E($e,t),t&&n(ga),t&&n(xt),t&&n(fa),E(ke,t),t&&n(_a),t&&n(Ge),t&&n(ba),E(xe,t),t&&n(va),E(Ee,t),t&&n(ya),t&&n(Ve),t&&n($a),E(Ut,t)}}}const sh={local:"tm-kim-ng-ngha-vi-faiss",sections:[{local:"s-dng-nhng-biu-din-t-cho-tm-kim-ng-ngha",title:"S\u1EED d\u1EE5ng nh\xFAng bi\u1EC3u di\u1EC5n t\u1EEB cho t\xECm ki\u1EBFm ng\u1EEF ngh\u0129a"},{local:"ti-v-chun-b-tp-d-liu",title:"T\u1EA3i v\xE0 chu\u1EA9n b\u1ECB t\u1EADp d\u1EEF li\u1EC7u"},{local:"to-ra-biu-din-vn-bn",title:"T\u1EA1o ra bi\u1EC3u di\u1EC5n v\u0103n b\u1EA3n"},{local:"s-dng-faiss-tm-kim-im-tng-ng-hiu-qu",title:"S\u1EED d\u1EE5ng FAISS \u0111\u1EC3 t\xECm ki\u1EBFm \u0111i\u1EC3m t\u01B0\u01A1ng \u0111\u1ED3ng hi\u1EC7u qu\u1EA3"}],title:"T\xECm ki\u1EBFm ng\u1EEF ngh\u0129a v\u1EDBi FAISS"};function ah(z,u,y){let d="pt";return Ur(()=>{const j=new URLSearchParams(window.location.search);y(0,d=j.get("fw")||"pt")}),[d]}class uh extends Fr{constructor(u){super();Mr(this,u,ah,nh,Lr,{})}}export{uh as default,sh as metadata};
