import{S as Wa,i as qa,s as Ia,e as n,k as l,w as f,t as g,M as Ha,c as r,d as t,m as p,a as i,x as u,h as v,b as s,G as a,g as h,y as c,L as Ba,q as m,o as d,B as w,v as Ca}from"../../chunks/vendor-hf-doc-builder.js";import{I as $}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ua}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{Q as y}from"../../chunks/Question-hf-doc-builder.js";function La(Vt){let x,Ue,k,q,we,F,kt,ge,bt,Le,Q,De,de,zt,Oe,b,I,ve,Y,_t,$e,Et,Me,j,Re,z,H,ye,G,Pt,J,Nt,xe,At,Tt,Fe,K,Qe,_,B,ke,V,St,be,Wt,Ye,X,je,E,C,ze,Z,qt,ee,It,_e,Ht,Bt,Ge,te,Je,P,U,Ee,ae,Ct,oe,Ut,Pe,Lt,Dt,Ke,ne,Ve,N,L,Ne,re,Ot,Ae,Mt,Xe,ie,Ze,A,D,Te,se,Rt,Se,Ft,et,he,tt,T,O,We,le,Qt,qe,Yt,at,pe,ot,S,M,Ie,fe,jt,He,Gt,nt,ue,rt,W,R,Be,ce,Jt,Ce,Kt,it,me,st;return F=new $({}),Q=new Ua({props:{chapter:6,classNames:"absolute z-10 right-0 top-0"}}),Y=new $({}),j=new y({props:{choices:[{text:"When your dataset is similar to that used by an existing pretrained model, and you want to pretrain a new model",explain:"In this case, to save time and compute resources, a better choice would be to use the same tokenizer as the pretrained model and fine-tune that model instead."},{text:"When your dataset is similar to that used by an existing pretrained model, and you want to fine-tune a new model using this pretrained model",explain:"To fine-tune a model from a pretrained model, you should always use the same tokenizer."},{text:"When your dataset is different from the one used by an existing pretrained model, and you want to pretrain a new model",explain:"Correct! In this case there's no advantage to using the same tokenizer.",correct:!0},{text:"When your dataset is different from the one used by an existing pretrained model, but you want to fine-tune a new model using this pretrained model",explain:"To fine-tune a model from a pretrained model, you should always use the same tokenizer."}]}}),G=new $({}),K=new y({props:{choices:[{text:"That's the only type the method <code>train_new_from_iterator()</code> accepts.",explain:"A list of lists of texts is a particular kind of generator of lists of texts, so the method will accept this too. Try again!"},{text:"You will avoid loading the whole dataset into memory at once.",explain:"Right! Each batch of texts will be released from memory when you iterate, and the gain will be especially visible if you use \u{1F917} Datasets to store your texts.",correct:!0},{text:"This will allow the \u{1F917} Tokenizers library to use multiprocessing.",explain:"No, it will use multiprocessing either way."},{text:"The tokenizer you train will generate better texts.",explain:"The tokenizer does not generate text -- are you confusing it with a language model?"}]}}),V=new $({}),X=new y({props:{choices:[{text:"It can process inputs faster than a slow tokenizer when you batch lots of inputs together.",explain:"Correct! Thanks to parallelism implemented in Rust, it will be faster on batches of inputs. What other benefit can you think of?",correct:!0},{text:"Fast tokenizers always tokenize faster than their slow counterparts.",explain:"A fast tokenizer can actually be slower when you only give it one or very few texts, since it can't use parallelism."},{text:"It can apply padding and truncation.",explain:"True, but slow tokenizers also do that."},{text:"It has some additional features allowing you to map tokens to the span of text that created them.",explain:"Indeed -- those are called offset mappings. That's not the only advantage, though.",correct:!0}]}}),Z=new $({}),te=new y({props:{choices:[{text:"The entities with the same label are merged into one entity.",explain:"That's oversimplifying things a little. Try again!"},{text:"There is a label for the beginning of an entity and a label for the continuation of an entity.",explain:"Correct!",correct:!0},{text:"In a given word, as long as the first token has the label of the entity, the whole word is considered labeled with that entity.",explain:"That's one strategy to handle entities. What other answers here apply?",correct:!0},{text:"When a token has the label of a given entity, any other following token with the same label is considered part of the same entity, unless it's labeled as the start of a new entity.",explain:"That's the most common way to group entities together -- it's not the only right answer, though.",correct:!0}]}}),ae=new $({}),ne=new y({props:{choices:[{text:"It doesn't really, as it truncates the long context at the maximum length accepted by the model.",explain:"There is a trick you can use to handle long contexts. Do you remember what it is?"},{text:"It splits the context into several parts and averages the results obtained.",explain:"No, it wouldn't make sense to average the results, as some parts of the context won't include the answer."},{text:"It splits the context into several parts (with overlap) and finds the maximum score for an answer in each part.",explain:"That's the correct answer!",correct:!0},{text:"It splits the context into several parts (without overlap, for efficiency) and finds the maximum score for an answer in each part.",explain:"No, it includes some overlap between the parts to avoid a situation where the answer would be split across two parts."}]}}),re=new $({}),ie=new y({props:{choices:[{text:"It's any cleanup the tokenizer performs on the texts in the initial stages.",explain:"That's correct -- for instance, it might involve removing accents or whitespace, or lowercasing the inputs.",correct:!0},{text:"It's a data augmentation technique that involves making the text more normal by removing rare words.",explain:"That's incorrect! Try again."},{text:"It's the final post-processing step where the tokenizer adds the special tokens.",explain:"That stage is simply called post-processing."},{text:"It's when the embeddings are made with mean 0 and standard deviation 1, by subtracting the mean and dividing by the std.",explain:"That process is commonly called normalization when applied to pixel values in computer vision, but it's not what normalization means in NLP."}]}}),se=new $({}),he=new y({props:{choices:[{text:"It's the step before the tokenization, where data augmentation (like random masking) is applied.",explain:"No, that step is part of the preprocessing."},{text:"It's the step before the tokenization, where the desired cleanup operations are applied to the text.",explain:"No, that's the normalization step."},{text:"It's the step before the tokenizer model is applied, to split the input into words.",explain:"That's the correct answer!",correct:!0},{text:"It's the step before the tokenizer model is applied, to split the input into tokens.",explain:"No, splitting into tokens is the job of the tokenizer model."}]}}),le=new $({}),pe=new y({props:{choices:[{text:"BPE is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.",explain:"That's the case indeed!",correct:!0},{text:"BPE is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.",explain:"No, that's the approach taken by a different tokenization algorithm."},{text:"BPE tokenizers learn merge rules by merging the pair of tokens that is the most frequent.",explain:"That's correct!",correct:!0},{text:"A BPE tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.",explain:"No, that's the strategy applied by another tokenization algorithm."},{text:"BPE tokenizes words into subwords by splitting them into characters and then applying the merge rules.",explain:"That's correct!",correct:!0},{text:"BPE tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.",explain:"No, that's another tokenization algorithm's way of doing things."}]}}),fe=new $({}),ue=new y({props:{choices:[{text:"WordPiece is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.",explain:"That's the case indeed!",correct:!0},{text:"WordPiece is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.",explain:"No, that's the approach taken by a different tokenization algorithm."},{text:"WordPiece tokenizers learn merge rules by merging the pair of tokens that is the most frequent.",explain:"No, that's the strategy applied by another tokenization algorithm."},{text:"A WordPiece tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.",explain:"That's correct!",correct:!0},{text:"WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.",explain:"No, that's how another tokenization algorithm works."},{text:"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.",explain:"Yes, this is how WordPiece proceeds for the encoding.",correct:!0}]}}),ce=new $({}),me=new y({props:{choices:[{text:"Unigram is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.",explain:"No, that's the approach taken by a different tokenization algorithm."},{text:"Unigram is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.",explain:"That's correct!",correct:!0},{text:"Unigram adapts its vocabulary by minimizing a loss computed over the whole corpus.",explain:"That's correct!",correct:!0},{text:"Unigram adapts its vocabulary by keeping the most frequent subwords.",explain:"No, this incorrect."},{text:"Unigram tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.",explain:"That's correct!",correct:!0},{text:"Unigram tokenizes words into subwords by splitting them into characters, then applying the merge rules.",explain:"No, that's how another tokenization algorithm works."}]}}),{c(){x=n("meta"),Ue=l(),k=n("h1"),q=n("a"),we=n("span"),f(F.$$.fragment),kt=l(),ge=n("span"),bt=g("\u0110\u1ED1 vui cu\u1ED1i ch\u01B0\u01A1ng"),Le=l(),f(Q.$$.fragment),De=l(),de=n("p"),zt=g("Let\u2019s test what you learned in this chapter!"),Oe=l(),b=n("h3"),I=n("a"),ve=n("span"),f(Y.$$.fragment),_t=l(),$e=n("span"),Et=g("1. When should you train a new tokenizer?"),Me=l(),f(j.$$.fragment),Re=l(),z=n("h3"),H=n("a"),ye=n("span"),f(G.$$.fragment),Pt=l(),J=n("span"),Nt=g("2. What is the advantage of using a generator of lists of texts compared to a list of lists of texts when using "),xe=n("code"),At=g("train_new_from_iterator()"),Tt=g("?"),Fe=l(),f(K.$$.fragment),Qe=l(),_=n("h3"),B=n("a"),ke=n("span"),f(V.$$.fragment),St=l(),be=n("span"),Wt=g("3. What are the advantages of using a \u201Cfast\u201D tokenizer?"),Ye=l(),f(X.$$.fragment),je=l(),E=n("h3"),C=n("a"),ze=n("span"),f(Z.$$.fragment),qt=l(),ee=n("span"),It=g("4. How does the "),_e=n("code"),Ht=g("token-classification"),Bt=g(" pipeline handle entities that span over several tokens?"),Ge=l(),f(te.$$.fragment),Je=l(),P=n("h3"),U=n("a"),Ee=n("span"),f(ae.$$.fragment),Ct=l(),oe=n("span"),Ut=g("5. How does the "),Pe=n("code"),Lt=g("question-answering"),Dt=g(" pipeline handle long contexts?"),Ke=l(),f(ne.$$.fragment),Ve=l(),N=n("h3"),L=n("a"),Ne=n("span"),f(re.$$.fragment),Ot=l(),Ae=n("span"),Mt=g("6. What is normalization?"),Xe=l(),f(ie.$$.fragment),Ze=l(),A=n("h3"),D=n("a"),Te=n("span"),f(se.$$.fragment),Rt=l(),Se=n("span"),Ft=g("7. What is pre-tokenization for a subword tokenizer?"),et=l(),f(he.$$.fragment),tt=l(),T=n("h3"),O=n("a"),We=n("span"),f(le.$$.fragment),Qt=l(),qe=n("span"),Yt=g("8. Select the sentences that apply to the BPE model of tokenization."),at=l(),f(pe.$$.fragment),ot=l(),S=n("h3"),M=n("a"),Ie=n("span"),f(fe.$$.fragment),jt=l(),He=n("span"),Gt=g("9. Select the sentences that apply to the WordPiece model of tokenization."),nt=l(),f(ue.$$.fragment),rt=l(),W=n("h3"),R=n("a"),Be=n("span"),f(ce.$$.fragment),Jt=l(),Ce=n("span"),Kt=g("10. Select the sentences that apply to the Unigram model of tokenization."),it=l(),f(me.$$.fragment),this.h()},l(e){const o=Ha('[data-svelte="svelte-1phssyn"]',document.head);x=r(o,"META",{name:!0,content:!0}),o.forEach(t),Ue=p(e),k=r(e,"H1",{class:!0});var ht=i(k);q=r(ht,"A",{id:!0,class:!0,href:!0});var Xt=i(q);we=r(Xt,"SPAN",{});var Zt=i(we);u(F.$$.fragment,Zt),Zt.forEach(t),Xt.forEach(t),kt=p(ht),ge=r(ht,"SPAN",{});var ea=i(ge);bt=v(ea,"\u0110\u1ED1 vui cu\u1ED1i ch\u01B0\u01A1ng"),ea.forEach(t),ht.forEach(t),Le=p(e),u(Q.$$.fragment,e),De=p(e),de=r(e,"P",{});var ta=i(de);zt=v(ta,"Let\u2019s test what you learned in this chapter!"),ta.forEach(t),Oe=p(e),b=r(e,"H3",{class:!0});var lt=i(b);I=r(lt,"A",{id:!0,class:!0,href:!0});var aa=i(I);ve=r(aa,"SPAN",{});var oa=i(ve);u(Y.$$.fragment,oa),oa.forEach(t),aa.forEach(t),_t=p(lt),$e=r(lt,"SPAN",{});var na=i($e);Et=v(na,"1. When should you train a new tokenizer?"),na.forEach(t),lt.forEach(t),Me=p(e),u(j.$$.fragment,e),Re=p(e),z=r(e,"H3",{class:!0});var pt=i(z);H=r(pt,"A",{id:!0,class:!0,href:!0});var ra=i(H);ye=r(ra,"SPAN",{});var ia=i(ye);u(G.$$.fragment,ia),ia.forEach(t),ra.forEach(t),Pt=p(pt),J=r(pt,"SPAN",{});var ft=i(J);Nt=v(ft,"2. What is the advantage of using a generator of lists of texts compared to a list of lists of texts when using "),xe=r(ft,"CODE",{});var sa=i(xe);At=v(sa,"train_new_from_iterator()"),sa.forEach(t),Tt=v(ft,"?"),ft.forEach(t),pt.forEach(t),Fe=p(e),u(K.$$.fragment,e),Qe=p(e),_=r(e,"H3",{class:!0});var ut=i(_);B=r(ut,"A",{id:!0,class:!0,href:!0});var ha=i(B);ke=r(ha,"SPAN",{});var la=i(ke);u(V.$$.fragment,la),la.forEach(t),ha.forEach(t),St=p(ut),be=r(ut,"SPAN",{});var pa=i(be);Wt=v(pa,"3. What are the advantages of using a \u201Cfast\u201D tokenizer?"),pa.forEach(t),ut.forEach(t),Ye=p(e),u(X.$$.fragment,e),je=p(e),E=r(e,"H3",{class:!0});var ct=i(E);C=r(ct,"A",{id:!0,class:!0,href:!0});var fa=i(C);ze=r(fa,"SPAN",{});var ua=i(ze);u(Z.$$.fragment,ua),ua.forEach(t),fa.forEach(t),qt=p(ct),ee=r(ct,"SPAN",{});var mt=i(ee);It=v(mt,"4. How does the "),_e=r(mt,"CODE",{});var ca=i(_e);Ht=v(ca,"token-classification"),ca.forEach(t),Bt=v(mt," pipeline handle entities that span over several tokens?"),mt.forEach(t),ct.forEach(t),Ge=p(e),u(te.$$.fragment,e),Je=p(e),P=r(e,"H3",{class:!0});var dt=i(P);U=r(dt,"A",{id:!0,class:!0,href:!0});var ma=i(U);Ee=r(ma,"SPAN",{});var da=i(Ee);u(ae.$$.fragment,da),da.forEach(t),ma.forEach(t),Ct=p(dt),oe=r(dt,"SPAN",{});var wt=i(oe);Ut=v(wt,"5. How does the "),Pe=r(wt,"CODE",{});var wa=i(Pe);Lt=v(wa,"question-answering"),wa.forEach(t),Dt=v(wt," pipeline handle long contexts?"),wt.forEach(t),dt.forEach(t),Ke=p(e),u(ne.$$.fragment,e),Ve=p(e),N=r(e,"H3",{class:!0});var gt=i(N);L=r(gt,"A",{id:!0,class:!0,href:!0});var ga=i(L);Ne=r(ga,"SPAN",{});var va=i(Ne);u(re.$$.fragment,va),va.forEach(t),ga.forEach(t),Ot=p(gt),Ae=r(gt,"SPAN",{});var $a=i(Ae);Mt=v($a,"6. What is normalization?"),$a.forEach(t),gt.forEach(t),Xe=p(e),u(ie.$$.fragment,e),Ze=p(e),A=r(e,"H3",{class:!0});var vt=i(A);D=r(vt,"A",{id:!0,class:!0,href:!0});var ya=i(D);Te=r(ya,"SPAN",{});var xa=i(Te);u(se.$$.fragment,xa),xa.forEach(t),ya.forEach(t),Rt=p(vt),Se=r(vt,"SPAN",{});var ka=i(Se);Ft=v(ka,"7. What is pre-tokenization for a subword tokenizer?"),ka.forEach(t),vt.forEach(t),et=p(e),u(he.$$.fragment,e),tt=p(e),T=r(e,"H3",{class:!0});var $t=i(T);O=r($t,"A",{id:!0,class:!0,href:!0});var ba=i(O);We=r(ba,"SPAN",{});var za=i(We);u(le.$$.fragment,za),za.forEach(t),ba.forEach(t),Qt=p($t),qe=r($t,"SPAN",{});var _a=i(qe);Yt=v(_a,"8. Select the sentences that apply to the BPE model of tokenization."),_a.forEach(t),$t.forEach(t),at=p(e),u(pe.$$.fragment,e),ot=p(e),S=r(e,"H3",{class:!0});var yt=i(S);M=r(yt,"A",{id:!0,class:!0,href:!0});var Ea=i(M);Ie=r(Ea,"SPAN",{});var Pa=i(Ie);u(fe.$$.fragment,Pa),Pa.forEach(t),Ea.forEach(t),jt=p(yt),He=r(yt,"SPAN",{});var Na=i(He);Gt=v(Na,"9. Select the sentences that apply to the WordPiece model of tokenization."),Na.forEach(t),yt.forEach(t),nt=p(e),u(ue.$$.fragment,e),rt=p(e),W=r(e,"H3",{class:!0});var xt=i(W);R=r(xt,"A",{id:!0,class:!0,href:!0});var Aa=i(R);Be=r(Aa,"SPAN",{});var Ta=i(Be);u(ce.$$.fragment,Ta),Ta.forEach(t),Aa.forEach(t),Jt=p(xt),Ce=r(xt,"SPAN",{});var Sa=i(Ce);Kt=v(Sa,"10. Select the sentences that apply to the Unigram model of tokenization."),Sa.forEach(t),xt.forEach(t),it=p(e),u(me.$$.fragment,e),this.h()},h(){s(x,"name","hf:doc:metadata"),s(x,"content",JSON.stringify(Da)),s(q,"id","vui-cui-chng"),s(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(q,"href","#vui-cui-chng"),s(k,"class","relative group"),s(I,"id","1.-when-should-you-train-a-new-tokenizer?"),s(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(I,"href","#1.-when-should-you-train-a-new-tokenizer?"),s(b,"class","relative group"),s(H,"id","2.-what-is-the-advantage-of-using-a-generator-of-lists-of-texts-compared-to-a-list-of-lists-of-texts-when-using-<code>train_new_from_iterator()</code>?"),s(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(H,"href","#2.-what-is-the-advantage-of-using-a-generator-of-lists-of-texts-compared-to-a-list-of-lists-of-texts-when-using-<code>train_new_from_iterator()</code>?"),s(z,"class","relative group"),s(B,"id","3.-what-are-the-advantages-of-using-a-\u201Cfast\u201D-tokenizer?"),s(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(B,"href","#3.-what-are-the-advantages-of-using-a-\u201Cfast\u201D-tokenizer?"),s(_,"class","relative group"),s(C,"id","4.-how-does-the-<code>token-classification</code>-pipeline-handle-entities-that-span-over-several-tokens?"),s(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(C,"href","#4.-how-does-the-<code>token-classification</code>-pipeline-handle-entities-that-span-over-several-tokens?"),s(E,"class","relative group"),s(U,"id","5.-how-does-the-<code>question-answering</code>-pipeline-handle-long-contexts?"),s(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(U,"href","#5.-how-does-the-<code>question-answering</code>-pipeline-handle-long-contexts?"),s(P,"class","relative group"),s(L,"id","6.-what-is-normalization?"),s(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(L,"href","#6.-what-is-normalization?"),s(N,"class","relative group"),s(D,"id","7.-what-is-pre-tokenization-for-a-subword-tokenizer?"),s(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(D,"href","#7.-what-is-pre-tokenization-for-a-subword-tokenizer?"),s(A,"class","relative group"),s(O,"id","8.-select-the-sentences-that-apply-to-the-bpe-model-of-tokenization."),s(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(O,"href","#8.-select-the-sentences-that-apply-to-the-bpe-model-of-tokenization."),s(T,"class","relative group"),s(M,"id","9.-select-the-sentences-that-apply-to-the-wordpiece-model-of-tokenization."),s(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(M,"href","#9.-select-the-sentences-that-apply-to-the-wordpiece-model-of-tokenization."),s(S,"class","relative group"),s(R,"id","10.-select-the-sentences-that-apply-to-the-unigram-model-of-tokenization."),s(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(R,"href","#10.-select-the-sentences-that-apply-to-the-unigram-model-of-tokenization."),s(W,"class","relative group")},m(e,o){a(document.head,x),h(e,Ue,o),h(e,k,o),a(k,q),a(q,we),c(F,we,null),a(k,kt),a(k,ge),a(ge,bt),h(e,Le,o),c(Q,e,o),h(e,De,o),h(e,de,o),a(de,zt),h(e,Oe,o),h(e,b,o),a(b,I),a(I,ve),c(Y,ve,null),a(b,_t),a(b,$e),a($e,Et),h(e,Me,o),c(j,e,o),h(e,Re,o),h(e,z,o),a(z,H),a(H,ye),c(G,ye,null),a(z,Pt),a(z,J),a(J,Nt),a(J,xe),a(xe,At),a(J,Tt),h(e,Fe,o),c(K,e,o),h(e,Qe,o),h(e,_,o),a(_,B),a(B,ke),c(V,ke,null),a(_,St),a(_,be),a(be,Wt),h(e,Ye,o),c(X,e,o),h(e,je,o),h(e,E,o),a(E,C),a(C,ze),c(Z,ze,null),a(E,qt),a(E,ee),a(ee,It),a(ee,_e),a(_e,Ht),a(ee,Bt),h(e,Ge,o),c(te,e,o),h(e,Je,o),h(e,P,o),a(P,U),a(U,Ee),c(ae,Ee,null),a(P,Ct),a(P,oe),a(oe,Ut),a(oe,Pe),a(Pe,Lt),a(oe,Dt),h(e,Ke,o),c(ne,e,o),h(e,Ve,o),h(e,N,o),a(N,L),a(L,Ne),c(re,Ne,null),a(N,Ot),a(N,Ae),a(Ae,Mt),h(e,Xe,o),c(ie,e,o),h(e,Ze,o),h(e,A,o),a(A,D),a(D,Te),c(se,Te,null),a(A,Rt),a(A,Se),a(Se,Ft),h(e,et,o),c(he,e,o),h(e,tt,o),h(e,T,o),a(T,O),a(O,We),c(le,We,null),a(T,Qt),a(T,qe),a(qe,Yt),h(e,at,o),c(pe,e,o),h(e,ot,o),h(e,S,o),a(S,M),a(M,Ie),c(fe,Ie,null),a(S,jt),a(S,He),a(He,Gt),h(e,nt,o),c(ue,e,o),h(e,rt,o),h(e,W,o),a(W,R),a(R,Be),c(ce,Be,null),a(W,Jt),a(W,Ce),a(Ce,Kt),h(e,it,o),c(me,e,o),st=!0},p:Ba,i(e){st||(m(F.$$.fragment,e),m(Q.$$.fragment,e),m(Y.$$.fragment,e),m(j.$$.fragment,e),m(G.$$.fragment,e),m(K.$$.fragment,e),m(V.$$.fragment,e),m(X.$$.fragment,e),m(Z.$$.fragment,e),m(te.$$.fragment,e),m(ae.$$.fragment,e),m(ne.$$.fragment,e),m(re.$$.fragment,e),m(ie.$$.fragment,e),m(se.$$.fragment,e),m(he.$$.fragment,e),m(le.$$.fragment,e),m(pe.$$.fragment,e),m(fe.$$.fragment,e),m(ue.$$.fragment,e),m(ce.$$.fragment,e),m(me.$$.fragment,e),st=!0)},o(e){d(F.$$.fragment,e),d(Q.$$.fragment,e),d(Y.$$.fragment,e),d(j.$$.fragment,e),d(G.$$.fragment,e),d(K.$$.fragment,e),d(V.$$.fragment,e),d(X.$$.fragment,e),d(Z.$$.fragment,e),d(te.$$.fragment,e),d(ae.$$.fragment,e),d(ne.$$.fragment,e),d(re.$$.fragment,e),d(ie.$$.fragment,e),d(se.$$.fragment,e),d(he.$$.fragment,e),d(le.$$.fragment,e),d(pe.$$.fragment,e),d(fe.$$.fragment,e),d(ue.$$.fragment,e),d(ce.$$.fragment,e),d(me.$$.fragment,e),st=!1},d(e){t(x),e&&t(Ue),e&&t(k),w(F),e&&t(Le),w(Q,e),e&&t(De),e&&t(de),e&&t(Oe),e&&t(b),w(Y),e&&t(Me),w(j,e),e&&t(Re),e&&t(z),w(G),e&&t(Fe),w(K,e),e&&t(Qe),e&&t(_),w(V),e&&t(Ye),w(X,e),e&&t(je),e&&t(E),w(Z),e&&t(Ge),w(te,e),e&&t(Je),e&&t(P),w(ae),e&&t(Ke),w(ne,e),e&&t(Ve),e&&t(N),w(re),e&&t(Xe),w(ie,e),e&&t(Ze),e&&t(A),w(se),e&&t(et),w(he,e),e&&t(tt),e&&t(T),w(le),e&&t(at),w(pe,e),e&&t(ot),e&&t(S),w(fe),e&&t(nt),w(ue,e),e&&t(rt),e&&t(W),w(ce),e&&t(it),w(me,e)}}}const Da={local:"vui-cui-chng",title:"\u0110\u1ED1 vui cu\u1ED1i ch\u01B0\u01A1ng"};function Oa(Vt){return Ca(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ya extends Wa{constructor(x){super();qa(this,x,Oa,La,Ia,{})}}export{Ya as default,Da as metadata};
