import{S as Ce,i as Me,s as Ie,e as r,k as f,w as ie,t as m,M as Se,c as l,d as a,m as c,a as s,x as de,h as p,b as n,G as o,g as i,y as fe,L as ke,q as ce,o as me,B as pe,v as Ge}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ne}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Re}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Be}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Ue(ue){let u,N,h,_,A,$,Q,C,j,R,b,B,w,U,v,D,M,K,V,X,x,W,Y,T,Z,z,q,ee,F,d,I,g,ae,oe,S,L,te,re,k,y,le,se,G,P,ne,H;return $=new Re({}),b=new Be({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),w=new Ne({props:{id:"d_ixlCubqQw"}}),{c(){u=r("meta"),N=f(),h=r("h1"),_=r("a"),A=r("span"),ie($.$$.fragment),Q=f(),C=r("span"),j=m("Modelos de decodificadores"),R=f(),ie(b.$$.fragment),B=f(),ie(w.$$.fragment),U=f(),v=r("p"),D=m("Los modelos de decodificadores usan \xFAnicamente el decodificador del Transformador. En cada etapa, para una palabra dada las capas de atenci\xF3n pueden acceder s\xF3lamente a las palabras que se ubican antes en la oraci\xF3n. Estos modelos se suelen llamar modelos "),M=r("em"),K=m("auto-regressive"),V=m("."),X=f(),x=r("p"),W=m("El preentrenamiento de los modelos de decodificadores generalmente gira en torno a la predicci\xF3n de la siguiente palabra en la oraci\xF3n."),Y=f(),T=r("p"),Z=m("Estos modelos son m\xE1s adecuados para tareas que implican la generaci\xF3n de texto."),z=f(),q=r("p"),ee=m("Los miembros de esta familia de modelos incluyen:"),F=f(),d=r("ul"),I=r("li"),g=r("a"),ae=m("CTRL"),oe=f(),S=r("li"),L=r("a"),te=m("GPT"),re=f(),k=r("li"),y=r("a"),le=m("GPT-2"),se=f(),G=r("li"),P=r("a"),ne=m("Transformer XL"),this.h()},l(e){const t=Se('[data-svelte="svelte-1phssyn"]',document.head);u=l(t,"META",{name:!0,content:!0}),t.forEach(a),N=c(e),h=l(e,"H1",{class:!0});var J=s(h);_=l(J,"A",{id:!0,class:!0,href:!0});var he=s(_);A=l(he,"SPAN",{});var _e=s(A);de($.$$.fragment,_e),_e.forEach(a),he.forEach(a),Q=c(J),C=l(J,"SPAN",{});var ve=s(C);j=p(ve,"Modelos de decodificadores"),ve.forEach(a),J.forEach(a),R=c(e),de(b.$$.fragment,e),B=c(e),de(w.$$.fragment,e),U=c(e),v=l(e,"P",{});var O=s(v);D=p(O,"Los modelos de decodificadores usan \xFAnicamente el decodificador del Transformador. En cada etapa, para una palabra dada las capas de atenci\xF3n pueden acceder s\xF3lamente a las palabras que se ubican antes en la oraci\xF3n. Estos modelos se suelen llamar modelos "),M=l(O,"EM",{});var Ee=s(M);K=p(Ee,"auto-regressive"),Ee.forEach(a),V=p(O,"."),O.forEach(a),X=c(e),x=l(e,"P",{});var $e=s(x);W=p($e,"El preentrenamiento de los modelos de decodificadores generalmente gira en torno a la predicci\xF3n de la siguiente palabra en la oraci\xF3n."),$e.forEach(a),Y=c(e),T=l(e,"P",{});var be=s(T);Z=p(be,"Estos modelos son m\xE1s adecuados para tareas que implican la generaci\xF3n de texto."),be.forEach(a),z=c(e),q=l(e,"P",{});var we=s(q);ee=p(we,"Los miembros de esta familia de modelos incluyen:"),we.forEach(a),F=c(e),d=l(e,"UL",{});var E=s(d);I=l(E,"LI",{});var ge=s(I);g=l(ge,"A",{href:!0,rel:!0});var Le=s(g);ae=p(Le,"CTRL"),Le.forEach(a),ge.forEach(a),oe=c(E),S=l(E,"LI",{});var ye=s(S);L=l(ye,"A",{href:!0,rel:!0});var Pe=s(L);te=p(Pe,"GPT"),Pe.forEach(a),ye.forEach(a),re=c(E),k=l(E,"LI",{});var xe=s(k);y=l(xe,"A",{href:!0,rel:!0});var Te=s(y);le=p(Te,"GPT-2"),Te.forEach(a),xe.forEach(a),se=c(E),G=l(E,"LI",{});var qe=s(G);P=l(qe,"A",{href:!0,rel:!0});var Ae=s(P);ne=p(Ae,"Transformer XL"),Ae.forEach(a),qe.forEach(a),E.forEach(a),this.h()},h(){n(u,"name","hf:doc:metadata"),n(u,"content",JSON.stringify(Xe)),n(_,"id","modelos-de-decodificadores"),n(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(_,"href","#modelos-de-decodificadores"),n(h,"class","relative group"),n(g,"href","https://huggingface.co/transformers/model_doc/ctrl.html"),n(g,"rel","nofollow"),n(L,"href","https://huggingface.co/transformers/model_doc/gpt.html"),n(L,"rel","nofollow"),n(y,"href","https://huggingface.co/transformers/model_doc/gpt2.html"),n(y,"rel","nofollow"),n(P,"href","https://huggingface.co/transformers/model_doc/transformerxl.html"),n(P,"rel","nofollow")},m(e,t){o(document.head,u),i(e,N,t),i(e,h,t),o(h,_),o(_,A),fe($,A,null),o(h,Q),o(h,C),o(C,j),i(e,R,t),fe(b,e,t),i(e,B,t),fe(w,e,t),i(e,U,t),i(e,v,t),o(v,D),o(v,M),o(M,K),o(v,V),i(e,X,t),i(e,x,t),o(x,W),i(e,Y,t),i(e,T,t),o(T,Z),i(e,z,t),i(e,q,t),o(q,ee),i(e,F,t),i(e,d,t),o(d,I),o(I,g),o(g,ae),o(d,oe),o(d,S),o(S,L),o(L,te),o(d,re),o(d,k),o(k,y),o(y,le),o(d,se),o(d,G),o(G,P),o(P,ne),H=!0},p:ke,i(e){H||(ce($.$$.fragment,e),ce(b.$$.fragment,e),ce(w.$$.fragment,e),H=!0)},o(e){me($.$$.fragment,e),me(b.$$.fragment,e),me(w.$$.fragment,e),H=!1},d(e){a(u),e&&a(N),e&&a(h),pe($),e&&a(R),pe(b,e),e&&a(B),pe(w,e),e&&a(U),e&&a(v),e&&a(X),e&&a(x),e&&a(Y),e&&a(T),e&&a(z),e&&a(q),e&&a(F),e&&a(d)}}}const Xe={local:"modelos-de-decodificadores",title:"Modelos de decodificadores"};function Ye(ue){return Ge(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Oe extends Ce{constructor(u){super();Me(this,u,Ye,Ue,Ie,{})}}export{Oe as default,Xe as metadata};
