import{S as at,i as rt,s as ot,e as r,k as n,w as Qe,t as l,M as st,c as o,d as a,m as d,a as s,x as Ve,h as i,b as w,G as e,g as E,y as We,L as nt,q as Ze,o as et,B as tt,v as lt}from"../../chunks/vendor-hf-doc-builder.js";import{I as dt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as it}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function ct(ge){let f,J,m,_,g,y,ae,q,re,Y,D,K,b,oe,x,se,ne,Q,$,le,V,R,G,u,H,de,ie,M,ce,fe,j,me,ue,p,h,I,pe,he,N,ve,Te,S,Ee,_e,v,k,be,Re,z,ye,De,O,$e,Ae,T,U,Be,Le,X,Ce,Pe,F,we,W;return y=new dt({}),D=new it({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),{c(){f=r("meta"),J=n(),m=r("h1"),_=r("a"),g=r("span"),Qe(y.$$.fragment),ae=n(),q=r("span"),re=l("Resumen"),Y=n(),Qe(D.$$.fragment),K=n(),b=r("p"),oe=l("En este cap\xEDtulo viste c\xF3mo abordar diferentes tareas de PLN usando la funci\xF3n de alto nivel "),x=r("code"),se=l("pipeline()"),ne=l(" de \u{1F917} Transformers. Tambi\xE9n viste como buscar modelos en el Hub, as\xED como usar la API de Inferencia para probar los modelos directamente en tu navegador."),Q=n(),$=r("p"),le=l("Discutimos brevemente el funcionamiento de los Transformadores y hablamos sobre la importancia de la transferencia de aprendizaje y el ajuste. Un aspecto clave es que puedes usar la arquitectura completa o s\xF3lo el codificador o decodificador, dependiendo de qu\xE9 tipo de tarea quieres resolver. La siguiente tabla resume lo anterior:"),V=n(),R=r("table"),G=r("thead"),u=r("tr"),H=r("th"),de=l("Modelo"),ie=n(),M=r("th"),ce=l("Ejemplos"),fe=n(),j=r("th"),me=l("Tareas"),ue=n(),p=r("tbody"),h=r("tr"),I=r("td"),pe=l("Codificador"),he=n(),N=r("td"),ve=l("ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),Te=n(),S=r("td"),Ee=l("Clasificaci\xF3n de oraciones, reconocimiento de entidades nombradas, respuesta extractiva a preguntas"),_e=n(),v=r("tr"),k=r("td"),be=l("Decodificador"),Re=n(),z=r("td"),ye=l("CTRL, GPT, GPT-2, Transformer XL"),De=n(),O=r("td"),$e=l("Generaci\xF3n de texto"),Ae=n(),T=r("tr"),U=r("td"),Be=l("Codificador-decodificador"),Le=n(),X=r("td"),Ce=l("BART, T5, Marian, mBART"),Pe=n(),F=r("td"),we=l("Resumen, traducci\xF3n, respuesta generativa a preguntas"),this.h()},l(t){const c=st('[data-svelte="svelte-1phssyn"]',document.head);f=o(c,"META",{name:!0,content:!0}),c.forEach(a),J=d(t),m=o(t,"H1",{class:!0});var Z=s(m);_=o(Z,"A",{id:!0,class:!0,href:!0});var qe=s(_);g=o(qe,"SPAN",{});var xe=s(g);Ve(y.$$.fragment,xe),xe.forEach(a),qe.forEach(a),ae=d(Z),q=o(Z,"SPAN",{});var Ge=s(q);re=i(Ge,"Resumen"),Ge.forEach(a),Z.forEach(a),Y=d(t),Ve(D.$$.fragment,t),K=d(t),b=o(t,"P",{});var ee=s(b);oe=i(ee,"En este cap\xEDtulo viste c\xF3mo abordar diferentes tareas de PLN usando la funci\xF3n de alto nivel "),x=o(ee,"CODE",{});var He=s(x);se=i(He,"pipeline()"),He.forEach(a),ne=i(ee," de \u{1F917} Transformers. Tambi\xE9n viste como buscar modelos en el Hub, as\xED como usar la API de Inferencia para probar los modelos directamente en tu navegador."),ee.forEach(a),Q=d(t),$=o(t,"P",{});var Me=s($);le=i(Me,"Discutimos brevemente el funcionamiento de los Transformadores y hablamos sobre la importancia de la transferencia de aprendizaje y el ajuste. Un aspecto clave es que puedes usar la arquitectura completa o s\xF3lo el codificador o decodificador, dependiendo de qu\xE9 tipo de tarea quieres resolver. La siguiente tabla resume lo anterior:"),Me.forEach(a),V=d(t),R=o(t,"TABLE",{});var te=s(R);G=o(te,"THEAD",{});var je=s(G);u=o(je,"TR",{});var A=s(u);H=o(A,"TH",{});var Ie=s(H);de=i(Ie,"Modelo"),Ie.forEach(a),ie=d(A),M=o(A,"TH",{});var Ne=s(M);ce=i(Ne,"Ejemplos"),Ne.forEach(a),fe=d(A),j=o(A,"TH",{});var Se=s(j);me=i(Se,"Tareas"),Se.forEach(a),A.forEach(a),je.forEach(a),ue=d(te),p=o(te,"TBODY",{});var B=s(p);h=o(B,"TR",{});var L=s(h);I=o(L,"TD",{});var ke=s(I);pe=i(ke,"Codificador"),ke.forEach(a),he=d(L),N=o(L,"TD",{});var ze=s(N);ve=i(ze,"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),ze.forEach(a),Te=d(L),S=o(L,"TD",{});var Oe=s(S);Ee=i(Oe,"Clasificaci\xF3n de oraciones, reconocimiento de entidades nombradas, respuesta extractiva a preguntas"),Oe.forEach(a),L.forEach(a),_e=d(B),v=o(B,"TR",{});var C=s(v);k=o(C,"TD",{});var Ue=s(k);be=i(Ue,"Decodificador"),Ue.forEach(a),Re=d(C),z=o(C,"TD",{});var Xe=s(z);ye=i(Xe,"CTRL, GPT, GPT-2, Transformer XL"),Xe.forEach(a),De=d(C),O=o(C,"TD",{});var Fe=s(O);$e=i(Fe,"Generaci\xF3n de texto"),Fe.forEach(a),C.forEach(a),Ae=d(B),T=o(B,"TR",{});var P=s(T);U=o(P,"TD",{});var Je=s(U);Be=i(Je,"Codificador-decodificador"),Je.forEach(a),Le=d(P),X=o(P,"TD",{});var Ye=s(X);Ce=i(Ye,"BART, T5, Marian, mBART"),Ye.forEach(a),Pe=d(P),F=o(P,"TD",{});var Ke=s(F);we=i(Ke,"Resumen, traducci\xF3n, respuesta generativa a preguntas"),Ke.forEach(a),P.forEach(a),B.forEach(a),te.forEach(a),this.h()},h(){w(f,"name","hf:doc:metadata"),w(f,"content",JSON.stringify(ft)),w(_,"id","resumen"),w(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(_,"href","#resumen"),w(m,"class","relative group")},m(t,c){e(document.head,f),E(t,J,c),E(t,m,c),e(m,_),e(_,g),We(y,g,null),e(m,ae),e(m,q),e(q,re),E(t,Y,c),We(D,t,c),E(t,K,c),E(t,b,c),e(b,oe),e(b,x),e(x,se),e(b,ne),E(t,Q,c),E(t,$,c),e($,le),E(t,V,c),E(t,R,c),e(R,G),e(G,u),e(u,H),e(H,de),e(u,ie),e(u,M),e(M,ce),e(u,fe),e(u,j),e(j,me),e(R,ue),e(R,p),e(p,h),e(h,I),e(I,pe),e(h,he),e(h,N),e(N,ve),e(h,Te),e(h,S),e(S,Ee),e(p,_e),e(p,v),e(v,k),e(k,be),e(v,Re),e(v,z),e(z,ye),e(v,De),e(v,O),e(O,$e),e(p,Ae),e(p,T),e(T,U),e(U,Be),e(T,Le),e(T,X),e(X,Ce),e(T,Pe),e(T,F),e(F,we),W=!0},p:nt,i(t){W||(Ze(y.$$.fragment,t),Ze(D.$$.fragment,t),W=!0)},o(t){et(y.$$.fragment,t),et(D.$$.fragment,t),W=!1},d(t){a(f),t&&a(J),t&&a(m),tt(y),t&&a(Y),tt(D,t),t&&a(K),t&&a(b),t&&a(Q),t&&a($),t&&a(V),t&&a(R)}}}const ft={local:"resumen",title:"Resumen"};function mt(ge){return lt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vt extends at{constructor(f){super();rt(this,f,mt,ct,ot,{})}}export{vt as default,ft as metadata};
