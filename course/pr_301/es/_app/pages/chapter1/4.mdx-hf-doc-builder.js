import{S as Zc,i as Kc,s as eu,e as o,k as u,w as h,t as n,M as au,c as s,d as r,m as p,a as t,x as g,h as l,b as c,N as m,G as a,g as d,y as E,L as ru,q as b,o as _,B as q,v as ou}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ii}from"../../chunks/Youtube-hf-doc-builder.js";import{I as N}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as su}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function tu(xi){let S,qo,R,re,ar,Te,nt,rr,lt,yo,Pe,jo,ga,it,To,D,oe,or,$e,dt,sr,ct,Po,Ea,ut,$o,B,ke,Mi,pt,we,Li,ko,se,mt,Ae,ft,vt,wo,f,tr,te,nr,ht,gt,Ie,Et,bt,_t,lr,ne,ir,qt,yt,xe,jt,Tt,Pt,dr,le,cr,$t,kt,Me,wt,At,It,ur,ie,pr,xt,Mt,Le,Lt,zt,Gt,mr,j,fr,Nt,St,ze,Rt,Dt,Ge,Bt,Ot,Ct,vr,T,hr,Vt,Ht,Ne,Ut,Ft,gr,Yt,Jt,Ao,ba,Xt,Io,P,Se,Qt,Er,Wt,Zt,Kt,Re,en,br,an,rn,on,De,sn,_r,tn,nn,xo,_a,ln,Mo,O,de,qr,Be,dn,yr,cn,Lo,ce,un,jr,pn,mn,zo,$,fn,Tr,vn,hn,Pr,gn,En,Go,k,bn,$r,_n,qn,kr,yn,jn,No,C,Oe,zi,Tn,Ce,Gi,So,ue,Pn,wr,$n,kn,Ro,V,Ve,Ni,wn,He,Si,Do,H,pe,Ar,Ue,An,Ir,In,Bo,qa,xn,Oo,Fe,Ye,Ri,Co,ya,Mn,Vo,U,Je,Di,Ln,Xe,Bi,Ho,Qe,Uo,ja,zn,Fo,Ta,Gn,Yo,Pa,Nn,Jo,F,me,xr,We,Sn,Mr,Rn,Xo,Ze,Qo,fe,Dn,Lr,Bn,On,Wo,Y,Ke,Oi,Cn,ea,Ci,Zo,$a,Vn,Ko,w,Hn,zr,Un,Fn,Gr,Yn,Jn,es,A,Nr,Xn,Qn,Sr,Wn,Zn,Rr,Kn,as,ve,el,Dr,al,rl,rs,J,aa,Vi,ol,ra,Hi,os,ka,sl,ss,wa,tl,ts,X,he,Br,oa,nl,Or,ll,ns,Aa,il,ls,sa,is,Q,ge,Cr,ta,dl,Vr,cl,ds,Ia,ul,cs,Ee,xa,Hr,pl,ml,fl,Ma,Ur,vl,hl,us,W,na,Ui,gl,la,Fi,ps,La,El,ms,I,za,Fr,bl,_l,ql,Ga,Yr,yl,jl,Tl,be,Jr,Pl,$l,Xr,kl,wl,fs,Na,Al,vs,Z,_e,Qr,ia,Il,Wr,xl,hs,x,Ml,Zr,Ll,zl,da,Gl,Nl,gs,Sa,Sl,Es,Ra,Rl,bs,Da,Dl,_s,K,qe,Kr,ca,Bl,eo,Ol,qs,Ba,Cl,ys,Oa,Vl,js,Ca,Hl,Ts,ee,ua,Yi,Ul,pa,Ji,Ps,Va,Fl,$s,ye,Yl,ao,Jl,Xl,ks,ae,je,ro,ma,Ql,oo,Wl,ws,v,Zl,so,Kl,ei,to,ai,ri,no,oi,si,lo,ti,ni,As,M,Ha,io,li,ii,di,Ua,co,ci,ui,pi,L,uo,mi,fi,po,vi,hi,mo,gi,Ei,Is,z,bi,fo,_i,qi,vo,yi,ji,xs;return Te=new N({}),Pe=new su({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),$e=new N({}),Be=new N({}),Ue=new N({}),Qe=new Ii({props:{id:"ftWlj4FBHTg"}}),We=new N({}),Ze=new Ii({props:{id:"BqqfQnyjmgg"}}),oa=new N({}),sa=new Ii({props:{id:"H39Z_720T5s"}}),ta=new N({}),ia=new N({}),ca=new N({}),ma=new N({}),{c(){S=o("meta"),qo=u(),R=o("h1"),re=o("a"),ar=o("span"),h(Te.$$.fragment),nt=u(),rr=o("span"),lt=n("\xBFC\xF3mo funcionan los Transformadores?"),yo=u(),h(Pe.$$.fragment),jo=u(),ga=o("p"),it=n("En esta secci\xF3n, daremos una mirada de alto nivel a la arquitectura de los Transformadores."),To=u(),D=o("h2"),oe=o("a"),or=o("span"),h($e.$$.fragment),dt=u(),sr=o("span"),ct=n("Un poco de historia sobre los Transformadores"),Po=u(),Ea=o("p"),ut=n("Estos son algunos hitos en la (corta) historia de los Transformadores:"),$o=u(),B=o("div"),ke=o("img"),pt=u(),we=o("img"),ko=u(),se=o("p"),mt=n("La "),Ae=o("a"),ft=n("arquitectura de los Transformadores"),vt=n(" fue presentada por primera vez en junio de 2017. El trabajo original se enfocaba en tareas de traducci\xF3n. A esto le sigu\xF3 la introducci\xF3n de numerosos modelos influyentes, que incluyen:"),wo=u(),f=o("ul"),tr=o("li"),te=o("p"),nr=o("strong"),ht=n("Junio de 2018"),gt=n(": "),Ie=o("a"),Et=n("GPT"),bt=n(", el primer modelo de Transformadores preentrenados, que fue usado para ajustar varias tareas de PLN y obtuvo resultados de vanguardia"),_t=u(),lr=o("li"),ne=o("p"),ir=o("strong"),qt=n("Octubre de 2018"),yt=n(": "),xe=o("a"),jt=n("BERT"),Tt=n(", otro gran modelo preentrenado, dise\xF1ado para producir mejores res\xFAmenes de oraciones (\xA1m\xE1s sobre esto en el siguiente cap\xEDtulo!)"),Pt=u(),dr=o("li"),le=o("p"),cr=o("strong"),$t=n("Febrero de 2019"),kt=n(": "),Me=o("a"),wt=n("GPT-2"),At=n(", una versi\xF3n mejorada (y m\xE1s grande) de GPT, que no se liber\xF3 inmediatamente al p\xFAblico por consideraciones \xE9ticas"),It=u(),ur=o("li"),ie=o("p"),pr=o("strong"),xt=n("Octubre de 2019"),Mt=n(": "),Le=o("a"),Lt=n("DistilBERT"),zt=n(", una versi\xF3n destilada de BERT que es 60% m\xE1s r\xE1pida, 40% m\xE1s ligera en memoria y que retiene el 97% del desempe\xF1o de BERT"),Gt=u(),mr=o("li"),j=o("p"),fr=o("strong"),Nt=n("Octubre de 2019"),St=n(": "),ze=o("a"),Rt=n("BART"),Dt=n(" y "),Ge=o("a"),Bt=n("T5"),Ot=n(", dos grandes modelos preentrenados usando la misma arquitectura del modelo original de Transformador (los primeros en hacerlo)"),Ct=u(),vr=o("li"),T=o("p"),hr=o("strong"),Vt=n("Mayo de 2020"),Ht=n(", "),Ne=o("a"),Ut=n("GPT-3"),Ft=n(", una versi\xF3n a\xFAn m\xE1s grande de GPT-2 con buen desempe\xF1o en una gran variedad de tareas sin la necesidad de ajustes (llamado "),gr=o("em"),Yt=n("zero-shot learning"),Jt=n(")"),Ao=u(),ba=o("p"),Xt=n("Esta lista est\xE1 lejos de ser exhaustiva y solo pretende resaltar algunos de los diferentes modelos de Transformadores. De manera general, estos pueden agruparse en tres categor\xEDas:"),Io=u(),P=o("ul"),Se=o("li"),Qt=n("Parecidos a GPT (tambi\xE9n llamados modelos "),Er=o("em"),Wt=n("auto-regressive"),Zt=n(")"),Kt=u(),Re=o("li"),en=n("Parecidos a BERT (tambi\xE9n llamados modelos "),br=o("em"),an=n("auto-encoding"),rn=n(")"),on=u(),De=o("li"),sn=n("Parecidos a BART/T5 (tambi\xE9n llamados modelos "),_r=o("em"),tn=n("sequence-to-sequence"),nn=n(")"),xo=u(),_a=o("p"),ln=n("Vamos a entrar en estas familias de modelos a profundidad m\xE1s adelante."),Mo=u(),O=o("h2"),de=o("a"),qr=o("span"),h(Be.$$.fragment),dn=u(),yr=o("span"),cn=n("Los Transformadores son modelos de lenguaje"),Lo=u(),ce=o("p"),un=n("Todos los modelos de Transformadores mencionados con anterioridad (GPT, BERT, BART, T5, etc.) han sido entrenados como "),jr=o("em"),pn=n("modelos de lenguaje"),mn=n(". Esto significa que han sido entrenados con grandes cantidades de texto crudo de una manera auto-supervisada. El aprendizaje auto-supervisado es un tipo de entrenamiento en el que el objetivo se computa autom\xE1ticamente de las entradas del modelo. \xA1Esto significa que no necesitan humanos que etiqueten los datos!"),zo=u(),$=o("p"),fn=n("Este tipo de modelos desarrolla un entendimiento estad\xEDstico del lenguaje sobre el que fue entrenado, pero no es muy \xFAtil para tareas pr\xE1cticas espec\xEDficas. Por lo anterior, el modelo general preentrenado pasa por un proceso llamado "),Tr=o("em"),vn=n("transferencia de aprendizaje"),hn=n(" (o "),Pr=o("em"),gn=n("transfer learning"),En=n(" en Ingl\xE9s). Durante este proceso, el modelo se ajusta de una forma supervisada \u2014 esto es, usando etiquetas hechas por humanos \u2014 para una tarea dada."),Go=u(),k=o("p"),bn=n("Un ejemplo de una tarea es predecir la palabra siguiente en una oraci\xF3n con base en las "),$r=o("em"),_n=n("n"),qn=n(" palabras previas. Esto se denomina "),kr=o("em"),yn=n("modelado de lenguaje causal"),jn=n(" porque la salida depende de las entradas pasadas y presentes, pero no en las futuras."),No=u(),C=o("div"),Oe=o("img"),Tn=u(),Ce=o("img"),So=u(),ue=o("p"),Pn=n("Otro ejemplo es el "),wr=o("em"),$n=n("modelado de leguaje oculto"),kn=n(", en el que el modelo predice una palabra oculta en la oraci\xF3n."),Ro=u(),V=o("div"),Ve=o("img"),wn=u(),He=o("img"),Do=u(),H=o("h2"),pe=o("a"),Ar=o("span"),h(Ue.$$.fragment),An=u(),Ir=o("span"),In=n("Los Transformadores son modelos grandes"),Bo=u(),qa=o("p"),xn=n("Excepto algunos casos at\xEDpicos (como DistilBERT), la estrategia general para mejorar el desempe\xF1o es incrementar el tama\xF1o de los modelos, as\xED como la cantidad de datos con los que est\xE1n preentrenados."),Oo=u(),Fe=o("div"),Ye=o("img"),Co=u(),ya=o("p"),Mn=n("Desafortunadamente, entrenar un modelo, especialmente uno grande, requiere de grandes cantidades de datos. Esto se vuelve muy costoso en t\xE9rminos de tiempo y recursos de computaci\xF3n, que se traduce incluso en impacto ambiental, como se puede ver en la siguiente gr\xE1fica."),Vo=u(),U=o("div"),Je=o("img"),Ln=u(),Xe=o("img"),Ho=u(),h(Qe.$$.fragment),Uo=u(),ja=o("p"),zn=n("Esto es ilustrativo para un proyecto que busca un modelo (muy grande), liderado por un equipo que intenta de manera consciente reducir el impacto ambiental del preentrenamiento. La huella de ejecutar muchas pruebas para encontrar los mejores hiperpar\xE1metros es a\xFAn mayor."),Fo=u(),Ta=o("p"),Gn=n("Ahora imag\xEDnate si cada vez que un equipo de investigaci\xF3n, una organizaci\xF3n estudiantil o una compa\xF1\xEDa intentaran entrenar un modelo, tuvieran que hacerlo desde cero. \xA1Esto implicar\xEDa costos globales enormes e innecesarios!"),Yo=u(),Pa=o("p"),Nn=n("Esta es la raz\xF3n por la que compartir modelos de lenguaje es fundamental: compartir los pesos entrenados y construir sobre los existentes reduce el costo general y la huella de carbono de la comunidad."),Jo=u(),F=o("h2"),me=o("a"),xr=o("span"),h(We.$$.fragment),Sn=u(),Mr=o("span"),Rn=n("Transferencia de aprendizaje (*Transfer learning*)"),Xo=u(),h(Ze.$$.fragment),Qo=u(),fe=o("p"),Dn=n("El "),Lr=o("em"),Bn=n("preentrenamiento"),On=n(" es el acto de entrenar un modelo desde cero: los pesos se inicializan de manera aleat\xF3ria y el entrenamiento empieza sin un conocimiento previo."),Wo=u(),Y=o("div"),Ke=o("img"),Cn=u(),ea=o("img"),Zo=u(),$a=o("p"),Vn=n("Este preentrenamiento se hace usualmente sobre grandes cantidades de datos. Por lo anterior, requiere un gran corpus de datos y el entrenamiento puede tomar varias semanas."),Ko=u(),w=o("p"),Hn=n("Por su parte, el ajuste (o "),zr=o("em"),Un=n("fine-tuning"),Fn=n(") es el entrenamiento realizado "),Gr=o("strong"),Yn=n("despu\xE9s"),Jn=n(" de que el modelo ha sido preentrenado. Para hacer el ajuste, comienzas con un modelo de lenguaje preentrenado y luego realizas un aprendizaje adicional con un conjunto de datos espec\xEDficos para tu tarea. Pero entonces \u2014 \xBFpor qu\xE9 no entrenar directamente para la tarea final? Hay un par de razones:"),es=u(),A=o("ul"),Nr=o("li"),Xn=n("El modelo preentrenado ya est\xE1 entrenado con un conjunto de datos parecido al conjunto de datos de ajuste. De esta manera, el proceso de ajuste puede hacer uso del conocimiento adquirido por el modelo inicial durante el preentrenamiento (por ejemplo, para problemas de PLN, el modelo preentrenado tendr\xE1 alg\xFAn tipo de entendimiento estad\xEDstico del idioma que est\xE1s usando para tu tarea)."),Qn=u(),Sr=o("li"),Wn=n("Dado que el modelo preentrenado fue entrenado con muchos datos, el ajuste requerir\xE1 menos datos para tener resultados decentes."),Zn=u(),Rr=o("li"),Kn=n("Por la misma raz\xF3n, la cantidad de tiempo y recursos necesarios para tener buenos resultados es mucho menor."),as=u(),ve=o("p"),el=n("Por ejemplo, se podr\xEDa aprovechar un modelo preentrenado en Ingl\xE9s y despu\xE9s ajustarlo con un corpus arXiv, teniendo como resultado un modelo basado en investigaci\xF3n cient\xEDfica. El ajuste solo requerir\xE1 una cantidad limitada de datos: el conocimiento que el modelo preentrenado ha adquirido se \u201Ctransfiere\u201D, de ah\xED el t\xE9rmino "),Dr=o("em"),al=n("transferencia de aprendizaje"),rl=n("."),rs=u(),J=o("div"),aa=o("img"),ol=u(),ra=o("img"),os=u(),ka=o("p"),sl=n("De ese modo, el ajuste de un modelo tendr\xE1 menos costos de tiempo, de datos, financieros y ambientales. Adem\xE1s es m\xE1s r\xE1pido y m\xE1s f\xE1cil de iterar en diferentes esquemas de ajuste, dado que el entrenamiento es menos restrictivo que un preentrenamiento completo."),ss=u(),wa=o("p"),tl=n("Este proceso tambi\xE9n conseguir\xE1 mejores resultados que entrenar desde cero (a menos que tengas una gran cantidad de datos), raz\xF3n por la cual siempre deber\xEDas intentar aprovechar un modelo preentrenado \u2014 uno que est\xE9 tan cerca como sea posible a la tarea respectiva \u2014 y ajustarlo."),ts=u(),X=o("h2"),he=o("a"),Br=o("span"),h(oa.$$.fragment),nl=u(),Or=o("span"),ll=n("Arquitectura general"),ns=u(),Aa=o("p"),il=n("En esta secci\xF3n, revisaremos la arquitectura general del Transformador. No te preocupes si no entiendes algunos de los conceptos; hay secciones detalladas m\xE1s adelante para cada uno de los componentes."),ls=u(),h(sa.$$.fragment),is=u(),Q=o("h2"),ge=o("a"),Cr=o("span"),h(ta.$$.fragment),dl=u(),Vr=o("span"),cl=n("Introducci\xF3n"),ds=u(),Ia=o("p"),ul=n("El modelo est\xE1 compuesto por dos bloques:"),cs=u(),Ee=o("ul"),xa=o("li"),Hr=o("strong"),pl=n("Codificador (izquierda)"),ml=n(": El codificador recibe una entrada y construye una representaci\xF3n de \xE9sta (sus caracter\xEDsticas). Esto significa que el modelo est\xE1 optimizado para conseguir un entendimiento a partir de la entrada."),fl=u(),Ma=o("li"),Ur=o("strong"),vl=n("Decodificador (derecha)"),hl=n(": El decodificador usa la representac\xF3n del codificador (caracter\xEDsticas) junto con otras entradas para generar una secuencia objetivo. Esto significa que el modelo est\xE1 optimizado para generar salidas."),us=u(),W=o("div"),na=o("img"),gl=u(),la=o("img"),ps=u(),La=o("p"),El=n("Cada una de estas partes puede ser usada de manera independiente, dependiendo de la tarea:"),ms=u(),I=o("ul"),za=o("li"),Fr=o("strong"),bl=n("Modelos con solo codificadores"),_l=n(": Buenos para las tareas que requieren el entendimiento de la entrada, como la clasificaci\xF3n de oraciones y reconocimiento de entidades nombradas."),ql=u(),Ga=o("li"),Yr=o("strong"),yl=n("Modelos con solo decodificadores"),jl=n(": Buenos para tareas generativas como la generaci\xF3n de textos."),Tl=u(),be=o("li"),Jr=o("strong"),Pl=n("Modelos con codificadores y decodificadores"),$l=n(" o "),Xr=o("strong"),kl=n("Modelos secuencia a secuencia"),wl=n(": Buenos para tareas generativas que requieren una entrada, como la traducci\xF3n o resumen."),fs=u(),Na=o("p"),Al=n("Vamos a abordar estas arquitecturas de manera independiente en secciones posteriores."),vs=u(),Z=o("h2"),_e=o("a"),Qr=o("span"),h(ia.$$.fragment),Il=u(),Wr=o("span"),xl=n("Capas de atenci\xF3n"),hs=u(),x=o("p"),Ml=n("Una caracter\xEDstica clave de los Transformadores es que est\xE1n construidos con capas especiales llamadas "),Zr=o("em"),Ll=n("capas de atenci\xF3n"),zl=n(". De hecho, el t\xEDtulo del trabajo que introdujo la arquitectura de los Transformadores fue "),da=o("a"),Gl=n("\u201CAttention Is All You Need\u201D"),Nl=n(". Vamos a explorar los detalles de las capas de atenci\xF3n m\xE1s adelante en el curso; por ahora, todo lo que tienes que saber es que esta capa va a indicarle al modelo que tiene que prestar especial atenci\xF3n a ciertas partes de la oraci\xF3n que le pasaste (y m\xE1s o menos ignorar las dem\xE1s), cuando trabaje con la representaci\xF3n de cada palabra."),gs=u(),Sa=o("p"),Sl=n("Para poner esto en contexto, piensa en la tarea de traducir texto de Ingl\xE9s a Franc\xE9s. Dada la entrada \u201CYou like this course\u201D, un modelo de traducci\xF3n necesitar\xE1 tener en cuenta la palabra adyacente \u201CYou\u201D para obtener la traducci\xF3n correcta de la palabra \u201Clike\u201D, porque en Franc\xE9s el verbo \u201Clike\u201D se conjuga de manera distinta dependiendo del sujeto. Sin embargo, el resto de la oraci\xF3n no es \xFAtil para la traducci\xF3n de esa palabra. En la misma l\xEDnea, al traducir \u201Cthis\u201D, el modelo tambi\xE9n deber\xE1 prestar atenci\xF3n a la palabra \u201Ccourse\u201D, porque \u201Cthis\u201D se traduce de manera distinta dependiendo de si el nombre asociado es masculino o femenino. De nuevo, las otras palabras en la oraci\xF3n no van a importar para la traducci\xF3n de \u201Cthis\u201D. Con oraciones (y reglas gramaticales) m\xE1s complejas, el modelo deber\xE1 prestar especial atenci\xF3n a palabras que pueden aparecer m\xE1s lejos en la oraci\xF3n para traducir correctamente cada palabra."),Es=u(),Ra=o("p"),Rl=n("El mismo concepto aplica para cualquier tarea asociada con lenguaje natural: una palabra por si misma tiene un significado, pero ese significado est\xE1 afectado profundamente por el contexto, que puede ser cualquier palabra (o palabras) antes o despu\xE9s de la palabra que est\xE1 siendo estudiada."),bs=u(),Da=o("p"),Dl=n("Ahora que tienes una idea de qu\xE9 son las capas de atenci\xF3n, echemos un vistazo m\xE1s de cerca a la arquitectura del Transformador."),_s=u(),K=o("h2"),qe=o("a"),Kr=o("span"),h(ca.$$.fragment),Bl=u(),eo=o("span"),Ol=n("La arquitectura original"),qs=u(),Ba=o("p"),Cl=n("La arquitectura del Transformador fue dise\xF1ada originalmente para traducci\xF3n. Durante el entrenamiento, el codificador recibe entradas (oraciones) en un idioma dado, mientras que el decodificador recibe las mismas oraciones en el idioma objetivo. En el codificador, las capas de atenci\xF3n pueden usar todas las palabras en una oraci\xF3n (dado que, como vimos, la traducci\xF3n de una palabra dada puede ser dependiente de lo que est\xE1 antes y despu\xE9s en la oraci\xF3n). Por su parte, el decodificador trabaja de manera secuencial y s\xF3lo le puede prestar atenci\xF3n a las palabras en la oraci\xF3n que ya ha traducido (es decir, s\xF3lo las palabras antes de que la palabra se ha generado). Por ejemplo, cuando hemos predecido las primeras tres palabras del objetivo de traducci\xF3n se las damos al decodificador, que luego usa todas las entradas del codificador para intentar predecir la cuarta palabra."),ys=u(),Oa=o("p"),Vl=n("Para acelerar el entrenamiento (cuando el modelo tiene acceso a las oraciones objetivo), al decodificador se le alimenta el objetivo completo, pero no puede usar palabras futuras (si tuviera acceso a la palabra en la posici\xF3n 2 cuando trata de predecir la palabra en la posici\xF3n 2, \xA1el problema no ser\xEDa muy dificil!). Por ejemplo, al intentar predecir la cuarta palabra, la capa de atenci\xF3n s\xF3lo tendr\xEDa acceso a las palabras en las posiciones 1 a 3."),js=u(),Ca=o("p"),Hl=n("La arquitectura original del Transformador se ve\xEDa as\xED, con el codificador a la izquierda y el decodificador a la derecha:"),Ts=u(),ee=o("div"),ua=o("img"),Ul=u(),pa=o("img"),Ps=u(),Va=o("p"),Fl=n("Observa que la primera capa de atenci\xF3n en un bloque de decodificador presta atenci\xF3n a todas las entradas (pasadas) al decodificador, mientras que la segunda capa de atenci\xF3n usa la salida del codificador. De esta manera puede acceder a toda la oraci\xF3n de entrada para predecir de mejor manera la palabra actual. Esto es muy \xFAtil dado que diferentes idiomas pueden tener reglas gramaticales que ponen las palabras en \xF3rden distinto o alg\xFAn contexto que se provee despu\xE9s puede ser \xFAtil para determinar la mejor traducci\xF3n de una palabra dada."),$s=u(),ye=o("p"),Yl=n("La "),ao=o("em"),Jl=n("m\xE1scara de atenci\xF3n"),Xl=n(" tambi\xE9n se puede usar en el codificador/decodificador para evitar que el modelo preste atenci\xF3n a algunas palabras especiales \u2014por ejemplo, la palabra especial de relleno que hace que todas las entradas sean de la misma longitud cuando se agrupan oraciones."),ks=u(),ae=o("h2"),je=o("a"),ro=o("span"),h(ma.$$.fragment),Ql=u(),oo=o("span"),Wl=n("Arquitecturas vs. puntos de control"),ws=u(),v=o("p"),Zl=n("A medida que estudiemos a profundidad los Transformadores, ver\xE1s menciones a "),so=o("em"),Kl=n("arquitecturas"),ei=n(", "),to=o("em"),ai=n("puntos de control"),ri=n(" ("),no=o("em"),oi=n("checkpoints"),si=n(") y "),lo=o("em"),ti=n("modelos"),ni=n(". Estos t\xE9rminos tienen significados ligeramentes diferentes:"),As=u(),M=o("ul"),Ha=o("li"),io=o("strong"),li=n("Arquitecturas"),ii=n(": Este es el esqueleto del modelo \u2014 la definici\xF3n de cada capa y cada operaci\xF3n que sucede al interior del modelo."),di=u(),Ua=o("li"),co=o("strong"),ci=n("Puntos de control"),ui=n(": Estos son los pesos que ser\xE1n cargados en una arquitectura dada."),pi=u(),L=o("li"),uo=o("strong"),mi=n("Modelo"),fi=n(": Esta es un t\xE9rmino sombrilla que no es tan preciso como \u201Carquitectura\u201D o \u201Cpunto de control\u201D y puede significar ambas cosas. Este curso especificar\xE1 "),po=o("em"),vi=n("arquitectura"),hi=n(" o "),mo=o("em"),gi=n("punto de control"),Ei=n(" cuando sea relevante para evitar ambig\xFCedades."),Is=u(),z=o("p"),bi=n("Por ejemplo, mientras que BERT es una arquitectura, "),fo=o("code"),_i=n("bert-base-cased"),qi=n(" - un conjunto de pesos entrenados por el equipo de Google para la primera versi\xF3n de BERT - es un punto de control. Sin embargo, se podr\xEDa decir \u201Cel modelo BERT\u201D y \u201Cel modelo "),vo=o("code"),yi=n("bert-base-cased"),ji=n("\u201C."),this.h()},l(e){const i=au('[data-svelte="svelte-1phssyn"]',document.head);S=s(i,"META",{name:!0,content:!0}),i.forEach(r),qo=p(e),R=s(e,"H1",{class:!0});var Ms=t(R);re=s(Ms,"A",{id:!0,class:!0,href:!0});var Xi=t(re);ar=s(Xi,"SPAN",{});var Qi=t(ar);g(Te.$$.fragment,Qi),Qi.forEach(r),Xi.forEach(r),nt=p(Ms),rr=s(Ms,"SPAN",{});var Wi=t(rr);lt=l(Wi,"\xBFC\xF3mo funcionan los Transformadores?"),Wi.forEach(r),Ms.forEach(r),yo=p(e),g(Pe.$$.fragment,e),jo=p(e),ga=s(e,"P",{});var Zi=t(ga);it=l(Zi,"En esta secci\xF3n, daremos una mirada de alto nivel a la arquitectura de los Transformadores."),Zi.forEach(r),To=p(e),D=s(e,"H2",{class:!0});var Ls=t(D);oe=s(Ls,"A",{id:!0,class:!0,href:!0});var Ki=t(oe);or=s(Ki,"SPAN",{});var ed=t(or);g($e.$$.fragment,ed),ed.forEach(r),Ki.forEach(r),dt=p(Ls),sr=s(Ls,"SPAN",{});var ad=t(sr);ct=l(ad,"Un poco de historia sobre los Transformadores"),ad.forEach(r),Ls.forEach(r),Po=p(e),Ea=s(e,"P",{});var rd=t(Ea);ut=l(rd,"Estos son algunos hitos en la (corta) historia de los Transformadores:"),rd.forEach(r),$o=p(e),B=s(e,"DIV",{class:!0});var zs=t(B);ke=s(zs,"IMG",{class:!0,src:!0,alt:!0}),pt=p(zs),we=s(zs,"IMG",{class:!0,src:!0,alt:!0}),zs.forEach(r),ko=p(e),se=s(e,"P",{});var Gs=t(se);mt=l(Gs,"La "),Ae=s(Gs,"A",{href:!0,rel:!0});var od=t(Ae);ft=l(od,"arquitectura de los Transformadores"),od.forEach(r),vt=l(Gs," fue presentada por primera vez en junio de 2017. El trabajo original se enfocaba en tareas de traducci\xF3n. A esto le sigu\xF3 la introducci\xF3n de numerosos modelos influyentes, que incluyen:"),Gs.forEach(r),wo=p(e),f=s(e,"UL",{});var y=t(f);tr=s(y,"LI",{});var sd=t(tr);te=s(sd,"P",{});var ho=t(te);nr=s(ho,"STRONG",{});var td=t(nr);ht=l(td,"Junio de 2018"),td.forEach(r),gt=l(ho,": "),Ie=s(ho,"A",{href:!0,rel:!0});var nd=t(Ie);Et=l(nd,"GPT"),nd.forEach(r),bt=l(ho,", el primer modelo de Transformadores preentrenados, que fue usado para ajustar varias tareas de PLN y obtuvo resultados de vanguardia"),ho.forEach(r),sd.forEach(r),_t=p(y),lr=s(y,"LI",{});var ld=t(lr);ne=s(ld,"P",{});var go=t(ne);ir=s(go,"STRONG",{});var id=t(ir);qt=l(id,"Octubre de 2018"),id.forEach(r),yt=l(go,": "),xe=s(go,"A",{href:!0,rel:!0});var dd=t(xe);jt=l(dd,"BERT"),dd.forEach(r),Tt=l(go,", otro gran modelo preentrenado, dise\xF1ado para producir mejores res\xFAmenes de oraciones (\xA1m\xE1s sobre esto en el siguiente cap\xEDtulo!)"),go.forEach(r),ld.forEach(r),Pt=p(y),dr=s(y,"LI",{});var cd=t(dr);le=s(cd,"P",{});var Eo=t(le);cr=s(Eo,"STRONG",{});var ud=t(cr);$t=l(ud,"Febrero de 2019"),ud.forEach(r),kt=l(Eo,": "),Me=s(Eo,"A",{href:!0,rel:!0});var pd=t(Me);wt=l(pd,"GPT-2"),pd.forEach(r),At=l(Eo,", una versi\xF3n mejorada (y m\xE1s grande) de GPT, que no se liber\xF3 inmediatamente al p\xFAblico por consideraciones \xE9ticas"),Eo.forEach(r),cd.forEach(r),It=p(y),ur=s(y,"LI",{});var md=t(ur);ie=s(md,"P",{});var bo=t(ie);pr=s(bo,"STRONG",{});var fd=t(pr);xt=l(fd,"Octubre de 2019"),fd.forEach(r),Mt=l(bo,": "),Le=s(bo,"A",{href:!0,rel:!0});var vd=t(Le);Lt=l(vd,"DistilBERT"),vd.forEach(r),zt=l(bo,", una versi\xF3n destilada de BERT que es 60% m\xE1s r\xE1pida, 40% m\xE1s ligera en memoria y que retiene el 97% del desempe\xF1o de BERT"),bo.forEach(r),md.forEach(r),Gt=p(y),mr=s(y,"LI",{});var hd=t(mr);j=s(hd,"P",{});var fa=t(j);fr=s(fa,"STRONG",{});var gd=t(fr);Nt=l(gd,"Octubre de 2019"),gd.forEach(r),St=l(fa,": "),ze=s(fa,"A",{href:!0,rel:!0});var Ed=t(ze);Rt=l(Ed,"BART"),Ed.forEach(r),Dt=l(fa," y "),Ge=s(fa,"A",{href:!0,rel:!0});var bd=t(Ge);Bt=l(bd,"T5"),bd.forEach(r),Ot=l(fa,", dos grandes modelos preentrenados usando la misma arquitectura del modelo original de Transformador (los primeros en hacerlo)"),fa.forEach(r),hd.forEach(r),Ct=p(y),vr=s(y,"LI",{});var _d=t(vr);T=s(_d,"P",{});var va=t(T);hr=s(va,"STRONG",{});var qd=t(hr);Vt=l(qd,"Mayo de 2020"),qd.forEach(r),Ht=l(va,", "),Ne=s(va,"A",{href:!0,rel:!0});var yd=t(Ne);Ut=l(yd,"GPT-3"),yd.forEach(r),Ft=l(va,", una versi\xF3n a\xFAn m\xE1s grande de GPT-2 con buen desempe\xF1o en una gran variedad de tareas sin la necesidad de ajustes (llamado "),gr=s(va,"EM",{});var jd=t(gr);Yt=l(jd,"zero-shot learning"),jd.forEach(r),Jt=l(va,")"),va.forEach(r),_d.forEach(r),y.forEach(r),Ao=p(e),ba=s(e,"P",{});var Td=t(ba);Xt=l(Td,"Esta lista est\xE1 lejos de ser exhaustiva y solo pretende resaltar algunos de los diferentes modelos de Transformadores. De manera general, estos pueden agruparse en tres categor\xEDas:"),Td.forEach(r),Io=p(e),P=s(e,"UL",{});var Fa=t(P);Se=s(Fa,"LI",{});var Ns=t(Se);Qt=l(Ns,"Parecidos a GPT (tambi\xE9n llamados modelos "),Er=s(Ns,"EM",{});var Pd=t(Er);Wt=l(Pd,"auto-regressive"),Pd.forEach(r),Zt=l(Ns,")"),Ns.forEach(r),Kt=p(Fa),Re=s(Fa,"LI",{});var Ss=t(Re);en=l(Ss,"Parecidos a BERT (tambi\xE9n llamados modelos "),br=s(Ss,"EM",{});var $d=t(br);an=l($d,"auto-encoding"),$d.forEach(r),rn=l(Ss,")"),Ss.forEach(r),on=p(Fa),De=s(Fa,"LI",{});var Rs=t(De);sn=l(Rs,"Parecidos a BART/T5 (tambi\xE9n llamados modelos "),_r=s(Rs,"EM",{});var kd=t(_r);tn=l(kd,"sequence-to-sequence"),kd.forEach(r),nn=l(Rs,")"),Rs.forEach(r),Fa.forEach(r),xo=p(e),_a=s(e,"P",{});var wd=t(_a);ln=l(wd,"Vamos a entrar en estas familias de modelos a profundidad m\xE1s adelante."),wd.forEach(r),Mo=p(e),O=s(e,"H2",{class:!0});var Ds=t(O);de=s(Ds,"A",{id:!0,class:!0,href:!0});var Ad=t(de);qr=s(Ad,"SPAN",{});var Id=t(qr);g(Be.$$.fragment,Id),Id.forEach(r),Ad.forEach(r),dn=p(Ds),yr=s(Ds,"SPAN",{});var xd=t(yr);cn=l(xd,"Los Transformadores son modelos de lenguaje"),xd.forEach(r),Ds.forEach(r),Lo=p(e),ce=s(e,"P",{});var Bs=t(ce);un=l(Bs,"Todos los modelos de Transformadores mencionados con anterioridad (GPT, BERT, BART, T5, etc.) han sido entrenados como "),jr=s(Bs,"EM",{});var Md=t(jr);pn=l(Md,"modelos de lenguaje"),Md.forEach(r),mn=l(Bs,". Esto significa que han sido entrenados con grandes cantidades de texto crudo de una manera auto-supervisada. El aprendizaje auto-supervisado es un tipo de entrenamiento en el que el objetivo se computa autom\xE1ticamente de las entradas del modelo. \xA1Esto significa que no necesitan humanos que etiqueten los datos!"),Bs.forEach(r),zo=p(e),$=s(e,"P",{});var Ya=t($);fn=l(Ya,"Este tipo de modelos desarrolla un entendimiento estad\xEDstico del lenguaje sobre el que fue entrenado, pero no es muy \xFAtil para tareas pr\xE1cticas espec\xEDficas. Por lo anterior, el modelo general preentrenado pasa por un proceso llamado "),Tr=s(Ya,"EM",{});var Ld=t(Tr);vn=l(Ld,"transferencia de aprendizaje"),Ld.forEach(r),hn=l(Ya," (o "),Pr=s(Ya,"EM",{});var zd=t(Pr);gn=l(zd,"transfer learning"),zd.forEach(r),En=l(Ya," en Ingl\xE9s). Durante este proceso, el modelo se ajusta de una forma supervisada \u2014 esto es, usando etiquetas hechas por humanos \u2014 para una tarea dada."),Ya.forEach(r),Go=p(e),k=s(e,"P",{});var Ja=t(k);bn=l(Ja,"Un ejemplo de una tarea es predecir la palabra siguiente en una oraci\xF3n con base en las "),$r=s(Ja,"EM",{});var Gd=t($r);_n=l(Gd,"n"),Gd.forEach(r),qn=l(Ja," palabras previas. Esto se denomina "),kr=s(Ja,"EM",{});var Nd=t(kr);yn=l(Nd,"modelado de lenguaje causal"),Nd.forEach(r),jn=l(Ja," porque la salida depende de las entradas pasadas y presentes, pero no en las futuras."),Ja.forEach(r),No=p(e),C=s(e,"DIV",{class:!0});var Os=t(C);Oe=s(Os,"IMG",{class:!0,src:!0,alt:!0}),Tn=p(Os),Ce=s(Os,"IMG",{class:!0,src:!0,alt:!0}),Os.forEach(r),So=p(e),ue=s(e,"P",{});var Cs=t(ue);Pn=l(Cs,"Otro ejemplo es el "),wr=s(Cs,"EM",{});var Sd=t(wr);$n=l(Sd,"modelado de leguaje oculto"),Sd.forEach(r),kn=l(Cs,", en el que el modelo predice una palabra oculta en la oraci\xF3n."),Cs.forEach(r),Ro=p(e),V=s(e,"DIV",{class:!0});var Vs=t(V);Ve=s(Vs,"IMG",{class:!0,src:!0,alt:!0}),wn=p(Vs),He=s(Vs,"IMG",{class:!0,src:!0,alt:!0}),Vs.forEach(r),Do=p(e),H=s(e,"H2",{class:!0});var Hs=t(H);pe=s(Hs,"A",{id:!0,class:!0,href:!0});var Rd=t(pe);Ar=s(Rd,"SPAN",{});var Dd=t(Ar);g(Ue.$$.fragment,Dd),Dd.forEach(r),Rd.forEach(r),An=p(Hs),Ir=s(Hs,"SPAN",{});var Bd=t(Ir);In=l(Bd,"Los Transformadores son modelos grandes"),Bd.forEach(r),Hs.forEach(r),Bo=p(e),qa=s(e,"P",{});var Od=t(qa);xn=l(Od,"Excepto algunos casos at\xEDpicos (como DistilBERT), la estrategia general para mejorar el desempe\xF1o es incrementar el tama\xF1o de los modelos, as\xED como la cantidad de datos con los que est\xE1n preentrenados."),Od.forEach(r),Oo=p(e),Fe=s(e,"DIV",{class:!0});var Cd=t(Fe);Ye=s(Cd,"IMG",{src:!0,alt:!0,width:!0}),Cd.forEach(r),Co=p(e),ya=s(e,"P",{});var Vd=t(ya);Mn=l(Vd,"Desafortunadamente, entrenar un modelo, especialmente uno grande, requiere de grandes cantidades de datos. Esto se vuelve muy costoso en t\xE9rminos de tiempo y recursos de computaci\xF3n, que se traduce incluso en impacto ambiental, como se puede ver en la siguiente gr\xE1fica."),Vd.forEach(r),Vo=p(e),U=s(e,"DIV",{class:!0});var Us=t(U);Je=s(Us,"IMG",{class:!0,src:!0,alt:!0}),Ln=p(Us),Xe=s(Us,"IMG",{class:!0,src:!0,alt:!0}),Us.forEach(r),Ho=p(e),g(Qe.$$.fragment,e),Uo=p(e),ja=s(e,"P",{});var Hd=t(ja);zn=l(Hd,"Esto es ilustrativo para un proyecto que busca un modelo (muy grande), liderado por un equipo que intenta de manera consciente reducir el impacto ambiental del preentrenamiento. La huella de ejecutar muchas pruebas para encontrar los mejores hiperpar\xE1metros es a\xFAn mayor."),Hd.forEach(r),Fo=p(e),Ta=s(e,"P",{});var Ud=t(Ta);Gn=l(Ud,"Ahora imag\xEDnate si cada vez que un equipo de investigaci\xF3n, una organizaci\xF3n estudiantil o una compa\xF1\xEDa intentaran entrenar un modelo, tuvieran que hacerlo desde cero. \xA1Esto implicar\xEDa costos globales enormes e innecesarios!"),Ud.forEach(r),Yo=p(e),Pa=s(e,"P",{});var Fd=t(Pa);Nn=l(Fd,"Esta es la raz\xF3n por la que compartir modelos de lenguaje es fundamental: compartir los pesos entrenados y construir sobre los existentes reduce el costo general y la huella de carbono de la comunidad."),Fd.forEach(r),Jo=p(e),F=s(e,"H2",{class:!0});var Fs=t(F);me=s(Fs,"A",{id:!0,class:!0,href:!0});var Yd=t(me);xr=s(Yd,"SPAN",{});var Jd=t(xr);g(We.$$.fragment,Jd),Jd.forEach(r),Yd.forEach(r),Sn=p(Fs),Mr=s(Fs,"SPAN",{});var Xd=t(Mr);Rn=l(Xd,"Transferencia de aprendizaje (*Transfer learning*)"),Xd.forEach(r),Fs.forEach(r),Xo=p(e),g(Ze.$$.fragment,e),Qo=p(e),fe=s(e,"P",{});var Ys=t(fe);Dn=l(Ys,"El "),Lr=s(Ys,"EM",{});var Qd=t(Lr);Bn=l(Qd,"preentrenamiento"),Qd.forEach(r),On=l(Ys," es el acto de entrenar un modelo desde cero: los pesos se inicializan de manera aleat\xF3ria y el entrenamiento empieza sin un conocimiento previo."),Ys.forEach(r),Wo=p(e),Y=s(e,"DIV",{class:!0});var Js=t(Y);Ke=s(Js,"IMG",{class:!0,src:!0,alt:!0}),Cn=p(Js),ea=s(Js,"IMG",{class:!0,src:!0,alt:!0}),Js.forEach(r),Zo=p(e),$a=s(e,"P",{});var Wd=t($a);Vn=l(Wd,"Este preentrenamiento se hace usualmente sobre grandes cantidades de datos. Por lo anterior, requiere un gran corpus de datos y el entrenamiento puede tomar varias semanas."),Wd.forEach(r),Ko=p(e),w=s(e,"P",{});var Xa=t(w);Hn=l(Xa,"Por su parte, el ajuste (o "),zr=s(Xa,"EM",{});var Zd=t(zr);Un=l(Zd,"fine-tuning"),Zd.forEach(r),Fn=l(Xa,") es el entrenamiento realizado "),Gr=s(Xa,"STRONG",{});var Kd=t(Gr);Yn=l(Kd,"despu\xE9s"),Kd.forEach(r),Jn=l(Xa," de que el modelo ha sido preentrenado. Para hacer el ajuste, comienzas con un modelo de lenguaje preentrenado y luego realizas un aprendizaje adicional con un conjunto de datos espec\xEDficos para tu tarea. Pero entonces \u2014 \xBFpor qu\xE9 no entrenar directamente para la tarea final? Hay un par de razones:"),Xa.forEach(r),es=p(e),A=s(e,"UL",{});var Qa=t(A);Nr=s(Qa,"LI",{});var ec=t(Nr);Xn=l(ec,"El modelo preentrenado ya est\xE1 entrenado con un conjunto de datos parecido al conjunto de datos de ajuste. De esta manera, el proceso de ajuste puede hacer uso del conocimiento adquirido por el modelo inicial durante el preentrenamiento (por ejemplo, para problemas de PLN, el modelo preentrenado tendr\xE1 alg\xFAn tipo de entendimiento estad\xEDstico del idioma que est\xE1s usando para tu tarea)."),ec.forEach(r),Qn=p(Qa),Sr=s(Qa,"LI",{});var ac=t(Sr);Wn=l(ac,"Dado que el modelo preentrenado fue entrenado con muchos datos, el ajuste requerir\xE1 menos datos para tener resultados decentes."),ac.forEach(r),Zn=p(Qa),Rr=s(Qa,"LI",{});var rc=t(Rr);Kn=l(rc,"Por la misma raz\xF3n, la cantidad de tiempo y recursos necesarios para tener buenos resultados es mucho menor."),rc.forEach(r),Qa.forEach(r),as=p(e),ve=s(e,"P",{});var Xs=t(ve);el=l(Xs,"Por ejemplo, se podr\xEDa aprovechar un modelo preentrenado en Ingl\xE9s y despu\xE9s ajustarlo con un corpus arXiv, teniendo como resultado un modelo basado en investigaci\xF3n cient\xEDfica. El ajuste solo requerir\xE1 una cantidad limitada de datos: el conocimiento que el modelo preentrenado ha adquirido se \u201Ctransfiere\u201D, de ah\xED el t\xE9rmino "),Dr=s(Xs,"EM",{});var oc=t(Dr);al=l(oc,"transferencia de aprendizaje"),oc.forEach(r),rl=l(Xs,"."),Xs.forEach(r),rs=p(e),J=s(e,"DIV",{class:!0});var Qs=t(J);aa=s(Qs,"IMG",{class:!0,src:!0,alt:!0}),ol=p(Qs),ra=s(Qs,"IMG",{class:!0,src:!0,alt:!0}),Qs.forEach(r),os=p(e),ka=s(e,"P",{});var sc=t(ka);sl=l(sc,"De ese modo, el ajuste de un modelo tendr\xE1 menos costos de tiempo, de datos, financieros y ambientales. Adem\xE1s es m\xE1s r\xE1pido y m\xE1s f\xE1cil de iterar en diferentes esquemas de ajuste, dado que el entrenamiento es menos restrictivo que un preentrenamiento completo."),sc.forEach(r),ss=p(e),wa=s(e,"P",{});var tc=t(wa);tl=l(tc,"Este proceso tambi\xE9n conseguir\xE1 mejores resultados que entrenar desde cero (a menos que tengas una gran cantidad de datos), raz\xF3n por la cual siempre deber\xEDas intentar aprovechar un modelo preentrenado \u2014 uno que est\xE9 tan cerca como sea posible a la tarea respectiva \u2014 y ajustarlo."),tc.forEach(r),ts=p(e),X=s(e,"H2",{class:!0});var Ws=t(X);he=s(Ws,"A",{id:!0,class:!0,href:!0});var nc=t(he);Br=s(nc,"SPAN",{});var lc=t(Br);g(oa.$$.fragment,lc),lc.forEach(r),nc.forEach(r),nl=p(Ws),Or=s(Ws,"SPAN",{});var ic=t(Or);ll=l(ic,"Arquitectura general"),ic.forEach(r),Ws.forEach(r),ns=p(e),Aa=s(e,"P",{});var dc=t(Aa);il=l(dc,"En esta secci\xF3n, revisaremos la arquitectura general del Transformador. No te preocupes si no entiendes algunos de los conceptos; hay secciones detalladas m\xE1s adelante para cada uno de los componentes."),dc.forEach(r),ls=p(e),g(sa.$$.fragment,e),is=p(e),Q=s(e,"H2",{class:!0});var Zs=t(Q);ge=s(Zs,"A",{id:!0,class:!0,href:!0});var cc=t(ge);Cr=s(cc,"SPAN",{});var uc=t(Cr);g(ta.$$.fragment,uc),uc.forEach(r),cc.forEach(r),dl=p(Zs),Vr=s(Zs,"SPAN",{});var pc=t(Vr);cl=l(pc,"Introducci\xF3n"),pc.forEach(r),Zs.forEach(r),ds=p(e),Ia=s(e,"P",{});var mc=t(Ia);ul=l(mc,"El modelo est\xE1 compuesto por dos bloques:"),mc.forEach(r),cs=p(e),Ee=s(e,"UL",{});var Ks=t(Ee);xa=s(Ks,"LI",{});var Ti=t(xa);Hr=s(Ti,"STRONG",{});var fc=t(Hr);pl=l(fc,"Codificador (izquierda)"),fc.forEach(r),ml=l(Ti,": El codificador recibe una entrada y construye una representaci\xF3n de \xE9sta (sus caracter\xEDsticas). Esto significa que el modelo est\xE1 optimizado para conseguir un entendimiento a partir de la entrada."),Ti.forEach(r),fl=p(Ks),Ma=s(Ks,"LI",{});var Pi=t(Ma);Ur=s(Pi,"STRONG",{});var vc=t(Ur);vl=l(vc,"Decodificador (derecha)"),vc.forEach(r),hl=l(Pi,": El decodificador usa la representac\xF3n del codificador (caracter\xEDsticas) junto con otras entradas para generar una secuencia objetivo. Esto significa que el modelo est\xE1 optimizado para generar salidas."),Pi.forEach(r),Ks.forEach(r),us=p(e),W=s(e,"DIV",{class:!0});var et=t(W);na=s(et,"IMG",{class:!0,src:!0,alt:!0}),gl=p(et),la=s(et,"IMG",{class:!0,src:!0,alt:!0}),et.forEach(r),ps=p(e),La=s(e,"P",{});var hc=t(La);El=l(hc,"Cada una de estas partes puede ser usada de manera independiente, dependiendo de la tarea:"),hc.forEach(r),ms=p(e),I=s(e,"UL",{});var Wa=t(I);za=s(Wa,"LI",{});var $i=t(za);Fr=s($i,"STRONG",{});var gc=t(Fr);bl=l(gc,"Modelos con solo codificadores"),gc.forEach(r),_l=l($i,": Buenos para las tareas que requieren el entendimiento de la entrada, como la clasificaci\xF3n de oraciones y reconocimiento de entidades nombradas."),$i.forEach(r),ql=p(Wa),Ga=s(Wa,"LI",{});var ki=t(Ga);Yr=s(ki,"STRONG",{});var Ec=t(Yr);yl=l(Ec,"Modelos con solo decodificadores"),Ec.forEach(r),jl=l(ki,": Buenos para tareas generativas como la generaci\xF3n de textos."),ki.forEach(r),Tl=p(Wa),be=s(Wa,"LI",{});var _o=t(be);Jr=s(_o,"STRONG",{});var bc=t(Jr);Pl=l(bc,"Modelos con codificadores y decodificadores"),bc.forEach(r),$l=l(_o," o "),Xr=s(_o,"STRONG",{});var _c=t(Xr);kl=l(_c,"Modelos secuencia a secuencia"),_c.forEach(r),wl=l(_o,": Buenos para tareas generativas que requieren una entrada, como la traducci\xF3n o resumen."),_o.forEach(r),Wa.forEach(r),fs=p(e),Na=s(e,"P",{});var qc=t(Na);Al=l(qc,"Vamos a abordar estas arquitecturas de manera independiente en secciones posteriores."),qc.forEach(r),vs=p(e),Z=s(e,"H2",{class:!0});var at=t(Z);_e=s(at,"A",{id:!0,class:!0,href:!0});var yc=t(_e);Qr=s(yc,"SPAN",{});var jc=t(Qr);g(ia.$$.fragment,jc),jc.forEach(r),yc.forEach(r),Il=p(at),Wr=s(at,"SPAN",{});var Tc=t(Wr);xl=l(Tc,"Capas de atenci\xF3n"),Tc.forEach(r),at.forEach(r),hs=p(e),x=s(e,"P",{});var Za=t(x);Ml=l(Za,"Una caracter\xEDstica clave de los Transformadores es que est\xE1n construidos con capas especiales llamadas "),Zr=s(Za,"EM",{});var Pc=t(Zr);Ll=l(Pc,"capas de atenci\xF3n"),Pc.forEach(r),zl=l(Za,". De hecho, el t\xEDtulo del trabajo que introdujo la arquitectura de los Transformadores fue "),da=s(Za,"A",{href:!0,rel:!0});var $c=t(da);Gl=l($c,"\u201CAttention Is All You Need\u201D"),$c.forEach(r),Nl=l(Za,". Vamos a explorar los detalles de las capas de atenci\xF3n m\xE1s adelante en el curso; por ahora, todo lo que tienes que saber es que esta capa va a indicarle al modelo que tiene que prestar especial atenci\xF3n a ciertas partes de la oraci\xF3n que le pasaste (y m\xE1s o menos ignorar las dem\xE1s), cuando trabaje con la representaci\xF3n de cada palabra."),Za.forEach(r),gs=p(e),Sa=s(e,"P",{});var kc=t(Sa);Sl=l(kc,"Para poner esto en contexto, piensa en la tarea de traducir texto de Ingl\xE9s a Franc\xE9s. Dada la entrada \u201CYou like this course\u201D, un modelo de traducci\xF3n necesitar\xE1 tener en cuenta la palabra adyacente \u201CYou\u201D para obtener la traducci\xF3n correcta de la palabra \u201Clike\u201D, porque en Franc\xE9s el verbo \u201Clike\u201D se conjuga de manera distinta dependiendo del sujeto. Sin embargo, el resto de la oraci\xF3n no es \xFAtil para la traducci\xF3n de esa palabra. En la misma l\xEDnea, al traducir \u201Cthis\u201D, el modelo tambi\xE9n deber\xE1 prestar atenci\xF3n a la palabra \u201Ccourse\u201D, porque \u201Cthis\u201D se traduce de manera distinta dependiendo de si el nombre asociado es masculino o femenino. De nuevo, las otras palabras en la oraci\xF3n no van a importar para la traducci\xF3n de \u201Cthis\u201D. Con oraciones (y reglas gramaticales) m\xE1s complejas, el modelo deber\xE1 prestar especial atenci\xF3n a palabras que pueden aparecer m\xE1s lejos en la oraci\xF3n para traducir correctamente cada palabra."),kc.forEach(r),Es=p(e),Ra=s(e,"P",{});var wc=t(Ra);Rl=l(wc,"El mismo concepto aplica para cualquier tarea asociada con lenguaje natural: una palabra por si misma tiene un significado, pero ese significado est\xE1 afectado profundamente por el contexto, que puede ser cualquier palabra (o palabras) antes o despu\xE9s de la palabra que est\xE1 siendo estudiada."),wc.forEach(r),bs=p(e),Da=s(e,"P",{});var Ac=t(Da);Dl=l(Ac,"Ahora que tienes una idea de qu\xE9 son las capas de atenci\xF3n, echemos un vistazo m\xE1s de cerca a la arquitectura del Transformador."),Ac.forEach(r),_s=p(e),K=s(e,"H2",{class:!0});var rt=t(K);qe=s(rt,"A",{id:!0,class:!0,href:!0});var Ic=t(qe);Kr=s(Ic,"SPAN",{});var xc=t(Kr);g(ca.$$.fragment,xc),xc.forEach(r),Ic.forEach(r),Bl=p(rt),eo=s(rt,"SPAN",{});var Mc=t(eo);Ol=l(Mc,"La arquitectura original"),Mc.forEach(r),rt.forEach(r),qs=p(e),Ba=s(e,"P",{});var Lc=t(Ba);Cl=l(Lc,"La arquitectura del Transformador fue dise\xF1ada originalmente para traducci\xF3n. Durante el entrenamiento, el codificador recibe entradas (oraciones) en un idioma dado, mientras que el decodificador recibe las mismas oraciones en el idioma objetivo. En el codificador, las capas de atenci\xF3n pueden usar todas las palabras en una oraci\xF3n (dado que, como vimos, la traducci\xF3n de una palabra dada puede ser dependiente de lo que est\xE1 antes y despu\xE9s en la oraci\xF3n). Por su parte, el decodificador trabaja de manera secuencial y s\xF3lo le puede prestar atenci\xF3n a las palabras en la oraci\xF3n que ya ha traducido (es decir, s\xF3lo las palabras antes de que la palabra se ha generado). Por ejemplo, cuando hemos predecido las primeras tres palabras del objetivo de traducci\xF3n se las damos al decodificador, que luego usa todas las entradas del codificador para intentar predecir la cuarta palabra."),Lc.forEach(r),ys=p(e),Oa=s(e,"P",{});var zc=t(Oa);Vl=l(zc,"Para acelerar el entrenamiento (cuando el modelo tiene acceso a las oraciones objetivo), al decodificador se le alimenta el objetivo completo, pero no puede usar palabras futuras (si tuviera acceso a la palabra en la posici\xF3n 2 cuando trata de predecir la palabra en la posici\xF3n 2, \xA1el problema no ser\xEDa muy dificil!). Por ejemplo, al intentar predecir la cuarta palabra, la capa de atenci\xF3n s\xF3lo tendr\xEDa acceso a las palabras en las posiciones 1 a 3."),zc.forEach(r),js=p(e),Ca=s(e,"P",{});var Gc=t(Ca);Hl=l(Gc,"La arquitectura original del Transformador se ve\xEDa as\xED, con el codificador a la izquierda y el decodificador a la derecha:"),Gc.forEach(r),Ts=p(e),ee=s(e,"DIV",{class:!0});var ot=t(ee);ua=s(ot,"IMG",{class:!0,src:!0,alt:!0}),Ul=p(ot),pa=s(ot,"IMG",{class:!0,src:!0,alt:!0}),ot.forEach(r),Ps=p(e),Va=s(e,"P",{});var Nc=t(Va);Fl=l(Nc,"Observa que la primera capa de atenci\xF3n en un bloque de decodificador presta atenci\xF3n a todas las entradas (pasadas) al decodificador, mientras que la segunda capa de atenci\xF3n usa la salida del codificador. De esta manera puede acceder a toda la oraci\xF3n de entrada para predecir de mejor manera la palabra actual. Esto es muy \xFAtil dado que diferentes idiomas pueden tener reglas gramaticales que ponen las palabras en \xF3rden distinto o alg\xFAn contexto que se provee despu\xE9s puede ser \xFAtil para determinar la mejor traducci\xF3n de una palabra dada."),Nc.forEach(r),$s=p(e),ye=s(e,"P",{});var st=t(ye);Yl=l(st,"La "),ao=s(st,"EM",{});var Sc=t(ao);Jl=l(Sc,"m\xE1scara de atenci\xF3n"),Sc.forEach(r),Xl=l(st," tambi\xE9n se puede usar en el codificador/decodificador para evitar que el modelo preste atenci\xF3n a algunas palabras especiales \u2014por ejemplo, la palabra especial de relleno que hace que todas las entradas sean de la misma longitud cuando se agrupan oraciones."),st.forEach(r),ks=p(e),ae=s(e,"H2",{class:!0});var tt=t(ae);je=s(tt,"A",{id:!0,class:!0,href:!0});var Rc=t(je);ro=s(Rc,"SPAN",{});var Dc=t(ro);g(ma.$$.fragment,Dc),Dc.forEach(r),Rc.forEach(r),Ql=p(tt),oo=s(tt,"SPAN",{});var Bc=t(oo);Wl=l(Bc,"Arquitecturas vs. puntos de control"),Bc.forEach(r),tt.forEach(r),ws=p(e),v=s(e,"P",{});var G=t(v);Zl=l(G,"A medida que estudiemos a profundidad los Transformadores, ver\xE1s menciones a "),so=s(G,"EM",{});var Oc=t(so);Kl=l(Oc,"arquitecturas"),Oc.forEach(r),ei=l(G,", "),to=s(G,"EM",{});var Cc=t(to);ai=l(Cc,"puntos de control"),Cc.forEach(r),ri=l(G," ("),no=s(G,"EM",{});var Vc=t(no);oi=l(Vc,"checkpoints"),Vc.forEach(r),si=l(G,") y "),lo=s(G,"EM",{});var Hc=t(lo);ti=l(Hc,"modelos"),Hc.forEach(r),ni=l(G,". Estos t\xE9rminos tienen significados ligeramentes diferentes:"),G.forEach(r),As=p(e),M=s(e,"UL",{});var Ka=t(M);Ha=s(Ka,"LI",{});var wi=t(Ha);io=s(wi,"STRONG",{});var Uc=t(io);li=l(Uc,"Arquitecturas"),Uc.forEach(r),ii=l(wi,": Este es el esqueleto del modelo \u2014 la definici\xF3n de cada capa y cada operaci\xF3n que sucede al interior del modelo."),wi.forEach(r),di=p(Ka),Ua=s(Ka,"LI",{});var Ai=t(Ua);co=s(Ai,"STRONG",{});var Fc=t(co);ci=l(Fc,"Puntos de control"),Fc.forEach(r),ui=l(Ai,": Estos son los pesos que ser\xE1n cargados en una arquitectura dada."),Ai.forEach(r),pi=p(Ka),L=s(Ka,"LI",{});var ha=t(L);uo=s(ha,"STRONG",{});var Yc=t(uo);mi=l(Yc,"Modelo"),Yc.forEach(r),fi=l(ha,": Esta es un t\xE9rmino sombrilla que no es tan preciso como \u201Carquitectura\u201D o \u201Cpunto de control\u201D y puede significar ambas cosas. Este curso especificar\xE1 "),po=s(ha,"EM",{});var Jc=t(po);vi=l(Jc,"arquitectura"),Jc.forEach(r),hi=l(ha," o "),mo=s(ha,"EM",{});var Xc=t(mo);gi=l(Xc,"punto de control"),Xc.forEach(r),Ei=l(ha," cuando sea relevante para evitar ambig\xFCedades."),ha.forEach(r),Ka.forEach(r),Is=p(e),z=s(e,"P",{});var er=t(z);bi=l(er,"Por ejemplo, mientras que BERT es una arquitectura, "),fo=s(er,"CODE",{});var Qc=t(fo);_i=l(Qc,"bert-base-cased"),Qc.forEach(r),qi=l(er," - un conjunto de pesos entrenados por el equipo de Google para la primera versi\xF3n de BERT - es un punto de control. Sin embargo, se podr\xEDa decir \u201Cel modelo BERT\u201D y \u201Cel modelo "),vo=s(er,"CODE",{});var Wc=t(vo);yi=l(Wc,"bert-base-cased"),Wc.forEach(r),ji=l(er,"\u201C."),er.forEach(r),this.h()},h(){c(S,"name","hf:doc:metadata"),c(S,"content",JSON.stringify(nu)),c(re,"id","cmo-funcionan-los-transformadores"),c(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(re,"href","#cmo-funcionan-los-transformadores"),c(R,"class","relative group"),c(oe,"id","un-poco-de-historia-sobre-los-transformadores"),c(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(oe,"href","#un-poco-de-historia-sobre-los-transformadores"),c(D,"class","relative group"),c(ke,"class","block dark:hidden"),m(ke.src,Mi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||c(ke,"src",Mi),c(ke,"alt","A brief chronology of Transformers models."),c(we,"class","hidden dark:block"),m(we.src,Li="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||c(we,"src",Li),c(we,"alt","A brief chronology of Transformers models."),c(B,"class","flex justify-center"),c(Ae,"href","https://arxiv.org/abs/1706.03762"),c(Ae,"rel","nofollow"),c(Ie,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),c(Ie,"rel","nofollow"),c(xe,"href","https://arxiv.org/abs/1810.04805"),c(xe,"rel","nofollow"),c(Me,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),c(Me,"rel","nofollow"),c(Le,"href","https://arxiv.org/abs/1910.01108"),c(Le,"rel","nofollow"),c(ze,"href","https://arxiv.org/abs/1910.13461"),c(ze,"rel","nofollow"),c(Ge,"href","https://arxiv.org/abs/1910.10683"),c(Ge,"rel","nofollow"),c(Ne,"href","https://arxiv.org/abs/2005.14165"),c(Ne,"rel","nofollow"),c(de,"id","los-transformadores-son-modelos-de-lenguaje"),c(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(de,"href","#los-transformadores-son-modelos-de-lenguaje"),c(O,"class","relative group"),c(Oe,"class","block dark:hidden"),m(Oe.src,zi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||c(Oe,"src",zi),c(Oe,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(Ce,"class","hidden dark:block"),m(Ce.src,Gi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||c(Ce,"src",Gi),c(Ce,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(C,"class","flex justify-center"),c(Ve,"class","block dark:hidden"),m(Ve.src,Ni="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||c(Ve,"src",Ni),c(Ve,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(He,"class","hidden dark:block"),m(He.src,Si="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||c(He,"src",Si),c(He,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(V,"class","flex justify-center"),c(pe,"id","los-transformadores-son-modelos-grandes"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#los-transformadores-son-modelos-grandes"),c(H,"class","relative group"),m(Ye.src,Ri="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||c(Ye,"src",Ri),c(Ye,"alt","Number of parameters of recent Transformers models"),c(Ye,"width","90%"),c(Fe,"class","flex justify-center"),c(Je,"class","block dark:hidden"),m(Je.src,Di="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||c(Je,"src",Di),c(Je,"alt","The carbon footprint of a large language model."),c(Xe,"class","hidden dark:block"),m(Xe.src,Bi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||c(Xe,"src",Bi),c(Xe,"alt","The carbon footprint of a large language model."),c(U,"class","flex justify-center"),c(me,"id","transferencia-de-aprendizaje-transfer-learning"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#transferencia-de-aprendizaje-transfer-learning"),c(F,"class","relative group"),c(Ke,"class","block dark:hidden"),m(Ke.src,Oi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||c(Ke,"src",Oi),c(Ke,"alt","The pretraining of a language model is costly in both time and money."),c(ea,"class","hidden dark:block"),m(ea.src,Ci="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||c(ea,"src",Ci),c(ea,"alt","The pretraining of a language model is costly in both time and money."),c(Y,"class","flex justify-center"),c(aa,"class","block dark:hidden"),m(aa.src,Vi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||c(aa,"src",Vi),c(aa,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(ra,"class","hidden dark:block"),m(ra.src,Hi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||c(ra,"src",Hi),c(ra,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(J,"class","flex justify-center"),c(he,"id","arquitectura-general"),c(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(he,"href","#arquitectura-general"),c(X,"class","relative group"),c(ge,"id","introduccin"),c(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ge,"href","#introduccin"),c(Q,"class","relative group"),c(na,"class","block dark:hidden"),m(na.src,Ui="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||c(na,"src",Ui),c(na,"alt","Architecture of a Transformers models"),c(la,"class","hidden dark:block"),m(la.src,Fi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||c(la,"src",Fi),c(la,"alt","Architecture of a Transformers models"),c(W,"class","flex justify-center"),c(_e,"id","capas-de-atencin"),c(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_e,"href","#capas-de-atencin"),c(Z,"class","relative group"),c(da,"href","https://arxiv.org/abs/1706.03762"),c(da,"rel","nofollow"),c(qe,"id","la-arquitectura-original"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#la-arquitectura-original"),c(K,"class","relative group"),c(ua,"class","block dark:hidden"),m(ua.src,Yi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||c(ua,"src",Yi),c(ua,"alt","Architecture of a Transformers models"),c(pa,"class","hidden dark:block"),m(pa.src,Ji="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||c(pa,"src",Ji),c(pa,"alt","Architecture of a Transformers models"),c(ee,"class","flex justify-center"),c(je,"id","arquitecturas-vs-puntos-de-control"),c(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(je,"href","#arquitecturas-vs-puntos-de-control"),c(ae,"class","relative group")},m(e,i){a(document.head,S),d(e,qo,i),d(e,R,i),a(R,re),a(re,ar),E(Te,ar,null),a(R,nt),a(R,rr),a(rr,lt),d(e,yo,i),E(Pe,e,i),d(e,jo,i),d(e,ga,i),a(ga,it),d(e,To,i),d(e,D,i),a(D,oe),a(oe,or),E($e,or,null),a(D,dt),a(D,sr),a(sr,ct),d(e,Po,i),d(e,Ea,i),a(Ea,ut),d(e,$o,i),d(e,B,i),a(B,ke),a(B,pt),a(B,we),d(e,ko,i),d(e,se,i),a(se,mt),a(se,Ae),a(Ae,ft),a(se,vt),d(e,wo,i),d(e,f,i),a(f,tr),a(tr,te),a(te,nr),a(nr,ht),a(te,gt),a(te,Ie),a(Ie,Et),a(te,bt),a(f,_t),a(f,lr),a(lr,ne),a(ne,ir),a(ir,qt),a(ne,yt),a(ne,xe),a(xe,jt),a(ne,Tt),a(f,Pt),a(f,dr),a(dr,le),a(le,cr),a(cr,$t),a(le,kt),a(le,Me),a(Me,wt),a(le,At),a(f,It),a(f,ur),a(ur,ie),a(ie,pr),a(pr,xt),a(ie,Mt),a(ie,Le),a(Le,Lt),a(ie,zt),a(f,Gt),a(f,mr),a(mr,j),a(j,fr),a(fr,Nt),a(j,St),a(j,ze),a(ze,Rt),a(j,Dt),a(j,Ge),a(Ge,Bt),a(j,Ot),a(f,Ct),a(f,vr),a(vr,T),a(T,hr),a(hr,Vt),a(T,Ht),a(T,Ne),a(Ne,Ut),a(T,Ft),a(T,gr),a(gr,Yt),a(T,Jt),d(e,Ao,i),d(e,ba,i),a(ba,Xt),d(e,Io,i),d(e,P,i),a(P,Se),a(Se,Qt),a(Se,Er),a(Er,Wt),a(Se,Zt),a(P,Kt),a(P,Re),a(Re,en),a(Re,br),a(br,an),a(Re,rn),a(P,on),a(P,De),a(De,sn),a(De,_r),a(_r,tn),a(De,nn),d(e,xo,i),d(e,_a,i),a(_a,ln),d(e,Mo,i),d(e,O,i),a(O,de),a(de,qr),E(Be,qr,null),a(O,dn),a(O,yr),a(yr,cn),d(e,Lo,i),d(e,ce,i),a(ce,un),a(ce,jr),a(jr,pn),a(ce,mn),d(e,zo,i),d(e,$,i),a($,fn),a($,Tr),a(Tr,vn),a($,hn),a($,Pr),a(Pr,gn),a($,En),d(e,Go,i),d(e,k,i),a(k,bn),a(k,$r),a($r,_n),a(k,qn),a(k,kr),a(kr,yn),a(k,jn),d(e,No,i),d(e,C,i),a(C,Oe),a(C,Tn),a(C,Ce),d(e,So,i),d(e,ue,i),a(ue,Pn),a(ue,wr),a(wr,$n),a(ue,kn),d(e,Ro,i),d(e,V,i),a(V,Ve),a(V,wn),a(V,He),d(e,Do,i),d(e,H,i),a(H,pe),a(pe,Ar),E(Ue,Ar,null),a(H,An),a(H,Ir),a(Ir,In),d(e,Bo,i),d(e,qa,i),a(qa,xn),d(e,Oo,i),d(e,Fe,i),a(Fe,Ye),d(e,Co,i),d(e,ya,i),a(ya,Mn),d(e,Vo,i),d(e,U,i),a(U,Je),a(U,Ln),a(U,Xe),d(e,Ho,i),E(Qe,e,i),d(e,Uo,i),d(e,ja,i),a(ja,zn),d(e,Fo,i),d(e,Ta,i),a(Ta,Gn),d(e,Yo,i),d(e,Pa,i),a(Pa,Nn),d(e,Jo,i),d(e,F,i),a(F,me),a(me,xr),E(We,xr,null),a(F,Sn),a(F,Mr),a(Mr,Rn),d(e,Xo,i),E(Ze,e,i),d(e,Qo,i),d(e,fe,i),a(fe,Dn),a(fe,Lr),a(Lr,Bn),a(fe,On),d(e,Wo,i),d(e,Y,i),a(Y,Ke),a(Y,Cn),a(Y,ea),d(e,Zo,i),d(e,$a,i),a($a,Vn),d(e,Ko,i),d(e,w,i),a(w,Hn),a(w,zr),a(zr,Un),a(w,Fn),a(w,Gr),a(Gr,Yn),a(w,Jn),d(e,es,i),d(e,A,i),a(A,Nr),a(Nr,Xn),a(A,Qn),a(A,Sr),a(Sr,Wn),a(A,Zn),a(A,Rr),a(Rr,Kn),d(e,as,i),d(e,ve,i),a(ve,el),a(ve,Dr),a(Dr,al),a(ve,rl),d(e,rs,i),d(e,J,i),a(J,aa),a(J,ol),a(J,ra),d(e,os,i),d(e,ka,i),a(ka,sl),d(e,ss,i),d(e,wa,i),a(wa,tl),d(e,ts,i),d(e,X,i),a(X,he),a(he,Br),E(oa,Br,null),a(X,nl),a(X,Or),a(Or,ll),d(e,ns,i),d(e,Aa,i),a(Aa,il),d(e,ls,i),E(sa,e,i),d(e,is,i),d(e,Q,i),a(Q,ge),a(ge,Cr),E(ta,Cr,null),a(Q,dl),a(Q,Vr),a(Vr,cl),d(e,ds,i),d(e,Ia,i),a(Ia,ul),d(e,cs,i),d(e,Ee,i),a(Ee,xa),a(xa,Hr),a(Hr,pl),a(xa,ml),a(Ee,fl),a(Ee,Ma),a(Ma,Ur),a(Ur,vl),a(Ma,hl),d(e,us,i),d(e,W,i),a(W,na),a(W,gl),a(W,la),d(e,ps,i),d(e,La,i),a(La,El),d(e,ms,i),d(e,I,i),a(I,za),a(za,Fr),a(Fr,bl),a(za,_l),a(I,ql),a(I,Ga),a(Ga,Yr),a(Yr,yl),a(Ga,jl),a(I,Tl),a(I,be),a(be,Jr),a(Jr,Pl),a(be,$l),a(be,Xr),a(Xr,kl),a(be,wl),d(e,fs,i),d(e,Na,i),a(Na,Al),d(e,vs,i),d(e,Z,i),a(Z,_e),a(_e,Qr),E(ia,Qr,null),a(Z,Il),a(Z,Wr),a(Wr,xl),d(e,hs,i),d(e,x,i),a(x,Ml),a(x,Zr),a(Zr,Ll),a(x,zl),a(x,da),a(da,Gl),a(x,Nl),d(e,gs,i),d(e,Sa,i),a(Sa,Sl),d(e,Es,i),d(e,Ra,i),a(Ra,Rl),d(e,bs,i),d(e,Da,i),a(Da,Dl),d(e,_s,i),d(e,K,i),a(K,qe),a(qe,Kr),E(ca,Kr,null),a(K,Bl),a(K,eo),a(eo,Ol),d(e,qs,i),d(e,Ba,i),a(Ba,Cl),d(e,ys,i),d(e,Oa,i),a(Oa,Vl),d(e,js,i),d(e,Ca,i),a(Ca,Hl),d(e,Ts,i),d(e,ee,i),a(ee,ua),a(ee,Ul),a(ee,pa),d(e,Ps,i),d(e,Va,i),a(Va,Fl),d(e,$s,i),d(e,ye,i),a(ye,Yl),a(ye,ao),a(ao,Jl),a(ye,Xl),d(e,ks,i),d(e,ae,i),a(ae,je),a(je,ro),E(ma,ro,null),a(ae,Ql),a(ae,oo),a(oo,Wl),d(e,ws,i),d(e,v,i),a(v,Zl),a(v,so),a(so,Kl),a(v,ei),a(v,to),a(to,ai),a(v,ri),a(v,no),a(no,oi),a(v,si),a(v,lo),a(lo,ti),a(v,ni),d(e,As,i),d(e,M,i),a(M,Ha),a(Ha,io),a(io,li),a(Ha,ii),a(M,di),a(M,Ua),a(Ua,co),a(co,ci),a(Ua,ui),a(M,pi),a(M,L),a(L,uo),a(uo,mi),a(L,fi),a(L,po),a(po,vi),a(L,hi),a(L,mo),a(mo,gi),a(L,Ei),d(e,Is,i),d(e,z,i),a(z,bi),a(z,fo),a(fo,_i),a(z,qi),a(z,vo),a(vo,yi),a(z,ji),xs=!0},p:ru,i(e){xs||(b(Te.$$.fragment,e),b(Pe.$$.fragment,e),b($e.$$.fragment,e),b(Be.$$.fragment,e),b(Ue.$$.fragment,e),b(Qe.$$.fragment,e),b(We.$$.fragment,e),b(Ze.$$.fragment,e),b(oa.$$.fragment,e),b(sa.$$.fragment,e),b(ta.$$.fragment,e),b(ia.$$.fragment,e),b(ca.$$.fragment,e),b(ma.$$.fragment,e),xs=!0)},o(e){_(Te.$$.fragment,e),_(Pe.$$.fragment,e),_($e.$$.fragment,e),_(Be.$$.fragment,e),_(Ue.$$.fragment,e),_(Qe.$$.fragment,e),_(We.$$.fragment,e),_(Ze.$$.fragment,e),_(oa.$$.fragment,e),_(sa.$$.fragment,e),_(ta.$$.fragment,e),_(ia.$$.fragment,e),_(ca.$$.fragment,e),_(ma.$$.fragment,e),xs=!1},d(e){r(S),e&&r(qo),e&&r(R),q(Te),e&&r(yo),q(Pe,e),e&&r(jo),e&&r(ga),e&&r(To),e&&r(D),q($e),e&&r(Po),e&&r(Ea),e&&r($o),e&&r(B),e&&r(ko),e&&r(se),e&&r(wo),e&&r(f),e&&r(Ao),e&&r(ba),e&&r(Io),e&&r(P),e&&r(xo),e&&r(_a),e&&r(Mo),e&&r(O),q(Be),e&&r(Lo),e&&r(ce),e&&r(zo),e&&r($),e&&r(Go),e&&r(k),e&&r(No),e&&r(C),e&&r(So),e&&r(ue),e&&r(Ro),e&&r(V),e&&r(Do),e&&r(H),q(Ue),e&&r(Bo),e&&r(qa),e&&r(Oo),e&&r(Fe),e&&r(Co),e&&r(ya),e&&r(Vo),e&&r(U),e&&r(Ho),q(Qe,e),e&&r(Uo),e&&r(ja),e&&r(Fo),e&&r(Ta),e&&r(Yo),e&&r(Pa),e&&r(Jo),e&&r(F),q(We),e&&r(Xo),q(Ze,e),e&&r(Qo),e&&r(fe),e&&r(Wo),e&&r(Y),e&&r(Zo),e&&r($a),e&&r(Ko),e&&r(w),e&&r(es),e&&r(A),e&&r(as),e&&r(ve),e&&r(rs),e&&r(J),e&&r(os),e&&r(ka),e&&r(ss),e&&r(wa),e&&r(ts),e&&r(X),q(oa),e&&r(ns),e&&r(Aa),e&&r(ls),q(sa,e),e&&r(is),e&&r(Q),q(ta),e&&r(ds),e&&r(Ia),e&&r(cs),e&&r(Ee),e&&r(us),e&&r(W),e&&r(ps),e&&r(La),e&&r(ms),e&&r(I),e&&r(fs),e&&r(Na),e&&r(vs),e&&r(Z),q(ia),e&&r(hs),e&&r(x),e&&r(gs),e&&r(Sa),e&&r(Es),e&&r(Ra),e&&r(bs),e&&r(Da),e&&r(_s),e&&r(K),q(ca),e&&r(qs),e&&r(Ba),e&&r(ys),e&&r(Oa),e&&r(js),e&&r(Ca),e&&r(Ts),e&&r(ee),e&&r(Ps),e&&r(Va),e&&r($s),e&&r(ye),e&&r(ks),e&&r(ae),q(ma),e&&r(ws),e&&r(v),e&&r(As),e&&r(M),e&&r(Is),e&&r(z)}}}const nu={local:"cmo-funcionan-los-transformadores",sections:[{local:"un-poco-de-historia-sobre-los-transformadores",title:"Un poco de historia sobre los Transformadores"},{local:"los-transformadores-son-modelos-de-lenguaje",title:"Los Transformadores son modelos de lenguaje"},{local:"los-transformadores-son-modelos-grandes",title:"Los Transformadores son modelos grandes"},{local:"transferencia-de-aprendizaje-transfer-learning",title:"Transferencia de aprendizaje (*Transfer learning*)"},{local:"arquitectura-general",title:"Arquitectura general"},{local:"introduccin",title:"Introducci\xF3n"},{local:"capas-de-atencin",title:"Capas de atenci\xF3n"},{local:"la-arquitectura-original",title:"La arquitectura original"},{local:"arquitecturas-vs-puntos-de-control",title:"Arquitecturas vs. puntos de control"}],title:"\xBFC\xF3mo funcionan los Transformadores?"};function lu(xi){return ou(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pu extends Zc{constructor(S){super();Kc(this,S,lu,tu,eu,{})}}export{pu as default,nu as metadata};
