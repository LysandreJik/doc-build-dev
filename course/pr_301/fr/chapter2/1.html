<meta charset="utf-8" /><meta http-equiv="content-security-policy" content=""><meta name="hf:doc:metadata" content="{&quot;local&quot;:&quot;introduction&quot;,&quot;title&quot;:&quot;Introduction&quot;}" data-svelte="svelte-1phssyn">
	<link rel="modulepreload" href="/docs/course/pr_301/fr/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
	<link rel="modulepreload" href="/docs/course/pr_301/fr/_app/start-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_301/fr/_app/chunks/vendor-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_301/fr/_app/chunks/paths-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_301/fr/_app/pages/__layout.svelte-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_301/fr/_app/pages/chapter2/1.mdx-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_301/fr/_app/chunks/Tip-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_301/fr/_app/chunks/IconCopyLink-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/pr_301/fr/_app/chunks/CourseFloatingBanner-hf-doc-builder.js"> 





<h1 class="relative group"><a id="introduction" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#introduction"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Introduction
	</span></h1>



<div class="flex space-x-1 absolute z-10 right-0 top-0"><a href="https://discuss.huggingface.co/t/chapter-2-questions" target="_blank"><img alt="Ask a Question" class="!m-0" src="https://img.shields.io/badge/Ask%20a%20question-ffcb4c.svg?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgLTEgMTA0IDEwNiI+PGRlZnM+PHN0eWxlPi5jbHMtMXtmaWxsOiMyMzFmMjA7fS5jbHMtMntmaWxsOiNmZmY5YWU7fS5jbHMtM3tmaWxsOiMwMGFlZWY7fS5jbHMtNHtmaWxsOiMwMGE5NGY7fS5jbHMtNXtmaWxsOiNmMTVkMjI7fS5jbHMtNntmaWxsOiNlMzFiMjM7fTwvc3R5bGU+PC9kZWZzPjx0aXRsZT5EaXNjb3Vyc2VfbG9nbzwvdGl0bGU+PGcgaWQ9IkxheWVyXzIiPjxnIGlkPSJMYXllcl8zIj48cGF0aCBjbGFzcz0iY2xzLTEiIGQ9Ik01MS44NywwQzIzLjcxLDAsMCwyMi44MywwLDUxYzAsLjkxLDAsNTIuODEsMCw1Mi44MWw1MS44Ni0uMDVjMjguMTYsMCw1MS0yMy43MSw1MS01MS44N1M4MCwwLDUxLjg3LDBaIi8+PHBhdGggY2xhc3M9ImNscy0yIiBkPSJNNTIuMzcsMTkuNzRBMzEuNjIsMzEuNjIsMCwwLDAsMjQuNTgsNjYuNDFsLTUuNzIsMTguNEwzOS40LDgwLjE3YTMxLjYxLDMxLjYxLDAsMSwwLDEzLTYwLjQzWiIvPjxwYXRoIGNsYXNzPSJjbHMtMyIgZD0iTTc3LjQ1LDMyLjEyYTMxLjYsMzEuNiwwLDAsMS0zOC4wNSw0OEwxOC44Niw4NC44MmwyMC45MS0yLjQ3QTMxLjYsMzEuNiwwLDAsMCw3Ny40NSwzMi4xMloiLz48cGF0aCBjbGFzcz0iY2xzLTQiIGQ9Ik03MS42MywyNi4yOUEzMS42LDMxLjYsMCwwLDEsMzguOCw3OEwxOC44Niw4NC44MiwzOS40LDgwLjE3QTMxLjYsMzEuNiwwLDAsMCw3MS42MywyNi4yOVoiLz48cGF0aCBjbGFzcz0iY2xzLTUiIGQ9Ik0yNi40Nyw2Ny4xMWEzMS42MSwzMS42MSwwLDAsMSw1MS0zNUEzMS42MSwzMS42MSwwLDAsMCwyNC41OCw2Ni40MWwtNS43MiwxOC40WiIvPjxwYXRoIGNsYXNzPSJjbHMtNiIgZD0iTTI0LjU4LDY2LjQxQTMxLjYxLDMxLjYxLDAsMCwxLDcxLjYzLDI2LjI5YTMxLjYxLDMxLjYxLDAsMCwwLTQ5LDM5LjYzbC0zLjc2LDE4LjlaIi8+PC9nPjwvZz48L3N2Zz4="></a>
	
	</div>
<p>Comme vous l’avez vu dans le <a href="/course/fr/chapter1">chapitre 1</a>, les <em>transformers</em> sont généralement très grands. Pouvant aller de plusieurs millions à des dizaines de milliards de paramètres, l’entraînement et le déploiement de ces modèles est une entreprise compliquée. De plus, avec de nouveaux modèles publiés presque quotidiennement et ayant chacun sa propre implémentation, les essayer tous n’est pas une tâche facile.</p>
<p>La bibliothèque 🤗 <em>Transformers</em> a été créée pour résoudre ce problème. Son objectif est de fournir une API unique à travers laquelle tout modèle de <em>transformers</em> peut être chargé, entraîné et sauvegardé. Les principales caractéristiques de la bibliothèque sont :</p>
<ul><li><strong>La facilité d’utilisation</strong> : en seulement deux lignes de code il est possible de télécharger, charger et utiliser un modèle de NLP à l’état de l’art pour faire de l’inférence,</li>
<li><strong>La flexibilité</strong> : au fond, tous les modèles sont de simples classes PyTorch <code>nn.Module</code> ou TensorFlow <code>tf.keras.Model</code> et peuvent être manipulés comme n’importe quel autre modèle dans leurs <em>frameworks</em> d’apprentissage automatique respectifs,</li>
<li><strong>La simplicité</strong> : pratiquement aucune abstraction n’est faite dans la bibliothèque. Avoir tout dans un fichier est un concept central : la passe avant d’un modèle est entièrement définie dans un seul fichier afin que le code lui-même soit compréhensible et piratable.</li></ul>
<p>Cette dernière caractéristique rend 🤗 <em>Transformers</em> très différent des autres bibliothèques d’apprentissage automatique.
Les modèles ne sont pas construits sur des modules partagés entre plusieurs fichiers. Au lieu de cela, chaque modèle possède ses propres couches.
En plus de rendre les modèles plus accessibles et compréhensibles, cela vous permet d’expérimenter des choses facilement sur un modèle sans affecter les autres.</p>
<p>Ce chapitre commence par un exemple de bout en bout où nous utilisons un modèle et un <em>tokenizer</em> ensemble pour reproduire la fonction <code>pipeline()</code> introduite dans le <a href="/course/fr/chapter1">chapitre 1</a>.
Ensuite, nous aborderons l’API <em>model</em> : nous nous plongerons dans les classes de modèle et de configuration, nous verrons comment charger un modèle et enfin comment il traite les entrées numériques pour produire des prédictions. </p>
<p>Nous examinerons ensuite l’API <em>tokenizer</em> qui est l’autre composant principal de la fonction <code>pipeline()</code>.
Les <em>tokenizers</em> s’occupent de la première et de la dernière étape du traitement en gérant la conversion du texte en entrées numériques pour le réseau neuronal et la reconversion en texte lorsqu’elle est nécessaire.
Enfin, nous montrerons comment gérer l’envoi de plusieurs phrases à travers un modèle dans un batch préparé et nous conclurons le tout en examinant de plus près la fonction <code>tokenizer()</code>.</p>


<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">⚠️ Afin de bénéficier de toutes les fonctionnalités disponibles avec le <i>Hub</i> et la bibliothèque 🤗 <i>Transformers</i>, nous vous recommandons <a href="https://huggingface.co/join">de créer un compte</a>.
</div>


		<script type="module" data-hydrate="1b72edz">
		import { start } from "/docs/course/pr_301/fr/_app/start-hf-doc-builder.js";
		start({
			target: document.querySelector('[data-hydrate="1b72edz"]').parentNode,
			paths: {"base":"/docs/course/pr_301/fr","assets":"/docs/course/pr_301/fr"},
			session: {},
			route: false,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/docs/course/pr_301/fr/_app/pages/__layout.svelte-hf-doc-builder.js"),
						import("/docs/course/pr_301/fr/_app/pages/chapter2/1.mdx-hf-doc-builder.js")
				],
				params: {}
			}
		});
	</script>
