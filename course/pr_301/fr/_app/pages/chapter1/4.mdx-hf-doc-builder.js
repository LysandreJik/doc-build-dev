import{S as Rp,i as jp,s as Sp,e as s,k as d,w as g,t as n,M as zp,c as a,d as r,m,a as o,x as E,h as l,b as c,N as h,G as t,g as u,y as _,L as Bp,q,o as b,B as x,v as Dp}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Yc}from"../../chunks/Youtube-hf-doc-builder.js";import{I as B}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Op}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Up(Fc){let D,da,O,fe,_r,Oe,nl,Ue,ll,qr,ol,il,ma,He,pa,he,ul,br,cl,dl,fa,U,ve,xr,Ve,ml,Ct,pl,$r,fl,ha,ge,hl,Pr,vl,gl,va,H,Ye,Jc,El,Fe,Xc,ga,Je,Ee,_l,kr,ql,bl,Ea,v,wr,$,Mr,xl,$l,Xe,Pl,kl,yr,wl,Ml,Lr,yl,Ll,Tl,Tr,_e,Ir,Il,Al,Qe,Cl,Nl,Gl,Ar,qe,Cr,Rl,jl,We,Sl,zl,Bl,Nr,be,Gr,Dl,Ol,Ze,Ul,Hl,Vl,Rr,P,jr,Yl,Fl,Ke,Jl,Xl,et,Ql,Wl,Sr,Zl,Kl,eo,zr,k,Br,to,ro,tt,so,ao,Dr,no,lo,Or,oo,io,_a,xe,uo,Ur,co,mo,qa,T,V,po,Hr,fo,ho,Vr,vo,go,Eo,Y,_o,Yr,qo,bo,Fr,xo,$o,Po,F,ko,Jr,wo,Mo,Xr,yo,Lo,ba,Nt,To,xa,J,$e,Qr,rt,Io,st,Ao,Wr,Co,No,$a,I,Go,Zr,Ro,jo,Kr,So,zo,Pa,Pe,Bo,es,Do,Oo,ka,A,Uo,ts,Ho,Vo,rs,Yo,Fo,wa,X,at,Qc,Jo,nt,Wc,Ma,ke,Xo,ss,Qo,Wo,ya,Q,lt,Zc,Zo,ot,Kc,La,W,we,as,it,Ko,ut,ei,ns,ti,ri,Ta,Gt,si,Ia,ct,dt,ed,Aa,Rt,ai,Ca,Z,mt,td,ni,pt,rd,Na,ft,Ga,jt,li,Ra,St,oi,ja,zt,ii,Sa,K,Me,ls,ht,ui,os,ci,za,vt,Ba,Bt,di,Da,ee,gt,sd,mi,Et,ad,Oa,Dt,pi,Ua,C,fi,is,hi,vi,us,gi,Ei,Ha,N,te,_i,cs,qi,bi,ds,xi,$i,Pi,_t,ki,ms,wi,Mi,yi,ps,Li,Va,G,Ti,fs,Ii,Ai,hs,Ci,Ni,Ya,re,qt,nd,Gi,bt,ld,Fa,R,Ri,vs,ji,Si,gs,zi,Bi,Ja,ye,Di,Es,Oi,Ui,Xa,se,Le,_s,xt,Hi,qs,Vi,Qa,Te,Yi,bs,Fi,Ji,Wa,$t,Za,ae,Ie,xs,Pt,Xi,$s,Qi,Ka,Ot,Wi,en,Ae,Ut,Ps,Zi,Ki,eu,Ht,ks,tu,ru,tn,ne,kt,od,su,wt,id,rn,Vt,au,sn,j,Yt,ws,nu,lu,ou,Ft,Ms,iu,uu,cu,Ce,ys,du,mu,Ls,pu,fu,an,Jt,hu,nn,le,Ne,Ts,Mt,vu,Is,gu,ln,w,Eu,As,_u,qu,Cs,bu,xu,yt,Ns,$u,Pu,on,p,ku,Gs,wu,Mu,Rs,yu,Lu,js,Tu,Iu,Ss,Au,Cu,zs,Nu,Gu,Bs,Ru,ju,Ds,Su,zu,Os,Bu,Du,un,Xt,Ou,cn,Ge,Uu,Us,Hu,Vu,dn,oe,Re,Hs,Lt,Yu,Vs,Fu,mn,je,Ju,Ys,Xu,Qu,pn,Qt,Wu,fn,Se,Zu,Fs,Ku,ec,hn,ie,Tt,ud,tc,It,cd,vn,Wt,rc,gn,S,sc,Js,ac,nc,Xs,lc,oc,En,ue,ze,Qs,At,ic,Zt,uc,Ws,cc,_n,Kt,dc,qn,er,mc,bn,tr,pc,xn,rr,fc,$n,z,sr,Zs,hc,vc,gc,ar,Ks,Ec,_c,qc,M,ea,bc,xc,ta,$c,Pc,ra,kc,wc,sa,Mc,yc,Pn,y,Lc,aa,Tc,Ic,na,Ac,Cc,la,Nc,Gc,kn;return Oe=new B({}),He=new Op({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),Ve=new B({}),rt=new B({}),it=new B({}),ft=new Yc({props:{id:"ftWlj4FBHTg"}}),ht=new B({}),vt=new Yc({props:{id:"BqqfQnyjmgg"}}),xt=new B({}),$t=new Yc({props:{id:"H39Z_720T5s"}}),Pt=new B({}),Mt=new B({}),Lt=new B({}),At=new B({}),{c(){D=s("meta"),da=d(),O=s("h1"),fe=s("a"),_r=s("span"),g(Oe.$$.fragment),nl=d(),Ue=s("span"),ll=n("Comment fonctionnent les "),qr=s("i"),ol=n("transformers"),il=n(" ?"),ma=d(),g(He.$$.fragment),pa=d(),he=s("p"),ul=n("Dans cette partie, nous allons jeter un coup d\u2019\u0153il \xE0 l\u2019architecture des "),br=s("em"),cl=n("transformers"),dl=n("."),fa=d(),U=s("h2"),ve=s("a"),xr=s("span"),g(Ve.$$.fragment),ml=d(),Ct=s("span"),pl=n("Court historique des "),$r=s("i"),fl=n("transformers"),ha=d(),ge=s("p"),hl=n("Voici quelques dates clefs dans la courte histoire des "),Pr=s("em"),vl=n("transformers"),gl=n(" :"),va=d(),H=s("div"),Ye=s("img"),El=d(),Fe=s("img"),ga=d(),Je=s("p"),Ee=s("a"),_l=n("L\u2019architecture "),kr=s("em"),ql=n("Transformer"),bl=n(" a \xE9t\xE9 pr\xE9sent\xE9e en juin 2017. Initialement, la recherche portait sur la t\xE2che de traduction. Elle a \xE9t\xE9 suivie par l\u2019introduction de plusieurs mod\xE8les influents, notamment :"),Ea=d(),v=s("ul"),wr=s("li"),$=s("p"),Mr=s("strong"),xl=n("Juin 2018"),$l=n(" : "),Xe=s("a"),Pl=n("GPT"),kl=n(", le premier "),yr=s("em"),wl=n("transformer"),Ml=n(" pr\xE9-entra\xEEn\xE9 et "),Lr=s("em"),yl=n("finetun\xE9"),Ll=n(" sur diff\xE9rentes t\xE2ches de NLP et ayant obtenu des r\xE9sultats \xE0 l\u2019\xE9tat de l\u2019art,"),Tl=d(),Tr=s("li"),_e=s("p"),Ir=s("strong"),Il=n("Octobre 2018"),Al=n(" : "),Qe=s("a"),Cl=n("BERT"),Nl=n(", autre grand mod\xE8le pr\xE9-entra\xEEn\xE9 ayant \xE9t\xE9 construit pour produire de meilleurs r\xE9sum\xE9s de texte (plus de d\xE9tails dans le chapitre suivant !),"),Gl=d(),Ar=s("li"),qe=s("p"),Cr=s("strong"),Rl=n("F\xE9vrier 2019"),jl=n(" : "),We=s("a"),Sl=n("GPT-2"),zl=n(", une version am\xE9lior\xE9e (et plus grande) de GPT qui n\u2019a pas \xE9t\xE9 directement rendu publique pour cause de raisons \xE9thiques,"),Bl=d(),Nr=s("li"),be=s("p"),Gr=s("strong"),Dl=n("Octobre 2019"),Ol=n(" : "),Ze=s("a"),Ul=n("DistilBERT"),Hl=n(", une version distill\xE9e de BERT \xE9tant 60% plus rapide, 40% plus l\xE9g\xE8re en m\xE9moire et conservant tout de m\xEAme 97% des performances initiales de BERT,"),Vl=d(),Rr=s("li"),P=s("p"),jr=s("strong"),Yl=n("Octobre 2019"),Fl=n(" : "),Ke=s("a"),Jl=n("BART"),Xl=n(" et "),et=s("a"),Ql=n("T5"),Wl=n(", deux mod\xE8les pr\xE9-entra\xEEn\xE9s utilisant la m\xEAme architecture que le "),Sr=s("em"),Zl=n("transformer"),Kl=n(" original (les premiers \xE0 faire cela),"),eo=d(),zr=s("li"),k=s("p"),Br=s("strong"),to=n("Mai 2020"),ro=n(" : "),tt=s("a"),so=n("GPT-3"),ao=n(", une version encore plus grande que GPT-2 ayant des performances tr\xE8s bonnes sur une vari\xE9t\xE9 de t\xE2ches ne n\xE9cessitant pas de "),Dr=s("em"),no=n("finetuning"),lo=n(" (appel\xE9 "),Or=s("em"),oo=n("zero-shot learning"),io=n(")."),_a=d(),xe=s("p"),uo=n("Cette liste est loin d\u2019\xEAtre exhaustive et met en lumi\xE8re certains "),Ur=s("em"),co=n("transformers"),mo=n(". Plus largement, ces mod\xE8les peuvent \xEAtre regroup\xE9s en trois cat\xE9gories :"),qa=d(),T=s("ul"),V=s("li"),po=n("ceux de type GPT (aussi appel\xE9s "),Hr=s("em"),fo=n("transformers"),ho=d(),Vr=s("em"),vo=n("autor\xE9gressifs"),go=n(")"),Eo=d(),Y=s("li"),_o=n("ceux de type BERT (aussi appel\xE9s "),Yr=s("em"),qo=n("transformers"),bo=d(),Fr=s("em"),xo=n("auto-encodeurs"),$o=n(")"),Po=d(),F=s("li"),ko=n("ceux de type BART/T5 (aussi appel\xE9s "),Jr=s("em"),wo=n("transformers"),Mo=d(),Xr=s("em"),yo=n("s\xE9quence-\xE0-s\xE9quence"),Lo=n(")"),ba=d(),Nt=s("p"),To=n("Nous verrons plus en profondeur ces familles de mod\xE8les plus tard."),xa=d(),J=s("h2"),$e=s("a"),Qr=s("span"),g(rt.$$.fragment),Io=d(),st=s("span"),Ao=n("Les "),Wr=s("i"),Co=n("transformers"),No=n(" sont des mod\xE8les de langage"),$a=d(),I=s("p"),Go=n("Tous les "),Zr=s("em"),Ro=n("transformers"),jo=n(" mentionn\xE9s ci-dessus (GPT, BERT, BART, T5, etc.) ont \xE9t\xE9 entra\xEEn\xE9s comme des "),Kr=s("em"),So=n("mod\xE8les de langage"),zo=n(". Cela signifie qu\u2019ils ont \xE9t\xE9 entra\xEEn\xE9s sur une large quantit\xE9 de textes bruts de mani\xE8re autosupervis\xE9e. L\u2019apprentissage autosupervis\xE9 est un type d\u2019entra\xEEnement dans lequel l\u2019objectif est automatiquement calcul\xE9 \xE0 partir des entr\xE9es du mod\xE8le. Cela signifie que les humains ne sont pas n\xE9cessaires pour \xE9tiqueter les donn\xE9es !"),Pa=d(),Pe=s("p"),Bo=n("Ce type de mod\xE8le d\xE9veloppe une compr\xE9hension statistique de la langue sur laquelle il a \xE9t\xE9 entra\xEEn\xE9, mais il n\u2019est pas tr\xE8s utile pour des t\xE2ches pratiques sp\xE9cifiques. Pour cette raison, le mod\xE8le pr\xE9-entra\xEEn\xE9 passe ensuite par un processus appel\xE9 apprentissage par transfert. Au cours de ce processus, le mod\xE8le est "),es=s("em"),Do=n("finetun\xE9"),Oo=n(" de mani\xE8re supervis\xE9e (c\u2019est-\xE0-dire en utilisant des \xE9tiquettes annot\xE9es par des humains) pour une t\xE2che donn\xE9e."),ka=d(),A=s("p"),Uo=n("Un exemple de t\xE2che consiste \xE0 pr\xE9dire le mot suivant dans une phrase apr\xE8s avoir lu les "),ts=s("em"),Ho=n("n"),Vo=n(" mots pr\xE9c\xE9dents. Cette t\xE2che est appel\xE9e "),rs=s("em"),Yo=n("mod\xE9lisation causale du langage"),Fo=n(" car la sortie d\xE9pend des entr\xE9es pass\xE9es et pr\xE9sentes, mais pas des entr\xE9es futures."),wa=d(),X=s("div"),at=s("img"),Jo=d(),nt=s("img"),Ma=d(),ke=s("p"),Xo=n("Un autre exemple est la "),ss=s("em"),Qo=n("mod\xE9lisation du langage masqu\xE9"),Wo=n(", dans laquelle le mod\xE8le pr\xE9dit un mot masqu\xE9 dans la phrase."),ya=d(),Q=s("div"),lt=s("img"),Zo=d(),ot=s("img"),La=d(),W=s("h2"),we=s("a"),as=s("span"),g(it.$$.fragment),Ko=d(),ut=s("span"),ei=n("Les "),ns=s("i"),ti=n("transformers"),ri=n(" sont \xE9normes"),Ta=d(),Gt=s("p"),si=n("En dehors de quelques exceptions (comme DistilBERT), la strat\xE9gie g\xE9n\xE9rale pour obtenir de meilleure performance consiste \xE0 augmenter la taille des mod\xE8les ainsi que la quantit\xE9 de donn\xE9es utilis\xE9es pour l\u2019entra\xEEnement de ces derniers."),Ia=d(),ct=s("div"),dt=s("img"),Aa=d(),Rt=s("p"),ai=n("Malheureusement, entra\xEEner un mod\xE8le et particuli\xE8rement un tr\xE8s grand mod\xE8le, n\xE9cessite une importante quantit\xE9 de donn\xE9es. Cela devient tr\xE8s co\xFBteux en termes de temps et de ressources de calcul. Cela se traduit m\xEAme par un impact environnemental comme le montre le graphique suivant."),Ca=d(),Z=s("div"),mt=s("img"),ni=d(),pt=s("img"),Na=d(),g(ft.$$.fragment),Ga=d(),jt=s("p"),li=n("L\u2019image montre l\u2019empreinte carbone pour un projet d\u2019entra\xEEnement d\u2019un (tr\xE8s grand) mod\xE8le men\xE9 par une \xE9quipe qui pourtant essaie consciemment de r\xE9duire l\u2019impact environnemental du pr\xE9-entra\xEEnement. L\u2019empreinte de l\u2019ex\xE9cution de nombreux essais pour obtenir les meilleurs hyperparam\xE8tres serait encore plus \xE9lev\xE9e."),Ra=d(),St=s("p"),oi=n("Imaginez qu\u2019\xE0 chaque fois qu\u2019une \xE9quipe de recherche, une association d\u2019\xE9tudiants ou une entreprise souhaite entra\xEEner un mod\xE8le, elle le fasse en partant de z\xE9ro. Cela entra\xEEnerait des co\xFBts globaux \xE9normes et inutiles !"),ja=d(),zt=s("p"),ii=n("C\u2019est pourquoi le partage des mod\xE8les du langage est primordial : partager les poids d\u2019entra\xEEnement et construire \xE0 partir de ces poids permet de r\xE9duire les co\xFBts de calcul globaux ainsi que l\u2019empreinte carbone de toute la communaut\xE9."),Sa=d(),K=s("h2"),Me=s("a"),ls=s("span"),g(ht.$$.fragment),ui=d(),os=s("span"),ci=n("L'apprentissage par transfert"),za=d(),g(vt.$$.fragment),Ba=d(),Bt=s("p"),di=n("Le pr\xE9-entra\xEEnement consiste \xE0 entra\xEEner un mod\xE8le \xE0 partir de z\xE9ro : les poids sont initialis\xE9s de mani\xE8re al\xE9atoire et l\u2019entra\xEEnement commence sans aucune connaissance pr\xE9alable."),Da=d(),ee=s("div"),gt=s("img"),mi=d(),Et=s("img"),Oa=d(),Dt=s("p"),pi=n("Ce pr\xE9-entra\xEEnement est g\xE9n\xE9ralement effectu\xE9 sur de tr\xE8s grandes quantit\xE9s de donn\xE9es. Il n\xE9cessite donc un tr\xE8s grand corpus de donn\xE9es et l\u2019entra\xEEnement peut prendre jusqu\u2019\xE0 plusieurs semaines."),Ua=d(),C=s("p"),fi=n("Le "),is=s("em"),hi=n("finetuning"),vi=n(", quant \xE0 lui, est l\u2019entrainement effectu\xE9 apr\xE8s qu\u2019un mod\xE8le ait \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Pour effectuer un "),us=s("em"),gi=n("finetuning"),Ei=n(", vous devez d\u2019abord acqu\xE9rir un mod\xE8le de langue pr\xE9-entra\xEEn\xE9, puis effectuer un entra\xEEnement suppl\xE9mentaire avec un jeu de donn\xE9es sp\xE9cifiques. Mais pourquoi ne pas entra\xEEner directement pour la t\xE2che finale ? Il y a plusieurs raisons \xE0 cela :"),Ha=d(),N=s("ul"),te=s("li"),_i=n("Le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur un jeu de donn\xE9es qui pr\xE9sente certaines similitudes avec le jeu de donn\xE9es de "),cs=s("em"),qi=n("finetuning"),bi=n(". Le processus de "),ds=s("em"),xi=n("finetuning"),$i=n(" est donc en mesure de tirer parti des connaissances acquises par le mod\xE8le initial lors du pr\xE9-entra\xEEnement (par exemple, pour les probl\xE8mes de langage naturel, le mod\xE8le pr\xE9-entra\xEEn\xE9 aura une certaine compr\xE9hension statistique de la langue que vous utilisez pour votre t\xE2che)"),Pi=d(),_t=s("li"),ki=n("Comme le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur de nombreuses donn\xE9es, le "),ms=s("em"),wi=n("finetuning"),Mi=n(" n\xE9cessite beaucoup moins de donn\xE9es pour obtenir des r\xE9sultats d\xE9cents."),yi=d(),ps=s("li"),Li=n("Pour la m\xEAme raison, le temps et les ressources n\xE9cessaires pour obtenir de bons r\xE9sultats sont beaucoup moins importants."),Va=d(),G=s("p"),Ti=n("Par exemple, il est possible d\u2019exploiter un mod\xE8le pr\xE9-entra\xEEn\xE9 entra\xEEn\xE9 sur la langue anglaise, puis de le "),fs=s("em"),Ii=n("finetuner"),Ai=n(" sur un corpus arXiv, pour obtenir un mod\xE8le bas\xE9 sur la science et la recherche. Le "),hs=s("em"),Ci=n("finetuning"),Ni=n(" ne n\xE9cessitera qu\u2019une quantit\xE9 limit\xE9e de donn\xE9es : les connaissances acquises par le mod\xE8le pr\xE9-entra\xEEn\xE9 sont \xAB transf\xE9r\xE9es \xBB, d\u2019o\xF9 le terme d\u2019apprentissage par transfert."),Ya=d(),re=s("div"),qt=s("img"),Gi=d(),bt=s("img"),Fa=d(),R=s("p"),Ri=n("Le "),vs=s("em"),ji=n("finetuning"),Si=n(" d\u2019un mod\xE8le a donc un co\xFBt moindre en termes de temps, de donn\xE9es, de finances et d\u2019environnement. Il est aussi plus rapide et plus facile d\u2019it\xE9rer sur diff\xE9rents sch\xE9mas de "),gs=s("em"),zi=n("finetuning"),Bi=n(" car l\u2019entra\xEEnement est moins contraignant qu\u2019un pr\xE9-entra\xEEnement complet."),Ja=d(),ye=s("p"),Di=n("Ce processus permet \xE9galement d\u2019obtenir de meilleurs r\xE9sultats que l\u2019entra\xEEnement \xE0 partir de z\xE9ro (\xE0 moins que vous ne disposiez d\u2019un grand nombre de donn\xE9es). C\u2019est pourquoi vous devez toujours essayer de tirer parti d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9, c\u2019est-\xE0-dire un mod\xE8le aussi proche que possible de la t\xE2che que vous avez \xE0 accomplir, et de le "),Es=s("em"),Oi=n("finetuner"),Ui=n("."),Xa=d(),se=s("h2"),Le=s("a"),_s=s("span"),g(xt.$$.fragment),Hi=d(),qs=s("span"),Vi=n("Architecture g\xE9n\xE9rale"),Qa=d(),Te=s("p"),Yi=n("Dans cette section, nous allons voir l\u2019architecture g\xE9n\xE9rale des "),bs=s("em"),Fi=n("transformers"),Ji=n(". Pas d\u2019inqui\xE9tudes si vous ne comprenez pas tous les concepts, des sections d\xE9taill\xE9es qui couvrent chaque composant seront abord\xE9es plus tard."),Wa=d(),g($t.$$.fragment),Za=d(),ae=s("h2"),Ie=s("a"),xs=s("span"),g(Pt.$$.fragment),Xi=d(),$s=s("span"),Qi=n("Introduction"),Ka=d(),Ot=s("p"),Wi=n("Le mod\xE8le est principalement compos\xE9 de deux blocs :"),en=d(),Ae=s("ul"),Ut=s("li"),Ps=s("strong"),Zi=n("Encodeur (\xE0 gauche)"),Ki=n(" : l\u2019encodeur re\xE7oit une entr\xE9e et construit une repr\xE9sentation de celle-ci (ses caract\xE9ristiques). Cela signifie que le mod\xE8le est optimis\xE9 pour acqu\xE9rir une compr\xE9hension venant de ces entr\xE9es."),eu=d(),Ht=s("li"),ks=s("strong"),tu=n("D\xE9codeur (\xE0 droite)"),ru=n(" : le d\xE9codeur utilise la repr\xE9sentation de l\u2019encodeur (les caract\xE9ristiques) en plus des autres entr\xE9es pour g\xE9n\xE9rer une s\xE9quence cible. Cela signifie que le mod\xE8le est optimis\xE9 pour g\xE9n\xE9rer des sorties."),tn=d(),ne=s("div"),kt=s("img"),su=d(),wt=s("img"),rn=d(),Vt=s("p"),au=n("Chacun de ces blocs peuvent \xEAtre utilis\xE9s ind\xE9pendamment en fonction de la t\xE2che que l\u2019on souhaite traiter :"),sn=d(),j=s("ul"),Yt=s("li"),ws=s("strong"),nu=n("Mod\xE8les uniquement encodeurs"),lu=n(" : adapt\xE9s pour des t\xE2ches qui n\xE9cessitent une compr\xE9hension de l\u2019entr\xE9e, comme la classification de phrases et la reconnaissance d\u2019entit\xE9s nomm\xE9es."),ou=d(),Ft=s("li"),Ms=s("strong"),iu=n("Mod\xE8les uniquement d\xE9codeurs"),uu=n(" : adapt\xE9s pour les t\xE2ches g\xE9n\xE9ratives telles que la g\xE9n\xE9ration de texte."),cu=d(),Ce=s("li"),ys=s("strong"),du=n("Mod\xE8les encodeurs-d\xE9codeurs"),mu=n(" (ou "),Ls=s("strong"),pu=n("mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),fu=n(") : adapt\xE9s aux t\xE2ches g\xE9n\xE9ratives qui n\xE9cessitent une entr\xE9e, telles que la traduction ou le r\xE9sum\xE9 de texte."),an=d(),Jt=s("p"),hu=n("Nous verrons plus en d\xE9tails chacune de ces architectures plus tard."),nn=d(),le=s("h2"),Ne=s("a"),Ts=s("span"),g(Mt.$$.fragment),vu=d(),Is=s("span"),gu=n("Les couches d'attention"),ln=d(),w=s("p"),Eu=n("Une caract\xE9ristique cl\xE9 des "),As=s("em"),_u=n("transformers"),qu=n(" est qu\u2019ils sont construits avec des couches sp\xE9ciales appel\xE9es couches d\u2019attention. En fait, le titre du papier introduisant l\u2019architecture "),Cs=s("em"),bu=n("transformer"),xu=n(" se nomme "),yt=s("a"),Ns=s("em"),$u=n("Attention Is All You Need"),Pu=n(" ! Nous explorerons les d\xE9tails des couches d\u2019attention plus tard dans le cours. Pour l\u2019instant, tout ce que vous devez savoir est que cette couche indique au mod\xE8le de pr\xEAter une attention sp\xE9cifique \xE0 certains mots de la phrase que vous lui avez pass\xE9e (et d\u2019ignorer plus ou moins les autres) lors du traitement de la repr\xE9sentation de chaque mot."),on=d(),p=s("p"),ku=n("Pour mettre cela en contexte, consid\xE9rons la t\xE2che de traduire un texte de l\u2019anglais au fran\xE7ais. \xC9tant donn\xE9 l\u2019entr\xE9e \xAB "),Gs=s("em"),wu=n("You like this course"),Mu=n(" \xBB, un mod\xE8le de traduction devra \xE9galement s\u2019int\xE9resser au mot adjacent \xAB "),Rs=s("em"),yu=n("You"),Lu=n(" \xBB pour obtenir la traduction correcte du mot \xAB "),js=s("em"),Tu=n("like"),Iu=n(" \xBB, car en fran\xE7ais le verbe \xAB "),Ss=s("em"),Au=n("like"),Cu=n(" \xBB se conjugue diff\xE9remment selon le sujet. Le reste de la phrase n\u2019est en revanche pas utile pour la traduction de ce mot. Dans le m\xEAme ordre d\u2019id\xE9es, pour traduire \xAB "),zs=s("em"),Nu=n("this"),Gu=n(" \xBB, le mod\xE8le devra \xE9galement faire attention au mot \xAB "),Bs=s("em"),Ru=n("course"),ju=n(" \xBB car \xAB "),Ds=s("em"),Su=n("this"),zu=n(" \xBB se traduit diff\xE9remment selon que le nom associ\xE9 est masculin ou f\xE9minin. L\xE0 encore, les autres mots de la phrase n\u2019auront aucune importance pour la traduction de \xAB "),Os=s("em"),Bu=n("this"),Du=n(" \xBB. Avec des phrases plus complexes (et des r\xE8gles de grammaire plus complexes), le mod\xE8le devra pr\xEAter une attention particuli\xE8re aux mots qui pourraient appara\xEEtre plus loin dans la phrase pour traduire correctement chaque mot."),un=d(),Xt=s("p"),Ou=n("Le m\xEAme concept s\u2019applique \xE0 toute t\xE2che associ\xE9e au langage naturel : un mot en lui-m\xEAme a un sens, mais ce sens est profond\xE9ment affect\xE9 par le contexte, qui peut \xEAtre n\u2019importe quel autre mot (ou mots) avant ou apr\xE8s le mot \xE9tudi\xE9."),cn=d(),Ge=s("p"),Uu=n("Maintenant que vous avez une id\xE9e plus pr\xE9cise des couches d\u2019attentions, nous allons regarder de plus pr\xE8s l\u2019architecture des "),Us=s("em"),Hu=n("transformers"),Vu=n("."),dn=d(),oe=s("h2"),Re=s("a"),Hs=s("span"),g(Lt.$$.fragment),Yu=d(),Vs=s("span"),Fu=n("L'architecture originale"),mn=d(),je=s("p"),Ju=n("L\u2019architecture du "),Ys=s("em"),Xu=n("transformer"),Qu=n(" a initialement \xE9t\xE9 construite pour la t\xE2che de traduction. Pendant l\u2019entra\xEEnement, l\u2019encodeur re\xE7oit des entr\xE9es (des phrases) dans une certaine langue, tandis que le d\xE9codeur re\xE7oit la m\xEAme phrase traduite dans la langue cible. Pour l\u2019encodeur, les couches d\u2019attention peuvent utiliser tous les mots d\u2019une phrase (puisque comme nous venons de le voir, la traduction d\u2019un mot donn\xE9 peut d\xE9pendre de ce qui le suit ou le pr\xE9c\xE8de dans la phrase). Le d\xE9codeur, quant \xE0 lui, fonctionne de fa\xE7on s\xE9quentielle et ne peut porter son attention qu\u2019aux mots d\xE9j\xE0 traduits dans la phrase (donc uniquement les mots g\xE9n\xE9r\xE9s avant le mot en cours). Par exemple, lorsqu\u2019on a pr\xE9dit les trois premiers mots de la phrase cible, on les donne au d\xE9codeur qui utilise alors toutes les entr\xE9es de l\u2019encodeur pour essayer de pr\xE9dire le quatri\xE8me mot."),pn=d(),Qt=s("p"),Wu=n("Pour acc\xE9l\xE9rer les choses pendant l\u2019apprentissage (lorsque le mod\xE8le a acc\xE8s aux phrases cibles), le d\xE9codeur est aliment\xE9 avec la cible enti\xE8re, mais il n\u2019est pas autoris\xE9 \xE0 utiliser les mots futurs (s\u2019il avait acc\xE8s au mot en position 2 lorsqu\u2019il essayait de pr\xE9dire le mot en position 2, le probl\xE8me ne serait pas tr\xE8s difficile !). Par exemple, en essayant de pr\xE9dire le quatri\xE8me mot, la couche d\u2019attention n\u2019aura acc\xE8s qu\u2019aux mots des positions 1 \xE0 3."),fn=d(),Se=s("p"),Zu=n("L\u2019architecture originale du "),Fs=s("em"),Ku=n("transformer"),ec=n(" ressemble \xE0 ceci, avec l\u2019encodeur \xE0 gauche et le d\xE9codeur \xE0 droite :"),hn=d(),ie=s("div"),Tt=s("img"),tc=d(),It=s("img"),vn=d(),Wt=s("p"),rc=n("Notez que la premi\xE8re couche d\u2019attention dans un bloc d\xE9codeur pr\xEAte attention \xE0 toutes les entr\xE9es (pass\xE9es) du d\xE9codeur, mais que la deuxi\xE8me couche d\u2019attention utilise la sortie de l\u2019encodeur. Elle peut donc acc\xE9der \xE0 l\u2019ensemble de la phrase d\u2019entr\xE9e pour pr\xE9dire au mieux le mot actuel. C\u2019est tr\xE8s utile, car diff\xE9rentes langues peuvent avoir des r\xE8gles grammaticales qui placent les mots dans un ordre diff\xE9rent, ou un contexte fourni plus tard dans la phrase peut \xEAtre utile pour d\xE9terminer la meilleure traduction d\u2019un mot donn\xE9."),gn=d(),S=s("p"),sc=n("Le "),Js=s("em"),ac=n("masque d\u2019attention"),nc=n(" peut \xE9galement \xEAtre utilis\xE9 dans l\u2019encodeur/d\xE9codeur pour emp\xEAcher le mod\xE8le de pr\xEAter attention \xE0 certains mots sp\xE9ciaux. Par exemple, le mot de remplissage sp\xE9cial (le "),Xs=s("em"),lc=n("padding"),oc=n(") utilis\xE9 pour que toutes les entr\xE9es aient la m\xEAme longueur lors du regroupement de phrases."),En=d(),ue=s("h2"),ze=s("a"),Qs=s("span"),g(At.$$.fragment),ic=d(),Zt=s("span"),uc=n("Architectures contre "),Ws=s("i"),cc=n("checkpoints"),_n=n(`

 
En approfondissant l'\xE9tude des `),Kt=s("i"),dc=n("transformers"),qn=n(" dans ce cours, vous verrez des mentions d'"),er=s("i"),mc=n("architectures"),bn=n(" et de "),tr=s("i"),pc=n("checkpoints"),xn=n(" ainsi que de "),rr=s("i"),fc=n("mod\xE8les"),$n=n(`. Ces termes ont tous des significations l\xE9g\xE8rement diff\xE9rentes :
`),z=s("ul"),sr=s("li"),Zs=s("strong"),hc=n("Architecture"),vc=n(" : c\u2019est le squelette du mod\xE8le, la d\xE9finition de chaque couche et chaque op\xE9ration qui se produit au sein du mod\xE8le."),gc=d(),ar=s("li"),Ks=s("strong"),Ec=n("Checkpoints"),_c=n(" : ce sont les poids qui seront charg\xE9s dans une architecture donn\xE9e."),qc=d(),M=s("li"),ea=s("strong"),bc=n("Mod\xE8le"),xc=n(" : c\u2019est un mot valise n\u2019\xE9tant pas aussi pr\xE9cis que les mots \xAB architecture \xBB ou \xAB "),ta=s("em"),$c=n("checkpoint"),Pc=n(" \xBB. Il peut d\xE9signer l\u2019un comme l\u2019autre. Dans ce cours, il sera sp\xE9cifi\xE9 "),ra=s("em"),kc=n("architecture"),wc=n(" ou "),sa=s("em"),Mc=n("checkpoint"),yc=n(" lorsqu\u2019il sera essentiel de r\xE9duire toute ambigu\xEFt\xE9."),Pn=d(),y=s("p"),Lc=n("Par exemple, BERT est une architecture alors que "),aa=s("code"),Tc=n("bert-base-cased"),Ic=n(" (un ensemble de poids entra\xEEn\xE9 par l\u2019\xE9quipe de Google lors de la premi\xE8re sortie de BERT) est un "),na=s("em"),Ac=n("checkpoint"),Cc=n(". Cependant, il est possible de dire \xAB le mod\xE8le BERT \xBB et \xAB le mod\xE8le "),la=s("code"),Nc=n("bert-base-cased"),Gc=n(" \xBB."),this.h()},l(e){const i=zp('[data-svelte="svelte-1phssyn"]',document.head);D=a(i,"META",{name:!0,content:!0}),i.forEach(r),da=m(e),O=a(e,"H1",{class:!0});var wn=o(O);fe=a(wn,"A",{id:!0,class:!0,href:!0});var dd=o(fe);_r=a(dd,"SPAN",{});var md=o(_r);E(Oe.$$.fragment,md),md.forEach(r),dd.forEach(r),nl=m(wn),Ue=a(wn,"SPAN",{});var Mn=o(Ue);ll=l(Mn,"Comment fonctionnent les "),qr=a(Mn,"I",{});var pd=o(qr);ol=l(pd,"transformers"),pd.forEach(r),il=l(Mn," ?"),Mn.forEach(r),wn.forEach(r),ma=m(e),E(He.$$.fragment,e),pa=m(e),he=a(e,"P",{});var yn=o(he);ul=l(yn,"Dans cette partie, nous allons jeter un coup d\u2019\u0153il \xE0 l\u2019architecture des "),br=a(yn,"EM",{});var fd=o(br);cl=l(fd,"transformers"),fd.forEach(r),dl=l(yn,"."),yn.forEach(r),fa=m(e),U=a(e,"H2",{class:!0});var Ln=o(U);ve=a(Ln,"A",{id:!0,class:!0,href:!0});var hd=o(ve);xr=a(hd,"SPAN",{});var vd=o(xr);E(Ve.$$.fragment,vd),vd.forEach(r),hd.forEach(r),ml=m(Ln),Ct=a(Ln,"SPAN",{});var Rc=o(Ct);pl=l(Rc,"Court historique des "),$r=a(Rc,"I",{});var gd=o($r);fl=l(gd,"transformers"),gd.forEach(r),Rc.forEach(r),Ln.forEach(r),ha=m(e),ge=a(e,"P",{});var Tn=o(ge);hl=l(Tn,"Voici quelques dates clefs dans la courte histoire des "),Pr=a(Tn,"EM",{});var Ed=o(Pr);vl=l(Ed,"transformers"),Ed.forEach(r),gl=l(Tn," :"),Tn.forEach(r),va=m(e),H=a(e,"DIV",{class:!0});var In=o(H);Ye=a(In,"IMG",{class:!0,src:!0,alt:!0}),El=m(In),Fe=a(In,"IMG",{class:!0,src:!0,alt:!0}),In.forEach(r),ga=m(e),Je=a(e,"P",{});var jc=o(Je);Ee=a(jc,"A",{href:!0,rel:!0});var Sc=o(Ee);_l=l(Sc,"L\u2019architecture "),kr=a(Sc,"EM",{});var _d=o(kr);ql=l(_d,"Transformer"),_d.forEach(r),Sc.forEach(r),bl=l(jc," a \xE9t\xE9 pr\xE9sent\xE9e en juin 2017. Initialement, la recherche portait sur la t\xE2che de traduction. Elle a \xE9t\xE9 suivie par l\u2019introduction de plusieurs mod\xE8les influents, notamment :"),jc.forEach(r),Ea=m(e),v=a(e,"UL",{});var L=o(v);wr=a(L,"LI",{});var qd=o(wr);$=a(qd,"P",{});var ce=o($);Mr=a(ce,"STRONG",{});var bd=o(Mr);xl=l(bd,"Juin 2018"),bd.forEach(r),$l=l(ce," : "),Xe=a(ce,"A",{href:!0,rel:!0});var xd=o(Xe);Pl=l(xd,"GPT"),xd.forEach(r),kl=l(ce,", le premier "),yr=a(ce,"EM",{});var $d=o(yr);wl=l($d,"transformer"),$d.forEach(r),Ml=l(ce," pr\xE9-entra\xEEn\xE9 et "),Lr=a(ce,"EM",{});var Pd=o(Lr);yl=l(Pd,"finetun\xE9"),Pd.forEach(r),Ll=l(ce," sur diff\xE9rentes t\xE2ches de NLP et ayant obtenu des r\xE9sultats \xE0 l\u2019\xE9tat de l\u2019art,"),ce.forEach(r),qd.forEach(r),Tl=m(L),Tr=a(L,"LI",{});var kd=o(Tr);_e=a(kd,"P",{});var oa=o(_e);Ir=a(oa,"STRONG",{});var wd=o(Ir);Il=l(wd,"Octobre 2018"),wd.forEach(r),Al=l(oa," : "),Qe=a(oa,"A",{href:!0,rel:!0});var Md=o(Qe);Cl=l(Md,"BERT"),Md.forEach(r),Nl=l(oa,", autre grand mod\xE8le pr\xE9-entra\xEEn\xE9 ayant \xE9t\xE9 construit pour produire de meilleurs r\xE9sum\xE9s de texte (plus de d\xE9tails dans le chapitre suivant !),"),oa.forEach(r),kd.forEach(r),Gl=m(L),Ar=a(L,"LI",{});var yd=o(Ar);qe=a(yd,"P",{});var ia=o(qe);Cr=a(ia,"STRONG",{});var Ld=o(Cr);Rl=l(Ld,"F\xE9vrier 2019"),Ld.forEach(r),jl=l(ia," : "),We=a(ia,"A",{href:!0,rel:!0});var Td=o(We);Sl=l(Td,"GPT-2"),Td.forEach(r),zl=l(ia,", une version am\xE9lior\xE9e (et plus grande) de GPT qui n\u2019a pas \xE9t\xE9 directement rendu publique pour cause de raisons \xE9thiques,"),ia.forEach(r),yd.forEach(r),Bl=m(L),Nr=a(L,"LI",{});var Id=o(Nr);be=a(Id,"P",{});var ua=o(be);Gr=a(ua,"STRONG",{});var Ad=o(Gr);Dl=l(Ad,"Octobre 2019"),Ad.forEach(r),Ol=l(ua," : "),Ze=a(ua,"A",{href:!0,rel:!0});var Cd=o(Ze);Ul=l(Cd,"DistilBERT"),Cd.forEach(r),Hl=l(ua,", une version distill\xE9e de BERT \xE9tant 60% plus rapide, 40% plus l\xE9g\xE8re en m\xE9moire et conservant tout de m\xEAme 97% des performances initiales de BERT,"),ua.forEach(r),Id.forEach(r),Vl=m(L),Rr=a(L,"LI",{});var Nd=o(Rr);P=a(Nd,"P",{});var de=o(P);jr=a(de,"STRONG",{});var Gd=o(jr);Yl=l(Gd,"Octobre 2019"),Gd.forEach(r),Fl=l(de," : "),Ke=a(de,"A",{href:!0,rel:!0});var Rd=o(Ke);Jl=l(Rd,"BART"),Rd.forEach(r),Xl=l(de," et "),et=a(de,"A",{href:!0,rel:!0});var jd=o(et);Ql=l(jd,"T5"),jd.forEach(r),Wl=l(de,", deux mod\xE8les pr\xE9-entra\xEEn\xE9s utilisant la m\xEAme architecture que le "),Sr=a(de,"EM",{});var Sd=o(Sr);Zl=l(Sd,"transformer"),Sd.forEach(r),Kl=l(de," original (les premiers \xE0 faire cela),"),de.forEach(r),Nd.forEach(r),eo=m(L),zr=a(L,"LI",{});var zd=o(zr);k=a(zd,"P",{});var me=o(k);Br=a(me,"STRONG",{});var Bd=o(Br);to=l(Bd,"Mai 2020"),Bd.forEach(r),ro=l(me," : "),tt=a(me,"A",{href:!0,rel:!0});var Dd=o(tt);so=l(Dd,"GPT-3"),Dd.forEach(r),ao=l(me,", une version encore plus grande que GPT-2 ayant des performances tr\xE8s bonnes sur une vari\xE9t\xE9 de t\xE2ches ne n\xE9cessitant pas de "),Dr=a(me,"EM",{});var Od=o(Dr);no=l(Od,"finetuning"),Od.forEach(r),lo=l(me," (appel\xE9 "),Or=a(me,"EM",{});var Ud=o(Or);oo=l(Ud,"zero-shot learning"),Ud.forEach(r),io=l(me,")."),me.forEach(r),zd.forEach(r),L.forEach(r),_a=m(e),xe=a(e,"P",{});var An=o(xe);uo=l(An,"Cette liste est loin d\u2019\xEAtre exhaustive et met en lumi\xE8re certains "),Ur=a(An,"EM",{});var Hd=o(Ur);co=l(Hd,"transformers"),Hd.forEach(r),mo=l(An,". Plus largement, ces mod\xE8les peuvent \xEAtre regroup\xE9s en trois cat\xE9gories :"),An.forEach(r),qa=m(e),T=a(e,"UL",{});var nr=o(T);V=a(nr,"LI",{});var lr=o(V);po=l(lr,"ceux de type GPT (aussi appel\xE9s "),Hr=a(lr,"EM",{});var Vd=o(Hr);fo=l(Vd,"transformers"),Vd.forEach(r),ho=m(lr),Vr=a(lr,"EM",{});var Yd=o(Vr);vo=l(Yd,"autor\xE9gressifs"),Yd.forEach(r),go=l(lr,")"),lr.forEach(r),Eo=m(nr),Y=a(nr,"LI",{});var or=o(Y);_o=l(or,"ceux de type BERT (aussi appel\xE9s "),Yr=a(or,"EM",{});var Fd=o(Yr);qo=l(Fd,"transformers"),Fd.forEach(r),bo=m(or),Fr=a(or,"EM",{});var Jd=o(Fr);xo=l(Jd,"auto-encodeurs"),Jd.forEach(r),$o=l(or,")"),or.forEach(r),Po=m(nr),F=a(nr,"LI",{});var ir=o(F);ko=l(ir,"ceux de type BART/T5 (aussi appel\xE9s "),Jr=a(ir,"EM",{});var Xd=o(Jr);wo=l(Xd,"transformers"),Xd.forEach(r),Mo=m(ir),Xr=a(ir,"EM",{});var Qd=o(Xr);yo=l(Qd,"s\xE9quence-\xE0-s\xE9quence"),Qd.forEach(r),Lo=l(ir,")"),ir.forEach(r),nr.forEach(r),ba=m(e),Nt=a(e,"P",{});var Wd=o(Nt);To=l(Wd,"Nous verrons plus en profondeur ces familles de mod\xE8les plus tard."),Wd.forEach(r),xa=m(e),J=a(e,"H2",{class:!0});var Cn=o(J);$e=a(Cn,"A",{id:!0,class:!0,href:!0});var Zd=o($e);Qr=a(Zd,"SPAN",{});var Kd=o(Qr);E(rt.$$.fragment,Kd),Kd.forEach(r),Zd.forEach(r),Io=m(Cn),st=a(Cn,"SPAN",{});var Nn=o(st);Ao=l(Nn,"Les "),Wr=a(Nn,"I",{});var em=o(Wr);Co=l(em,"transformers"),em.forEach(r),No=l(Nn," sont des mod\xE8les de langage"),Nn.forEach(r),Cn.forEach(r),$a=m(e),I=a(e,"P",{});var ur=o(I);Go=l(ur,"Tous les "),Zr=a(ur,"EM",{});var tm=o(Zr);Ro=l(tm,"transformers"),tm.forEach(r),jo=l(ur," mentionn\xE9s ci-dessus (GPT, BERT, BART, T5, etc.) ont \xE9t\xE9 entra\xEEn\xE9s comme des "),Kr=a(ur,"EM",{});var rm=o(Kr);So=l(rm,"mod\xE8les de langage"),rm.forEach(r),zo=l(ur,". Cela signifie qu\u2019ils ont \xE9t\xE9 entra\xEEn\xE9s sur une large quantit\xE9 de textes bruts de mani\xE8re autosupervis\xE9e. L\u2019apprentissage autosupervis\xE9 est un type d\u2019entra\xEEnement dans lequel l\u2019objectif est automatiquement calcul\xE9 \xE0 partir des entr\xE9es du mod\xE8le. Cela signifie que les humains ne sont pas n\xE9cessaires pour \xE9tiqueter les donn\xE9es !"),ur.forEach(r),Pa=m(e),Pe=a(e,"P",{});var Gn=o(Pe);Bo=l(Gn,"Ce type de mod\xE8le d\xE9veloppe une compr\xE9hension statistique de la langue sur laquelle il a \xE9t\xE9 entra\xEEn\xE9, mais il n\u2019est pas tr\xE8s utile pour des t\xE2ches pratiques sp\xE9cifiques. Pour cette raison, le mod\xE8le pr\xE9-entra\xEEn\xE9 passe ensuite par un processus appel\xE9 apprentissage par transfert. Au cours de ce processus, le mod\xE8le est "),es=a(Gn,"EM",{});var sm=o(es);Do=l(sm,"finetun\xE9"),sm.forEach(r),Oo=l(Gn," de mani\xE8re supervis\xE9e (c\u2019est-\xE0-dire en utilisant des \xE9tiquettes annot\xE9es par des humains) pour une t\xE2che donn\xE9e."),Gn.forEach(r),ka=m(e),A=a(e,"P",{});var cr=o(A);Uo=l(cr,"Un exemple de t\xE2che consiste \xE0 pr\xE9dire le mot suivant dans une phrase apr\xE8s avoir lu les "),ts=a(cr,"EM",{});var am=o(ts);Ho=l(am,"n"),am.forEach(r),Vo=l(cr," mots pr\xE9c\xE9dents. Cette t\xE2che est appel\xE9e "),rs=a(cr,"EM",{});var nm=o(rs);Yo=l(nm,"mod\xE9lisation causale du langage"),nm.forEach(r),Fo=l(cr," car la sortie d\xE9pend des entr\xE9es pass\xE9es et pr\xE9sentes, mais pas des entr\xE9es futures."),cr.forEach(r),wa=m(e),X=a(e,"DIV",{class:!0});var Rn=o(X);at=a(Rn,"IMG",{class:!0,src:!0,alt:!0}),Jo=m(Rn),nt=a(Rn,"IMG",{class:!0,src:!0,alt:!0}),Rn.forEach(r),Ma=m(e),ke=a(e,"P",{});var jn=o(ke);Xo=l(jn,"Un autre exemple est la "),ss=a(jn,"EM",{});var lm=o(ss);Qo=l(lm,"mod\xE9lisation du langage masqu\xE9"),lm.forEach(r),Wo=l(jn,", dans laquelle le mod\xE8le pr\xE9dit un mot masqu\xE9 dans la phrase."),jn.forEach(r),ya=m(e),Q=a(e,"DIV",{class:!0});var Sn=o(Q);lt=a(Sn,"IMG",{class:!0,src:!0,alt:!0}),Zo=m(Sn),ot=a(Sn,"IMG",{class:!0,src:!0,alt:!0}),Sn.forEach(r),La=m(e),W=a(e,"H2",{class:!0});var zn=o(W);we=a(zn,"A",{id:!0,class:!0,href:!0});var om=o(we);as=a(om,"SPAN",{});var im=o(as);E(it.$$.fragment,im),im.forEach(r),om.forEach(r),Ko=m(zn),ut=a(zn,"SPAN",{});var Bn=o(ut);ei=l(Bn,"Les "),ns=a(Bn,"I",{});var um=o(ns);ti=l(um,"transformers"),um.forEach(r),ri=l(Bn," sont \xE9normes"),Bn.forEach(r),zn.forEach(r),Ta=m(e),Gt=a(e,"P",{});var cm=o(Gt);si=l(cm,"En dehors de quelques exceptions (comme DistilBERT), la strat\xE9gie g\xE9n\xE9rale pour obtenir de meilleure performance consiste \xE0 augmenter la taille des mod\xE8les ainsi que la quantit\xE9 de donn\xE9es utilis\xE9es pour l\u2019entra\xEEnement de ces derniers."),cm.forEach(r),Ia=m(e),ct=a(e,"DIV",{class:!0});var dm=o(ct);dt=a(dm,"IMG",{src:!0,alt:!0,width:!0}),dm.forEach(r),Aa=m(e),Rt=a(e,"P",{});var mm=o(Rt);ai=l(mm,"Malheureusement, entra\xEEner un mod\xE8le et particuli\xE8rement un tr\xE8s grand mod\xE8le, n\xE9cessite une importante quantit\xE9 de donn\xE9es. Cela devient tr\xE8s co\xFBteux en termes de temps et de ressources de calcul. Cela se traduit m\xEAme par un impact environnemental comme le montre le graphique suivant."),mm.forEach(r),Ca=m(e),Z=a(e,"DIV",{class:!0});var Dn=o(Z);mt=a(Dn,"IMG",{class:!0,src:!0,alt:!0}),ni=m(Dn),pt=a(Dn,"IMG",{class:!0,src:!0,alt:!0}),Dn.forEach(r),Na=m(e),E(ft.$$.fragment,e),Ga=m(e),jt=a(e,"P",{});var pm=o(jt);li=l(pm,"L\u2019image montre l\u2019empreinte carbone pour un projet d\u2019entra\xEEnement d\u2019un (tr\xE8s grand) mod\xE8le men\xE9 par une \xE9quipe qui pourtant essaie consciemment de r\xE9duire l\u2019impact environnemental du pr\xE9-entra\xEEnement. L\u2019empreinte de l\u2019ex\xE9cution de nombreux essais pour obtenir les meilleurs hyperparam\xE8tres serait encore plus \xE9lev\xE9e."),pm.forEach(r),Ra=m(e),St=a(e,"P",{});var fm=o(St);oi=l(fm,"Imaginez qu\u2019\xE0 chaque fois qu\u2019une \xE9quipe de recherche, une association d\u2019\xE9tudiants ou une entreprise souhaite entra\xEEner un mod\xE8le, elle le fasse en partant de z\xE9ro. Cela entra\xEEnerait des co\xFBts globaux \xE9normes et inutiles !"),fm.forEach(r),ja=m(e),zt=a(e,"P",{});var hm=o(zt);ii=l(hm,"C\u2019est pourquoi le partage des mod\xE8les du langage est primordial : partager les poids d\u2019entra\xEEnement et construire \xE0 partir de ces poids permet de r\xE9duire les co\xFBts de calcul globaux ainsi que l\u2019empreinte carbone de toute la communaut\xE9."),hm.forEach(r),Sa=m(e),K=a(e,"H2",{class:!0});var On=o(K);Me=a(On,"A",{id:!0,class:!0,href:!0});var vm=o(Me);ls=a(vm,"SPAN",{});var gm=o(ls);E(ht.$$.fragment,gm),gm.forEach(r),vm.forEach(r),ui=m(On),os=a(On,"SPAN",{});var Em=o(os);ci=l(Em,"L'apprentissage par transfert"),Em.forEach(r),On.forEach(r),za=m(e),E(vt.$$.fragment,e),Ba=m(e),Bt=a(e,"P",{});var _m=o(Bt);di=l(_m,"Le pr\xE9-entra\xEEnement consiste \xE0 entra\xEEner un mod\xE8le \xE0 partir de z\xE9ro : les poids sont initialis\xE9s de mani\xE8re al\xE9atoire et l\u2019entra\xEEnement commence sans aucune connaissance pr\xE9alable."),_m.forEach(r),Da=m(e),ee=a(e,"DIV",{class:!0});var Un=o(ee);gt=a(Un,"IMG",{class:!0,src:!0,alt:!0}),mi=m(Un),Et=a(Un,"IMG",{class:!0,src:!0,alt:!0}),Un.forEach(r),Oa=m(e),Dt=a(e,"P",{});var qm=o(Dt);pi=l(qm,"Ce pr\xE9-entra\xEEnement est g\xE9n\xE9ralement effectu\xE9 sur de tr\xE8s grandes quantit\xE9s de donn\xE9es. Il n\xE9cessite donc un tr\xE8s grand corpus de donn\xE9es et l\u2019entra\xEEnement peut prendre jusqu\u2019\xE0 plusieurs semaines."),qm.forEach(r),Ua=m(e),C=a(e,"P",{});var dr=o(C);fi=l(dr,"Le "),is=a(dr,"EM",{});var bm=o(is);hi=l(bm,"finetuning"),bm.forEach(r),vi=l(dr,", quant \xE0 lui, est l\u2019entrainement effectu\xE9 apr\xE8s qu\u2019un mod\xE8le ait \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Pour effectuer un "),us=a(dr,"EM",{});var xm=o(us);gi=l(xm,"finetuning"),xm.forEach(r),Ei=l(dr,", vous devez d\u2019abord acqu\xE9rir un mod\xE8le de langue pr\xE9-entra\xEEn\xE9, puis effectuer un entra\xEEnement suppl\xE9mentaire avec un jeu de donn\xE9es sp\xE9cifiques. Mais pourquoi ne pas entra\xEEner directement pour la t\xE2che finale ? Il y a plusieurs raisons \xE0 cela :"),dr.forEach(r),Ha=m(e),N=a(e,"UL",{});var mr=o(N);te=a(mr,"LI",{});var pr=o(te);_i=l(pr,"Le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur un jeu de donn\xE9es qui pr\xE9sente certaines similitudes avec le jeu de donn\xE9es de "),cs=a(pr,"EM",{});var $m=o(cs);qi=l($m,"finetuning"),$m.forEach(r),bi=l(pr,". Le processus de "),ds=a(pr,"EM",{});var Pm=o(ds);xi=l(Pm,"finetuning"),Pm.forEach(r),$i=l(pr," est donc en mesure de tirer parti des connaissances acquises par le mod\xE8le initial lors du pr\xE9-entra\xEEnement (par exemple, pour les probl\xE8mes de langage naturel, le mod\xE8le pr\xE9-entra\xEEn\xE9 aura une certaine compr\xE9hension statistique de la langue que vous utilisez pour votre t\xE2che)"),pr.forEach(r),Pi=m(mr),_t=a(mr,"LI",{});var Hn=o(_t);ki=l(Hn,"Comme le mod\xE8le pr\xE9-entra\xEEn\xE9 a d\xE9j\xE0 \xE9t\xE9 entra\xEEn\xE9 sur de nombreuses donn\xE9es, le "),ms=a(Hn,"EM",{});var km=o(ms);wi=l(km,"finetuning"),km.forEach(r),Mi=l(Hn," n\xE9cessite beaucoup moins de donn\xE9es pour obtenir des r\xE9sultats d\xE9cents."),Hn.forEach(r),yi=m(mr),ps=a(mr,"LI",{});var wm=o(ps);Li=l(wm,"Pour la m\xEAme raison, le temps et les ressources n\xE9cessaires pour obtenir de bons r\xE9sultats sont beaucoup moins importants."),wm.forEach(r),mr.forEach(r),Va=m(e),G=a(e,"P",{});var fr=o(G);Ti=l(fr,"Par exemple, il est possible d\u2019exploiter un mod\xE8le pr\xE9-entra\xEEn\xE9 entra\xEEn\xE9 sur la langue anglaise, puis de le "),fs=a(fr,"EM",{});var Mm=o(fs);Ii=l(Mm,"finetuner"),Mm.forEach(r),Ai=l(fr," sur un corpus arXiv, pour obtenir un mod\xE8le bas\xE9 sur la science et la recherche. Le "),hs=a(fr,"EM",{});var ym=o(hs);Ci=l(ym,"finetuning"),ym.forEach(r),Ni=l(fr," ne n\xE9cessitera qu\u2019une quantit\xE9 limit\xE9e de donn\xE9es : les connaissances acquises par le mod\xE8le pr\xE9-entra\xEEn\xE9 sont \xAB transf\xE9r\xE9es \xBB, d\u2019o\xF9 le terme d\u2019apprentissage par transfert."),fr.forEach(r),Ya=m(e),re=a(e,"DIV",{class:!0});var Vn=o(re);qt=a(Vn,"IMG",{class:!0,src:!0,alt:!0}),Gi=m(Vn),bt=a(Vn,"IMG",{class:!0,src:!0,alt:!0}),Vn.forEach(r),Fa=m(e),R=a(e,"P",{});var hr=o(R);Ri=l(hr,"Le "),vs=a(hr,"EM",{});var Lm=o(vs);ji=l(Lm,"finetuning"),Lm.forEach(r),Si=l(hr," d\u2019un mod\xE8le a donc un co\xFBt moindre en termes de temps, de donn\xE9es, de finances et d\u2019environnement. Il est aussi plus rapide et plus facile d\u2019it\xE9rer sur diff\xE9rents sch\xE9mas de "),gs=a(hr,"EM",{});var Tm=o(gs);zi=l(Tm,"finetuning"),Tm.forEach(r),Bi=l(hr," car l\u2019entra\xEEnement est moins contraignant qu\u2019un pr\xE9-entra\xEEnement complet."),hr.forEach(r),Ja=m(e),ye=a(e,"P",{});var Yn=o(ye);Di=l(Yn,"Ce processus permet \xE9galement d\u2019obtenir de meilleurs r\xE9sultats que l\u2019entra\xEEnement \xE0 partir de z\xE9ro (\xE0 moins que vous ne disposiez d\u2019un grand nombre de donn\xE9es). C\u2019est pourquoi vous devez toujours essayer de tirer parti d\u2019un mod\xE8le pr\xE9-entra\xEEn\xE9, c\u2019est-\xE0-dire un mod\xE8le aussi proche que possible de la t\xE2che que vous avez \xE0 accomplir, et de le "),Es=a(Yn,"EM",{});var Im=o(Es);Oi=l(Im,"finetuner"),Im.forEach(r),Ui=l(Yn,"."),Yn.forEach(r),Xa=m(e),se=a(e,"H2",{class:!0});var Fn=o(se);Le=a(Fn,"A",{id:!0,class:!0,href:!0});var Am=o(Le);_s=a(Am,"SPAN",{});var Cm=o(_s);E(xt.$$.fragment,Cm),Cm.forEach(r),Am.forEach(r),Hi=m(Fn),qs=a(Fn,"SPAN",{});var Nm=o(qs);Vi=l(Nm,"Architecture g\xE9n\xE9rale"),Nm.forEach(r),Fn.forEach(r),Qa=m(e),Te=a(e,"P",{});var Jn=o(Te);Yi=l(Jn,"Dans cette section, nous allons voir l\u2019architecture g\xE9n\xE9rale des "),bs=a(Jn,"EM",{});var Gm=o(bs);Fi=l(Gm,"transformers"),Gm.forEach(r),Ji=l(Jn,". Pas d\u2019inqui\xE9tudes si vous ne comprenez pas tous les concepts, des sections d\xE9taill\xE9es qui couvrent chaque composant seront abord\xE9es plus tard."),Jn.forEach(r),Wa=m(e),E($t.$$.fragment,e),Za=m(e),ae=a(e,"H2",{class:!0});var Xn=o(ae);Ie=a(Xn,"A",{id:!0,class:!0,href:!0});var Rm=o(Ie);xs=a(Rm,"SPAN",{});var jm=o(xs);E(Pt.$$.fragment,jm),jm.forEach(r),Rm.forEach(r),Xi=m(Xn),$s=a(Xn,"SPAN",{});var Sm=o($s);Qi=l(Sm,"Introduction"),Sm.forEach(r),Xn.forEach(r),Ka=m(e),Ot=a(e,"P",{});var zm=o(Ot);Wi=l(zm,"Le mod\xE8le est principalement compos\xE9 de deux blocs :"),zm.forEach(r),en=m(e),Ae=a(e,"UL",{});var Qn=o(Ae);Ut=a(Qn,"LI",{});var zc=o(Ut);Ps=a(zc,"STRONG",{});var Bm=o(Ps);Zi=l(Bm,"Encodeur (\xE0 gauche)"),Bm.forEach(r),Ki=l(zc," : l\u2019encodeur re\xE7oit une entr\xE9e et construit une repr\xE9sentation de celle-ci (ses caract\xE9ristiques). Cela signifie que le mod\xE8le est optimis\xE9 pour acqu\xE9rir une compr\xE9hension venant de ces entr\xE9es."),zc.forEach(r),eu=m(Qn),Ht=a(Qn,"LI",{});var Bc=o(Ht);ks=a(Bc,"STRONG",{});var Dm=o(ks);tu=l(Dm,"D\xE9codeur (\xE0 droite)"),Dm.forEach(r),ru=l(Bc," : le d\xE9codeur utilise la repr\xE9sentation de l\u2019encodeur (les caract\xE9ristiques) en plus des autres entr\xE9es pour g\xE9n\xE9rer une s\xE9quence cible. Cela signifie que le mod\xE8le est optimis\xE9 pour g\xE9n\xE9rer des sorties."),Bc.forEach(r),Qn.forEach(r),tn=m(e),ne=a(e,"DIV",{class:!0});var Wn=o(ne);kt=a(Wn,"IMG",{class:!0,src:!0,alt:!0}),su=m(Wn),wt=a(Wn,"IMG",{class:!0,src:!0,alt:!0}),Wn.forEach(r),rn=m(e),Vt=a(e,"P",{});var Om=o(Vt);au=l(Om,"Chacun de ces blocs peuvent \xEAtre utilis\xE9s ind\xE9pendamment en fonction de la t\xE2che que l\u2019on souhaite traiter :"),Om.forEach(r),sn=m(e),j=a(e,"UL",{});var vr=o(j);Yt=a(vr,"LI",{});var Dc=o(Yt);ws=a(Dc,"STRONG",{});var Um=o(ws);nu=l(Um,"Mod\xE8les uniquement encodeurs"),Um.forEach(r),lu=l(Dc," : adapt\xE9s pour des t\xE2ches qui n\xE9cessitent une compr\xE9hension de l\u2019entr\xE9e, comme la classification de phrases et la reconnaissance d\u2019entit\xE9s nomm\xE9es."),Dc.forEach(r),ou=m(vr),Ft=a(vr,"LI",{});var Oc=o(Ft);Ms=a(Oc,"STRONG",{});var Hm=o(Ms);iu=l(Hm,"Mod\xE8les uniquement d\xE9codeurs"),Hm.forEach(r),uu=l(Oc," : adapt\xE9s pour les t\xE2ches g\xE9n\xE9ratives telles que la g\xE9n\xE9ration de texte."),Oc.forEach(r),cu=m(vr),Ce=a(vr,"LI",{});var ca=o(Ce);ys=a(ca,"STRONG",{});var Vm=o(ys);du=l(Vm,"Mod\xE8les encodeurs-d\xE9codeurs"),Vm.forEach(r),mu=l(ca," (ou "),Ls=a(ca,"STRONG",{});var Ym=o(Ls);pu=l(Ym,"mod\xE8les de s\xE9quence-\xE0-s\xE9quence"),Ym.forEach(r),fu=l(ca,") : adapt\xE9s aux t\xE2ches g\xE9n\xE9ratives qui n\xE9cessitent une entr\xE9e, telles que la traduction ou le r\xE9sum\xE9 de texte."),ca.forEach(r),vr.forEach(r),an=m(e),Jt=a(e,"P",{});var Fm=o(Jt);hu=l(Fm,"Nous verrons plus en d\xE9tails chacune de ces architectures plus tard."),Fm.forEach(r),nn=m(e),le=a(e,"H2",{class:!0});var Zn=o(le);Ne=a(Zn,"A",{id:!0,class:!0,href:!0});var Jm=o(Ne);Ts=a(Jm,"SPAN",{});var Xm=o(Ts);E(Mt.$$.fragment,Xm),Xm.forEach(r),Jm.forEach(r),vu=m(Zn),Is=a(Zn,"SPAN",{});var Qm=o(Is);gu=l(Qm,"Les couches d'attention"),Qm.forEach(r),Zn.forEach(r),ln=m(e),w=a(e,"P",{});var Be=o(w);Eu=l(Be,"Une caract\xE9ristique cl\xE9 des "),As=a(Be,"EM",{});var Wm=o(As);_u=l(Wm,"transformers"),Wm.forEach(r),qu=l(Be," est qu\u2019ils sont construits avec des couches sp\xE9ciales appel\xE9es couches d\u2019attention. En fait, le titre du papier introduisant l\u2019architecture "),Cs=a(Be,"EM",{});var Zm=o(Cs);bu=l(Zm,"transformer"),Zm.forEach(r),xu=l(Be," se nomme "),yt=a(Be,"A",{href:!0,rel:!0});var Km=o(yt);Ns=a(Km,"EM",{});var ep=o(Ns);$u=l(ep,"Attention Is All You Need"),ep.forEach(r),Km.forEach(r),Pu=l(Be," ! Nous explorerons les d\xE9tails des couches d\u2019attention plus tard dans le cours. Pour l\u2019instant, tout ce que vous devez savoir est que cette couche indique au mod\xE8le de pr\xEAter une attention sp\xE9cifique \xE0 certains mots de la phrase que vous lui avez pass\xE9e (et d\u2019ignorer plus ou moins les autres) lors du traitement de la repr\xE9sentation de chaque mot."),Be.forEach(r),on=m(e),p=a(e,"P",{});var f=o(p);ku=l(f,"Pour mettre cela en contexte, consid\xE9rons la t\xE2che de traduire un texte de l\u2019anglais au fran\xE7ais. \xC9tant donn\xE9 l\u2019entr\xE9e \xAB "),Gs=a(f,"EM",{});var tp=o(Gs);wu=l(tp,"You like this course"),tp.forEach(r),Mu=l(f," \xBB, un mod\xE8le de traduction devra \xE9galement s\u2019int\xE9resser au mot adjacent \xAB "),Rs=a(f,"EM",{});var rp=o(Rs);yu=l(rp,"You"),rp.forEach(r),Lu=l(f," \xBB pour obtenir la traduction correcte du mot \xAB "),js=a(f,"EM",{});var sp=o(js);Tu=l(sp,"like"),sp.forEach(r),Iu=l(f," \xBB, car en fran\xE7ais le verbe \xAB "),Ss=a(f,"EM",{});var ap=o(Ss);Au=l(ap,"like"),ap.forEach(r),Cu=l(f," \xBB se conjugue diff\xE9remment selon le sujet. Le reste de la phrase n\u2019est en revanche pas utile pour la traduction de ce mot. Dans le m\xEAme ordre d\u2019id\xE9es, pour traduire \xAB "),zs=a(f,"EM",{});var np=o(zs);Nu=l(np,"this"),np.forEach(r),Gu=l(f," \xBB, le mod\xE8le devra \xE9galement faire attention au mot \xAB "),Bs=a(f,"EM",{});var lp=o(Bs);Ru=l(lp,"course"),lp.forEach(r),ju=l(f," \xBB car \xAB "),Ds=a(f,"EM",{});var op=o(Ds);Su=l(op,"this"),op.forEach(r),zu=l(f," \xBB se traduit diff\xE9remment selon que le nom associ\xE9 est masculin ou f\xE9minin. L\xE0 encore, les autres mots de la phrase n\u2019auront aucune importance pour la traduction de \xAB "),Os=a(f,"EM",{});var ip=o(Os);Bu=l(ip,"this"),ip.forEach(r),Du=l(f," \xBB. Avec des phrases plus complexes (et des r\xE8gles de grammaire plus complexes), le mod\xE8le devra pr\xEAter une attention particuli\xE8re aux mots qui pourraient appara\xEEtre plus loin dans la phrase pour traduire correctement chaque mot."),f.forEach(r),un=m(e),Xt=a(e,"P",{});var up=o(Xt);Ou=l(up,"Le m\xEAme concept s\u2019applique \xE0 toute t\xE2che associ\xE9e au langage naturel : un mot en lui-m\xEAme a un sens, mais ce sens est profond\xE9ment affect\xE9 par le contexte, qui peut \xEAtre n\u2019importe quel autre mot (ou mots) avant ou apr\xE8s le mot \xE9tudi\xE9."),up.forEach(r),cn=m(e),Ge=a(e,"P",{});var Kn=o(Ge);Uu=l(Kn,"Maintenant que vous avez une id\xE9e plus pr\xE9cise des couches d\u2019attentions, nous allons regarder de plus pr\xE8s l\u2019architecture des "),Us=a(Kn,"EM",{});var cp=o(Us);Hu=l(cp,"transformers"),cp.forEach(r),Vu=l(Kn,"."),Kn.forEach(r),dn=m(e),oe=a(e,"H2",{class:!0});var el=o(oe);Re=a(el,"A",{id:!0,class:!0,href:!0});var dp=o(Re);Hs=a(dp,"SPAN",{});var mp=o(Hs);E(Lt.$$.fragment,mp),mp.forEach(r),dp.forEach(r),Yu=m(el),Vs=a(el,"SPAN",{});var pp=o(Vs);Fu=l(pp,"L'architecture originale"),pp.forEach(r),el.forEach(r),mn=m(e),je=a(e,"P",{});var tl=o(je);Ju=l(tl,"L\u2019architecture du "),Ys=a(tl,"EM",{});var fp=o(Ys);Xu=l(fp,"transformer"),fp.forEach(r),Qu=l(tl," a initialement \xE9t\xE9 construite pour la t\xE2che de traduction. Pendant l\u2019entra\xEEnement, l\u2019encodeur re\xE7oit des entr\xE9es (des phrases) dans une certaine langue, tandis que le d\xE9codeur re\xE7oit la m\xEAme phrase traduite dans la langue cible. Pour l\u2019encodeur, les couches d\u2019attention peuvent utiliser tous les mots d\u2019une phrase (puisque comme nous venons de le voir, la traduction d\u2019un mot donn\xE9 peut d\xE9pendre de ce qui le suit ou le pr\xE9c\xE8de dans la phrase). Le d\xE9codeur, quant \xE0 lui, fonctionne de fa\xE7on s\xE9quentielle et ne peut porter son attention qu\u2019aux mots d\xE9j\xE0 traduits dans la phrase (donc uniquement les mots g\xE9n\xE9r\xE9s avant le mot en cours). Par exemple, lorsqu\u2019on a pr\xE9dit les trois premiers mots de la phrase cible, on les donne au d\xE9codeur qui utilise alors toutes les entr\xE9es de l\u2019encodeur pour essayer de pr\xE9dire le quatri\xE8me mot."),tl.forEach(r),pn=m(e),Qt=a(e,"P",{});var hp=o(Qt);Wu=l(hp,"Pour acc\xE9l\xE9rer les choses pendant l\u2019apprentissage (lorsque le mod\xE8le a acc\xE8s aux phrases cibles), le d\xE9codeur est aliment\xE9 avec la cible enti\xE8re, mais il n\u2019est pas autoris\xE9 \xE0 utiliser les mots futurs (s\u2019il avait acc\xE8s au mot en position 2 lorsqu\u2019il essayait de pr\xE9dire le mot en position 2, le probl\xE8me ne serait pas tr\xE8s difficile !). Par exemple, en essayant de pr\xE9dire le quatri\xE8me mot, la couche d\u2019attention n\u2019aura acc\xE8s qu\u2019aux mots des positions 1 \xE0 3."),hp.forEach(r),fn=m(e),Se=a(e,"P",{});var rl=o(Se);Zu=l(rl,"L\u2019architecture originale du "),Fs=a(rl,"EM",{});var vp=o(Fs);Ku=l(vp,"transformer"),vp.forEach(r),ec=l(rl," ressemble \xE0 ceci, avec l\u2019encodeur \xE0 gauche et le d\xE9codeur \xE0 droite :"),rl.forEach(r),hn=m(e),ie=a(e,"DIV",{class:!0});var sl=o(ie);Tt=a(sl,"IMG",{class:!0,src:!0,alt:!0}),tc=m(sl),It=a(sl,"IMG",{class:!0,src:!0,alt:!0}),sl.forEach(r),vn=m(e),Wt=a(e,"P",{});var gp=o(Wt);rc=l(gp,"Notez que la premi\xE8re couche d\u2019attention dans un bloc d\xE9codeur pr\xEAte attention \xE0 toutes les entr\xE9es (pass\xE9es) du d\xE9codeur, mais que la deuxi\xE8me couche d\u2019attention utilise la sortie de l\u2019encodeur. Elle peut donc acc\xE9der \xE0 l\u2019ensemble de la phrase d\u2019entr\xE9e pour pr\xE9dire au mieux le mot actuel. C\u2019est tr\xE8s utile, car diff\xE9rentes langues peuvent avoir des r\xE8gles grammaticales qui placent les mots dans un ordre diff\xE9rent, ou un contexte fourni plus tard dans la phrase peut \xEAtre utile pour d\xE9terminer la meilleure traduction d\u2019un mot donn\xE9."),gp.forEach(r),gn=m(e),S=a(e,"P",{});var gr=o(S);sc=l(gr,"Le "),Js=a(gr,"EM",{});var Ep=o(Js);ac=l(Ep,"masque d\u2019attention"),Ep.forEach(r),nc=l(gr," peut \xE9galement \xEAtre utilis\xE9 dans l\u2019encodeur/d\xE9codeur pour emp\xEAcher le mod\xE8le de pr\xEAter attention \xE0 certains mots sp\xE9ciaux. Par exemple, le mot de remplissage sp\xE9cial (le "),Xs=a(gr,"EM",{});var _p=o(Xs);lc=l(_p,"padding"),_p.forEach(r),oc=l(gr,") utilis\xE9 pour que toutes les entr\xE9es aient la m\xEAme longueur lors du regroupement de phrases."),gr.forEach(r),En=m(e),ue=a(e,"H2",{class:!0});var al=o(ue);ze=a(al,"A",{id:!0,class:!0,href:!0});var qp=o(ze);Qs=a(qp,"SPAN",{});var bp=o(Qs);E(At.$$.fragment,bp),bp.forEach(r),qp.forEach(r),ic=m(al),Zt=a(al,"SPAN",{});var Uc=o(Zt);uc=l(Uc,"Architectures contre "),Ws=a(Uc,"I",{});var xp=o(Ws);cc=l(xp,"checkpoints"),xp.forEach(r),Uc.forEach(r),al.forEach(r),_n=l(e,`

 
En approfondissant l'\xE9tude des `),Kt=a(e,"I",{});var $p=o(Kt);dc=l($p,"transformers"),$p.forEach(r),qn=l(e," dans ce cours, vous verrez des mentions d'"),er=a(e,"I",{});var Pp=o(er);mc=l(Pp,"architectures"),Pp.forEach(r),bn=l(e," et de "),tr=a(e,"I",{});var kp=o(tr);pc=l(kp,"checkpoints"),kp.forEach(r),xn=l(e," ainsi que de "),rr=a(e,"I",{});var wp=o(rr);fc=l(wp,"mod\xE8les"),wp.forEach(r),$n=l(e,`. Ces termes ont tous des significations l\xE9g\xE8rement diff\xE9rentes :
`),z=a(e,"UL",{});var Er=o(z);sr=a(Er,"LI",{});var Hc=o(sr);Zs=a(Hc,"STRONG",{});var Mp=o(Zs);hc=l(Mp,"Architecture"),Mp.forEach(r),vc=l(Hc," : c\u2019est le squelette du mod\xE8le, la d\xE9finition de chaque couche et chaque op\xE9ration qui se produit au sein du mod\xE8le."),Hc.forEach(r),gc=m(Er),ar=a(Er,"LI",{});var Vc=o(ar);Ks=a(Vc,"STRONG",{});var yp=o(Ks);Ec=l(yp,"Checkpoints"),yp.forEach(r),_c=l(Vc," : ce sont les poids qui seront charg\xE9s dans une architecture donn\xE9e."),Vc.forEach(r),qc=m(Er),M=a(Er,"LI",{});var pe=o(M);ea=a(pe,"STRONG",{});var Lp=o(ea);bc=l(Lp,"Mod\xE8le"),Lp.forEach(r),xc=l(pe," : c\u2019est un mot valise n\u2019\xE9tant pas aussi pr\xE9cis que les mots \xAB architecture \xBB ou \xAB "),ta=a(pe,"EM",{});var Tp=o(ta);$c=l(Tp,"checkpoint"),Tp.forEach(r),Pc=l(pe," \xBB. Il peut d\xE9signer l\u2019un comme l\u2019autre. Dans ce cours, il sera sp\xE9cifi\xE9 "),ra=a(pe,"EM",{});var Ip=o(ra);kc=l(Ip,"architecture"),Ip.forEach(r),wc=l(pe," ou "),sa=a(pe,"EM",{});var Ap=o(sa);Mc=l(Ap,"checkpoint"),Ap.forEach(r),yc=l(pe," lorsqu\u2019il sera essentiel de r\xE9duire toute ambigu\xEFt\xE9."),pe.forEach(r),Er.forEach(r),Pn=m(e),y=a(e,"P",{});var De=o(y);Lc=l(De,"Par exemple, BERT est une architecture alors que "),aa=a(De,"CODE",{});var Cp=o(aa);Tc=l(Cp,"bert-base-cased"),Cp.forEach(r),Ic=l(De," (un ensemble de poids entra\xEEn\xE9 par l\u2019\xE9quipe de Google lors de la premi\xE8re sortie de BERT) est un "),na=a(De,"EM",{});var Np=o(na);Ac=l(Np,"checkpoint"),Np.forEach(r),Cc=l(De,". Cependant, il est possible de dire \xAB le mod\xE8le BERT \xBB et \xAB le mod\xE8le "),la=a(De,"CODE",{});var Gp=o(la);Nc=l(Gp,"bert-base-cased"),Gp.forEach(r),Gc=l(De," \xBB."),De.forEach(r),this.h()},h(){c(D,"name","hf:doc:metadata"),c(D,"content",JSON.stringify(Hp)),c(fe,"id","comment-fonctionnent-les-itransformersi"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#comment-fonctionnent-les-itransformersi"),c(O,"class","relative group"),c(ve,"id","court-historique-des-itransformersi"),c(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ve,"href","#court-historique-des-itransformersi"),c(U,"class","relative group"),c(Ye,"class","block dark:hidden"),h(Ye.src,Jc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||c(Ye,"src",Jc),c(Ye,"alt","A brief chronology of Transformers models."),c(Fe,"class","hidden dark:block"),h(Fe.src,Xc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||c(Fe,"src",Xc),c(Fe,"alt","A brief chronology of Transformers models."),c(H,"class","flex justify-center"),c(Ee,"href","https://arxiv.org/abs/1706.03762"),c(Ee,"rel","nofollow"),c(Xe,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),c(Xe,"rel","nofollow"),c(Qe,"href","https://arxiv.org/abs/1810.04805"),c(Qe,"rel","nofollow"),c(We,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),c(We,"rel","nofollow"),c(Ze,"href","https://arxiv.org/abs/1910.01108"),c(Ze,"rel","nofollow"),c(Ke,"href","https://arxiv.org/abs/1910.13461"),c(Ke,"rel","nofollow"),c(et,"href","https://arxiv.org/abs/1910.10683"),c(et,"rel","nofollow"),c(tt,"href","https://arxiv.org/abs/2005.14165"),c(tt,"rel","nofollow"),c($e,"id","les-itransformersi-sont-des-modles-de-langage"),c($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($e,"href","#les-itransformersi-sont-des-modles-de-langage"),c(J,"class","relative group"),c(at,"class","block dark:hidden"),h(at.src,Qc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||c(at,"src",Qc),c(at,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(nt,"class","hidden dark:block"),h(nt.src,Wc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||c(nt,"src",Wc),c(nt,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(X,"class","flex justify-center"),c(lt,"class","block dark:hidden"),h(lt.src,Zc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||c(lt,"src",Zc),c(lt,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(ot,"class","hidden dark:block"),h(ot.src,Kc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||c(ot,"src",Kc),c(ot,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(Q,"class","flex justify-center"),c(we,"id","les-itransformersi-sont-normes"),c(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(we,"href","#les-itransformersi-sont-normes"),c(W,"class","relative group"),h(dt.src,ed="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||c(dt,"src",ed),c(dt,"alt","Number of parameters of recent Transformers models"),c(dt,"width","90%"),c(ct,"class","flex justify-center"),c(mt,"class","block dark:hidden"),h(mt.src,td="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||c(mt,"src",td),c(mt,"alt","The carbon footprint of a large language model."),c(pt,"class","hidden dark:block"),h(pt.src,rd="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||c(pt,"src",rd),c(pt,"alt","The carbon footprint of a large language model."),c(Z,"class","flex justify-center"),c(Me,"id","lapprentissage-par-transfert"),c(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Me,"href","#lapprentissage-par-transfert"),c(K,"class","relative group"),c(gt,"class","block dark:hidden"),h(gt.src,sd="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||c(gt,"src",sd),c(gt,"alt","The pretraining of a language model is costly in both time and money."),c(Et,"class","hidden dark:block"),h(Et.src,ad="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||c(Et,"src",ad),c(Et,"alt","The pretraining of a language model is costly in both time and money."),c(ee,"class","flex justify-center"),c(qt,"class","block dark:hidden"),h(qt.src,nd="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||c(qt,"src",nd),c(qt,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(bt,"class","hidden dark:block"),h(bt.src,ld="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||c(bt,"src",ld),c(bt,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(re,"class","flex justify-center"),c(Le,"id","architecture-gnrale"),c(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Le,"href","#architecture-gnrale"),c(se,"class","relative group"),c(Ie,"id","introduction"),c(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ie,"href","#introduction"),c(ae,"class","relative group"),c(kt,"class","block dark:hidden"),h(kt.src,od="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||c(kt,"src",od),c(kt,"alt","Architecture of a Transformers models"),c(wt,"class","hidden dark:block"),h(wt.src,id="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||c(wt,"src",id),c(wt,"alt","Architecture of a Transformers models"),c(ne,"class","flex justify-center"),c(Ne,"id","les-couches-dattention"),c(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ne,"href","#les-couches-dattention"),c(le,"class","relative group"),c(yt,"href","https://arxiv.org/abs/1706.03762"),c(yt,"rel","nofollow"),c(Re,"id","larchitecture-originale"),c(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Re,"href","#larchitecture-originale"),c(oe,"class","relative group"),c(Tt,"class","block dark:hidden"),h(Tt.src,ud="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||c(Tt,"src",ud),c(Tt,"alt","Architecture of a Transformers models"),c(It,"class","hidden dark:block"),h(It.src,cd="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||c(It,"src",cd),c(It,"alt","Architecture of a Transformers models"),c(ie,"class","flex justify-center"),c(ze,"id","architectures-contre-icheckpointsi"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#architectures-contre-icheckpointsi"),c(ue,"class","relative group")},m(e,i){t(document.head,D),u(e,da,i),u(e,O,i),t(O,fe),t(fe,_r),_(Oe,_r,null),t(O,nl),t(O,Ue),t(Ue,ll),t(Ue,qr),t(qr,ol),t(Ue,il),u(e,ma,i),_(He,e,i),u(e,pa,i),u(e,he,i),t(he,ul),t(he,br),t(br,cl),t(he,dl),u(e,fa,i),u(e,U,i),t(U,ve),t(ve,xr),_(Ve,xr,null),t(U,ml),t(U,Ct),t(Ct,pl),t(Ct,$r),t($r,fl),u(e,ha,i),u(e,ge,i),t(ge,hl),t(ge,Pr),t(Pr,vl),t(ge,gl),u(e,va,i),u(e,H,i),t(H,Ye),t(H,El),t(H,Fe),u(e,ga,i),u(e,Je,i),t(Je,Ee),t(Ee,_l),t(Ee,kr),t(kr,ql),t(Je,bl),u(e,Ea,i),u(e,v,i),t(v,wr),t(wr,$),t($,Mr),t(Mr,xl),t($,$l),t($,Xe),t(Xe,Pl),t($,kl),t($,yr),t(yr,wl),t($,Ml),t($,Lr),t(Lr,yl),t($,Ll),t(v,Tl),t(v,Tr),t(Tr,_e),t(_e,Ir),t(Ir,Il),t(_e,Al),t(_e,Qe),t(Qe,Cl),t(_e,Nl),t(v,Gl),t(v,Ar),t(Ar,qe),t(qe,Cr),t(Cr,Rl),t(qe,jl),t(qe,We),t(We,Sl),t(qe,zl),t(v,Bl),t(v,Nr),t(Nr,be),t(be,Gr),t(Gr,Dl),t(be,Ol),t(be,Ze),t(Ze,Ul),t(be,Hl),t(v,Vl),t(v,Rr),t(Rr,P),t(P,jr),t(jr,Yl),t(P,Fl),t(P,Ke),t(Ke,Jl),t(P,Xl),t(P,et),t(et,Ql),t(P,Wl),t(P,Sr),t(Sr,Zl),t(P,Kl),t(v,eo),t(v,zr),t(zr,k),t(k,Br),t(Br,to),t(k,ro),t(k,tt),t(tt,so),t(k,ao),t(k,Dr),t(Dr,no),t(k,lo),t(k,Or),t(Or,oo),t(k,io),u(e,_a,i),u(e,xe,i),t(xe,uo),t(xe,Ur),t(Ur,co),t(xe,mo),u(e,qa,i),u(e,T,i),t(T,V),t(V,po),t(V,Hr),t(Hr,fo),t(V,ho),t(V,Vr),t(Vr,vo),t(V,go),t(T,Eo),t(T,Y),t(Y,_o),t(Y,Yr),t(Yr,qo),t(Y,bo),t(Y,Fr),t(Fr,xo),t(Y,$o),t(T,Po),t(T,F),t(F,ko),t(F,Jr),t(Jr,wo),t(F,Mo),t(F,Xr),t(Xr,yo),t(F,Lo),u(e,ba,i),u(e,Nt,i),t(Nt,To),u(e,xa,i),u(e,J,i),t(J,$e),t($e,Qr),_(rt,Qr,null),t(J,Io),t(J,st),t(st,Ao),t(st,Wr),t(Wr,Co),t(st,No),u(e,$a,i),u(e,I,i),t(I,Go),t(I,Zr),t(Zr,Ro),t(I,jo),t(I,Kr),t(Kr,So),t(I,zo),u(e,Pa,i),u(e,Pe,i),t(Pe,Bo),t(Pe,es),t(es,Do),t(Pe,Oo),u(e,ka,i),u(e,A,i),t(A,Uo),t(A,ts),t(ts,Ho),t(A,Vo),t(A,rs),t(rs,Yo),t(A,Fo),u(e,wa,i),u(e,X,i),t(X,at),t(X,Jo),t(X,nt),u(e,Ma,i),u(e,ke,i),t(ke,Xo),t(ke,ss),t(ss,Qo),t(ke,Wo),u(e,ya,i),u(e,Q,i),t(Q,lt),t(Q,Zo),t(Q,ot),u(e,La,i),u(e,W,i),t(W,we),t(we,as),_(it,as,null),t(W,Ko),t(W,ut),t(ut,ei),t(ut,ns),t(ns,ti),t(ut,ri),u(e,Ta,i),u(e,Gt,i),t(Gt,si),u(e,Ia,i),u(e,ct,i),t(ct,dt),u(e,Aa,i),u(e,Rt,i),t(Rt,ai),u(e,Ca,i),u(e,Z,i),t(Z,mt),t(Z,ni),t(Z,pt),u(e,Na,i),_(ft,e,i),u(e,Ga,i),u(e,jt,i),t(jt,li),u(e,Ra,i),u(e,St,i),t(St,oi),u(e,ja,i),u(e,zt,i),t(zt,ii),u(e,Sa,i),u(e,K,i),t(K,Me),t(Me,ls),_(ht,ls,null),t(K,ui),t(K,os),t(os,ci),u(e,za,i),_(vt,e,i),u(e,Ba,i),u(e,Bt,i),t(Bt,di),u(e,Da,i),u(e,ee,i),t(ee,gt),t(ee,mi),t(ee,Et),u(e,Oa,i),u(e,Dt,i),t(Dt,pi),u(e,Ua,i),u(e,C,i),t(C,fi),t(C,is),t(is,hi),t(C,vi),t(C,us),t(us,gi),t(C,Ei),u(e,Ha,i),u(e,N,i),t(N,te),t(te,_i),t(te,cs),t(cs,qi),t(te,bi),t(te,ds),t(ds,xi),t(te,$i),t(N,Pi),t(N,_t),t(_t,ki),t(_t,ms),t(ms,wi),t(_t,Mi),t(N,yi),t(N,ps),t(ps,Li),u(e,Va,i),u(e,G,i),t(G,Ti),t(G,fs),t(fs,Ii),t(G,Ai),t(G,hs),t(hs,Ci),t(G,Ni),u(e,Ya,i),u(e,re,i),t(re,qt),t(re,Gi),t(re,bt),u(e,Fa,i),u(e,R,i),t(R,Ri),t(R,vs),t(vs,ji),t(R,Si),t(R,gs),t(gs,zi),t(R,Bi),u(e,Ja,i),u(e,ye,i),t(ye,Di),t(ye,Es),t(Es,Oi),t(ye,Ui),u(e,Xa,i),u(e,se,i),t(se,Le),t(Le,_s),_(xt,_s,null),t(se,Hi),t(se,qs),t(qs,Vi),u(e,Qa,i),u(e,Te,i),t(Te,Yi),t(Te,bs),t(bs,Fi),t(Te,Ji),u(e,Wa,i),_($t,e,i),u(e,Za,i),u(e,ae,i),t(ae,Ie),t(Ie,xs),_(Pt,xs,null),t(ae,Xi),t(ae,$s),t($s,Qi),u(e,Ka,i),u(e,Ot,i),t(Ot,Wi),u(e,en,i),u(e,Ae,i),t(Ae,Ut),t(Ut,Ps),t(Ps,Zi),t(Ut,Ki),t(Ae,eu),t(Ae,Ht),t(Ht,ks),t(ks,tu),t(Ht,ru),u(e,tn,i),u(e,ne,i),t(ne,kt),t(ne,su),t(ne,wt),u(e,rn,i),u(e,Vt,i),t(Vt,au),u(e,sn,i),u(e,j,i),t(j,Yt),t(Yt,ws),t(ws,nu),t(Yt,lu),t(j,ou),t(j,Ft),t(Ft,Ms),t(Ms,iu),t(Ft,uu),t(j,cu),t(j,Ce),t(Ce,ys),t(ys,du),t(Ce,mu),t(Ce,Ls),t(Ls,pu),t(Ce,fu),u(e,an,i),u(e,Jt,i),t(Jt,hu),u(e,nn,i),u(e,le,i),t(le,Ne),t(Ne,Ts),_(Mt,Ts,null),t(le,vu),t(le,Is),t(Is,gu),u(e,ln,i),u(e,w,i),t(w,Eu),t(w,As),t(As,_u),t(w,qu),t(w,Cs),t(Cs,bu),t(w,xu),t(w,yt),t(yt,Ns),t(Ns,$u),t(w,Pu),u(e,on,i),u(e,p,i),t(p,ku),t(p,Gs),t(Gs,wu),t(p,Mu),t(p,Rs),t(Rs,yu),t(p,Lu),t(p,js),t(js,Tu),t(p,Iu),t(p,Ss),t(Ss,Au),t(p,Cu),t(p,zs),t(zs,Nu),t(p,Gu),t(p,Bs),t(Bs,Ru),t(p,ju),t(p,Ds),t(Ds,Su),t(p,zu),t(p,Os),t(Os,Bu),t(p,Du),u(e,un,i),u(e,Xt,i),t(Xt,Ou),u(e,cn,i),u(e,Ge,i),t(Ge,Uu),t(Ge,Us),t(Us,Hu),t(Ge,Vu),u(e,dn,i),u(e,oe,i),t(oe,Re),t(Re,Hs),_(Lt,Hs,null),t(oe,Yu),t(oe,Vs),t(Vs,Fu),u(e,mn,i),u(e,je,i),t(je,Ju),t(je,Ys),t(Ys,Xu),t(je,Qu),u(e,pn,i),u(e,Qt,i),t(Qt,Wu),u(e,fn,i),u(e,Se,i),t(Se,Zu),t(Se,Fs),t(Fs,Ku),t(Se,ec),u(e,hn,i),u(e,ie,i),t(ie,Tt),t(ie,tc),t(ie,It),u(e,vn,i),u(e,Wt,i),t(Wt,rc),u(e,gn,i),u(e,S,i),t(S,sc),t(S,Js),t(Js,ac),t(S,nc),t(S,Xs),t(Xs,lc),t(S,oc),u(e,En,i),u(e,ue,i),t(ue,ze),t(ze,Qs),_(At,Qs,null),t(ue,ic),t(ue,Zt),t(Zt,uc),t(Zt,Ws),t(Ws,cc),u(e,_n,i),u(e,Kt,i),t(Kt,dc),u(e,qn,i),u(e,er,i),t(er,mc),u(e,bn,i),u(e,tr,i),t(tr,pc),u(e,xn,i),u(e,rr,i),t(rr,fc),u(e,$n,i),u(e,z,i),t(z,sr),t(sr,Zs),t(Zs,hc),t(sr,vc),t(z,gc),t(z,ar),t(ar,Ks),t(Ks,Ec),t(ar,_c),t(z,qc),t(z,M),t(M,ea),t(ea,bc),t(M,xc),t(M,ta),t(ta,$c),t(M,Pc),t(M,ra),t(ra,kc),t(M,wc),t(M,sa),t(sa,Mc),t(M,yc),u(e,Pn,i),u(e,y,i),t(y,Lc),t(y,aa),t(aa,Tc),t(y,Ic),t(y,na),t(na,Ac),t(y,Cc),t(y,la),t(la,Nc),t(y,Gc),kn=!0},p:Bp,i(e){kn||(q(Oe.$$.fragment,e),q(He.$$.fragment,e),q(Ve.$$.fragment,e),q(rt.$$.fragment,e),q(it.$$.fragment,e),q(ft.$$.fragment,e),q(ht.$$.fragment,e),q(vt.$$.fragment,e),q(xt.$$.fragment,e),q($t.$$.fragment,e),q(Pt.$$.fragment,e),q(Mt.$$.fragment,e),q(Lt.$$.fragment,e),q(At.$$.fragment,e),kn=!0)},o(e){b(Oe.$$.fragment,e),b(He.$$.fragment,e),b(Ve.$$.fragment,e),b(rt.$$.fragment,e),b(it.$$.fragment,e),b(ft.$$.fragment,e),b(ht.$$.fragment,e),b(vt.$$.fragment,e),b(xt.$$.fragment,e),b($t.$$.fragment,e),b(Pt.$$.fragment,e),b(Mt.$$.fragment,e),b(Lt.$$.fragment,e),b(At.$$.fragment,e),kn=!1},d(e){r(D),e&&r(da),e&&r(O),x(Oe),e&&r(ma),x(He,e),e&&r(pa),e&&r(he),e&&r(fa),e&&r(U),x(Ve),e&&r(ha),e&&r(ge),e&&r(va),e&&r(H),e&&r(ga),e&&r(Je),e&&r(Ea),e&&r(v),e&&r(_a),e&&r(xe),e&&r(qa),e&&r(T),e&&r(ba),e&&r(Nt),e&&r(xa),e&&r(J),x(rt),e&&r($a),e&&r(I),e&&r(Pa),e&&r(Pe),e&&r(ka),e&&r(A),e&&r(wa),e&&r(X),e&&r(Ma),e&&r(ke),e&&r(ya),e&&r(Q),e&&r(La),e&&r(W),x(it),e&&r(Ta),e&&r(Gt),e&&r(Ia),e&&r(ct),e&&r(Aa),e&&r(Rt),e&&r(Ca),e&&r(Z),e&&r(Na),x(ft,e),e&&r(Ga),e&&r(jt),e&&r(Ra),e&&r(St),e&&r(ja),e&&r(zt),e&&r(Sa),e&&r(K),x(ht),e&&r(za),x(vt,e),e&&r(Ba),e&&r(Bt),e&&r(Da),e&&r(ee),e&&r(Oa),e&&r(Dt),e&&r(Ua),e&&r(C),e&&r(Ha),e&&r(N),e&&r(Va),e&&r(G),e&&r(Ya),e&&r(re),e&&r(Fa),e&&r(R),e&&r(Ja),e&&r(ye),e&&r(Xa),e&&r(se),x(xt),e&&r(Qa),e&&r(Te),e&&r(Wa),x($t,e),e&&r(Za),e&&r(ae),x(Pt),e&&r(Ka),e&&r(Ot),e&&r(en),e&&r(Ae),e&&r(tn),e&&r(ne),e&&r(rn),e&&r(Vt),e&&r(sn),e&&r(j),e&&r(an),e&&r(Jt),e&&r(nn),e&&r(le),x(Mt),e&&r(ln),e&&r(w),e&&r(on),e&&r(p),e&&r(un),e&&r(Xt),e&&r(cn),e&&r(Ge),e&&r(dn),e&&r(oe),x(Lt),e&&r(mn),e&&r(je),e&&r(pn),e&&r(Qt),e&&r(fn),e&&r(Se),e&&r(hn),e&&r(ie),e&&r(vn),e&&r(Wt),e&&r(gn),e&&r(S),e&&r(En),e&&r(ue),x(At),e&&r(_n),e&&r(Kt),e&&r(qn),e&&r(er),e&&r(bn),e&&r(tr),e&&r(xn),e&&r(rr),e&&r($n),e&&r(z),e&&r(Pn),e&&r(y)}}}const Hp={local:"comment-fonctionnent-les-itransformersi",sections:[{local:"court-historique-des-itransformersi",title:"Court historique des <i>transformers</i>"},{local:"les-itransformersi-sont-des-modles-de-langage",title:"Les <i>transformers</i> sont des mod\xE8les de langage"},{local:"les-itransformersi-sont-normes",title:"Les <i>transformers</i> sont \xE9normes"},{local:"lapprentissage-par-transfert",title:"L'apprentissage par transfert"},{local:"architecture-gnrale",title:"Architecture g\xE9n\xE9rale"},{local:"introduction",title:"Introduction"},{local:"les-couches-dattention",title:"Les couches d'attention"},{local:"larchitecture-originale",title:"L'architecture originale"},{local:"architectures-contre-icheckpointsi",title:"Architectures contre <i>checkpoints</i>"}],title:"Comment fonctionnent les <i>transformers</i> ?"};function Vp(Fc){return Dp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qp extends Rp{constructor(D){super();jp(this,D,Vp,Up,Sp,{})}}export{Qp as default,Hp as metadata};
