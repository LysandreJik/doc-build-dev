import{S as Ct,i as Dt,s as St,e as o,k as v,w as yt,t as n,M as Bt,c as a,d as r,m as _,a as l,x as It,h as s,b as h,G as e,g as m,y as Tt,L as Ut,q as At,o as Nt,B as jt,v as Ft}from"../../chunks/vendor-hf-doc-builder.js";import{I as Gt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ht}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Jt(lt){let k,ee,q,x,C,P,ce,D,me,te,w,re,u,pe,y,fe,de,S,he,ve,B,_e,Ee,U,ke,qe,F,ze,be,ne,c,xe,G,ge,Me,g,$e,H,Pe,we,J,Le,ye,M,Ie,O,Te,Ae,R,Ne,je,se,I,Ce,oe,p,z,De,K,Se,Be,Q,Ue,Fe,Ge,L,He,V,Je,Oe,Re,W,Ke,Qe,b,Ve,X,We,Xe,Y,Ye,Ze,ae,E,et,T,tt,rt,Z,nt,st,le;return P=new Gt({}),w=new Ht({props:{chapter:6,classNames:"absolute z-10 right-0 top-0"}}),{c(){k=o("meta"),ee=v(),q=o("h1"),x=o("a"),C=o("span"),yt(P.$$.fragment),ce=v(),D=o("span"),me=n("Introduction"),te=v(),yt(w.$$.fragment),re=v(),u=o("p"),pe=n("Dans le "),y=o("a"),fe=n("chapitre 3"),de=n(", nous avons vu comment "),S=o("em"),he=n("finetuner"),ve=n(" un mod\xE8le sur une t\xE2che donn\xE9e. Pour ce faire, nous utilisons le m\xEAme "),B=o("em"),_e=n("tokenizer"),Ee=n(" que celui avec lequel le mod\xE8le a \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Mais que faisons-nous lorsque nous voulons entra\xEEner un mod\xE8le \xE0 partir de z\xE9ro ? Dans ces cas, l\u2019utilisation d\u2019un "),U=o("em"),ke=n("tokenizer"),qe=n(" qui a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 sur un corpus d\u2019un autre domaine ou d\u2019une autre langue est g\xE9n\xE9ralement sous-optimale. Par exemple, un "),F=o("em"),ze=n("tokenizer"),be=n(" entra\xEEn\xE9 sur un corpus anglais sera peu performant sur un corpus de textes japonais car l\u2019utilisation des espaces et de la ponctuation est tr\xE8s diff\xE9rente entre les deux langues."),ne=v(),c=o("p"),xe=n("Dans ce chapitre, vous apprendrez \xE0 entra\xEEner un tout nouveau "),G=o("em"),ge=n("tokenizer"),Me=n(" sur un corpus de textes afin qu\u2019il puisse ensuite \xEAtre utilis\xE9 pour pr\xE9-entra\xEEner un mod\xE8le de langue. Tout cela se fera \xE0 l\u2019aide de la biblioth\xE8que "),g=o("a"),$e=n("\u{1F917} "),H=o("em"),Pe=n("Tokenizers"),we=n(", qui fournit les "),J=o("em"),Le=n("tokenizers"),ye=n(" \xAB rapides \xBB de la biblioth\xE8que "),M=o("a"),Ie=n("\u{1F917} "),O=o("em"),Te=n("Transformers"),Ae=n(". Nous examinerons de pr\xE8s les fonctionnalit\xE9s offertes par cette biblioth\xE8que et nous \xE9tudierons comment les "),R=o("em"),Ne=n("tokenizers"),je=n(" rapides diff\xE8rent des versions \xAB lentes \xBB."),se=v(),I=o("p"),Ce=n("Les sujets que nous couvrirons comprennent :"),oe=v(),p=o("ul"),z=o("li"),De=n("comment entra\xEEner sur un nouveau corpus de textes, un nouveau "),K=o("em"),Se=n("tokenizer"),Be=n(" similaire \xE0 celui utilis\xE9 par un "),Q=o("em"),Ue=n("checkpoint"),Fe=n(" donn\xE9,"),Ge=v(),L=o("li"),He=n("les caract\xE9ristiques sp\xE9ciales des "),V=o("em"),Je=n("tokenizers"),Oe=n(" rapides,"),Re=v(),W=o("li"),Ke=n("les diff\xE9rences entre les trois principaux algorithmes de tok\xE9nisation utilis\xE9s aujourd\u2019hui en NLP,"),Qe=v(),b=o("li"),Ve=n("comment construire un "),X=o("em"),We=n("tokenizer"),Xe=n(" \xE0 partir de z\xE9ro avec la biblioth\xE8que \u{1F917} "),Y=o("em"),Ye=n("Tokenizers"),Ze=n(" et l\u2019entra\xEEner sur des donn\xE9es."),ae=v(),E=o("p"),et=n("Les techniques pr\xE9sent\xE9es dans ce chapitre vous pr\xE9pareront \xE0 la section du "),T=o("a"),tt=n("chapitre 7"),rt=n(" o\xF9 nous verrons comment cr\xE9er un mod\xE8le de langue pour le langage Python. Commen\xE7ons par examiner ce que signifie \xAB entra\xEEner \xBB un "),Z=o("em"),nt=n("tokenizer"),st=n("."),this.h()},l(t){const i=Bt('[data-svelte="svelte-1phssyn"]',document.head);k=a(i,"META",{name:!0,content:!0}),i.forEach(r),ee=_(t),q=a(t,"H1",{class:!0});var ie=l(q);x=a(ie,"A",{id:!0,class:!0,href:!0});var it=l(x);C=a(it,"SPAN",{});var ut=l(C);It(P.$$.fragment,ut),ut.forEach(r),it.forEach(r),ce=_(ie),D=a(ie,"SPAN",{});var ct=l(D);me=s(ct,"Introduction"),ct.forEach(r),ie.forEach(r),te=_(t),It(w.$$.fragment,t),re=_(t),u=a(t,"P",{});var f=l(u);pe=s(f,"Dans le "),y=a(f,"A",{href:!0});var mt=l(y);fe=s(mt,"chapitre 3"),mt.forEach(r),de=s(f,", nous avons vu comment "),S=a(f,"EM",{});var pt=l(S);he=s(pt,"finetuner"),pt.forEach(r),ve=s(f," un mod\xE8le sur une t\xE2che donn\xE9e. Pour ce faire, nous utilisons le m\xEAme "),B=a(f,"EM",{});var ft=l(B);_e=s(ft,"tokenizer"),ft.forEach(r),Ee=s(f," que celui avec lequel le mod\xE8le a \xE9t\xE9 pr\xE9-entra\xEEn\xE9. Mais que faisons-nous lorsque nous voulons entra\xEEner un mod\xE8le \xE0 partir de z\xE9ro ? Dans ces cas, l\u2019utilisation d\u2019un "),U=a(f,"EM",{});var dt=l(U);ke=s(dt,"tokenizer"),dt.forEach(r),qe=s(f," qui a \xE9t\xE9 pr\xE9-entra\xEEn\xE9 sur un corpus d\u2019un autre domaine ou d\u2019une autre langue est g\xE9n\xE9ralement sous-optimale. Par exemple, un "),F=a(f,"EM",{});var ht=l(F);ze=s(ht,"tokenizer"),ht.forEach(r),be=s(f," entra\xEEn\xE9 sur un corpus anglais sera peu performant sur un corpus de textes japonais car l\u2019utilisation des espaces et de la ponctuation est tr\xE8s diff\xE9rente entre les deux langues."),f.forEach(r),ne=_(t),c=a(t,"P",{});var d=l(c);xe=s(d,"Dans ce chapitre, vous apprendrez \xE0 entra\xEEner un tout nouveau "),G=a(d,"EM",{});var vt=l(G);ge=s(vt,"tokenizer"),vt.forEach(r),Me=s(d," sur un corpus de textes afin qu\u2019il puisse ensuite \xEAtre utilis\xE9 pour pr\xE9-entra\xEEner un mod\xE8le de langue. Tout cela se fera \xE0 l\u2019aide de la biblioth\xE8que "),g=a(d,"A",{href:!0,rel:!0});var ot=l(g);$e=s(ot,"\u{1F917} "),H=a(ot,"EM",{});var _t=l(H);Pe=s(_t,"Tokenizers"),_t.forEach(r),ot.forEach(r),we=s(d,", qui fournit les "),J=a(d,"EM",{});var Et=l(J);Le=s(Et,"tokenizers"),Et.forEach(r),ye=s(d," \xAB rapides \xBB de la biblioth\xE8que "),M=a(d,"A",{href:!0,rel:!0});var at=l(M);Ie=s(at,"\u{1F917} "),O=a(at,"EM",{});var kt=l(O);Te=s(kt,"Transformers"),kt.forEach(r),at.forEach(r),Ae=s(d,". Nous examinerons de pr\xE8s les fonctionnalit\xE9s offertes par cette biblioth\xE8que et nous \xE9tudierons comment les "),R=a(d,"EM",{});var qt=l(R);Ne=s(qt,"tokenizers"),qt.forEach(r),je=s(d," rapides diff\xE8rent des versions \xAB lentes \xBB."),d.forEach(r),se=_(t),I=a(t,"P",{});var zt=l(I);Ce=s(zt,"Les sujets que nous couvrirons comprennent :"),zt.forEach(r),oe=_(t),p=a(t,"UL",{});var $=l(p);z=a($,"LI",{});var A=l(z);De=s(A,"comment entra\xEEner sur un nouveau corpus de textes, un nouveau "),K=a(A,"EM",{});var bt=l(K);Se=s(bt,"tokenizer"),bt.forEach(r),Be=s(A," similaire \xE0 celui utilis\xE9 par un "),Q=a(A,"EM",{});var xt=l(Q);Ue=s(xt,"checkpoint"),xt.forEach(r),Fe=s(A," donn\xE9,"),A.forEach(r),Ge=_($),L=a($,"LI",{});var ue=l(L);He=s(ue,"les caract\xE9ristiques sp\xE9ciales des "),V=a(ue,"EM",{});var gt=l(V);Je=s(gt,"tokenizers"),gt.forEach(r),Oe=s(ue," rapides,"),ue.forEach(r),Re=_($),W=a($,"LI",{});var Mt=l(W);Ke=s(Mt,"les diff\xE9rences entre les trois principaux algorithmes de tok\xE9nisation utilis\xE9s aujourd\u2019hui en NLP,"),Mt.forEach(r),Qe=_($),b=a($,"LI",{});var N=l(b);Ve=s(N,"comment construire un "),X=a(N,"EM",{});var $t=l(X);We=s($t,"tokenizer"),$t.forEach(r),Xe=s(N," \xE0 partir de z\xE9ro avec la biblioth\xE8que \u{1F917} "),Y=a(N,"EM",{});var Pt=l(Y);Ye=s(Pt,"Tokenizers"),Pt.forEach(r),Ze=s(N," et l\u2019entra\xEEner sur des donn\xE9es."),N.forEach(r),$.forEach(r),ae=_(t),E=a(t,"P",{});var j=l(E);et=s(j,"Les techniques pr\xE9sent\xE9es dans ce chapitre vous pr\xE9pareront \xE0 la section du "),T=a(j,"A",{href:!0});var wt=l(T);tt=s(wt,"chapitre 7"),wt.forEach(r),rt=s(j," o\xF9 nous verrons comment cr\xE9er un mod\xE8le de langue pour le langage Python. Commen\xE7ons par examiner ce que signifie \xAB entra\xEEner \xBB un "),Z=a(j,"EM",{});var Lt=l(Z);nt=s(Lt,"tokenizer"),Lt.forEach(r),st=s(j,"."),j.forEach(r),this.h()},h(){h(k,"name","hf:doc:metadata"),h(k,"content",JSON.stringify(Ot)),h(x,"id","introduction"),h(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(x,"href","#introduction"),h(q,"class","relative group"),h(y,"href","/course/fr/chapter3"),h(g,"href","https://github.com/huggingface/tokenizers"),h(g,"rel","nofollow"),h(M,"href","https://github.com/huggingface/transformers"),h(M,"rel","nofollow"),h(T,"href","/course/fr/chapter7/6")},m(t,i){e(document.head,k),m(t,ee,i),m(t,q,i),e(q,x),e(x,C),Tt(P,C,null),e(q,ce),e(q,D),e(D,me),m(t,te,i),Tt(w,t,i),m(t,re,i),m(t,u,i),e(u,pe),e(u,y),e(y,fe),e(u,de),e(u,S),e(S,he),e(u,ve),e(u,B),e(B,_e),e(u,Ee),e(u,U),e(U,ke),e(u,qe),e(u,F),e(F,ze),e(u,be),m(t,ne,i),m(t,c,i),e(c,xe),e(c,G),e(G,ge),e(c,Me),e(c,g),e(g,$e),e(g,H),e(H,Pe),e(c,we),e(c,J),e(J,Le),e(c,ye),e(c,M),e(M,Ie),e(M,O),e(O,Te),e(c,Ae),e(c,R),e(R,Ne),e(c,je),m(t,se,i),m(t,I,i),e(I,Ce),m(t,oe,i),m(t,p,i),e(p,z),e(z,De),e(z,K),e(K,Se),e(z,Be),e(z,Q),e(Q,Ue),e(z,Fe),e(p,Ge),e(p,L),e(L,He),e(L,V),e(V,Je),e(L,Oe),e(p,Re),e(p,W),e(W,Ke),e(p,Qe),e(p,b),e(b,Ve),e(b,X),e(X,We),e(b,Xe),e(b,Y),e(Y,Ye),e(b,Ze),m(t,ae,i),m(t,E,i),e(E,et),e(E,T),e(T,tt),e(E,rt),e(E,Z),e(Z,nt),e(E,st),le=!0},p:Ut,i(t){le||(At(P.$$.fragment,t),At(w.$$.fragment,t),le=!0)},o(t){Nt(P.$$.fragment,t),Nt(w.$$.fragment,t),le=!1},d(t){r(k),t&&r(ee),t&&r(q),jt(P),t&&r(te),jt(w,t),t&&r(re),t&&r(u),t&&r(ne),t&&r(c),t&&r(se),t&&r(I),t&&r(oe),t&&r(p),t&&r(ae),t&&r(E)}}}const Ot={local:"introduction",title:"Introduction"};function Rt(lt){return Ft(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Wt extends Ct{constructor(k){super();Dt(this,k,Rt,Jt,St,{})}}export{Wt as default,Ot as metadata};
