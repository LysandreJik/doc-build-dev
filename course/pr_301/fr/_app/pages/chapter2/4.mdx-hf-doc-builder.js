import{S as Sc,i as Hc,s as Uc,e as o,k as m,w as _,t as n,M as Oc,c as a,d as s,m as p,x as b,a as l,h as r,b as d,N as ds,G as t,g as u,y as E,o as v,p as Bc,q as h,B as $,v as Vc,n as Gc}from"../../chunks/vendor-hf-doc-builder.js";import{T as Jc}from"../../chunks/Tip-hf-doc-builder.js";import{Y as or}from"../../chunks/Youtube-hf-doc-builder.js";import{I as de}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Y}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as Dc}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{F as Rc}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Fc(W){let c,q;return c=new Dc({props:{chapter:2,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_tf.ipynb"}]}}),{c(){_(c.$$.fragment)},l(f){b(c.$$.fragment,f)},m(f,w){E(c,f,w),q=!0},i(f){q||(h(c.$$.fragment,f),q=!0)},o(f){v(c.$$.fragment,f),q=!1},d(f){$(c,f)}}}function Kc(W){let c,q;return c=new Dc({props:{chapter:2,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"}]}}),{c(){_(c.$$.fragment)},l(f){b(c.$$.fragment,f)},m(f,w){E(c,f,w),q=!0},i(f){q||(h(c.$$.fragment,f),q=!0)},o(f){v(c.$$.fragment,f),q=!1},d(f){$(c,f)}}}function Yc(W){let c,q,f,w,y,g,D,C,M,T,B,z,P,V,x,j,G;return{c(){c=o("p"),q=n("Similaire \xE0 "),f=o("code"),w=n("TFAutoModel"),y=n(", la classe "),g=o("code"),D=n("AutoTokenizer"),C=n(" r\xE9cup\xE8re la classe de "),M=o("em"),T=n("tokenizer"),B=n(" appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),z=o("em"),P=n("checkpoint"),V=n(". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),x=o("em"),j=n("checkpoint"),G=n(" :")},l(L){c=a(L,"P",{});var k=l(c);q=r(k,"Similaire \xE0 "),f=a(k,"CODE",{});var S=l(f);w=r(S,"TFAutoModel"),S.forEach(s),y=r(k,", la classe "),g=a(k,"CODE",{});var fe=l(g);D=r(fe,"AutoTokenizer"),fe.forEach(s),C=r(k," r\xE9cup\xE8re la classe de "),M=a(k,"EM",{});var te=l(M);T=r(te,"tokenizer"),te.forEach(s),B=r(k," appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),z=a(k,"EM",{});var ve=l(z);P=r(ve,"checkpoint"),ve.forEach(s),V=r(k,". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),x=a(k,"EM",{});var he=l(x);j=r(he,"checkpoint"),he.forEach(s),G=r(k," :"),k.forEach(s)},m(L,k){u(L,c,k),t(c,q),t(c,f),t(f,w),t(c,y),t(c,g),t(g,D),t(c,C),t(c,M),t(M,T),t(c,B),t(c,z),t(z,P),t(c,V),t(c,x),t(x,j),t(c,G)},d(L){L&&s(c)}}}function Wc(W){let c,q,f,w,y,g,D,C,M,T,B,z,P,V,x,j,G;return{c(){c=o("p"),q=n("Similaire \xE0 "),f=o("code"),w=n("AutoModel"),y=n(", la classe "),g=o("code"),D=n("AutoTokenizer"),C=n(" r\xE9cup\xE8re la classe de "),M=o("em"),T=n("tokenizer"),B=n(" appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),z=o("em"),P=n("checkpoint"),V=n(". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),x=o("em"),j=n("checkpoint"),G=n(" :")},l(L){c=a(L,"P",{});var k=l(c);q=r(k,"Similaire \xE0 "),f=a(k,"CODE",{});var S=l(f);w=r(S,"AutoModel"),S.forEach(s),y=r(k,", la classe "),g=a(k,"CODE",{});var fe=l(g);D=r(fe,"AutoTokenizer"),fe.forEach(s),C=r(k," r\xE9cup\xE8re la classe de "),M=a(k,"EM",{});var te=l(M);T=r(te,"tokenizer"),te.forEach(s),B=r(k," appropri\xE9e dans la biblioth\xE8que bas\xE9e sur le nom du "),z=a(k,"EM",{});var ve=l(z);P=r(ve,"checkpoint"),ve.forEach(s),V=r(k,". Elle peut \xEAtre utilis\xE9e directement avec n\u2019importe quel "),x=a(k,"EM",{});var he=l(x);j=r(he,"checkpoint"),he.forEach(s),G=r(k," :"),k.forEach(s)},m(L,k){u(L,c,k),t(c,q),t(c,f),t(f,w),t(c,y),t(c,g),t(g,D),t(c,C),t(c,M),t(M,T),t(c,B),t(c,z),t(z,P),t(c,V),t(c,x),t(x,j),t(c,G)},d(L){L&&s(c)}}}function Qc(W){let c,q,f,w,y,g,D,C,M,T,B;return{c(){c=o("p"),q=n("\u270F\uFE0F "),f=o("strong"),w=n("Essayez !"),y=n(" Reproduisez les deux derni\xE8res \xE9tapes (tok\xE9nisation et conversion en identifiants d\u2019entr\xE9e) sur les phrases des entr\xE9es que nous avons utilis\xE9es dans la section 2 (\xAB "),g=o("i"),D=n("I\u2019ve been waiting for a HuggingFace course my whole life."),C=n(" \xBB et \xAB "),M=o("i"),T=n("I hate this so much!"),B=n(" \xBB). V\xE9rifiez que vous obtenez les m\xEAmes identifiants d\u2019entr\xE9e que nous avons obtenus pr\xE9c\xE9demment !")},l(z){c=a(z,"P",{});var P=l(c);q=r(P,"\u270F\uFE0F "),f=a(P,"STRONG",{});var V=l(f);w=r(V,"Essayez !"),V.forEach(s),y=r(P," Reproduisez les deux derni\xE8res \xE9tapes (tok\xE9nisation et conversion en identifiants d\u2019entr\xE9e) sur les phrases des entr\xE9es que nous avons utilis\xE9es dans la section 2 (\xAB "),g=a(P,"I",{});var x=l(g);D=r(x,"I\u2019ve been waiting for a HuggingFace course my whole life."),x.forEach(s),C=r(P," \xBB et \xAB "),M=a(P,"I",{});var j=l(M);T=r(j,"I hate this so much!"),j.forEach(s),B=r(P," \xBB). V\xE9rifiez que vous obtenez les m\xEAmes identifiants d\u2019entr\xE9e que nous avons obtenus pr\xE9c\xE9demment !"),P.forEach(s)},m(z,P){u(z,c,P),t(c,q),t(c,f),t(f,w),t(c,y),t(c,g),t(g,D),t(c,C),t(c,M),t(M,T),t(c,B)},d(z){z&&s(c)}}}function Xc(W){let c,q,f,w,y,g,D,C,M,T,B,z,P,V,x,j,G,L,k,S,fe,te,ve,he,fs,sa,na,ar,Vt,ra,lr,nt,ir,Me,oa,vs,aa,la,ur,Gt,ia,mr,ke,Ae,hs,rt,ua,Jt,ks,ma,pa,pr,ot,cr,Ce,ca,_s,da,fa,dr,_e,at,km,va,lt,_m,fr,Le,ha,bs,ka,_a,vr,it,hr,ut,kr,Q,ba,Es,Ea,$a,$s,qa,ga,qs,xa,za,_r,Rt,wa,br,re,Pa,gs,ja,ya,xs,Ma,Aa,Er,se,Ca,zs,La,Ta,ws,Ia,Na,ne,Da,Ps,Sa,Ha,js,Ua,Oa,ys,Ba,Va,$r,oe,Ga,Ms,Ja,Ra,As,Fa,Ka,qr,be,Te,Cs,mt,Ya,Ft,Ls,Wa,Qa,gr,pt,xr,Ie,Xa,Ts,Za,el,zr,Ne,Is,tl,sl,ct,nl,Ns,rl,ol,wr,Kt,al,Pr,Ee,dt,bm,ll,ft,Em,jr,Yt,il,yr,H,ul,Ds,ml,pl,Ss,cl,dl,Hs,fl,vl,Us,hl,kl,Os,_l,bl,Mr,De,El,Bs,$l,ql,Ar,$e,Se,Vs,vt,gl,Gs,xl,Cr,ht,Lr,Wt,zl,Tr,Qt,wl,Ir,He,Pl,Js,jl,yl,Nr,qe,kt,$m,Ml,_t,qm,Dr,I,Al,Rs,Cl,Ll,Fs,Tl,Il,Ks,Nl,Dl,Ys,Sl,Hl,Ws,Ul,Ol,Qs,Bl,Vl,Sr,Xt,Gl,Hr,ge,Ue,Xs,bt,Jl,Zs,Rl,Ur,Zt,Fl,Or,ae,Et,Kl,en,Yl,Wl,Ql,$t,Xl,tn,Zl,ei,ti,Oe,sn,si,ni,nn,ri,oi,Br,es,ai,Vr,xe,Be,rn,qt,li,on,ii,Gr,N,ui,an,mi,pi,ln,ci,di,un,fi,vi,mn,hi,ki,pn,_i,bi,cn,Ei,$i,Jr,X,qi,dn,gi,xi,fn,zi,wi,vn,Pi,ji,Rr,gt,Fr,ts,xt,Kr,Ve,yi,hn,Mi,Ai,Yr,zt,Wr,wt,Qr,ss,Ci,Xr,Pt,Zr,U,Li,kn,Ti,Ii,ns,Ni,Di,_n,Si,Hi,bn,Ui,Oi,En,Bi,Vi,eo,ze,Ge,$n,jt,Gi,qn,Ji,to,yt,so,Je,Ri,gn,Fi,Ki,no,le,Yi,xn,Wi,Qi,zn,Xi,Zi,ro,J,eu,wn,tu,su,Pn,nu,ru,jn,ou,au,yn,lu,iu,oo,Re,uu,Mn,mu,pu,ao,we,Fe,An,Mt,cu,Cn,du,lo,ie,fu,Ln,vu,hu,Tn,ku,_u,io,At,uo,Ke,bu,In,Eu,$u,mo,Ct,po,A,qu,Nn,gu,xu,Dn,zu,wu,Sn,Pu,ju,Hn,yu,Mu,Un,Au,Cu,On,Lu,Tu,Bn,Iu,Nu,co,Pe,Ye,Vn,Lt,Du,Tt,Su,Gn,Hu,Uu,fo,ue,Ou,Jn,Bu,Vu,Rn,Gu,Ju,vo,It,ho,Nt,ko,We,Ru,Fn,Fu,Ku,_o,Qe,bo,je,Xe,Kn,Dt,Yu,Yn,Wu,Eo,me,Qu,Wn,Xu,Zu,Qn,em,tm,$o,St,qo,Ht,go,R,sm,Xn,nm,rm,Zn,om,am,er,lm,im,tr,um,mm,xo,Ze,pm,sr,cm,dm,zo;f=new Rc({props:{fw:W[0]}}),C=new de({});const gm=[Kc,Fc],Ut=[];function xm(e,i){return e[0]==="pt"?0:1}x=xm(W),j=Ut[x]=gm[x](W),L=new or({props:{id:"VFp38yj8h3A"}}),nt=new Y({props:{code:"Jim Henson was a puppeteer # Jim Henson \xE9tait un marionnettiste",highlighted:'Jim Henson was <span class="hljs-keyword">a</span> puppeteer <span class="hljs-comment"># Jim Henson \xE9tait un marionnettiste</span>'}}),rt=new de({}),ot=new or({props:{id:"nhJxYji1aho"}}),it=new Y({props:{code:`tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)`,highlighted:`tokenized_text = <span class="hljs-string">&quot;Jim Henson was a puppeteer&quot;</span>.split()
<span class="hljs-built_in">print</span>(tokenized_text)`}}),ut=new Y({props:{code:"['Jim', 'Henson', 'was', 'a', 'puppeteer'] # ['Jim', 'Henson', \xE9tait, 'un', 'marionnettiste']",highlighted:'[<span class="hljs-string">&#x27;Jim&#x27;</span>, <span class="hljs-string">&#x27;Henson&#x27;</span>, <span class="hljs-string">&#x27;was&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;puppeteer&#x27;</span>] <span class="hljs-comment"># [&#x27;Jim&#x27;, &#x27;Henson&#x27;, \xE9tait, &#x27;un&#x27;, &#x27;marionnettiste&#x27;]</span>'}}),mt=new de({}),pt=new or({props:{id:"ssLq_EK2jLE"}}),vt=new de({}),ht=new or({props:{id:"zHvTiHr506c"}}),bt=new de({}),qt=new de({}),gt=new Y({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}});function zm(e,i){return e[0]==="pt"?Wc:Yc}let wo=zm(W),ye=wo(W);return xt=new Y({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),zt=new Y({props:{code:'tokenizer("Using a Transformer network is simple")',highlighted:'tokenizer(<span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>)'}}),wt=new Y({props:{code:`{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}`,highlighted:`{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Pt=new Y({props:{code:'tokenizer.save_pretrained("directory_on_my_computer")',highlighted:'tokenizer.save_pretrained(<span class="hljs-string">&quot;directory_on_my_computer&quot;</span>)'}}),jt=new de({}),yt=new or({props:{id:"Yffk5aydLzg"}}),Mt=new de({}),At=new Y({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

sequence = <span class="hljs-string">&quot;Using a Transformer network is simple&quot;</span>
tokens = tokenizer.tokenize(sequence)

<span class="hljs-built_in">print</span>(tokens)`}}),Ct=new Y({props:{code:"['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']",highlighted:'[<span class="hljs-string">&#x27;Using&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;transform&#x27;</span>, <span class="hljs-string">&#x27;##er&#x27;</span>, <span class="hljs-string">&#x27;network&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;simple&#x27;</span>]'}}),Lt=new de({}),It=new Y({props:{code:`ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)`,highlighted:`ids = tokenizer.convert_tokens_to_ids(tokens)

<span class="hljs-built_in">print</span>(ids)`}}),Nt=new Y({props:{code:"[7993, 170, 11303, 1200, 2443, 1110, 3014]",highlighted:'[<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>]'}}),Qe=new Jc({props:{$$slots:{default:[Qc]},$$scope:{ctx:W}}}),Dt=new de({}),St=new Y({props:{code:`decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)`,highlighted:`decoded_string = tokenizer.decode([<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>])
<span class="hljs-built_in">print</span>(decoded_string)`}}),Ht=new Y({props:{code:"'Using a Transformer network is simple'",highlighted:'<span class="hljs-string">&#x27;Using a Transformer network is simple&#x27;</span>'}}),{c(){c=o("meta"),q=m(),_(f.$$.fragment),w=m(),y=o("h1"),g=o("a"),D=o("span"),_(C.$$.fragment),M=m(),T=o("span"),B=n("Les "),z=o("i"),P=n("tokenizers"),V=m(),j.c(),G=m(),_(L.$$.fragment),k=m(),S=o("p"),fe=n("Les "),te=o("em"),ve=n("tokenizers"),he=n(" sont l\u2019un des principaux composants du pipeline de NLP. Ils ont un seul objectif : traduire le texte en donn\xE9es pouvant \xEAtre trait\xE9es par le mod\xE8le. Les mod\xE8les ne pouvant traiter que des nombres, les "),fs=o("em"),sa=n("tokenizers"),na=n(" doivent convertir nos entr\xE9es textuelles en donn\xE9es num\xE9riques. Dans cette section, nous allons explorer ce qui se passe exactement dans le pipeline de tok\xE9nisation."),ar=m(),Vt=o("p"),ra=n("Dans les t\xE2ches de NLP, les donn\xE9es trait\xE9es sont g\xE9n\xE9ralement du texte brut. Voici un exemple de ce type de texte :"),lr=m(),_(nt.$$.fragment),ir=m(),Me=o("p"),oa=n("Les mod\xE8les ne pouvant traiter que des nombres, nous devons trouver un moyen de convertir le texte brut en nombres. C\u2019est ce que font les "),vs=o("em"),aa=n("tokenizers"),la=n(" et il existe de nombreuses fa\xE7ons de proc\xE9der. L\u2019objectif est de trouver la repr\xE9sentation la plus significative, c\u2019est-\xE0-dire celle qui a le plus de sens pour le mod\xE8le, et si possible qui soit la plus petite."),ur=m(),Gt=o("p"),ia=n("Voyons quelques exemples d\u2019algorithmes de tok\xE9nisation et essayons de r\xE9pondre \xE0 certaines des questions que vous pouvez vous poser \xE0 ce sujet."),mr=m(),ke=o("h2"),Ae=o("a"),hs=o("span"),_(rt.$$.fragment),ua=m(),Jt=o("span"),ks=o("i"),ma=n("Tokenizer"),pa=n(" bas\xE9 sur les mots"),pr=m(),_(ot.$$.fragment),cr=m(),Ce=o("p"),ca=n("Le premier type de "),_s=o("em"),da=n("tokenizer"),fa=n(" qui vient \xE0 l\u2019esprit est celui bas\xE9 sur les mots. Il est g\xE9n\xE9ralement tr\xE8s facile \xE0 utiliser et configurable avec seulement quelques r\xE8gles. Il donne souvent des r\xE9sultats d\xE9cents. Par exemple, dans l\u2019image ci-dessous, l\u2019objectif est de diviser le texte brut en mots et de trouver une repr\xE9sentation num\xE9rique pour chacun d\u2019eux :"),dr=m(),_e=o("div"),at=o("img"),va=m(),lt=o("img"),fr=m(),Le=o("p"),ha=n("Il existe diff\xE9rentes fa\xE7ons de diviser le texte. Par exemple, nous pouvons utiliser les espaces pour segmenter le texte en mots en appliquant la fonction "),bs=o("code"),ka=n("split()"),_a=n(" de Python :"),vr=m(),_(it.$$.fragment),hr=m(),_(ut.$$.fragment),kr=m(),Q=o("p"),ba=n("Il existe \xE9galement des variantes des "),Es=o("em"),Ea=n("tokenizers"),$a=n(" bas\xE9s sur les mots qui ont des r\xE8gles suppl\xE9mentaires pour la ponctuation. Avec ce type de "),$s=o("em"),qa=n("tokenizers"),ga=n(" nous pouvons nous retrouver avec des \xAB vocabulaires \xBB assez larges, o\xF9 un vocabulaire est d\xE9fini par le nombre total de "),qs=o("em"),xa=n("tokens"),za=n(" ind\xE9pendants que nous avons dans notre corpus."),_r=m(),Rt=o("p"),wa=n("Un identifiant est attribu\xE9 \xE0 chaque mot, en commen\xE7ant par 0 et en allant jusqu\u2019\xE0 la taille du vocabulaire. Le mod\xE8le utilise ces identifiants pour identifier chaque mot."),br=m(),re=o("p"),Pa=n("Si nous voulons couvrir compl\xE8tement une langue avec un "),gs=o("em"),ja=n("tokenizer"),ya=n(" bas\xE9 sur les mots, nous devons avoir un identifiant pour chaque mot de la langue que nous traitons, ce qui g\xE9n\xE8re une \xE9norme quantit\xE9 de "),xs=o("em"),Ma=n("tokens"),Aa=n(". Par exemple, il y a plus de 500 000 mots dans la langue anglaise. Ainsi pour associer chaque mot \xE0 un identifiant, nous devrions garder la trace d\u2019autant d\u2019identifiants. De plus, des mots comme \xAB chien \xBB sont repr\xE9sent\xE9s diff\xE9remment de mots comme \xAB chiens \xBB. Le mod\xE8le n\u2019a initialement aucun moyen de savoir que \xAB chien \xBB et \xAB chiens \xBB sont similaires : il identifie les deux mots comme non apparent\xE9s. Il en va de m\xEAme pour d\u2019autres mots similaires, comme \xAB maison \xBB et \xAB maisonnette \xBB que le mod\xE8le ne consid\xE9rera pas comme similaires au d\xE9part."),Er=m(),se=o("p"),Ca=n("Enfin, nous avons besoin d\u2019un "),zs=o("em"),La=n("token"),Ta=n(" personnalis\xE9 pour repr\xE9senter les mots qui ne font pas partie de notre vocabulaire. C\u2019est ce qu\u2019on appelle le "),ws=o("em"),Ia=n("token"),Na=n(" \xAB inconnu \xBB souvent repr\xE9sent\xE9 par \xAB [UNK] \xBB (de l\u2019anglais \xAB unknown \xBB) ou \xAB "),ne=o("unk"),Da=n("; \xBB. C\u2019est g\xE9n\xE9ralement un mauvais signe si vous constatez que le "),Ps=o("em"),Sa=n("tokenizer"),Ha=n(" produit un nombre important de ce jeton sp\xE9cial. Cela signifie qu\u2019il n\u2019a pas \xE9t\xE9 en mesure de r\xE9cup\xE9rer une repr\xE9sentation sens\xE9e d\u2019un mot et que vous perdez des informations en cours de route. L\u2019objectif de l\u2019\xE9laboration du vocabulaire est de faire en sorte que le "),js=o("em"),Ua=n("tokenizer"),Oa=n(" transforme le moins de mots possible en "),ys=o("em"),Ba=n("token"),Va=n(" inconnu."),$r=m(),oe=o("p"),Ga=n("Une fa\xE7on de r\xE9duire la quantit\xE9 de "),Ms=o("em"),Ja=n("tokens"),Ra=n(" inconnus est d\u2019aller un niveau plus profond, en utilisant un "),As=o("em"),Fa=n("tokenizer"),Ka=n(" bas\xE9 sur les caract\xE8res."),qr=m(),be=o("h2"),Te=o("a"),Cs=o("span"),_(mt.$$.fragment),Ya=m(),Ft=o("span"),Ls=o("i"),Wa=n("Tokenizer"),Qa=n("  bas\xE9 sur les caract\xE8res"),gr=m(),_(pt.$$.fragment),xr=m(),Ie=o("p"),Xa=n("Les "),Ts=o("em"),Za=n("tokenizers"),el=n(" bas\xE9s sur les caract\xE8res divisent le texte en caract\xE8res, plut\xF4t qu\u2019en mots. Cela pr\xE9sente deux avantages principaux :"),zr=m(),Ne=o("ul"),Is=o("li"),tl=n("le vocabulaire est beaucoup plus petit"),sl=m(),ct=o("li"),nl=n("il y a beaucoup moins de "),Ns=o("em"),rl=n("tokens"),ol=n(" hors vocabulaire (inconnus) puisque chaque mot peut \xEAtre construit \xE0 partir de caract\xE8res."),wr=m(),Kt=o("p"),al=n("Mais l\xE0 aussi, des questions se posent concernant les espaces et la ponctuation :"),Pr=m(),Ee=o("div"),dt=o("img"),ll=m(),ft=o("img"),jr=m(),Yt=o("p"),il=n("Cette approche n\u2019est pas non plus parfaite. Puisque la repr\xE9sentation est maintenant bas\xE9e sur des caract\xE8res plut\xF4t que sur des mots, on pourrait dire intuitivement qu\u2019elle est moins significative : chaque caract\xE8re ne signifie pas grand-chose en soi, alors que c\u2019est le cas pour les mots. Toutefois, l\xE0 encore, cela diff\xE8re selon la langue. En chinois, par exemple, chaque caract\xE8re est porteur de plus d\u2019informations qu\u2019un caract\xE8re dans une langue latine."),yr=m(),H=o("p"),ul=n("Un autre \xE9l\xE9ment \xE0 prendre en compte est que nous nous retrouverons avec une tr\xE8s grande quantit\xE9 de "),Ds=o("em"),ml=n("tokens"),pl=n(" \xE0 traiter par notre mod\xE8le. Alors qu\u2019avec un "),Ss=o("em"),cl=n("tokenizer"),dl=n(" bas\xE9 sur les mots, pour un mot donn\xE9 on aurait qu\u2019un seul "),Hs=o("em"),fl=n("token"),vl=n(", avec un "),Us=o("em"),hl=n("tokenizer"),kl=n(" bas\xE9 sur les caract\xE8res, cela peut facilement se transformer en 10 "),Os=o("em"),_l=n("tokens"),bl=n(" voire plus."),Mr=m(),De=o("p"),El=n("Pour obtenir le meilleur des deux mondes, nous pouvons utiliser une troisi\xE8me technique qui combine les deux approches : la "),Bs=o("em"),$l=n("tok\xE9nisation en sous-mots"),ql=n("."),Ar=m(),$e=o("h2"),Se=o("a"),Vs=o("span"),_(vt.$$.fragment),gl=m(),Gs=o("span"),xl=n("Tok\xE9nisation en sous-mots"),Cr=m(),_(ht.$$.fragment),Lr=m(),Wt=o("p"),zl=n("Les algorithmes de tokenisation en sous-mots reposent sur le principe selon lequel les mots fr\xE9quemment utilis\xE9s ne doivent pas \xEAtre divis\xE9s en sous-mots plus petits, mais les mots rares doivent \xEAtre d\xE9compos\xE9s en sous-mots significatifs."),Tr=m(),Qt=o("p"),wl=n("Par exemple, le mot \xAB maisonnette \xBB peut \xEAtre consid\xE9r\xE9 comme un mot rare et peut \xEAtre d\xE9compos\xE9 en \xAB maison \xBB et \xAB ette \xBB. Ces deux mots sont susceptibles d\u2019appara\xEEtre plus fr\xE9quemment en tant que sous-mots autonomes, alors qu\u2019en m\xEAme temps le sens de \xAB maison \xBB est conserv\xE9 par le sens composite de \xAB maison \xBB et \xAB ette \xBB."),Ir=m(),He=o("p"),Pl=n("Voici un exemple montrant comment un algorithme de tokenisation en sous-mots tokeniserait la s\xE9quence \xAB "),Js=o("em"),jl=n("Let\u2019s do tokenization"),yl=n(" ! \xBB :"),Nr=m(),qe=o("div"),kt=o("img"),Ml=m(),_t=o("img"),Dr=m(),I=o("p"),Al=n("Ces sous-mots finissent par fournir beaucoup de sens s\xE9mantique. Par exemple, ci-dessus, \xAB "),Rs=o("em"),Cl=n("tokenization"),Ll=n(" \xBB a \xE9t\xE9 divis\xE9 en \xAB "),Fs=o("em"),Tl=n("token"),Il=n(" \xBB et \xAB "),Ks=o("em"),Nl=n("ization"),Dl=n(" \xBB : deux "),Ys=o("em"),Sl=n("tokens"),Hl=n(" qui ont un sens s\xE9mantique tout en \xE9tant peu encombrants (seuls deux "),Ws=o("em"),Ul=n("tokens"),Ol=n(" sont n\xE9cessaires pour repr\xE9senter un long mot). Cela nous permet d\u2019avoir une couverture relativement bonne avec de petits vocabulaires et presque aucun "),Qs=o("em"),Bl=n("token"),Vl=n(" inconnu."),Sr=m(),Xt=o("p"),Gl=n("Cette approche est particuli\xE8rement utile dans les langues agglutinantes comme le turc, o\xF9 l\u2019on peut former des mots complexes (presque) arbitrairement longs en encha\xEEnant des sous-mots."),Hr=m(),ge=o("h3"),Ue=o("a"),Xs=o("span"),_(bt.$$.fragment),Jl=m(),Zs=o("span"),Rl=n("Et plus encore !"),Ur=m(),Zt=o("p"),Fl=n("Il existe de nombreuses autres techniques. Pour n\u2019en citer que quelques-unes :"),Or=m(),ae=o("ul"),Et=o("li"),Kl=n("le "),en=o("em"),Yl=n("Byte-level BPE"),Wl=n(" utilis\xE9 par exemple dans le GPT-2"),Ql=m(),$t=o("li"),Xl=n("le "),tn=o("em"),Zl=n("WordPiece"),ei=n(" utilis\xE9 par exemple dans BERT"),ti=m(),Oe=o("li"),sn=o("em"),si=n("SentencePiece"),ni=n(" ou "),nn=o("em"),ri=n("Unigram"),oi=n(", utilis\xE9s dans plusieurs mod\xE8les multilingues."),Br=m(),es=o("p"),ai=n("Vous devriez maintenant avoir une connaissance suffisante du fonctionnement des tokenizers pour commencer \xE0 utiliser l\u2019API."),Vr=m(),xe=o("h2"),Be=o("a"),rn=o("span"),_(qt.$$.fragment),li=m(),on=o("span"),ii=n("Chargement et sauvegarde"),Gr=m(),N=o("p"),ui=n("Le chargement et la sauvegarde des "),an=o("em"),mi=n("tokenizers"),pi=n(" est aussi simple que pour les mod\xE8les. En fait, c\u2019est bas\xE9 sur les deux m\xEAmes m\xE9thodes : "),ln=o("code"),ci=n("from_pretrained()"),di=n(" et "),un=o("code"),fi=n("save_pretrained()"),vi=n(". Ces m\xE9thodes vont charger ou sauvegarder l\u2019algorithme utilis\xE9 par le "),mn=o("em"),hi=n("tokenizer"),ki=n(" (un peu comme l\u2019"),pn=o("em"),_i=n("architecture"),bi=n(" du mod\xE8le) ainsi que son vocabulaire (un peu comme les "),cn=o("em"),Ei=n("poids"),$i=n(" du mod\xE8le)."),Jr=m(),X=o("p"),qi=n("Le chargement du "),dn=o("em"),gi=n("tokenizer"),xi=n(" de BERT entra\xEEn\xE9 avec le m\xEAme "),fn=o("em"),zi=n("checkpoint"),wi=n(" que BERT se fait de la m\xEAme mani\xE8re que le chargement du mod\xE8le, sauf que nous utilisons la classe "),vn=o("code"),Pi=n("BertTokenizer"),ji=n(" :"),Rr=m(),_(gt.$$.fragment),Fr=m(),ye.c(),ts=m(),_(xt.$$.fragment),Kr=m(),Ve=o("p"),yi=n("Nous pouvons \xE0 pr\xE9sent utiliser le "),hn=o("em"),Mi=n("tokenizer"),Ai=n(" comme indiqu\xE9 dans la section pr\xE9c\xE9dente :"),Yr=m(),_(zt.$$.fragment),Wr=m(),_(wt.$$.fragment),Qr=m(),ss=o("p"),Ci=n("La sauvegarde d\u2019un tokenizer est identique \xE0 celle d\u2019un mod\xE8le :"),Xr=m(),_(Pt.$$.fragment),Zr=m(),U=o("p"),Li=n("Nous parlerons plus en d\xE9tail des "),kn=o("code"),Ti=n("token_type_ids"),Ii=n(" au "),ns=o("a"),Ni=n("chapitre 3"),Di=n(" et nous expliquerons la cl\xE9 "),_n=o("code"),Si=n("attention_mask"),Hi=n(" un peu plus tard. Tout d\u2019abord, voyons comment les "),bn=o("code"),Ui=n("input_ids"),Oi=n(" sont g\xE9n\xE9r\xE9s. Pour ce faire, nous devons examiner les m\xE9thodes interm\xE9diaires du "),En=o("em"),Bi=n("tokenizer"),Vi=n("."),eo=m(),ze=o("h2"),Ge=o("a"),$n=o("span"),_(jt.$$.fragment),Gi=m(),qn=o("span"),Ji=n("Encodage"),to=m(),_(yt.$$.fragment),so=m(),Je=o("p"),Ri=n("La traduction d\u2019un texte en chiffres est connue sous le nom d\u2019"),gn=o("em"),Fi=n("encodage"),Ki=n(". L\u2019encodage se fait en deux \xE9tapes : la tokenisation, suivie de la conversion en identifiants d\u2019entr\xE9e."),no=m(),le=o("p"),Yi=n("Comme nous l\u2019avons vu, la premi\xE8re \xE9tape consiste \xE0 diviser le texte en mots (ou parties de mots, symboles de ponctuation, etc.), g\xE9n\xE9ralement appel\xE9s "),xn=o("em"),Wi=n("tokens"),Qi=n(". De nombreuses r\xE8gles peuvent r\xE9gir ce processus. C\u2019est pourquoi nous devons instancier le "),zn=o("em"),Xi=n("tokenizer"),Zi=n(" en utilisant le nom du mod\xE8le afin de nous assurer que nous utilisons les m\xEAmes r\xE8gles que celles utilis\xE9es lors du pr\xE9-entra\xEEnement du mod\xE8le."),ro=m(),J=o("p"),eu=n("La deuxi\xE8me \xE9tape consiste \xE0 convertir ces "),wn=o("em"),tu=n("tokens"),su=n(" en nombres afin de construire un tenseur \xE0 partir de ceux-ci ainsi que de les transmettre au mod\xE8le. Pour ce faire, le "),Pn=o("em"),nu=n("tokenizer"),ru=n(" poss\xE8de un "),jn=o("em"),ou=n("vocabulaire"),au=n(", qui est la partie que nous t\xE9l\xE9chargeons lorsque nous l\u2019instancions avec la m\xE9thode "),yn=o("code"),lu=n("from_pretrained()"),iu=n(". Encore une fois, nous devons utiliser le m\xEAme vocabulaire que celui utilis\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le."),oo=m(),Re=o("p"),uu=n("Pour mieux comprendre les deux \xE9tapes, nous allons les explorer s\xE9par\xE9ment. A noter que nous utilisons des m\xE9thodes effectuant s\xE9par\xE9ment des parties du pipeline de tokenisation afin de montrer les r\xE9sultats interm\xE9diaires de ces \xE9tapes. N\xE9anmoins, en pratique, il faut appeler le "),Mn=o("em"),mu=n("tokenizer"),pu=n(" directement sur vos entr\xE9es (comme indiqu\xE9 dans la section 2)."),ao=m(),we=o("h3"),Fe=o("a"),An=o("span"),_(Mt.$$.fragment),cu=m(),Cn=o("span"),du=n("Tokenisation"),lo=m(),ie=o("p"),fu=n("Le processus de tokenisation est effectu\xE9 par la m\xE9thode "),Ln=o("code"),vu=n("tokenize()"),hu=n(" du "),Tn=o("em"),ku=n("tokenizer"),_u=n(" :"),io=m(),_(At.$$.fragment),uo=m(),Ke=o("p"),bu=n("La sortie de cette m\xE9thode est une liste de cha\xEEnes de caract\xE8res ou de "),In=o("em"),Eu=n("tokens"),$u=n(" :"),mo=m(),_(Ct.$$.fragment),po=m(),A=o("p"),qu=n("Ce "),Nn=o("em"),gu=n("tokenizer"),xu=n(" est un "),Dn=o("em"),zu=n("tokenizer"),wu=n(" de sous-mots : il d\xE9coupe les mots jusqu\u2019\xE0 obtenir des "),Sn=o("em"),Pu=n("tokens"),ju=n(" qui peuvent \xEAtre repr\xE9sent\xE9s par son vocabulaire. C\u2019est le cas ici avec "),Hn=o("code"),yu=n("transformer"),Mu=n(" qui est divis\xE9 en deux "),Un=o("em"),Au=n("tokens"),Cu=n(" : "),On=o("code"),Lu=n("transform"),Tu=n(" et "),Bn=o("code"),Iu=n("##er"),Nu=n("."),co=m(),Pe=o("h3"),Ye=o("a"),Vn=o("span"),_(Lt.$$.fragment),Du=m(),Tt=o("span"),Su=n("De "),Gn=o("i"),Hu=n("tokens"),Uu=n(" aux identifiants d'entr\xE9e"),fo=m(),ue=o("p"),Ou=n("La conversion en identifiants d\u2019entr\xE9e est g\xE9r\xE9e par la m\xE9thode "),Jn=o("code"),Bu=n("convert_tokens_to_ids()"),Vu=n(" du "),Rn=o("em"),Gu=n("tokenizer"),Ju=n(" :"),vo=m(),_(It.$$.fragment),ho=m(),_(Nt.$$.fragment),ko=m(),We=o("p"),Ru=n("Une fois converties en tenseur dans le "),Fn=o("em"),Fu=n("framework"),Ku=n(" appropri\xE9, ces sorties peuvent ensuite \xEAtre utilis\xE9es comme entr\xE9es d\u2019un mod\xE8le, comme nous l\u2019avons vu pr\xE9c\xE9demment dans ce chapitre."),_o=m(),_(Qe.$$.fragment),bo=m(),je=o("h2"),Xe=o("a"),Kn=o("span"),_(Dt.$$.fragment),Yu=m(),Yn=o("span"),Wu=n("D\xE9codage"),Eo=m(),me=o("p"),Qu=n("Le "),Wn=o("em"),Xu=n("d\xE9codage"),Zu=n(" va dans l\u2019autre sens : \xE0 partir d\u2019indices du vocabulaire nous voulons obtenir une cha\xEEne de caract\xE8res. Cela peut \xEAtre fait avec la m\xE9thode "),Qn=o("code"),em=n("decode()"),tm=n(" comme suit :"),$o=m(),_(St.$$.fragment),qo=m(),_(Ht.$$.fragment),go=m(),R=o("p"),sm=n("Notez que la m\xE9thode "),Xn=o("code"),nm=n("decode"),rm=n(" non seulement reconvertit les indices en "),Zn=o("em"),om=n("tokens"),am=n(" mais regroupe \xE9galement les "),er=o("em"),lm=n("tokens"),im=n(" faisant partie des m\xEAmes mots. Le but \xE9tant de produire une phrase lisible. Ce comportement sera extr\xEAmement utile lorsque dans la suite du cours nous utiliserons des mod\xE8les pouvant produire du nouveau texte (soit du texte g\xE9n\xE9r\xE9 \xE0 partir d\u2019un "),tr=o("em"),um=n("prompt"),mm=n(", soit pour des probl\xE8mes de s\xE9quence \xE0 s\xE9quence comme la traduction ou le r\xE9sum\xE9 de texte)."),xo=m(),Ze=o("p"),pm=n("Vous devriez maintenant comprendre les op\xE9rations atomiques qu\u2019un "),sr=o("em"),cm=n("tokenizer"),dm=n(" peut g\xE9rer : tokenisation, conversion en identifiants, et reconversion des identifiants en cha\xEEne de caract\xE8res. Cependant, nous n\u2019avons fait qu\u2019effleurer la partie \xE9merg\xE9e de l\u2019iceberg. Dans la section suivante, nous allons pousser notre approche jusqu\u2019\xE0 ses limites et voir comment les surmonter."),this.h()},l(e){const i=Oc('[data-svelte="svelte-1phssyn"]',document.head);c=a(i,"META",{name:!0,content:!0}),i.forEach(s),q=p(e),b(f.$$.fragment,e),w=p(e),y=a(e,"H1",{class:!0});var Ot=l(y);g=a(Ot,"A",{id:!0,class:!0,href:!0});var rs=l(g);D=a(rs,"SPAN",{});var nr=l(D);b(C.$$.fragment,nr),nr.forEach(s),rs.forEach(s),M=p(Ot),T=a(Ot,"SPAN",{});var fm=l(T);B=r(fm,"Les "),z=a(fm,"I",{});var wm=l(z);P=r(wm,"tokenizers"),wm.forEach(s),fm.forEach(s),Ot.forEach(s),V=p(e),j.l(e),G=p(e),b(L.$$.fragment,e),k=p(e),S=a(e,"P",{});var os=l(S);fe=r(os,"Les "),te=a(os,"EM",{});var Pm=l(te);ve=r(Pm,"tokenizers"),Pm.forEach(s),he=r(os," sont l\u2019un des principaux composants du pipeline de NLP. Ils ont un seul objectif : traduire le texte en donn\xE9es pouvant \xEAtre trait\xE9es par le mod\xE8le. Les mod\xE8les ne pouvant traiter que des nombres, les "),fs=a(os,"EM",{});var jm=l(fs);sa=r(jm,"tokenizers"),jm.forEach(s),na=r(os," doivent convertir nos entr\xE9es textuelles en donn\xE9es num\xE9riques. Dans cette section, nous allons explorer ce qui se passe exactement dans le pipeline de tok\xE9nisation."),os.forEach(s),ar=p(e),Vt=a(e,"P",{});var ym=l(Vt);ra=r(ym,"Dans les t\xE2ches de NLP, les donn\xE9es trait\xE9es sont g\xE9n\xE9ralement du texte brut. Voici un exemple de ce type de texte :"),ym.forEach(s),lr=p(e),b(nt.$$.fragment,e),ir=p(e),Me=a(e,"P",{});var Po=l(Me);oa=r(Po,"Les mod\xE8les ne pouvant traiter que des nombres, nous devons trouver un moyen de convertir le texte brut en nombres. C\u2019est ce que font les "),vs=a(Po,"EM",{});var Mm=l(vs);aa=r(Mm,"tokenizers"),Mm.forEach(s),la=r(Po," et il existe de nombreuses fa\xE7ons de proc\xE9der. L\u2019objectif est de trouver la repr\xE9sentation la plus significative, c\u2019est-\xE0-dire celle qui a le plus de sens pour le mod\xE8le, et si possible qui soit la plus petite."),Po.forEach(s),ur=p(e),Gt=a(e,"P",{});var Am=l(Gt);ia=r(Am,"Voyons quelques exemples d\u2019algorithmes de tok\xE9nisation et essayons de r\xE9pondre \xE0 certaines des questions que vous pouvez vous poser \xE0 ce sujet."),Am.forEach(s),mr=p(e),ke=a(e,"H2",{class:!0});var jo=l(ke);Ae=a(jo,"A",{id:!0,class:!0,href:!0});var Cm=l(Ae);hs=a(Cm,"SPAN",{});var Lm=l(hs);b(rt.$$.fragment,Lm),Lm.forEach(s),Cm.forEach(s),ua=p(jo),Jt=a(jo,"SPAN",{});var vm=l(Jt);ks=a(vm,"I",{});var Tm=l(ks);ma=r(Tm,"Tokenizer"),Tm.forEach(s),pa=r(vm," bas\xE9 sur les mots"),vm.forEach(s),jo.forEach(s),pr=p(e),b(ot.$$.fragment,e),cr=p(e),Ce=a(e,"P",{});var yo=l(Ce);ca=r(yo,"Le premier type de "),_s=a(yo,"EM",{});var Im=l(_s);da=r(Im,"tokenizer"),Im.forEach(s),fa=r(yo," qui vient \xE0 l\u2019esprit est celui bas\xE9 sur les mots. Il est g\xE9n\xE9ralement tr\xE8s facile \xE0 utiliser et configurable avec seulement quelques r\xE8gles. Il donne souvent des r\xE9sultats d\xE9cents. Par exemple, dans l\u2019image ci-dessous, l\u2019objectif est de diviser le texte brut en mots et de trouver une repr\xE9sentation num\xE9rique pour chacun d\u2019eux :"),yo.forEach(s),dr=p(e),_e=a(e,"DIV",{class:!0});var Mo=l(_e);at=a(Mo,"IMG",{class:!0,src:!0,alt:!0}),va=p(Mo),lt=a(Mo,"IMG",{class:!0,src:!0,alt:!0}),Mo.forEach(s),fr=p(e),Le=a(e,"P",{});var Ao=l(Le);ha=r(Ao,"Il existe diff\xE9rentes fa\xE7ons de diviser le texte. Par exemple, nous pouvons utiliser les espaces pour segmenter le texte en mots en appliquant la fonction "),bs=a(Ao,"CODE",{});var Nm=l(bs);ka=r(Nm,"split()"),Nm.forEach(s),_a=r(Ao," de Python :"),Ao.forEach(s),vr=p(e),b(it.$$.fragment,e),hr=p(e),b(ut.$$.fragment,e),kr=p(e),Q=a(e,"P",{});var et=l(Q);ba=r(et,"Il existe \xE9galement des variantes des "),Es=a(et,"EM",{});var Dm=l(Es);Ea=r(Dm,"tokenizers"),Dm.forEach(s),$a=r(et," bas\xE9s sur les mots qui ont des r\xE8gles suppl\xE9mentaires pour la ponctuation. Avec ce type de "),$s=a(et,"EM",{});var Sm=l($s);qa=r(Sm,"tokenizers"),Sm.forEach(s),ga=r(et," nous pouvons nous retrouver avec des \xAB vocabulaires \xBB assez larges, o\xF9 un vocabulaire est d\xE9fini par le nombre total de "),qs=a(et,"EM",{});var Hm=l(qs);xa=r(Hm,"tokens"),Hm.forEach(s),za=r(et," ind\xE9pendants que nous avons dans notre corpus."),et.forEach(s),_r=p(e),Rt=a(e,"P",{});var Um=l(Rt);wa=r(Um,"Un identifiant est attribu\xE9 \xE0 chaque mot, en commen\xE7ant par 0 et en allant jusqu\u2019\xE0 la taille du vocabulaire. Le mod\xE8le utilise ces identifiants pour identifier chaque mot."),Um.forEach(s),br=p(e),re=a(e,"P",{});var as=l(re);Pa=r(as,"Si nous voulons couvrir compl\xE8tement une langue avec un "),gs=a(as,"EM",{});var Om=l(gs);ja=r(Om,"tokenizer"),Om.forEach(s),ya=r(as," bas\xE9 sur les mots, nous devons avoir un identifiant pour chaque mot de la langue que nous traitons, ce qui g\xE9n\xE8re une \xE9norme quantit\xE9 de "),xs=a(as,"EM",{});var Bm=l(xs);Ma=r(Bm,"tokens"),Bm.forEach(s),Aa=r(as,". Par exemple, il y a plus de 500 000 mots dans la langue anglaise. Ainsi pour associer chaque mot \xE0 un identifiant, nous devrions garder la trace d\u2019autant d\u2019identifiants. De plus, des mots comme \xAB chien \xBB sont repr\xE9sent\xE9s diff\xE9remment de mots comme \xAB chiens \xBB. Le mod\xE8le n\u2019a initialement aucun moyen de savoir que \xAB chien \xBB et \xAB chiens \xBB sont similaires : il identifie les deux mots comme non apparent\xE9s. Il en va de m\xEAme pour d\u2019autres mots similaires, comme \xAB maison \xBB et \xAB maisonnette \xBB que le mod\xE8le ne consid\xE9rera pas comme similaires au d\xE9part."),as.forEach(s),Er=p(e),se=a(e,"P",{});var Bt=l(se);Ca=r(Bt,"Enfin, nous avons besoin d\u2019un "),zs=a(Bt,"EM",{});var Vm=l(zs);La=r(Vm,"token"),Vm.forEach(s),Ta=r(Bt," personnalis\xE9 pour repr\xE9senter les mots qui ne font pas partie de notre vocabulaire. C\u2019est ce qu\u2019on appelle le "),ws=a(Bt,"EM",{});var Gm=l(ws);Ia=r(Gm,"token"),Gm.forEach(s),Na=r(Bt," \xAB inconnu \xBB souvent repr\xE9sent\xE9 par \xAB [UNK] \xBB (de l\u2019anglais \xAB unknown \xBB) ou \xAB "),ne=a(Bt,"UNK",{});var tt=l(ne);Da=r(tt,"; \xBB. C\u2019est g\xE9n\xE9ralement un mauvais signe si vous constatez que le "),Ps=a(tt,"EM",{});var Jm=l(Ps);Sa=r(Jm,"tokenizer"),Jm.forEach(s),Ha=r(tt," produit un nombre important de ce jeton sp\xE9cial. Cela signifie qu\u2019il n\u2019a pas \xE9t\xE9 en mesure de r\xE9cup\xE9rer une repr\xE9sentation sens\xE9e d\u2019un mot et que vous perdez des informations en cours de route. L\u2019objectif de l\u2019\xE9laboration du vocabulaire est de faire en sorte que le "),js=a(tt,"EM",{});var Rm=l(js);Ua=r(Rm,"tokenizer"),Rm.forEach(s),Oa=r(tt," transforme le moins de mots possible en "),ys=a(tt,"EM",{});var Fm=l(ys);Ba=r(Fm,"token"),Fm.forEach(s),Va=r(tt," inconnu."),tt.forEach(s),Bt.forEach(s),$r=p(e),oe=a(e,"P",{});var ls=l(oe);Ga=r(ls,"Une fa\xE7on de r\xE9duire la quantit\xE9 de "),Ms=a(ls,"EM",{});var Km=l(Ms);Ja=r(Km,"tokens"),Km.forEach(s),Ra=r(ls," inconnus est d\u2019aller un niveau plus profond, en utilisant un "),As=a(ls,"EM",{});var Ym=l(As);Fa=r(Ym,"tokenizer"),Ym.forEach(s),Ka=r(ls," bas\xE9 sur les caract\xE8res."),ls.forEach(s),qr=p(e),be=a(e,"H2",{class:!0});var Co=l(be);Te=a(Co,"A",{id:!0,class:!0,href:!0});var Wm=l(Te);Cs=a(Wm,"SPAN",{});var Qm=l(Cs);b(mt.$$.fragment,Qm),Qm.forEach(s),Wm.forEach(s),Ya=p(Co),Ft=a(Co,"SPAN",{});var hm=l(Ft);Ls=a(hm,"I",{});var Xm=l(Ls);Wa=r(Xm,"Tokenizer"),Xm.forEach(s),Qa=r(hm,"  bas\xE9 sur les caract\xE8res"),hm.forEach(s),Co.forEach(s),gr=p(e),b(pt.$$.fragment,e),xr=p(e),Ie=a(e,"P",{});var Lo=l(Ie);Xa=r(Lo,"Les "),Ts=a(Lo,"EM",{});var Zm=l(Ts);Za=r(Zm,"tokenizers"),Zm.forEach(s),el=r(Lo," bas\xE9s sur les caract\xE8res divisent le texte en caract\xE8res, plut\xF4t qu\u2019en mots. Cela pr\xE9sente deux avantages principaux :"),Lo.forEach(s),zr=p(e),Ne=a(e,"UL",{});var To=l(Ne);Is=a(To,"LI",{});var ep=l(Is);tl=r(ep,"le vocabulaire est beaucoup plus petit"),ep.forEach(s),sl=p(To),ct=a(To,"LI",{});var Io=l(ct);nl=r(Io,"il y a beaucoup moins de "),Ns=a(Io,"EM",{});var tp=l(Ns);rl=r(tp,"tokens"),tp.forEach(s),ol=r(Io," hors vocabulaire (inconnus) puisque chaque mot peut \xEAtre construit \xE0 partir de caract\xE8res."),Io.forEach(s),To.forEach(s),wr=p(e),Kt=a(e,"P",{});var sp=l(Kt);al=r(sp,"Mais l\xE0 aussi, des questions se posent concernant les espaces et la ponctuation :"),sp.forEach(s),Pr=p(e),Ee=a(e,"DIV",{class:!0});var No=l(Ee);dt=a(No,"IMG",{class:!0,src:!0,alt:!0}),ll=p(No),ft=a(No,"IMG",{class:!0,src:!0,alt:!0}),No.forEach(s),jr=p(e),Yt=a(e,"P",{});var np=l(Yt);il=r(np,"Cette approche n\u2019est pas non plus parfaite. Puisque la repr\xE9sentation est maintenant bas\xE9e sur des caract\xE8res plut\xF4t que sur des mots, on pourrait dire intuitivement qu\u2019elle est moins significative : chaque caract\xE8re ne signifie pas grand-chose en soi, alors que c\u2019est le cas pour les mots. Toutefois, l\xE0 encore, cela diff\xE8re selon la langue. En chinois, par exemple, chaque caract\xE8re est porteur de plus d\u2019informations qu\u2019un caract\xE8re dans une langue latine."),np.forEach(s),yr=p(e),H=a(e,"P",{});var Z=l(H);ul=r(Z,"Un autre \xE9l\xE9ment \xE0 prendre en compte est que nous nous retrouverons avec une tr\xE8s grande quantit\xE9 de "),Ds=a(Z,"EM",{});var rp=l(Ds);ml=r(rp,"tokens"),rp.forEach(s),pl=r(Z," \xE0 traiter par notre mod\xE8le. Alors qu\u2019avec un "),Ss=a(Z,"EM",{});var op=l(Ss);cl=r(op,"tokenizer"),op.forEach(s),dl=r(Z," bas\xE9 sur les mots, pour un mot donn\xE9 on aurait qu\u2019un seul "),Hs=a(Z,"EM",{});var ap=l(Hs);fl=r(ap,"token"),ap.forEach(s),vl=r(Z,", avec un "),Us=a(Z,"EM",{});var lp=l(Us);hl=r(lp,"tokenizer"),lp.forEach(s),kl=r(Z," bas\xE9 sur les caract\xE8res, cela peut facilement se transformer en 10 "),Os=a(Z,"EM",{});var ip=l(Os);_l=r(ip,"tokens"),ip.forEach(s),bl=r(Z," voire plus."),Z.forEach(s),Mr=p(e),De=a(e,"P",{});var Do=l(De);El=r(Do,"Pour obtenir le meilleur des deux mondes, nous pouvons utiliser une troisi\xE8me technique qui combine les deux approches : la "),Bs=a(Do,"EM",{});var up=l(Bs);$l=r(up,"tok\xE9nisation en sous-mots"),up.forEach(s),ql=r(Do,"."),Do.forEach(s),Ar=p(e),$e=a(e,"H2",{class:!0});var So=l($e);Se=a(So,"A",{id:!0,class:!0,href:!0});var mp=l(Se);Vs=a(mp,"SPAN",{});var pp=l(Vs);b(vt.$$.fragment,pp),pp.forEach(s),mp.forEach(s),gl=p(So),Gs=a(So,"SPAN",{});var cp=l(Gs);xl=r(cp,"Tok\xE9nisation en sous-mots"),cp.forEach(s),So.forEach(s),Cr=p(e),b(ht.$$.fragment,e),Lr=p(e),Wt=a(e,"P",{});var dp=l(Wt);zl=r(dp,"Les algorithmes de tokenisation en sous-mots reposent sur le principe selon lequel les mots fr\xE9quemment utilis\xE9s ne doivent pas \xEAtre divis\xE9s en sous-mots plus petits, mais les mots rares doivent \xEAtre d\xE9compos\xE9s en sous-mots significatifs."),dp.forEach(s),Tr=p(e),Qt=a(e,"P",{});var fp=l(Qt);wl=r(fp,"Par exemple, le mot \xAB maisonnette \xBB peut \xEAtre consid\xE9r\xE9 comme un mot rare et peut \xEAtre d\xE9compos\xE9 en \xAB maison \xBB et \xAB ette \xBB. Ces deux mots sont susceptibles d\u2019appara\xEEtre plus fr\xE9quemment en tant que sous-mots autonomes, alors qu\u2019en m\xEAme temps le sens de \xAB maison \xBB est conserv\xE9 par le sens composite de \xAB maison \xBB et \xAB ette \xBB."),fp.forEach(s),Ir=p(e),He=a(e,"P",{});var Ho=l(He);Pl=r(Ho,"Voici un exemple montrant comment un algorithme de tokenisation en sous-mots tokeniserait la s\xE9quence \xAB "),Js=a(Ho,"EM",{});var vp=l(Js);jl=r(vp,"Let\u2019s do tokenization"),vp.forEach(s),yl=r(Ho," ! \xBB :"),Ho.forEach(s),Nr=p(e),qe=a(e,"DIV",{class:!0});var Uo=l(qe);kt=a(Uo,"IMG",{class:!0,src:!0,alt:!0}),Ml=p(Uo),_t=a(Uo,"IMG",{class:!0,src:!0,alt:!0}),Uo.forEach(s),Dr=p(e),I=a(e,"P",{});var F=l(I);Al=r(F,"Ces sous-mots finissent par fournir beaucoup de sens s\xE9mantique. Par exemple, ci-dessus, \xAB "),Rs=a(F,"EM",{});var hp=l(Rs);Cl=r(hp,"tokenization"),hp.forEach(s),Ll=r(F," \xBB a \xE9t\xE9 divis\xE9 en \xAB "),Fs=a(F,"EM",{});var kp=l(Fs);Tl=r(kp,"token"),kp.forEach(s),Il=r(F," \xBB et \xAB "),Ks=a(F,"EM",{});var _p=l(Ks);Nl=r(_p,"ization"),_p.forEach(s),Dl=r(F," \xBB : deux "),Ys=a(F,"EM",{});var bp=l(Ys);Sl=r(bp,"tokens"),bp.forEach(s),Hl=r(F," qui ont un sens s\xE9mantique tout en \xE9tant peu encombrants (seuls deux "),Ws=a(F,"EM",{});var Ep=l(Ws);Ul=r(Ep,"tokens"),Ep.forEach(s),Ol=r(F," sont n\xE9cessaires pour repr\xE9senter un long mot). Cela nous permet d\u2019avoir une couverture relativement bonne avec de petits vocabulaires et presque aucun "),Qs=a(F,"EM",{});var $p=l(Qs);Bl=r($p,"token"),$p.forEach(s),Vl=r(F," inconnu."),F.forEach(s),Sr=p(e),Xt=a(e,"P",{});var qp=l(Xt);Gl=r(qp,"Cette approche est particuli\xE8rement utile dans les langues agglutinantes comme le turc, o\xF9 l\u2019on peut former des mots complexes (presque) arbitrairement longs en encha\xEEnant des sous-mots."),qp.forEach(s),Hr=p(e),ge=a(e,"H3",{class:!0});var Oo=l(ge);Ue=a(Oo,"A",{id:!0,class:!0,href:!0});var gp=l(Ue);Xs=a(gp,"SPAN",{});var xp=l(Xs);b(bt.$$.fragment,xp),xp.forEach(s),gp.forEach(s),Jl=p(Oo),Zs=a(Oo,"SPAN",{});var zp=l(Zs);Rl=r(zp,"Et plus encore !"),zp.forEach(s),Oo.forEach(s),Ur=p(e),Zt=a(e,"P",{});var wp=l(Zt);Fl=r(wp,"Il existe de nombreuses autres techniques. Pour n\u2019en citer que quelques-unes :"),wp.forEach(s),Or=p(e),ae=a(e,"UL",{});var is=l(ae);Et=a(is,"LI",{});var Bo=l(Et);Kl=r(Bo,"le "),en=a(Bo,"EM",{});var Pp=l(en);Yl=r(Pp,"Byte-level BPE"),Pp.forEach(s),Wl=r(Bo," utilis\xE9 par exemple dans le GPT-2"),Bo.forEach(s),Ql=p(is),$t=a(is,"LI",{});var Vo=l($t);Xl=r(Vo,"le "),tn=a(Vo,"EM",{});var jp=l(tn);Zl=r(jp,"WordPiece"),jp.forEach(s),ei=r(Vo," utilis\xE9 par exemple dans BERT"),Vo.forEach(s),ti=p(is),Oe=a(is,"LI",{});var rr=l(Oe);sn=a(rr,"EM",{});var yp=l(sn);si=r(yp,"SentencePiece"),yp.forEach(s),ni=r(rr," ou "),nn=a(rr,"EM",{});var Mp=l(nn);ri=r(Mp,"Unigram"),Mp.forEach(s),oi=r(rr,", utilis\xE9s dans plusieurs mod\xE8les multilingues."),rr.forEach(s),is.forEach(s),Br=p(e),es=a(e,"P",{});var Ap=l(es);ai=r(Ap,"Vous devriez maintenant avoir une connaissance suffisante du fonctionnement des tokenizers pour commencer \xE0 utiliser l\u2019API."),Ap.forEach(s),Vr=p(e),xe=a(e,"H2",{class:!0});var Go=l(xe);Be=a(Go,"A",{id:!0,class:!0,href:!0});var Cp=l(Be);rn=a(Cp,"SPAN",{});var Lp=l(rn);b(qt.$$.fragment,Lp),Lp.forEach(s),Cp.forEach(s),li=p(Go),on=a(Go,"SPAN",{});var Tp=l(on);ii=r(Tp,"Chargement et sauvegarde"),Tp.forEach(s),Go.forEach(s),Gr=p(e),N=a(e,"P",{});var K=l(N);ui=r(K,"Le chargement et la sauvegarde des "),an=a(K,"EM",{});var Ip=l(an);mi=r(Ip,"tokenizers"),Ip.forEach(s),pi=r(K," est aussi simple que pour les mod\xE8les. En fait, c\u2019est bas\xE9 sur les deux m\xEAmes m\xE9thodes : "),ln=a(K,"CODE",{});var Np=l(ln);ci=r(Np,"from_pretrained()"),Np.forEach(s),di=r(K," et "),un=a(K,"CODE",{});var Dp=l(un);fi=r(Dp,"save_pretrained()"),Dp.forEach(s),vi=r(K,". Ces m\xE9thodes vont charger ou sauvegarder l\u2019algorithme utilis\xE9 par le "),mn=a(K,"EM",{});var Sp=l(mn);hi=r(Sp,"tokenizer"),Sp.forEach(s),ki=r(K," (un peu comme l\u2019"),pn=a(K,"EM",{});var Hp=l(pn);_i=r(Hp,"architecture"),Hp.forEach(s),bi=r(K," du mod\xE8le) ainsi que son vocabulaire (un peu comme les "),cn=a(K,"EM",{});var Up=l(cn);Ei=r(Up,"poids"),Up.forEach(s),$i=r(K," du mod\xE8le)."),K.forEach(s),Jr=p(e),X=a(e,"P",{});var st=l(X);qi=r(st,"Le chargement du "),dn=a(st,"EM",{});var Op=l(dn);gi=r(Op,"tokenizer"),Op.forEach(s),xi=r(st," de BERT entra\xEEn\xE9 avec le m\xEAme "),fn=a(st,"EM",{});var Bp=l(fn);zi=r(Bp,"checkpoint"),Bp.forEach(s),wi=r(st," que BERT se fait de la m\xEAme mani\xE8re que le chargement du mod\xE8le, sauf que nous utilisons la classe "),vn=a(st,"CODE",{});var Vp=l(vn);Pi=r(Vp,"BertTokenizer"),Vp.forEach(s),ji=r(st," :"),st.forEach(s),Rr=p(e),b(gt.$$.fragment,e),Fr=p(e),ye.l(e),ts=p(e),b(xt.$$.fragment,e),Kr=p(e),Ve=a(e,"P",{});var Jo=l(Ve);yi=r(Jo,"Nous pouvons \xE0 pr\xE9sent utiliser le "),hn=a(Jo,"EM",{});var Gp=l(hn);Mi=r(Gp,"tokenizer"),Gp.forEach(s),Ai=r(Jo," comme indiqu\xE9 dans la section pr\xE9c\xE9dente :"),Jo.forEach(s),Yr=p(e),b(zt.$$.fragment,e),Wr=p(e),b(wt.$$.fragment,e),Qr=p(e),ss=a(e,"P",{});var Jp=l(ss);Ci=r(Jp,"La sauvegarde d\u2019un tokenizer est identique \xE0 celle d\u2019un mod\xE8le :"),Jp.forEach(s),Xr=p(e),b(Pt.$$.fragment,e),Zr=p(e),U=a(e,"P",{});var ee=l(U);Li=r(ee,"Nous parlerons plus en d\xE9tail des "),kn=a(ee,"CODE",{});var Rp=l(kn);Ti=r(Rp,"token_type_ids"),Rp.forEach(s),Ii=r(ee," au "),ns=a(ee,"A",{href:!0});var Fp=l(ns);Ni=r(Fp,"chapitre 3"),Fp.forEach(s),Di=r(ee," et nous expliquerons la cl\xE9 "),_n=a(ee,"CODE",{});var Kp=l(_n);Si=r(Kp,"attention_mask"),Kp.forEach(s),Hi=r(ee," un peu plus tard. Tout d\u2019abord, voyons comment les "),bn=a(ee,"CODE",{});var Yp=l(bn);Ui=r(Yp,"input_ids"),Yp.forEach(s),Oi=r(ee," sont g\xE9n\xE9r\xE9s. Pour ce faire, nous devons examiner les m\xE9thodes interm\xE9diaires du "),En=a(ee,"EM",{});var Wp=l(En);Bi=r(Wp,"tokenizer"),Wp.forEach(s),Vi=r(ee,"."),ee.forEach(s),eo=p(e),ze=a(e,"H2",{class:!0});var Ro=l(ze);Ge=a(Ro,"A",{id:!0,class:!0,href:!0});var Qp=l(Ge);$n=a(Qp,"SPAN",{});var Xp=l($n);b(jt.$$.fragment,Xp),Xp.forEach(s),Qp.forEach(s),Gi=p(Ro),qn=a(Ro,"SPAN",{});var Zp=l(qn);Ji=r(Zp,"Encodage"),Zp.forEach(s),Ro.forEach(s),to=p(e),b(yt.$$.fragment,e),so=p(e),Je=a(e,"P",{});var Fo=l(Je);Ri=r(Fo,"La traduction d\u2019un texte en chiffres est connue sous le nom d\u2019"),gn=a(Fo,"EM",{});var ec=l(gn);Fi=r(ec,"encodage"),ec.forEach(s),Ki=r(Fo,". L\u2019encodage se fait en deux \xE9tapes : la tokenisation, suivie de la conversion en identifiants d\u2019entr\xE9e."),Fo.forEach(s),no=p(e),le=a(e,"P",{});var us=l(le);Yi=r(us,"Comme nous l\u2019avons vu, la premi\xE8re \xE9tape consiste \xE0 diviser le texte en mots (ou parties de mots, symboles de ponctuation, etc.), g\xE9n\xE9ralement appel\xE9s "),xn=a(us,"EM",{});var tc=l(xn);Wi=r(tc,"tokens"),tc.forEach(s),Qi=r(us,". De nombreuses r\xE8gles peuvent r\xE9gir ce processus. C\u2019est pourquoi nous devons instancier le "),zn=a(us,"EM",{});var sc=l(zn);Xi=r(sc,"tokenizer"),sc.forEach(s),Zi=r(us," en utilisant le nom du mod\xE8le afin de nous assurer que nous utilisons les m\xEAmes r\xE8gles que celles utilis\xE9es lors du pr\xE9-entra\xEEnement du mod\xE8le."),us.forEach(s),ro=p(e),J=a(e,"P",{});var pe=l(J);eu=r(pe,"La deuxi\xE8me \xE9tape consiste \xE0 convertir ces "),wn=a(pe,"EM",{});var nc=l(wn);tu=r(nc,"tokens"),nc.forEach(s),su=r(pe," en nombres afin de construire un tenseur \xE0 partir de ceux-ci ainsi que de les transmettre au mod\xE8le. Pour ce faire, le "),Pn=a(pe,"EM",{});var rc=l(Pn);nu=r(rc,"tokenizer"),rc.forEach(s),ru=r(pe," poss\xE8de un "),jn=a(pe,"EM",{});var oc=l(jn);ou=r(oc,"vocabulaire"),oc.forEach(s),au=r(pe,", qui est la partie que nous t\xE9l\xE9chargeons lorsque nous l\u2019instancions avec la m\xE9thode "),yn=a(pe,"CODE",{});var ac=l(yn);lu=r(ac,"from_pretrained()"),ac.forEach(s),iu=r(pe,". Encore une fois, nous devons utiliser le m\xEAme vocabulaire que celui utilis\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le."),pe.forEach(s),oo=p(e),Re=a(e,"P",{});var Ko=l(Re);uu=r(Ko,"Pour mieux comprendre les deux \xE9tapes, nous allons les explorer s\xE9par\xE9ment. A noter que nous utilisons des m\xE9thodes effectuant s\xE9par\xE9ment des parties du pipeline de tokenisation afin de montrer les r\xE9sultats interm\xE9diaires de ces \xE9tapes. N\xE9anmoins, en pratique, il faut appeler le "),Mn=a(Ko,"EM",{});var lc=l(Mn);mu=r(lc,"tokenizer"),lc.forEach(s),pu=r(Ko," directement sur vos entr\xE9es (comme indiqu\xE9 dans la section 2)."),Ko.forEach(s),ao=p(e),we=a(e,"H3",{class:!0});var Yo=l(we);Fe=a(Yo,"A",{id:!0,class:!0,href:!0});var ic=l(Fe);An=a(ic,"SPAN",{});var uc=l(An);b(Mt.$$.fragment,uc),uc.forEach(s),ic.forEach(s),cu=p(Yo),Cn=a(Yo,"SPAN",{});var mc=l(Cn);du=r(mc,"Tokenisation"),mc.forEach(s),Yo.forEach(s),lo=p(e),ie=a(e,"P",{});var ms=l(ie);fu=r(ms,"Le processus de tokenisation est effectu\xE9 par la m\xE9thode "),Ln=a(ms,"CODE",{});var pc=l(Ln);vu=r(pc,"tokenize()"),pc.forEach(s),hu=r(ms," du "),Tn=a(ms,"EM",{});var cc=l(Tn);ku=r(cc,"tokenizer"),cc.forEach(s),_u=r(ms," :"),ms.forEach(s),io=p(e),b(At.$$.fragment,e),uo=p(e),Ke=a(e,"P",{});var Wo=l(Ke);bu=r(Wo,"La sortie de cette m\xE9thode est une liste de cha\xEEnes de caract\xE8res ou de "),In=a(Wo,"EM",{});var dc=l(In);Eu=r(dc,"tokens"),dc.forEach(s),$u=r(Wo," :"),Wo.forEach(s),mo=p(e),b(Ct.$$.fragment,e),po=p(e),A=a(e,"P",{});var O=l(A);qu=r(O,"Ce "),Nn=a(O,"EM",{});var fc=l(Nn);gu=r(fc,"tokenizer"),fc.forEach(s),xu=r(O," est un "),Dn=a(O,"EM",{});var vc=l(Dn);zu=r(vc,"tokenizer"),vc.forEach(s),wu=r(O," de sous-mots : il d\xE9coupe les mots jusqu\u2019\xE0 obtenir des "),Sn=a(O,"EM",{});var hc=l(Sn);Pu=r(hc,"tokens"),hc.forEach(s),ju=r(O," qui peuvent \xEAtre repr\xE9sent\xE9s par son vocabulaire. C\u2019est le cas ici avec "),Hn=a(O,"CODE",{});var kc=l(Hn);yu=r(kc,"transformer"),kc.forEach(s),Mu=r(O," qui est divis\xE9 en deux "),Un=a(O,"EM",{});var _c=l(Un);Au=r(_c,"tokens"),_c.forEach(s),Cu=r(O," : "),On=a(O,"CODE",{});var bc=l(On);Lu=r(bc,"transform"),bc.forEach(s),Tu=r(O," et "),Bn=a(O,"CODE",{});var Ec=l(Bn);Iu=r(Ec,"##er"),Ec.forEach(s),Nu=r(O,"."),O.forEach(s),co=p(e),Pe=a(e,"H3",{class:!0});var Qo=l(Pe);Ye=a(Qo,"A",{id:!0,class:!0,href:!0});var $c=l(Ye);Vn=a($c,"SPAN",{});var qc=l(Vn);b(Lt.$$.fragment,qc),qc.forEach(s),$c.forEach(s),Du=p(Qo),Tt=a(Qo,"SPAN",{});var Xo=l(Tt);Su=r(Xo,"De "),Gn=a(Xo,"I",{});var gc=l(Gn);Hu=r(gc,"tokens"),gc.forEach(s),Uu=r(Xo," aux identifiants d'entr\xE9e"),Xo.forEach(s),Qo.forEach(s),fo=p(e),ue=a(e,"P",{});var ps=l(ue);Ou=r(ps,"La conversion en identifiants d\u2019entr\xE9e est g\xE9r\xE9e par la m\xE9thode "),Jn=a(ps,"CODE",{});var xc=l(Jn);Bu=r(xc,"convert_tokens_to_ids()"),xc.forEach(s),Vu=r(ps," du "),Rn=a(ps,"EM",{});var zc=l(Rn);Gu=r(zc,"tokenizer"),zc.forEach(s),Ju=r(ps," :"),ps.forEach(s),vo=p(e),b(It.$$.fragment,e),ho=p(e),b(Nt.$$.fragment,e),ko=p(e),We=a(e,"P",{});var Zo=l(We);Ru=r(Zo,"Une fois converties en tenseur dans le "),Fn=a(Zo,"EM",{});var wc=l(Fn);Fu=r(wc,"framework"),wc.forEach(s),Ku=r(Zo," appropri\xE9, ces sorties peuvent ensuite \xEAtre utilis\xE9es comme entr\xE9es d\u2019un mod\xE8le, comme nous l\u2019avons vu pr\xE9c\xE9demment dans ce chapitre."),Zo.forEach(s),_o=p(e),b(Qe.$$.fragment,e),bo=p(e),je=a(e,"H2",{class:!0});var ea=l(je);Xe=a(ea,"A",{id:!0,class:!0,href:!0});var Pc=l(Xe);Kn=a(Pc,"SPAN",{});var jc=l(Kn);b(Dt.$$.fragment,jc),jc.forEach(s),Pc.forEach(s),Yu=p(ea),Yn=a(ea,"SPAN",{});var yc=l(Yn);Wu=r(yc,"D\xE9codage"),yc.forEach(s),ea.forEach(s),Eo=p(e),me=a(e,"P",{});var cs=l(me);Qu=r(cs,"Le "),Wn=a(cs,"EM",{});var Mc=l(Wn);Xu=r(Mc,"d\xE9codage"),Mc.forEach(s),Zu=r(cs," va dans l\u2019autre sens : \xE0 partir d\u2019indices du vocabulaire nous voulons obtenir une cha\xEEne de caract\xE8res. Cela peut \xEAtre fait avec la m\xE9thode "),Qn=a(cs,"CODE",{});var Ac=l(Qn);em=r(Ac,"decode()"),Ac.forEach(s),tm=r(cs," comme suit :"),cs.forEach(s),$o=p(e),b(St.$$.fragment,e),qo=p(e),b(Ht.$$.fragment,e),go=p(e),R=a(e,"P",{});var ce=l(R);sm=r(ce,"Notez que la m\xE9thode "),Xn=a(ce,"CODE",{});var Cc=l(Xn);nm=r(Cc,"decode"),Cc.forEach(s),rm=r(ce," non seulement reconvertit les indices en "),Zn=a(ce,"EM",{});var Lc=l(Zn);om=r(Lc,"tokens"),Lc.forEach(s),am=r(ce," mais regroupe \xE9galement les "),er=a(ce,"EM",{});var Tc=l(er);lm=r(Tc,"tokens"),Tc.forEach(s),im=r(ce," faisant partie des m\xEAmes mots. Le but \xE9tant de produire une phrase lisible. Ce comportement sera extr\xEAmement utile lorsque dans la suite du cours nous utiliserons des mod\xE8les pouvant produire du nouveau texte (soit du texte g\xE9n\xE9r\xE9 \xE0 partir d\u2019un "),tr=a(ce,"EM",{});var Ic=l(tr);um=r(Ic,"prompt"),Ic.forEach(s),mm=r(ce,", soit pour des probl\xE8mes de s\xE9quence \xE0 s\xE9quence comme la traduction ou le r\xE9sum\xE9 de texte)."),ce.forEach(s),xo=p(e),Ze=a(e,"P",{});var ta=l(Ze);pm=r(ta,"Vous devriez maintenant comprendre les op\xE9rations atomiques qu\u2019un "),sr=a(ta,"EM",{});var Nc=l(sr);cm=r(Nc,"tokenizer"),Nc.forEach(s),dm=r(ta," peut g\xE9rer : tokenisation, conversion en identifiants, et reconversion des identifiants en cha\xEEne de caract\xE8res. Cependant, nous n\u2019avons fait qu\u2019effleurer la partie \xE9merg\xE9e de l\u2019iceberg. Dans la section suivante, nous allons pousser notre approche jusqu\u2019\xE0 ses limites et voir comment les surmonter."),ta.forEach(s),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(Zc)),d(g,"id","les-itokenizersi"),d(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(g,"href","#les-itokenizersi"),d(y,"class","relative group"),d(Ae,"id","itokenizeri-bas-sur-les-mots"),d(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ae,"href","#itokenizeri-bas-sur-les-mots"),d(ke,"class","relative group"),d(at,"class","block dark:hidden"),ds(at.src,km="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg")||d(at,"src",km),d(at,"alt","An example of word-based tokenization."),d(lt,"class","hidden dark:block"),ds(lt.src,_m="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg")||d(lt,"src",_m),d(lt,"alt","An example of word-based tokenization."),d(_e,"class","flex justify-center"),d(Te,"id","itokenizeri-bas-sur-les-caractres"),d(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Te,"href","#itokenizeri-bas-sur-les-caractres"),d(be,"class","relative group"),d(dt,"class","block dark:hidden"),ds(dt.src,bm="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg")||d(dt,"src",bm),d(dt,"alt","An example of character-based tokenization."),d(ft,"class","hidden dark:block"),ds(ft.src,Em="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg")||d(ft,"src",Em),d(ft,"alt","An example of character-based tokenization."),d(Ee,"class","flex justify-center"),d(Se,"id","toknisation-en-sousmots"),d(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Se,"href","#toknisation-en-sousmots"),d($e,"class","relative group"),d(kt,"class","block dark:hidden"),ds(kt.src,$m="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg")||d(kt,"src",$m),d(kt,"alt","A subword tokenization algorithm."),d(_t,"class","hidden dark:block"),ds(_t.src,qm="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg")||d(_t,"src",qm),d(_t,"alt","A subword tokenization algorithm."),d(qe,"class","flex justify-center"),d(Ue,"id","et-plus-encore"),d(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ue,"href","#et-plus-encore"),d(ge,"class","relative group"),d(Be,"id","chargement-et-sauvegarde"),d(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Be,"href","#chargement-et-sauvegarde"),d(xe,"class","relative group"),d(ns,"href","/course/fr/chapter3"),d(Ge,"id","encodage"),d(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ge,"href","#encodage"),d(ze,"class","relative group"),d(Fe,"id","tokenisation"),d(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Fe,"href","#tokenisation"),d(we,"class","relative group"),d(Ye,"id","de-itokensi-aux-identifiants-dentre"),d(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ye,"href","#de-itokensi-aux-identifiants-dentre"),d(Pe,"class","relative group"),d(Xe,"id","dcodage"),d(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xe,"href","#dcodage"),d(je,"class","relative group")},m(e,i){t(document.head,c),u(e,q,i),E(f,e,i),u(e,w,i),u(e,y,i),t(y,g),t(g,D),E(C,D,null),t(y,M),t(y,T),t(T,B),t(T,z),t(z,P),u(e,V,i),Ut[x].m(e,i),u(e,G,i),E(L,e,i),u(e,k,i),u(e,S,i),t(S,fe),t(S,te),t(te,ve),t(S,he),t(S,fs),t(fs,sa),t(S,na),u(e,ar,i),u(e,Vt,i),t(Vt,ra),u(e,lr,i),E(nt,e,i),u(e,ir,i),u(e,Me,i),t(Me,oa),t(Me,vs),t(vs,aa),t(Me,la),u(e,ur,i),u(e,Gt,i),t(Gt,ia),u(e,mr,i),u(e,ke,i),t(ke,Ae),t(Ae,hs),E(rt,hs,null),t(ke,ua),t(ke,Jt),t(Jt,ks),t(ks,ma),t(Jt,pa),u(e,pr,i),E(ot,e,i),u(e,cr,i),u(e,Ce,i),t(Ce,ca),t(Ce,_s),t(_s,da),t(Ce,fa),u(e,dr,i),u(e,_e,i),t(_e,at),t(_e,va),t(_e,lt),u(e,fr,i),u(e,Le,i),t(Le,ha),t(Le,bs),t(bs,ka),t(Le,_a),u(e,vr,i),E(it,e,i),u(e,hr,i),E(ut,e,i),u(e,kr,i),u(e,Q,i),t(Q,ba),t(Q,Es),t(Es,Ea),t(Q,$a),t(Q,$s),t($s,qa),t(Q,ga),t(Q,qs),t(qs,xa),t(Q,za),u(e,_r,i),u(e,Rt,i),t(Rt,wa),u(e,br,i),u(e,re,i),t(re,Pa),t(re,gs),t(gs,ja),t(re,ya),t(re,xs),t(xs,Ma),t(re,Aa),u(e,Er,i),u(e,se,i),t(se,Ca),t(se,zs),t(zs,La),t(se,Ta),t(se,ws),t(ws,Ia),t(se,Na),t(se,ne),t(ne,Da),t(ne,Ps),t(Ps,Sa),t(ne,Ha),t(ne,js),t(js,Ua),t(ne,Oa),t(ne,ys),t(ys,Ba),t(ne,Va),u(e,$r,i),u(e,oe,i),t(oe,Ga),t(oe,Ms),t(Ms,Ja),t(oe,Ra),t(oe,As),t(As,Fa),t(oe,Ka),u(e,qr,i),u(e,be,i),t(be,Te),t(Te,Cs),E(mt,Cs,null),t(be,Ya),t(be,Ft),t(Ft,Ls),t(Ls,Wa),t(Ft,Qa),u(e,gr,i),E(pt,e,i),u(e,xr,i),u(e,Ie,i),t(Ie,Xa),t(Ie,Ts),t(Ts,Za),t(Ie,el),u(e,zr,i),u(e,Ne,i),t(Ne,Is),t(Is,tl),t(Ne,sl),t(Ne,ct),t(ct,nl),t(ct,Ns),t(Ns,rl),t(ct,ol),u(e,wr,i),u(e,Kt,i),t(Kt,al),u(e,Pr,i),u(e,Ee,i),t(Ee,dt),t(Ee,ll),t(Ee,ft),u(e,jr,i),u(e,Yt,i),t(Yt,il),u(e,yr,i),u(e,H,i),t(H,ul),t(H,Ds),t(Ds,ml),t(H,pl),t(H,Ss),t(Ss,cl),t(H,dl),t(H,Hs),t(Hs,fl),t(H,vl),t(H,Us),t(Us,hl),t(H,kl),t(H,Os),t(Os,_l),t(H,bl),u(e,Mr,i),u(e,De,i),t(De,El),t(De,Bs),t(Bs,$l),t(De,ql),u(e,Ar,i),u(e,$e,i),t($e,Se),t(Se,Vs),E(vt,Vs,null),t($e,gl),t($e,Gs),t(Gs,xl),u(e,Cr,i),E(ht,e,i),u(e,Lr,i),u(e,Wt,i),t(Wt,zl),u(e,Tr,i),u(e,Qt,i),t(Qt,wl),u(e,Ir,i),u(e,He,i),t(He,Pl),t(He,Js),t(Js,jl),t(He,yl),u(e,Nr,i),u(e,qe,i),t(qe,kt),t(qe,Ml),t(qe,_t),u(e,Dr,i),u(e,I,i),t(I,Al),t(I,Rs),t(Rs,Cl),t(I,Ll),t(I,Fs),t(Fs,Tl),t(I,Il),t(I,Ks),t(Ks,Nl),t(I,Dl),t(I,Ys),t(Ys,Sl),t(I,Hl),t(I,Ws),t(Ws,Ul),t(I,Ol),t(I,Qs),t(Qs,Bl),t(I,Vl),u(e,Sr,i),u(e,Xt,i),t(Xt,Gl),u(e,Hr,i),u(e,ge,i),t(ge,Ue),t(Ue,Xs),E(bt,Xs,null),t(ge,Jl),t(ge,Zs),t(Zs,Rl),u(e,Ur,i),u(e,Zt,i),t(Zt,Fl),u(e,Or,i),u(e,ae,i),t(ae,Et),t(Et,Kl),t(Et,en),t(en,Yl),t(Et,Wl),t(ae,Ql),t(ae,$t),t($t,Xl),t($t,tn),t(tn,Zl),t($t,ei),t(ae,ti),t(ae,Oe),t(Oe,sn),t(sn,si),t(Oe,ni),t(Oe,nn),t(nn,ri),t(Oe,oi),u(e,Br,i),u(e,es,i),t(es,ai),u(e,Vr,i),u(e,xe,i),t(xe,Be),t(Be,rn),E(qt,rn,null),t(xe,li),t(xe,on),t(on,ii),u(e,Gr,i),u(e,N,i),t(N,ui),t(N,an),t(an,mi),t(N,pi),t(N,ln),t(ln,ci),t(N,di),t(N,un),t(un,fi),t(N,vi),t(N,mn),t(mn,hi),t(N,ki),t(N,pn),t(pn,_i),t(N,bi),t(N,cn),t(cn,Ei),t(N,$i),u(e,Jr,i),u(e,X,i),t(X,qi),t(X,dn),t(dn,gi),t(X,xi),t(X,fn),t(fn,zi),t(X,wi),t(X,vn),t(vn,Pi),t(X,ji),u(e,Rr,i),E(gt,e,i),u(e,Fr,i),ye.m(e,i),u(e,ts,i),E(xt,e,i),u(e,Kr,i),u(e,Ve,i),t(Ve,yi),t(Ve,hn),t(hn,Mi),t(Ve,Ai),u(e,Yr,i),E(zt,e,i),u(e,Wr,i),E(wt,e,i),u(e,Qr,i),u(e,ss,i),t(ss,Ci),u(e,Xr,i),E(Pt,e,i),u(e,Zr,i),u(e,U,i),t(U,Li),t(U,kn),t(kn,Ti),t(U,Ii),t(U,ns),t(ns,Ni),t(U,Di),t(U,_n),t(_n,Si),t(U,Hi),t(U,bn),t(bn,Ui),t(U,Oi),t(U,En),t(En,Bi),t(U,Vi),u(e,eo,i),u(e,ze,i),t(ze,Ge),t(Ge,$n),E(jt,$n,null),t(ze,Gi),t(ze,qn),t(qn,Ji),u(e,to,i),E(yt,e,i),u(e,so,i),u(e,Je,i),t(Je,Ri),t(Je,gn),t(gn,Fi),t(Je,Ki),u(e,no,i),u(e,le,i),t(le,Yi),t(le,xn),t(xn,Wi),t(le,Qi),t(le,zn),t(zn,Xi),t(le,Zi),u(e,ro,i),u(e,J,i),t(J,eu),t(J,wn),t(wn,tu),t(J,su),t(J,Pn),t(Pn,nu),t(J,ru),t(J,jn),t(jn,ou),t(J,au),t(J,yn),t(yn,lu),t(J,iu),u(e,oo,i),u(e,Re,i),t(Re,uu),t(Re,Mn),t(Mn,mu),t(Re,pu),u(e,ao,i),u(e,we,i),t(we,Fe),t(Fe,An),E(Mt,An,null),t(we,cu),t(we,Cn),t(Cn,du),u(e,lo,i),u(e,ie,i),t(ie,fu),t(ie,Ln),t(Ln,vu),t(ie,hu),t(ie,Tn),t(Tn,ku),t(ie,_u),u(e,io,i),E(At,e,i),u(e,uo,i),u(e,Ke,i),t(Ke,bu),t(Ke,In),t(In,Eu),t(Ke,$u),u(e,mo,i),E(Ct,e,i),u(e,po,i),u(e,A,i),t(A,qu),t(A,Nn),t(Nn,gu),t(A,xu),t(A,Dn),t(Dn,zu),t(A,wu),t(A,Sn),t(Sn,Pu),t(A,ju),t(A,Hn),t(Hn,yu),t(A,Mu),t(A,Un),t(Un,Au),t(A,Cu),t(A,On),t(On,Lu),t(A,Tu),t(A,Bn),t(Bn,Iu),t(A,Nu),u(e,co,i),u(e,Pe,i),t(Pe,Ye),t(Ye,Vn),E(Lt,Vn,null),t(Pe,Du),t(Pe,Tt),t(Tt,Su),t(Tt,Gn),t(Gn,Hu),t(Tt,Uu),u(e,fo,i),u(e,ue,i),t(ue,Ou),t(ue,Jn),t(Jn,Bu),t(ue,Vu),t(ue,Rn),t(Rn,Gu),t(ue,Ju),u(e,vo,i),E(It,e,i),u(e,ho,i),E(Nt,e,i),u(e,ko,i),u(e,We,i),t(We,Ru),t(We,Fn),t(Fn,Fu),t(We,Ku),u(e,_o,i),E(Qe,e,i),u(e,bo,i),u(e,je,i),t(je,Xe),t(Xe,Kn),E(Dt,Kn,null),t(je,Yu),t(je,Yn),t(Yn,Wu),u(e,Eo,i),u(e,me,i),t(me,Qu),t(me,Wn),t(Wn,Xu),t(me,Zu),t(me,Qn),t(Qn,em),t(me,tm),u(e,$o,i),E(St,e,i),u(e,qo,i),E(Ht,e,i),u(e,go,i),u(e,R,i),t(R,sm),t(R,Xn),t(Xn,nm),t(R,rm),t(R,Zn),t(Zn,om),t(R,am),t(R,er),t(er,lm),t(R,im),t(R,tr),t(tr,um),t(R,mm),u(e,xo,i),u(e,Ze,i),t(Ze,pm),t(Ze,sr),t(sr,cm),t(Ze,dm),zo=!0},p(e,[i]){const Ot={};i&1&&(Ot.fw=e[0]),f.$set(Ot);let rs=x;x=xm(e),x!==rs&&(Gc(),v(Ut[rs],1,1,()=>{Ut[rs]=null}),Bc(),j=Ut[x],j||(j=Ut[x]=gm[x](e),j.c()),h(j,1),j.m(G.parentNode,G)),wo!==(wo=zm(e))&&(ye.d(1),ye=wo(e),ye&&(ye.c(),ye.m(ts.parentNode,ts)));const nr={};i&2&&(nr.$$scope={dirty:i,ctx:e}),Qe.$set(nr)},i(e){zo||(h(f.$$.fragment,e),h(C.$$.fragment,e),h(j),h(L.$$.fragment,e),h(nt.$$.fragment,e),h(rt.$$.fragment,e),h(ot.$$.fragment,e),h(it.$$.fragment,e),h(ut.$$.fragment,e),h(mt.$$.fragment,e),h(pt.$$.fragment,e),h(vt.$$.fragment,e),h(ht.$$.fragment,e),h(bt.$$.fragment,e),h(qt.$$.fragment,e),h(gt.$$.fragment,e),h(xt.$$.fragment,e),h(zt.$$.fragment,e),h(wt.$$.fragment,e),h(Pt.$$.fragment,e),h(jt.$$.fragment,e),h(yt.$$.fragment,e),h(Mt.$$.fragment,e),h(At.$$.fragment,e),h(Ct.$$.fragment,e),h(Lt.$$.fragment,e),h(It.$$.fragment,e),h(Nt.$$.fragment,e),h(Qe.$$.fragment,e),h(Dt.$$.fragment,e),h(St.$$.fragment,e),h(Ht.$$.fragment,e),zo=!0)},o(e){v(f.$$.fragment,e),v(C.$$.fragment,e),v(j),v(L.$$.fragment,e),v(nt.$$.fragment,e),v(rt.$$.fragment,e),v(ot.$$.fragment,e),v(it.$$.fragment,e),v(ut.$$.fragment,e),v(mt.$$.fragment,e),v(pt.$$.fragment,e),v(vt.$$.fragment,e),v(ht.$$.fragment,e),v(bt.$$.fragment,e),v(qt.$$.fragment,e),v(gt.$$.fragment,e),v(xt.$$.fragment,e),v(zt.$$.fragment,e),v(wt.$$.fragment,e),v(Pt.$$.fragment,e),v(jt.$$.fragment,e),v(yt.$$.fragment,e),v(Mt.$$.fragment,e),v(At.$$.fragment,e),v(Ct.$$.fragment,e),v(Lt.$$.fragment,e),v(It.$$.fragment,e),v(Nt.$$.fragment,e),v(Qe.$$.fragment,e),v(Dt.$$.fragment,e),v(St.$$.fragment,e),v(Ht.$$.fragment,e),zo=!1},d(e){s(c),e&&s(q),$(f,e),e&&s(w),e&&s(y),$(C),e&&s(V),Ut[x].d(e),e&&s(G),$(L,e),e&&s(k),e&&s(S),e&&s(ar),e&&s(Vt),e&&s(lr),$(nt,e),e&&s(ir),e&&s(Me),e&&s(ur),e&&s(Gt),e&&s(mr),e&&s(ke),$(rt),e&&s(pr),$(ot,e),e&&s(cr),e&&s(Ce),e&&s(dr),e&&s(_e),e&&s(fr),e&&s(Le),e&&s(vr),$(it,e),e&&s(hr),$(ut,e),e&&s(kr),e&&s(Q),e&&s(_r),e&&s(Rt),e&&s(br),e&&s(re),e&&s(Er),e&&s(se),e&&s($r),e&&s(oe),e&&s(qr),e&&s(be),$(mt),e&&s(gr),$(pt,e),e&&s(xr),e&&s(Ie),e&&s(zr),e&&s(Ne),e&&s(wr),e&&s(Kt),e&&s(Pr),e&&s(Ee),e&&s(jr),e&&s(Yt),e&&s(yr),e&&s(H),e&&s(Mr),e&&s(De),e&&s(Ar),e&&s($e),$(vt),e&&s(Cr),$(ht,e),e&&s(Lr),e&&s(Wt),e&&s(Tr),e&&s(Qt),e&&s(Ir),e&&s(He),e&&s(Nr),e&&s(qe),e&&s(Dr),e&&s(I),e&&s(Sr),e&&s(Xt),e&&s(Hr),e&&s(ge),$(bt),e&&s(Ur),e&&s(Zt),e&&s(Or),e&&s(ae),e&&s(Br),e&&s(es),e&&s(Vr),e&&s(xe),$(qt),e&&s(Gr),e&&s(N),e&&s(Jr),e&&s(X),e&&s(Rr),$(gt,e),e&&s(Fr),ye.d(e),e&&s(ts),$(xt,e),e&&s(Kr),e&&s(Ve),e&&s(Yr),$(zt,e),e&&s(Wr),$(wt,e),e&&s(Qr),e&&s(ss),e&&s(Xr),$(Pt,e),e&&s(Zr),e&&s(U),e&&s(eo),e&&s(ze),$(jt),e&&s(to),$(yt,e),e&&s(so),e&&s(Je),e&&s(no),e&&s(le),e&&s(ro),e&&s(J),e&&s(oo),e&&s(Re),e&&s(ao),e&&s(we),$(Mt),e&&s(lo),e&&s(ie),e&&s(io),$(At,e),e&&s(uo),e&&s(Ke),e&&s(mo),$(Ct,e),e&&s(po),e&&s(A),e&&s(co),e&&s(Pe),$(Lt),e&&s(fo),e&&s(ue),e&&s(vo),$(It,e),e&&s(ho),$(Nt,e),e&&s(ko),e&&s(We),e&&s(_o),$(Qe,e),e&&s(bo),e&&s(je),$(Dt),e&&s(Eo),e&&s(me),e&&s($o),$(St,e),e&&s(qo),$(Ht,e),e&&s(go),e&&s(R),e&&s(xo),e&&s(Ze)}}}const Zc={local:"les-itokenizersi",sections:[{local:"itokenizeri-bas-sur-les-mots",title:"<i>Tokenizer</i> bas\xE9 sur les mots"},{local:"itokenizeri-bas-sur-les-caractres",title:"<i>Tokenizer</i>  bas\xE9 sur les caract\xE8res"},{local:"toknisation-en-sousmots",sections:[{local:"et-plus-encore",title:"Et plus encore !"}],title:"Tok\xE9nisation en sous-mots"},{local:"chargement-et-sauvegarde",title:"Chargement et sauvegarde"},{local:"encodage",sections:[{local:"tokenisation",title:"Tokenisation"},{local:"de-itokensi-aux-identifiants-dentre",title:"De <i>tokens</i> aux identifiants d'entr\xE9e"}],title:"Encodage"},{local:"dcodage",title:"D\xE9codage"}],title:"Les <i>tokenizers</i>"};function ed(W,c,q){let f="pt";return Vc(()=>{const w=new URLSearchParams(window.location.search);q(0,f=w.get("fw")||"pt")}),[f]}class id extends Sc{constructor(c){super();Hc(this,c,ed,Xc,Uc,{})}}export{id as default,Zc as metadata};
