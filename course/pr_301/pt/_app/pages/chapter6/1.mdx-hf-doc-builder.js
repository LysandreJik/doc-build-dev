import{S as Ne,i as je,s as Le,e as t,k as p,w as Pe,t as i,M as Te,c as r,d as a,m as c,a as n,x as we,h as u,b as d,G as o,g as m,y as ye,L as Se,q as Ae,o as Ce,B as Ie,v as Oe}from"../../chunks/vendor-hf-doc-builder.js";import{I as Me}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Be}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function De(ce){let h,T,v,z,A,g,J,C,R,S,q,O,_,K,x,V,W,M,f,X,E,Y,Z,$,ee,oe,B,P,ae,D,l,I,te,re,N,se,ie,j,ne,ue,L,me,Q,k,le,w,de,pe,U;return g=new Me({}),q=new Be({props:{chapter:6,classNames:"absolute z-10 right-0 top-0"}}),{c(){h=t("meta"),T=p(),v=t("h1"),z=t("a"),A=t("span"),Pe(g.$$.fragment),J=p(),C=t("span"),R=i("Introdu\xE7\xE3o"),S=p(),Pe(q.$$.fragment),O=p(),_=t("p"),K=i("No "),x=t("a"),V=i("Cap\xEDtulo 3"),W=i(", n\xF3s estudamos como realizar o ajuste fino em um modelo para uma dada tarefa. Quando n\xF3s fazemos isso, usamos o mesmo tokenizador utilizado pelo modelo pr\xE9-treinado \u2014 mas o que podemos fazer quando queremos treinar um modelo do in\xEDcio? Nestes casos, utilizar um tokenizador que foi pr\xE9-treinado em um corpus de outro dom\xEDnio ou linguagem \xE9 tipicamente sub\xF3timo. Por exemplo, um tokenizador que \xE9 treinado em um corpus de lingua inglesa ter\xE1 um desempenho ruim em um corpus de textos em japon\xEAs, visto que o uso de espa\xE7os e pontua\xE7\xF5es \xE9 muito diferente nestes dois idiomas."),M=p(),f=t("p"),X=i("Neste cap\xEDtulo, voc\xEA aprender\xE1 como treinar um novo tokenizador em um corpus de textos, para ent\xE3o ser usado no treinamento de um modelo de linguagem. Isto tudo ser\xE1 feito com ajuda da biblioteca "),E=t("a"),Y=i("\u{1F917} Tokenizers"),Z=i(", que prov\xEA o tokenizador r\xE1pido na biblioteca "),$=t("a"),ee=i("\u{1F917} Transformers"),oe=i(". Daremos uma olhada a fundo sobre as funcionalidades oferecidas pela biblioteca, e explorar como os tokenizadores r\xE1pidos diferem das vers\xF5es \u201Clentas\u201D."),B=p(),P=t("p"),ae=i("Os t\xF3picos que iremos cobrir incluem:"),D=p(),l=t("ul"),I=t("li"),te=i("Como treinar um novo tokenizador semelhante ao usado por um determinado checkpoint em um novo corpus de textos"),re=p(),N=t("li"),se=i("Os recursos especiais dos tokenizadores r\xE1pidos"),ie=p(),j=t("li"),ne=i("As diferen\xE7as entre os tr\xEAs principais algoritmos de tokeniza\xE7\xE3o de subpalavras usados \u200B\u200Bno processamento de linguagem natural hoje"),ue=p(),L=t("li"),me=i("Como construir um tokenizador do zero com a biblioteca \u{1F917} Tokenizers e trein\xE1-lo em alguns dados"),Q=p(),k=t("p"),le=i("As t\xE9cnicas introduzidas neste cap\xEDtulo ir\xE3o te preparar para a se\xE7\xE3o no "),w=t("a"),de=i("Cap\xEDtulo 7"),pe=i(" onde iremos analisar a cria\xE7\xE3o de um modelo de linguagem para a linguagem Python. Primeiramente, vamos come\xE7ar analisando o que significa \u201Ctreinar\u201D um tokenizador."),this.h()},l(e){const s=Te('[data-svelte="svelte-1phssyn"]',document.head);h=r(s,"META",{name:!0,content:!0}),s.forEach(a),T=c(e),v=r(e,"H1",{class:!0});var F=n(v);z=r(F,"A",{id:!0,class:!0,href:!0});var fe=n(z);A=r(fe,"SPAN",{});var he=n(A);we(g.$$.fragment,he),he.forEach(a),fe.forEach(a),J=c(F),C=r(F,"SPAN",{});var ve=n(C);R=u(ve,"Introdu\xE7\xE3o"),ve.forEach(a),F.forEach(a),S=c(e),we(q.$$.fragment,e),O=c(e),_=r(e,"P",{});var G=n(_);K=u(G,"No "),x=r(G,"A",{href:!0});var ze=n(x);V=u(ze,"Cap\xEDtulo 3"),ze.forEach(a),W=u(G,", n\xF3s estudamos como realizar o ajuste fino em um modelo para uma dada tarefa. Quando n\xF3s fazemos isso, usamos o mesmo tokenizador utilizado pelo modelo pr\xE9-treinado \u2014 mas o que podemos fazer quando queremos treinar um modelo do in\xEDcio? Nestes casos, utilizar um tokenizador que foi pr\xE9-treinado em um corpus de outro dom\xEDnio ou linguagem \xE9 tipicamente sub\xF3timo. Por exemplo, um tokenizador que \xE9 treinado em um corpus de lingua inglesa ter\xE1 um desempenho ruim em um corpus de textos em japon\xEAs, visto que o uso de espa\xE7os e pontua\xE7\xF5es \xE9 muito diferente nestes dois idiomas."),G.forEach(a),M=c(e),f=r(e,"P",{});var y=n(f);X=u(y,"Neste cap\xEDtulo, voc\xEA aprender\xE1 como treinar um novo tokenizador em um corpus de textos, para ent\xE3o ser usado no treinamento de um modelo de linguagem. Isto tudo ser\xE1 feito com ajuda da biblioteca "),E=r(y,"A",{href:!0,rel:!0});var _e=n(E);Y=u(_e,"\u{1F917} Tokenizers"),_e.forEach(a),Z=u(y,", que prov\xEA o tokenizador r\xE1pido na biblioteca "),$=r(y,"A",{href:!0,rel:!0});var ke=n($);ee=u(ke,"\u{1F917} Transformers"),ke.forEach(a),oe=u(y,". Daremos uma olhada a fundo sobre as funcionalidades oferecidas pela biblioteca, e explorar como os tokenizadores r\xE1pidos diferem das vers\xF5es \u201Clentas\u201D."),y.forEach(a),B=c(e),P=r(e,"P",{});var be=n(P);ae=u(be,"Os t\xF3picos que iremos cobrir incluem:"),be.forEach(a),D=c(e),l=r(e,"UL",{});var b=n(l);I=r(b,"LI",{});var ge=n(I);te=u(ge,"Como treinar um novo tokenizador semelhante ao usado por um determinado checkpoint em um novo corpus de textos"),ge.forEach(a),re=c(b),N=r(b,"LI",{});var qe=n(N);se=u(qe,"Os recursos especiais dos tokenizadores r\xE1pidos"),qe.forEach(a),ie=c(b),j=r(b,"LI",{});var Ee=n(j);ne=u(Ee,"As diferen\xE7as entre os tr\xEAs principais algoritmos de tokeniza\xE7\xE3o de subpalavras usados \u200B\u200Bno processamento de linguagem natural hoje"),Ee.forEach(a),ue=c(b),L=r(b,"LI",{});var $e=n(L);me=u($e,"Como construir um tokenizador do zero com a biblioteca \u{1F917} Tokenizers e trein\xE1-lo em alguns dados"),$e.forEach(a),b.forEach(a),Q=c(e),k=r(e,"P",{});var H=n(k);le=u(H,"As t\xE9cnicas introduzidas neste cap\xEDtulo ir\xE3o te preparar para a se\xE7\xE3o no "),w=r(H,"A",{href:!0});var xe=n(w);de=u(xe,"Cap\xEDtulo 7"),xe.forEach(a),pe=u(H," onde iremos analisar a cria\xE7\xE3o de um modelo de linguagem para a linguagem Python. Primeiramente, vamos come\xE7ar analisando o que significa \u201Ctreinar\u201D um tokenizador."),H.forEach(a),this.h()},h(){d(h,"name","hf:doc:metadata"),d(h,"content",JSON.stringify(Qe)),d(z,"id","introduo"),d(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(z,"href","#introduo"),d(v,"class","relative group"),d(x,"href","/course/chapter3"),d(E,"href","https://github.com/huggingface/tokenizers"),d(E,"rel","nofollow"),d($,"href","https://github.com/huggingface/transformers"),d($,"rel","nofollow"),d(w,"href","/course/chapter7/6")},m(e,s){o(document.head,h),m(e,T,s),m(e,v,s),o(v,z),o(z,A),ye(g,A,null),o(v,J),o(v,C),o(C,R),m(e,S,s),ye(q,e,s),m(e,O,s),m(e,_,s),o(_,K),o(_,x),o(x,V),o(_,W),m(e,M,s),m(e,f,s),o(f,X),o(f,E),o(E,Y),o(f,Z),o(f,$),o($,ee),o(f,oe),m(e,B,s),m(e,P,s),o(P,ae),m(e,D,s),m(e,l,s),o(l,I),o(I,te),o(l,re),o(l,N),o(N,se),o(l,ie),o(l,j),o(j,ne),o(l,ue),o(l,L),o(L,me),m(e,Q,s),m(e,k,s),o(k,le),o(k,w),o(w,de),o(k,pe),U=!0},p:Se,i(e){U||(Ae(g.$$.fragment,e),Ae(q.$$.fragment,e),U=!0)},o(e){Ce(g.$$.fragment,e),Ce(q.$$.fragment,e),U=!1},d(e){a(h),e&&a(T),e&&a(v),Ie(g),e&&a(S),Ie(q,e),e&&a(O),e&&a(_),e&&a(M),e&&a(f),e&&a(B),e&&a(P),e&&a(D),e&&a(l),e&&a(Q),e&&a(k)}}}const Qe={local:"introduo",title:"Introdu\xE7\xE3o"};function Ue(ce){return Oe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Je extends Ne{constructor(h){super();je(this,h,Ue,De,Le,{})}}export{Je as default,Qe as metadata};
