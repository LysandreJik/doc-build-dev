import{S as to,i as io,s as mo,e as t,k as h,w as Ne,t as r,M as lo,c as i,d as a,m as _,a as m,x as je,h as s,b as P,G as o,g as n,y as De,q as Fe,o as Be,B as Ge,v as no}from"../../chunks/vendor-hf-doc-builder.js";import{T as co}from"../../chunks/Tip-hf-doc-builder.js";import{I as uo}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as po}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function fo(K){let c,v,p,f;return{c(){c=r("\u26A0\uFE0F Para beneficiar-se de todos os recursos dispon\xEDveis com o Model Hub e \u{1F917} Transformers, recomendamos  "),v=t("a"),p=r("criar uma conta"),f=r("."),this.h()},l(d){c=s(d,"\u26A0\uFE0F Para beneficiar-se de todos os recursos dispon\xEDveis com o Model Hub e \u{1F917} Transformers, recomendamos  "),v=i(d,"A",{href:!0});var u=m(v);p=s(u,"criar uma conta"),u.forEach(a),f=s(d,"."),this.h()},h(){P(v,"href","https://huggingface.co/join")},m(d,u){n(d,c,u),n(d,v,u),o(v,p),n(d,f,u)},d(d){d&&a(c),d&&a(v),d&&a(f)}}}function vo(K){let c,v,p,f,d,u,se,j,te,V,A,W,b,ie,w,me,le,D,ne,de,X,C,ce,Y,E,z,F,ue,pe,fe,$,B,ve,he,G,_e,be,R,Ee,$e,qe,y,H,ge,Pe,Z,I,Te,ee,q,Ae,Q,xe,ke,M,we,Ce,oe,g,ze,U,ye,Ie,J,Me,Oe,ae,T,re;return u=new uo({}),A=new po({props:{chapter:2,classNames:"absolute z-10 right-0 top-0"}}),T=new co({props:{$$slots:{default:[fo]},$$scope:{ctx:K}}}),{c(){c=t("meta"),v=h(),p=t("h1"),f=t("a"),d=t("span"),Ne(u.$$.fragment),se=h(),j=t("span"),te=r("Introdu\xE7\xE3o"),V=h(),Ne(A.$$.fragment),W=h(),b=t("p"),ie=r("Como voc\xEA viu no "),w=t("a"),me=r("Capitulo 1"),le=r(", normalmente modelos Transformers s\xE3o muito grandes. Com milh\xF5es a dezenas de "),D=t("em"),ne=r("bilh\xF5es"),de=r(" de par\xE2metros, o treinamento e o deploy destes modelos \xE9 uma tarefa complicado. Al\xE9m disso, com novos modelos sendo lan\xE7ados quase diariamente e cada um tendo sua pr\xF3pria implementa\xE7\xE3o, experiment\xE1-los a todos n\xE3o \xE9 tarefa f\xE1cil."),X=h(),C=t("p"),ce=r("A biblioteca \u{1F917} Transformers foi criado para resolver este problema. Seu objetivo \xE9 fornecer uma API \xFAnica atrav\xE9s do qual qualquer modelo Transformer possa ser carregado, treinado e salvo. As principais caracter\xEDsticas da biblioteca s\xE3o:"),Y=h(),E=t("ul"),z=t("li"),F=t("strong"),ue=r("F\xE1cil de usar"),pe=r(": Baixar, carregar e usar um modelo de processamento natural de linguagem (PNL) de \xFAltima gera\xE7\xE3o para infer\xEAncia pode ser feito em apenas duas linhas de c\xF3digo"),fe=h(),$=t("li"),B=t("strong"),ve=r("Flexibilidade"),he=r(": Em sua ess\xEAncia, todos os modelos s\xE3o uma simples classe PyTorch "),G=t("code"),_e=r("nn.Module"),be=r(" ou TensorFlow "),R=t("code"),Ee=r("tf.keras.Model"),$e=r(" e podem ser tratadas como qualquer outro modelo em seus respectivos frameworks de machine learning (ML)."),qe=h(),y=t("li"),H=t("strong"),ge=r("Simplicidade"),Pe=r(": Quase nenhuma abstra\xE7\xE3o \xE9 feita em toda a biblioteca. O \u201CTudo em um arquivo\u201D \xE9 um conceito principal: o \u201Cpasse para frente\u201D de um modelo \xE9 inteiramente definido em um \xFAnico arquivo, de modo que o c\xF3digo em si seja compreens\xEDvel e hacke\xE1vel."),Z=h(),I=t("p"),Te=r("Esta \xFAltima caracter\xEDstica torna \u{1F917} Transformers bem diferente de outras bibliotecas ML que modelos e/ou configura\xE7\xF5es s\xE3o compartilhados entre arquivos; em vez disso, cada modelo tem suas pr\xF3prias camadas. Al\xE9m de tornar os modelos mais acess\xEDveis e compreens\xEDveis, isto permite que voc\xEA experimente facilmente um modelo sem afetar outros."),ee=h(),q=t("p"),Ae=r("Este cap\xEDtulo come\xE7ar\xE1 com um exemplo de ponta a ponta onde usamos um modelo e um tokenizer juntos para replicar a fun\xE7\xE3o "),Q=t("code"),xe=r("pipeline()"),ke=r(" introduzida no "),M=t("a"),we=r("Capitulo 1"),Ce=r(". A seguir, discutiremos o modelo da API: onde veremos profundamente as classes de modelo e configura\xE7\xE3o, al\xE9m de mostrar como carregar um modelo e como ele processa as entradas num\xE9ricas para as previs\xF5es de sa\xEDda."),oe=h(),g=t("p"),ze=r("Depois veremos a API do tokenizer, que \xE9 o outro componente principal da fun\xE7\xE3o "),U=t("code"),ye=r("pipeline()"),Ie=r(". Os Tokenizers cuidam da primeira e da \xFAltima etapa do processamento, cuidando da convers\xE3o de texto para entradas num\xE9ricas para a rede neural, e da convers\xE3o de volta ao texto quando for necess\xE1rio. Por fim, mostraremos a voc\xEA como lidar com o envio de v\xE1rias frases atrav\xE9s de um modelo em um batch preparado, depois olharemos tudo mais atentamente a fun\xE7\xE3o de alto n\xEDvel "),J=t("code"),Me=r("tokenizer()"),Oe=r("."),ae=h(),Ne(T.$$.fragment),this.h()},l(e){const l=lo('[data-svelte="svelte-1phssyn"]',document.head);c=i(l,"META",{name:!0,content:!0}),l.forEach(a),v=_(e),p=i(e,"H1",{class:!0});var x=m(p);f=i(x,"A",{id:!0,class:!0,href:!0});var Re=m(f);d=i(Re,"SPAN",{});var He=m(d);je(u.$$.fragment,He),He.forEach(a),Re.forEach(a),se=_(x),j=i(x,"SPAN",{});var Qe=m(j);te=s(Qe,"Introdu\xE7\xE3o"),Qe.forEach(a),x.forEach(a),V=_(e),je(A.$$.fragment,e),W=_(e),b=i(e,"P",{});var O=m(b);ie=s(O,"Como voc\xEA viu no "),w=i(O,"A",{href:!0});var Ue=m(w);me=s(Ue,"Capitulo 1"),Ue.forEach(a),le=s(O,", normalmente modelos Transformers s\xE3o muito grandes. Com milh\xF5es a dezenas de "),D=i(O,"EM",{});var Je=m(D);ne=s(Je,"bilh\xF5es"),Je.forEach(a),de=s(O," de par\xE2metros, o treinamento e o deploy destes modelos \xE9 uma tarefa complicado. Al\xE9m disso, com novos modelos sendo lan\xE7ados quase diariamente e cada um tendo sua pr\xF3pria implementa\xE7\xE3o, experiment\xE1-los a todos n\xE3o \xE9 tarefa f\xE1cil."),O.forEach(a),X=_(e),C=i(e,"P",{});var Ke=m(C);ce=s(Ke,"A biblioteca \u{1F917} Transformers foi criado para resolver este problema. Seu objetivo \xE9 fornecer uma API \xFAnica atrav\xE9s do qual qualquer modelo Transformer possa ser carregado, treinado e salvo. As principais caracter\xEDsticas da biblioteca s\xE3o:"),Ke.forEach(a),Y=_(e),E=i(e,"UL",{});var S=m(E);z=i(S,"LI",{});var Se=m(z);F=i(Se,"STRONG",{});var Ve=m(F);ue=s(Ve,"F\xE1cil de usar"),Ve.forEach(a),pe=s(Se,": Baixar, carregar e usar um modelo de processamento natural de linguagem (PNL) de \xFAltima gera\xE7\xE3o para infer\xEAncia pode ser feito em apenas duas linhas de c\xF3digo"),Se.forEach(a),fe=_(S),$=i(S,"LI",{});var k=m($);B=i(k,"STRONG",{});var We=m(B);ve=s(We,"Flexibilidade"),We.forEach(a),he=s(k,": Em sua ess\xEAncia, todos os modelos s\xE3o uma simples classe PyTorch "),G=i(k,"CODE",{});var Xe=m(G);_e=s(Xe,"nn.Module"),Xe.forEach(a),be=s(k," ou TensorFlow "),R=i(k,"CODE",{});var Ye=m(R);Ee=s(Ye,"tf.keras.Model"),Ye.forEach(a),$e=s(k," e podem ser tratadas como qualquer outro modelo em seus respectivos frameworks de machine learning (ML)."),k.forEach(a),qe=_(S),y=i(S,"LI",{});var Le=m(y);H=i(Le,"STRONG",{});var Ze=m(H);ge=s(Ze,"Simplicidade"),Ze.forEach(a),Pe=s(Le,": Quase nenhuma abstra\xE7\xE3o \xE9 feita em toda a biblioteca. O \u201CTudo em um arquivo\u201D \xE9 um conceito principal: o \u201Cpasse para frente\u201D de um modelo \xE9 inteiramente definido em um \xFAnico arquivo, de modo que o c\xF3digo em si seja compreens\xEDvel e hacke\xE1vel."),Le.forEach(a),S.forEach(a),Z=_(e),I=i(e,"P",{});var eo=m(I);Te=s(eo,"Esta \xFAltima caracter\xEDstica torna \u{1F917} Transformers bem diferente de outras bibliotecas ML que modelos e/ou configura\xE7\xF5es s\xE3o compartilhados entre arquivos; em vez disso, cada modelo tem suas pr\xF3prias camadas. Al\xE9m de tornar os modelos mais acess\xEDveis e compreens\xEDveis, isto permite que voc\xEA experimente facilmente um modelo sem afetar outros."),eo.forEach(a),ee=_(e),q=i(e,"P",{});var L=m(q);Ae=s(L,"Este cap\xEDtulo come\xE7ar\xE1 com um exemplo de ponta a ponta onde usamos um modelo e um tokenizer juntos para replicar a fun\xE7\xE3o "),Q=i(L,"CODE",{});var oo=m(Q);xe=s(oo,"pipeline()"),oo.forEach(a),ke=s(L," introduzida no "),M=i(L,"A",{href:!0});var ao=m(M);we=s(ao,"Capitulo 1"),ao.forEach(a),Ce=s(L,". A seguir, discutiremos o modelo da API: onde veremos profundamente as classes de modelo e configura\xE7\xE3o, al\xE9m de mostrar como carregar um modelo e como ele processa as entradas num\xE9ricas para as previs\xF5es de sa\xEDda."),L.forEach(a),oe=_(e),g=i(e,"P",{});var N=m(g);ze=s(N,"Depois veremos a API do tokenizer, que \xE9 o outro componente principal da fun\xE7\xE3o "),U=i(N,"CODE",{});var ro=m(U);ye=s(ro,"pipeline()"),ro.forEach(a),Ie=s(N,". Os Tokenizers cuidam da primeira e da \xFAltima etapa do processamento, cuidando da convers\xE3o de texto para entradas num\xE9ricas para a rede neural, e da convers\xE3o de volta ao texto quando for necess\xE1rio. Por fim, mostraremos a voc\xEA como lidar com o envio de v\xE1rias frases atrav\xE9s de um modelo em um batch preparado, depois olharemos tudo mais atentamente a fun\xE7\xE3o de alto n\xEDvel "),J=i(N,"CODE",{});var so=m(J);Me=s(so,"tokenizer()"),so.forEach(a),Oe=s(N,"."),N.forEach(a),ae=_(e),je(T.$$.fragment,e),this.h()},h(){P(c,"name","hf:doc:metadata"),P(c,"content",JSON.stringify(ho)),P(f,"id","introduo"),P(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(f,"href","#introduo"),P(p,"class","relative group"),P(w,"href","/course/pt/chapter1"),P(M,"href","/course/pt/chapter1")},m(e,l){o(document.head,c),n(e,v,l),n(e,p,l),o(p,f),o(f,d),De(u,d,null),o(p,se),o(p,j),o(j,te),n(e,V,l),De(A,e,l),n(e,W,l),n(e,b,l),o(b,ie),o(b,w),o(w,me),o(b,le),o(b,D),o(D,ne),o(b,de),n(e,X,l),n(e,C,l),o(C,ce),n(e,Y,l),n(e,E,l),o(E,z),o(z,F),o(F,ue),o(z,pe),o(E,fe),o(E,$),o($,B),o(B,ve),o($,he),o($,G),o(G,_e),o($,be),o($,R),o(R,Ee),o($,$e),o(E,qe),o(E,y),o(y,H),o(H,ge),o(y,Pe),n(e,Z,l),n(e,I,l),o(I,Te),n(e,ee,l),n(e,q,l),o(q,Ae),o(q,Q),o(Q,xe),o(q,ke),o(q,M),o(M,we),o(q,Ce),n(e,oe,l),n(e,g,l),o(g,ze),o(g,U),o(U,ye),o(g,Ie),o(g,J),o(J,Me),o(g,Oe),n(e,ae,l),De(T,e,l),re=!0},p(e,[l]){const x={};l&2&&(x.$$scope={dirty:l,ctx:e}),T.$set(x)},i(e){re||(Fe(u.$$.fragment,e),Fe(A.$$.fragment,e),Fe(T.$$.fragment,e),re=!0)},o(e){Be(u.$$.fragment,e),Be(A.$$.fragment,e),Be(T.$$.fragment,e),re=!1},d(e){a(c),e&&a(v),e&&a(p),Ge(u),e&&a(V),Ge(A,e),e&&a(W),e&&a(b),e&&a(X),e&&a(C),e&&a(Y),e&&a(E),e&&a(Z),e&&a(I),e&&a(ee),e&&a(q),e&&a(oe),e&&a(g),e&&a(ae),Ge(T,e)}}}const ho={local:"introduo",title:"Introdu\xE7\xE3o"};function _o(K){return no(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class go extends to{constructor(c){super();io(this,c,_o,vo,mo,{})}}export{go as default,ho as metadata};
