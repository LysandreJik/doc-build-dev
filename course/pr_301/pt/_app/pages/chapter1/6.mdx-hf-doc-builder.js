import{S as qe,i as Me,s as Oe,e as r,k as m,w as Pe,t as f,M as Ce,c as s,d as a,m as c,a as t,x as xe,h as p,b as d,G as o,g as n,y as Le,L as Ie,q as ye,o as Te,B as Ae,v as Se}from"../../chunks/vendor-hf-doc-builder.js";import{I as be}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ke}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Ge(le){let u,k,h,v,q,$,J,M,j,G,g,N,_,D,O,K,Q,R,y,V,B,T,W,U,A,Y,X,i,C,w,Z,ee,I,P,ae,oe,S,x,re,se,b,L,te,z;return $=new be({}),g=new ke({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),{c(){u=r("meta"),k=m(),h=r("h1"),v=r("a"),q=r("span"),Pe($.$$.fragment),J=m(),M=r("span"),j=f("Modelos decodificadores"),G=m(),Pe(g.$$.fragment),N=m(),_=r("p"),D=f("Os modelos de decodificador usam apenas o decodificador de um modelo Transformer. Em cada etapa, para uma determinada palavra, as camadas de aten\xE7\xE3o s\xF3 podem acessar as palavras posicionadas antes dela na frase. Esses modelos geralmente s\xE3o chamados de\xA0"),O=r("em"),K=f("modelos auto-regressivos"),Q=f("."),R=m(),y=r("p"),V=f("O pr\xE9-treinamento de modelos de decodificadores geralmente gira em torno de prever a pr\xF3xima palavra na frase."),B=m(),T=r("p"),W=f("Esses modelos s\xE3o mais adequados para tarefas que envolvem gera\xE7\xE3o de texto."),U=m(),A=r("p"),Y=f("Os representantes desta fam\xEDlia de modelos incluem:"),X=m(),i=r("ul"),C=r("li"),w=r("a"),Z=f("CTRL"),ee=m(),I=r("li"),P=r("a"),ae=f("GPT"),oe=m(),S=r("li"),x=r("a"),re=f("GPT-2"),se=m(),b=r("li"),L=r("a"),te=f("Transformer XL"),this.h()},l(e){const l=Ce('[data-svelte="svelte-1phssyn"]',document.head);u=s(l,"META",{name:!0,content:!0}),l.forEach(a),k=c(e),h=s(e,"H1",{class:!0});var F=t(h);v=s(F,"A",{id:!0,class:!0,href:!0});var de=t(v);q=s(de,"SPAN",{});var ne=t(q);xe($.$$.fragment,ne),ne.forEach(a),de.forEach(a),J=c(F),M=s(F,"SPAN",{});var ie=t(M);j=p(ie,"Modelos decodificadores"),ie.forEach(a),F.forEach(a),G=c(e),xe(g.$$.fragment,e),N=c(e),_=s(e,"P",{});var H=t(_);D=p(H,"Os modelos de decodificador usam apenas o decodificador de um modelo Transformer. Em cada etapa, para uma determinada palavra, as camadas de aten\xE7\xE3o s\xF3 podem acessar as palavras posicionadas antes dela na frase. Esses modelos geralmente s\xE3o chamados de\xA0"),O=s(H,"EM",{});var me=t(O);K=p(me,"modelos auto-regressivos"),me.forEach(a),Q=p(H,"."),H.forEach(a),R=c(e),y=s(e,"P",{});var fe=t(y);V=p(fe,"O pr\xE9-treinamento de modelos de decodificadores geralmente gira em torno de prever a pr\xF3xima palavra na frase."),fe.forEach(a),B=c(e),T=s(e,"P",{});var ce=t(T);W=p(ce,"Esses modelos s\xE3o mais adequados para tarefas que envolvem gera\xE7\xE3o de texto."),ce.forEach(a),U=c(e),A=s(e,"P",{});var pe=t(A);Y=p(pe,"Os representantes desta fam\xEDlia de modelos incluem:"),pe.forEach(a),X=c(e),i=s(e,"UL",{});var E=t(i);C=s(E,"LI",{});var ue=t(C);w=s(ue,"A",{href:!0,rel:!0});var he=t(w);Z=p(he,"CTRL"),he.forEach(a),ue.forEach(a),ee=c(E),I=s(E,"LI",{});var ve=t(I);P=s(ve,"A",{href:!0,rel:!0});var _e=t(P);ae=p(_e,"GPT"),_e.forEach(a),ve.forEach(a),oe=c(E),S=s(E,"LI",{});var Ee=t(S);x=s(Ee,"A",{href:!0,rel:!0});var $e=t(x);re=p($e,"GPT-2"),$e.forEach(a),Ee.forEach(a),se=c(E),b=s(E,"LI",{});var ge=t(b);L=s(ge,"A",{href:!0,rel:!0});var we=t(L);te=p(we,"Transformer XL"),we.forEach(a),ge.forEach(a),E.forEach(a),this.h()},h(){d(u,"name","hf:doc:metadata"),d(u,"content",JSON.stringify(Ne)),d(v,"id","modelos-decodificadores"),d(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(v,"href","#modelos-decodificadores"),d(h,"class","relative group"),d(w,"href","https://huggingface.co/transformers/model_doc/ctrl.html"),d(w,"rel","nofollow"),d(P,"href","https://huggingface.co/transformers/model_doc/gpt.html"),d(P,"rel","nofollow"),d(x,"href","https://huggingface.co/transformers/model_doc/gpt2.html"),d(x,"rel","nofollow"),d(L,"href","https://huggingface.co/transformers/model_doc/transfo-xl.html"),d(L,"rel","nofollow")},m(e,l){o(document.head,u),n(e,k,l),n(e,h,l),o(h,v),o(v,q),Le($,q,null),o(h,J),o(h,M),o(M,j),n(e,G,l),Le(g,e,l),n(e,N,l),n(e,_,l),o(_,D),o(_,O),o(O,K),o(_,Q),n(e,R,l),n(e,y,l),o(y,V),n(e,B,l),n(e,T,l),o(T,W),n(e,U,l),n(e,A,l),o(A,Y),n(e,X,l),n(e,i,l),o(i,C),o(C,w),o(w,Z),o(i,ee),o(i,I),o(I,P),o(P,ae),o(i,oe),o(i,S),o(S,x),o(x,re),o(i,se),o(i,b),o(b,L),o(L,te),z=!0},p:Ie,i(e){z||(ye($.$$.fragment,e),ye(g.$$.fragment,e),z=!0)},o(e){Te($.$$.fragment,e),Te(g.$$.fragment,e),z=!1},d(e){a(u),e&&a(k),e&&a(h),Ae($),e&&a(G),Ae(g,e),e&&a(N),e&&a(_),e&&a(R),e&&a(y),e&&a(B),e&&a(T),e&&a(U),e&&a(A),e&&a(X),e&&a(i)}}}const Ne={local:"modelos-decodificadores",title:"Modelos decodificadores"};function Re(le){return Se(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ze extends qe{constructor(u){super();Me(this,u,Re,Ge,Oe,{})}}export{ze as default,Ne as metadata};
