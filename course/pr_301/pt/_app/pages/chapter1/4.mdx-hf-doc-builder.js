import{S as Cm,i as Vm,s as Um,e as r,k as u,w as v,t as i,M as Hm,c as s,d as o,m as p,a as t,x as g,h as d,b as m,N as c,G as a,g as n,y as h,L as Jm,q,o as E,B as _,v as Fm}from"../../chunks/vendor-hf-doc-builder.js";import{Y as bl}from"../../chunks/Youtube-hf-doc-builder.js";import{I as y}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Xm}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Ym(Tl){let O,qr,G,ee,Ka,$e,rt,eo,st,Er,Pe,_r,ha,tt,br,M,ae,ao,ke,it,oo,dt,Tr,qa,lt,$r,R,Ae,$l,nt,we,Pl,Pr,oe,mt,xe,ut,pt,kr,f,ro,re,so,ct,ft,ze,vt,gt,ht,to,se,io,qt,Et,je,_t,bt,Tt,lo,te,no,$t,Pt,Ie,kt,At,wt,mo,ie,uo,xt,zt,Ne,jt,It,Nt,po,$,co,yt,Ot,ye,Gt,Mt,Oe,Rt,St,Lt,fo,P,vo,Bt,Dt,Ge,Ct,Vt,go,Ut,Ht,Ar,Ea,Jt,wr,k,Me,Ft,ho,Xt,Yt,Qt,Re,Wt,qo,Zt,Kt,ei,Se,ai,Eo,oi,ri,xr,_a,si,zr,S,de,_o,Le,ti,bo,ii,jr,le,di,To,li,ni,Ir,ne,mi,$o,ui,pi,Nr,A,ci,Po,fi,vi,ko,gi,hi,yr,L,Be,kl,qi,De,Al,Or,me,Ei,Ao,_i,bi,Gr,B,Ce,wl,Ti,Ve,xl,Mr,D,ue,wo,Ue,$i,xo,Pi,Rr,ba,ki,Sr,He,Je,zl,Lr,Ta,Ai,Br,C,Fe,jl,wi,Xe,Il,Dr,Ye,Cr,$a,xi,Vr,Pa,zi,Ur,ka,ji,Hr,V,pe,zo,Qe,Ii,jo,Ni,Jr,We,Fr,Ze,Io,yi,Oi,Xr,U,Ke,Nl,Gi,ea,yl,Yr,Aa,Mi,Qr,H,No,Ri,Si,yo,Li,Bi,Wr,w,Oo,Di,Ci,Go,Vi,Ui,Mo,Hi,Zr,ce,Ji,Ro,Fi,Xi,Kr,J,aa,Ol,Yi,oa,Gl,es,wa,Qi,as,xa,Wi,os,F,fe,So,ra,Zi,Lo,Ki,rs,za,ed,ss,sa,ts,X,ve,Bo,ta,ad,Do,od,is,ja,rd,ds,ge,Ia,Co,sd,td,id,Na,Vo,dd,ld,ls,Y,ia,Ml,nd,da,Rl,ns,ya,md,ms,x,Oa,Uo,ud,pd,cd,Ga,Ho,fd,vd,gd,he,Jo,hd,qd,Fo,Ed,_d,us,Ma,bd,ps,Q,qe,Xo,la,Td,Yo,$d,cs,z,Pd,Qo,kd,Ad,na,wd,xd,fs,Ra,zd,vs,Sa,jd,gs,La,Id,hs,W,Ee,Wo,ma,Nd,Zo,yd,qs,Ba,Od,Es,Da,Gd,_s,Ca,Md,bs,Z,ua,Sl,Rd,pa,Ll,Ts,Va,Sd,$s,_e,Ld,Ko,Bd,Dd,Ps,K,be,er,ca,Cd,ar,Vd,ks,b,Ud,or,Hd,Jd,rr,Fd,Xd,sr,Yd,Qd,As,j,Ua,tr,Wd,Zd,Kd,Ha,ir,el,al,ol,I,dr,rl,sl,lr,tl,il,nr,dl,ll,ws,N,nl,mr,ml,ul,ur,pl,cl,xs;return $e=new y({}),Pe=new Xm({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),ke=new y({}),Le=new y({}),Ue=new y({}),Ye=new bl({props:{id:"ftWlj4FBHTg"}}),Qe=new y({}),We=new bl({props:{id:"BqqfQnyjmgg"}}),ra=new y({}),sa=new bl({props:{id:"H39Z_720T5s"}}),ta=new y({}),la=new y({}),ma=new y({}),ca=new y({}),{c(){O=r("meta"),qr=u(),G=r("h1"),ee=r("a"),Ka=r("span"),v($e.$$.fragment),rt=u(),eo=r("span"),st=i("Como os Transformers trabalham?"),Er=u(),v(Pe.$$.fragment),_r=u(),ha=r("p"),tt=i("Nessa se\xE7\xE3o, n\xF3s olharemos para o alto n\xEDvel de arquitetura dos modelos Transformers."),br=u(),M=r("h2"),ae=r("a"),ao=r("span"),v(ke.$$.fragment),it=u(),oo=r("span"),dt=i("Um pouco da hist\xF3ria dos Transformers"),Tr=u(),qa=r("p"),lt=i("Aqui alguns pontos de refer\xEAncia na (pequena) hist\xF3ria dos modelos Transformers:"),$r=u(),R=r("div"),Ae=r("img"),nt=u(),we=r("img"),Pr=u(),oe=r("p"),mt=i("A "),xe=r("a"),ut=i("arquitetura Transformer"),pt=i(" foi introduzida em Junho de 2017. O foco de pesquisa original foi para tarefas de tradu\xE7\xE3o. Isso foi seguido pela introdu\xE7\xE3o de muitos modelos influentes, incluindo:"),kr=u(),f=r("ul"),ro=r("li"),re=r("p"),so=r("strong"),ct=i("Junho de 2018"),ft=i(": "),ze=r("a"),vt=i("GPT"),gt=i(", o primeiro modelo Transformer pr\xE9-treinado, usado para ajuste-fino em v\xE1rias tarefas de NLP e obtendo resultados estado-da-arte"),ht=u(),to=r("li"),se=r("p"),io=r("strong"),qt=i("Outubro de 2018"),Et=i(": "),je=r("a"),_t=i("BERT"),bt=i(", outro grande modelo pr\xE9-treinado, esse outro foi designado para produzir melhores resumos de senten\xE7as(mais sobre isso no pr\xF3ximo cap\xEDtulo!)"),Tt=u(),lo=r("li"),te=r("p"),no=r("strong"),$t=i("Fevereiro de 2019"),Pt=i(": "),Ie=r("a"),kt=i("GPT-2"),At=i(", uma melhor (e maior) vers\xE3o da GPT que n\xE3o foi imediatamente publicizado o seu lan\xE7amento devido a preocupa\xE7\xF5es \xE9ticas [N.T.: n\xE3o apenas por isso]"),wt=u(),mo=r("li"),ie=r("p"),uo=r("strong"),xt=i("Outubro de 2019"),zt=i(": "),Ne=r("a"),jt=i("DistilBERT"),It=i(", uma vers\xE3o destilada do BERT que \xE9 60% mais r\xE1pidam 40% mais leve em mem\xF3ria, e ainda ret\xE9m 97% da performance do BERT"),Nt=u(),po=r("li"),$=r("p"),co=r("strong"),yt=i("Outubro de 2019"),Ot=i(": "),ye=r("a"),Gt=i("BART"),Mt=i(" e "),Oe=r("a"),Rt=i("T5"),St=i(", dois grandes modelos pr\xE9-treinados usando a mesma arquitetura do modelo original Transformer (os primeiros a fazerem at\xE9 ent\xE3o)"),Lt=u(),fo=r("li"),P=r("p"),vo=r("strong"),Bt=i("Maio de 2020"),Dt=i(", "),Ge=r("a"),Ct=i("GPT-3"),Vt=i(", uma vers\xE3o ainda maior da GPT-2 que \xE9 capaz de performar bem em uma variedade de tarefas sem a necessidade de ajuste-fino (chamado de aprendizagem"),go=r("em"),Ut=i("zero-shot"),Ht=i(")"),Ar=u(),Ea=r("p"),Jt=i("Esta lista est\xE1 longe de ser abrangente e destina-se apenas a destacar alguns dos diferentes tipos de modelos de Transformers. Em linhas gerais, eles podem ser agrupados em tr\xEAs categorias:"),wr=u(),k=r("ul"),Me=r("li"),Ft=i("GPT-like (tamb\xE9m chamados de modelos Transformers "),ho=r("em"),Xt=i("auto-regressivos"),Yt=i(")"),Qt=u(),Re=r("li"),Wt=i("BERT-like (tamb\xE9m chamados de modelos Transformers "),qo=r("em"),Zt=i("auto-codificadores"),Kt=i(")"),ei=u(),Se=r("li"),ai=i("BART/T5-like (tamb\xE9m chamados de modelos Transformers "),Eo=r("em"),oi=i("sequence-to-sequence"),ri=i(")"),xr=u(),_a=r("p"),si=i("Vamos mergulhar nessas fam\xEDlias com mais profundidade mais adiante"),zr=u(),S=r("h2"),de=r("a"),_o=r("span"),v(Le.$$.fragment),ti=u(),bo=r("span"),ii=i("Transformers s\xE3o modelos de linguagem"),jr=u(),le=r("p"),di=i("Todos os modelos de Transformer mencionados acima (GPT, BERT, BART, T5, etc.) foram treinados como "),To=r("em"),li=i("modelos de linguagem"),ni=i(". Isso significa que eles foram treinados em grandes quantidades de texto bruto de forma auto-supervisionada. O aprendizado autossupervisionado \xE9 um tipo de treinamento no qual o objetivo \xE9 calculado automaticamente a partir das entradas do modelo. Isso significa que os humanos n\xE3o s\xE3o necess\xE1rios para rotular os dados!"),Ir=u(),ne=r("p"),mi=i("Este tipo de modelo desenvolve uma compreens\xE3o estat\xEDstica da linguagem em que foi treinado, mas n\xE3o \xE9 muito \xFAtil para tarefas pr\xE1ticas espec\xEDficas. Por causa disso, o modelo geral pr\xE9-treinado passa por um processo chamado "),$o=r("em"),ui=i("aprendizagem de transfer\xEAncia"),pi=i(". Durante esse processo, o modelo \xE9 ajustado de maneira supervisionada - ou seja, usando r\xF3tulos anotados por humanos - em uma determinada tarefa."),Nr=u(),A=r("p"),ci=i("Um exemplo de tarefa \xE9 prever a pr\xF3xima palavra em uma frase depois de ler as "),Po=r("em"),fi=i("n"),vi=i(" palavras anteriores. Isso \xE9 chamado de "),ko=r("em"),gi=i("modelagem de linguagem causal"),hi=i(" porque a sa\xEDda depende das entradas passadas e presentes, mas n\xE3o das futuras."),yr=u(),L=r("div"),Be=r("img"),qi=u(),De=r("img"),Or=u(),me=r("p"),Ei=i("Outro exemplo \xE9 a "),Ao=r("em"),_i=i("modelagem de linguagem mascarada"),bi=i(", na qual o modelo prev\xEA uma palavra mascarada na frase."),Gr=u(),B=r("div"),Ce=r("img"),Ti=u(),Ve=r("img"),Mr=u(),D=r("h2"),ue=r("a"),wo=r("span"),v(Ue.$$.fragment),$i=u(),xo=r("span"),Pi=i("Transformers s\xE3o modelos grandes"),Rr=u(),ba=r("p"),ki=i("Al\xE9m de alguns outliers (como o DistilBERT), a estrat\xE9gia geral para obter melhor desempenho \xE9 aumentar os tamanhos dos modelos, bem como a quantidade de dados em que s\xE3o pr\xE9-treinados."),Sr=u(),He=r("div"),Je=r("img"),Lr=u(),Ta=r("p"),Ai=i("Infelizmente, treinar um modelo, especialmente um grande, requer uma grande quantidade de dados. Isso se torna muito caro em termos de tempo e recursos de computa\xE7\xE3o. At\xE9 se traduz em impacto ambiental, como pode ser visto no gr\xE1fico a seguir."),Br=u(),C=r("div"),Fe=r("img"),wi=u(),Xe=r("img"),Dr=u(),v(Ye.$$.fragment),Cr=u(),$a=r("p"),xi=i("E isso mostra um projeto para um modelo (muito grande) liderado por uma equipe que tenta conscientemente reduzir o impacto ambiental do pr\xE9-treinamento. Os gastos de executar muitos testes para obter os melhores hiperpar\xE2metros seria ainda maior."),Vr=u(),Pa=r("p"),zi=i("Imagine se cada vez que uma equipe de pesquisa, uma organiza\xE7\xE3o estudantil ou uma empresa quisesse treinar um modelo, o fizesse do zero. Isso levaria a custos globais enormes e desnecess\xE1rios!"),Ur=u(),ka=r("p"),ji=i("\xC9 por isso que compartilhar modelos de linguagem \xE9 fundamental: compartilhar os pesos treinados e construir em cima dos pesos j\xE1 treinados reduz o custo geral de computa\xE7\xE3o e os gastos de carbono da comunidade."),Hr=u(),V=r("h2"),pe=r("a"),zo=r("span"),v(Qe.$$.fragment),Ii=u(),jo=r("span"),Ni=i("Transfer\xEAncia de Aprendizagem"),Jr=u(),v(We.$$.fragment),Fr=u(),Ze=r("p"),Io=r("em"),yi=i("Pr\xE9-treinamento"),Oi=i(" \xE9 o ato de treinar um modelo do zero: os pesos s\xE3o inicializados aleatoriamente e o treinamento come\xE7a sem nenhum conhecimento pr\xE9vio."),Xr=u(),U=r("div"),Ke=r("img"),Gi=u(),ea=r("img"),Yr=u(),Aa=r("p"),Mi=i("Esse pr\xE9-treinamento geralmente \xE9 feito em grandes quantidades de dados. Portanto, requer um corpus de dados muito grande e o treinamento pode levar v\xE1rias semanas."),Qr=u(),H=r("p"),No=r("em"),Ri=i("Ajuste fino"),Si=i(", por outro lado, \xE9 o treinamento feito "),yo=r("strong"),Li=i("ap\xF3s"),Bi=i(" um modelo ter sido pr\xE9-treinado. Para realizar o ajuste fino, primeiro voc\xEA adquire um modelo de linguagem pr\xE9-treinado e, em seguida, realiza treinamento adicional com um conjunto de dados espec\xEDfico para sua tarefa. Espere - por que n\xE3o simplesmente treinar diretamente para a tarefa final? Existem algumas raz\xF5es:"),Wr=u(),w=r("ul"),Oo=r("li"),Di=i("O modelo pr\xE9-treinado j\xE1 foi treinado em um conjunto de dados que possui algumas semelhan\xE7as com o conjunto de dados de ajuste fino. O processo de ajuste fino \xE9, portanto, capaz de aproveitar o conhecimento adquirido pelo modelo inicial durante o pr\xE9-treinamento (por exemplo, com problemas de NLP, o modelo pr\xE9-treinado ter\xE1 algum tipo de compreens\xE3o estat\xEDstica da linguagem que voc\xEA est\xE1 usando para sua tarefa)."),Ci=u(),Go=r("li"),Vi=i("Como o modelo pr\xE9-treinado j\xE1 foi treinado com muitos dados, o ajuste fino requer muito menos dados para obter resultados decentes."),Ui=u(),Mo=r("li"),Hi=i("Pela mesma raz\xE3o, a quantidade de tempo e recursos necess\xE1rios para obter bons resultados s\xE3o muito menores."),Zr=u(),ce=r("p"),Ji=i("Por exemplo, pode-se alavancar um modelo pr\xE9-treinado treinado no idioma ingl\xEAs e depois ajust\xE1-lo em um corpus arXiv, resultando em um modelo baseado em ci\xEAncia/pesquisa. O ajuste fino exigir\xE1 apenas uma quantidade limitada de dados: o conhecimento que o modelo pr\xE9-treinado adquiriu \xE9 \u201Ctransferido\u201D, da\xED o termo "),Ro=r("em"),Fi=i("aprendizagem de transfer\xEAncia"),Xi=i("."),Kr=u(),J=r("div"),aa=r("img"),Yi=u(),oa=r("img"),es=u(),wa=r("p"),Qi=i("O ajuste fino de um modelo, portanto, tem menores custos de tempo, dados, financeiros e ambientais. Tamb\xE9m \xE9 mais r\xE1pido e f\xE1cil iterar em diferentes esquemas de ajuste fino, pois o treinamento \xE9 menos restritivo do que um pr\xE9-treinamento completo."),as=u(),xa=r("p"),Wi=i("Esse processo tamb\xE9m alcan\xE7ar\xE1 melhores resultados do que treinar do zero (a menos que voc\xEA tenha muitos dados), e \xE9 por isso que voc\xEA deve sempre tentar alavancar um modelo pr\xE9-treinado - um o mais pr\xF3ximo poss\xEDvel da tarefa que voc\xEA tem em m\xE3os - e  ent\xE3o fazer seu ajuste fino."),os=u(),F=r("h2"),fe=r("a"),So=r("span"),v(ra.$$.fragment),Zi=u(),Lo=r("span"),Ki=i("Arquitetura geral"),rs=u(),za=r("p"),ed=i("Nesta se\xE7\xE3o, veremos a arquitetura geral do modelo Transformer. N\xE3o se preocupe se voc\xEA n\xE3o entender alguns dos conceitos; h\xE1 se\xE7\xF5es detalhadas posteriormente cobrindo cada um dos componentes."),ss=u(),v(sa.$$.fragment),ts=u(),X=r("h2"),ve=r("a"),Bo=r("span"),v(ta.$$.fragment),ad=u(),Do=r("span"),od=i("Introdu\xE7\xE3o"),is=u(),ja=r("p"),rd=i("O modelo \xE9 principalmente composto por dois blocos:"),ds=u(),ge=r("ul"),Ia=r("li"),Co=r("strong"),sd=i("Codificador (esquerda)"),td=i(": O codificador recebe uma entrada e constr\xF3i uma representa\xE7\xE3o dela (seus recursos). Isso significa que o modelo \xE9 otimizado para adquirir entendimento da entrada."),id=u(),Na=r("li"),Vo=r("strong"),dd=i("Decodificador (\xE0 direita)"),ld=i(": O decodificador usa a representa\xE7\xE3o do codificador (recursos) junto com outras entradas para gerar uma sequ\xEAncia de destino. Isso significa que o modelo \xE9 otimizado para gerar sa\xEDdas."),ls=u(),Y=r("div"),ia=r("img"),nd=u(),da=r("img"),ns=u(),ya=r("p"),md=i("Cada uma dessas partes pode ser usada de forma independente, dependendo da tarefa:"),ms=u(),x=r("ul"),Oa=r("li"),Uo=r("strong"),ud=i("Modelos somente de codificador"),pd=i(": bom para tarefas que exigem compreens\xE3o da entrada, como classifica\xE7\xE3o de senten\xE7a e reconhecimento de entidade nomeada."),cd=u(),Ga=r("li"),Ho=r("strong"),fd=i("Modelos somente decodificadores"),vd=i(": bom para tarefas generativas, como gera\xE7\xE3o de texto."),gd=u(),he=r("li"),Jo=r("strong"),hd=i("Modelos de codificador-decodificador"),qd=i(" ou "),Fo=r("strong"),Ed=i("modelos de sequ\xEAncia a sequ\xEAncia"),_d=i(": bom para tarefas generativas que exigem uma entrada, como tradu\xE7\xE3o ou resumo. (corrigit sequence to sequence)"),us=u(),Ma=r("p"),bd=i("Vamos mergulhar nessas arquiteturas de forma independente em se\xE7\xF5es posteriores."),ps=u(),Q=r("h2"),qe=r("a"),Xo=r("span"),v(la.$$.fragment),Td=u(),Yo=r("span"),$d=i("Camadas de Aten\xE7\xE3o"),cs=u(),z=r("p"),Pd=i("Uma caracter\xEDstica chave dos modelos Transformer \xE9 que eles s\xE3o constru\xEDdos com camadas especiais chamadas "),Qo=r("em"),kd=i("camadas de aten\xE7\xE3o"),Ad=i(". Na verdade, o t\xEDtulo do artigo que apresenta a arquitetura do Transformer era "),na=r("a"),wd=i("\u201CAten\xE7\xE3o \xE9 tudo que voc\xEA precisa\u201D"),xd=i("! Exploraremos os detalhes das camadas de aten\xE7\xE3o posteriormente no curso; por enquanto, tudo o que voc\xEA precisa saber \xE9 que essa camada dir\xE1 ao modelo para prestar aten\xE7\xE3o espec\xEDfica a certas palavras na frase que voc\xEA passou (e mais ou menos ignorar as outras) ao lidar com a representa\xE7\xE3o de cada palavra."),fs=u(),Ra=r("p"),zd=i("Para contextualizar, considere a tarefa de traduzir o texto do portugu\xEAs para o franc\xEAs. Dada a entrada \u201CVoc\xEA gosta deste curso\u201D, um modelo de tradu\xE7\xE3o precisar\xE1 atender tamb\xE9m \xE0 palavra adjacente \u201CVoc\xEA\u201D para obter a tradu\xE7\xE3o adequada para a palavra \u201Cgosta\u201D, pois em franc\xEAs o verbo \u201Cgostar\u201D \xE9 conjugado de forma diferente dependendo o sujeito. O resto da frase, no entanto, n\xE3o \xE9 \xFAtil para a tradu\xE7\xE3o dessa palavra. Na mesma linha, ao traduzir \u201Cdeste\u201D o modelo tamb\xE9m precisar\xE1 prestar aten\xE7\xE3o \xE0 palavra \u201Ccurso\u201D, pois \u201Cdeste\u201D traduz-se de forma diferente dependendo se o substantivo associado \xE9 masculino ou feminino. Novamente, as outras palavras na frase n\xE3o importar\xE3o para a tradu\xE7\xE3o de \u201Cdeste\u201D. Com frases mais complexas (e regras gramaticais mais complexas), o modelo precisaria prestar aten\xE7\xE3o especial \xE0s palavras que podem aparecer mais distantes na frase para traduzir adequadamente cada palavra."),vs=u(),Sa=r("p"),jd=i("O mesmo conceito se aplica a qualquer tarefa associada \xE0 linguagem natural: uma palavra por si s\xF3 tem um significado, mas esse significado \xE9 profundamente afetado pelo contexto, que pode ser qualquer outra palavra (ou palavras) antes ou depois da palavra que est\xE1 sendo estudada."),gs=u(),La=r("p"),Id=i("Agora que voc\xEA tem uma ideia do que s\xE3o as camadas de aten\xE7\xE3o, vamos dar uma olhada mais de perto na arquitetura do Transformer."),hs=u(),W=r("h2"),Ee=r("a"),Wo=r("span"),v(ma.$$.fragment),Nd=u(),Zo=r("span"),yd=i("A arquitetura original"),qs=u(),Ba=r("p"),Od=i("A arquitetura Transformer foi originalmente projetada para tradu\xE7\xE3o. Durante o treinamento, o codificador recebe entradas (frases) em um determinado idioma, enquanto o decodificador recebe as mesmas frases no idioma de destino desejado. No codificador, as camadas de aten\xE7\xE3o podem usar todas as palavras em uma frase (j\xE1 que, como acabamos de ver, a tradu\xE7\xE3o de uma determinada palavra pode ser dependente do que est\xE1 depois e antes dela na frase). O decodificador, no entanto, funciona sequencialmente e s\xF3 pode prestar aten\xE7\xE3o nas palavras da frase que ele j\xE1 traduziu (portanto, apenas as palavras anteriores \xE0 palavra que est\xE1 sendo gerada no momento). Por exemplo, quando previmos as tr\xEAs primeiras palavras do alvo traduzido, as entregamos ao decodificador que ent\xE3o usa todas as entradas do codificador para tentar prever a quarta palavra."),Es=u(),Da=r("p"),Gd=i("Para acelerar as coisas durante o treinamento (quando o modelo tem acesso \xE0s frases alvo), o decodificador \xE9 alimentado com todo o alvo, mas n\xE3o \xE9 permitido usar palavras futuras (se teve acesso \xE0 palavra na posi\xE7\xE3o 2 ao tentar prever a palavra na posi\xE7\xE3o 2, o problema n\xE3o seria muito dif\xEDcil!). Por exemplo, ao tentar prever a quarta palavra, a camada de aten\xE7\xE3o s\xF3 ter\xE1 acesso \xE0s palavras nas posi\xE7\xF5es 1 a 3."),_s=u(),Ca=r("p"),Md=i("A arquitetura original do Transformer ficou assim, com o codificador \xE0 esquerda e o decodificador \xE0 direita:"),bs=u(),Z=r("div"),ua=r("img"),Rd=u(),pa=r("img"),Ts=u(),Va=r("p"),Sd=i("Observe que a primeira camada de aten\xE7\xE3o em um bloco decodificador presta aten\xE7\xE3o a todas as entradas (passadas) do decodificador, mas a segunda camada de aten\xE7\xE3o usa a sa\xEDda do codificador. Ele pode, assim, acessar toda a frase de entrada para melhor prever a palavra atual. Isso \xE9 muito \xFAtil, pois diferentes idiomas podem ter regras gramaticais que colocam as palavras em ordens diferentes, ou algum contexto fornecido posteriormente na frase pode ser \xFAtil para determinar a melhor tradu\xE7\xE3o de uma determinada palavra."),$s=u(),_e=r("p"),Ld=i("A "),Ko=r("em"),Bd=i("m\xE1scara de aten\xE7\xE3o"),Dd=i(" tamb\xE9m pode ser usada no codificador/decodificador para evitar que o modelo preste aten\xE7\xE3o a algumas palavras especiais - por exemplo, a palavra de preenchimento especial usada para fazer com que todas as entradas tenham o mesmo comprimento ao agrupar frases."),Ps=u(),K=r("h2"),be=r("a"),er=r("span"),v(ca.$$.fragment),Cd=u(),ar=r("span"),Vd=i("Arquiteturas vs. checkpoints"),ks=u(),b=r("p"),Ud=i("\xC0 medida que nos aprofundarmos nos modelos do Transformer neste curso, voc\xEA ver\xE1 men\xE7\xF5es a "),or=r("em"),Hd=i("arquiteturas"),Jd=i(" e "),rr=r("em"),Fd=i("checkpoints"),Xd=i(", bem como "),sr=r("em"),Yd=i("modelos"),Qd=i(". Todos esses termos t\xEAm significados ligeiramente diferentes:"),As=u(),j=r("ul"),Ua=r("li"),tr=r("strong"),Wd=i("Arquitetura"),Zd=i(": Este \xE9 o esqueleto do modelo \u2014 a defini\xE7\xE3o de cada camada e cada opera\xE7\xE3o que acontece dentro do modelo."),Kd=u(),Ha=r("li"),ir=r("strong"),el=i("Checkpoints"),al=i(": Esses s\xE3o os pesos que ser\xE3o carregados em uma determinada arquitetura."),ol=u(),I=r("li"),dr=r("strong"),rl=i("Modelos"),sl=i(": Este \xE9 um termo abrangente que n\xE3o \xE9 t\xE3o preciso quanto \u201Carquitetura\u201D ou \u201Ccheckpoint\u201D: pode significar ambos. Este curso especificar\xE1 "),lr=r("em"),tl=i("arquitetura"),il=i(" ou "),nr=r("em"),dl=i("checkpoint"),ll=i(" quando for necess\xE1rio reduzir a ambiguidade."),ws=u(),N=r("p"),nl=i("Por exemplo, BERT \xE9 uma arquitetura enquanto "),mr=r("code"),ml=i("bert-base-cased"),ul=i(", um conjunto de pesos treinados pela equipe do Google para a primeira vers\xE3o do BERT, \xE9 um checkpoint. No entanto, pode-se dizer \u201Co modelo BERT\u201D e \u201Co modelo "),ur=r("code"),pl=i("bert-base-cased"),cl=i("\u201C."),this.h()},l(e){const l=Hm('[data-svelte="svelte-1phssyn"]',document.head);O=s(l,"META",{name:!0,content:!0}),l.forEach(o),qr=p(e),G=s(e,"H1",{class:!0});var zs=t(G);ee=s(zs,"A",{id:!0,class:!0,href:!0});var Bl=t(ee);Ka=s(Bl,"SPAN",{});var Dl=t(Ka);g($e.$$.fragment,Dl),Dl.forEach(o),Bl.forEach(o),rt=p(zs),eo=s(zs,"SPAN",{});var Cl=t(eo);st=d(Cl,"Como os Transformers trabalham?"),Cl.forEach(o),zs.forEach(o),Er=p(e),g(Pe.$$.fragment,e),_r=p(e),ha=s(e,"P",{});var Vl=t(ha);tt=d(Vl,"Nessa se\xE7\xE3o, n\xF3s olharemos para o alto n\xEDvel de arquitetura dos modelos Transformers."),Vl.forEach(o),br=p(e),M=s(e,"H2",{class:!0});var js=t(M);ae=s(js,"A",{id:!0,class:!0,href:!0});var Ul=t(ae);ao=s(Ul,"SPAN",{});var Hl=t(ao);g(ke.$$.fragment,Hl),Hl.forEach(o),Ul.forEach(o),it=p(js),oo=s(js,"SPAN",{});var Jl=t(oo);dt=d(Jl,"Um pouco da hist\xF3ria dos Transformers"),Jl.forEach(o),js.forEach(o),Tr=p(e),qa=s(e,"P",{});var Fl=t(qa);lt=d(Fl,"Aqui alguns pontos de refer\xEAncia na (pequena) hist\xF3ria dos modelos Transformers:"),Fl.forEach(o),$r=p(e),R=s(e,"DIV",{class:!0});var Is=t(R);Ae=s(Is,"IMG",{class:!0,src:!0,alt:!0}),nt=p(Is),we=s(Is,"IMG",{class:!0,src:!0,alt:!0}),Is.forEach(o),Pr=p(e),oe=s(e,"P",{});var Ns=t(oe);mt=d(Ns,"A "),xe=s(Ns,"A",{href:!0,rel:!0});var Xl=t(xe);ut=d(Xl,"arquitetura Transformer"),Xl.forEach(o),pt=d(Ns," foi introduzida em Junho de 2017. O foco de pesquisa original foi para tarefas de tradu\xE7\xE3o. Isso foi seguido pela introdu\xE7\xE3o de muitos modelos influentes, incluindo:"),Ns.forEach(o),kr=p(e),f=s(e,"UL",{});var T=t(f);ro=s(T,"LI",{});var Yl=t(ro);re=s(Yl,"P",{});var pr=t(re);so=s(pr,"STRONG",{});var Ql=t(so);ct=d(Ql,"Junho de 2018"),Ql.forEach(o),ft=d(pr,": "),ze=s(pr,"A",{href:!0,rel:!0});var Wl=t(ze);vt=d(Wl,"GPT"),Wl.forEach(o),gt=d(pr,", o primeiro modelo Transformer pr\xE9-treinado, usado para ajuste-fino em v\xE1rias tarefas de NLP e obtendo resultados estado-da-arte"),pr.forEach(o),Yl.forEach(o),ht=p(T),to=s(T,"LI",{});var Zl=t(to);se=s(Zl,"P",{});var cr=t(se);io=s(cr,"STRONG",{});var Kl=t(io);qt=d(Kl,"Outubro de 2018"),Kl.forEach(o),Et=d(cr,": "),je=s(cr,"A",{href:!0,rel:!0});var en=t(je);_t=d(en,"BERT"),en.forEach(o),bt=d(cr,", outro grande modelo pr\xE9-treinado, esse outro foi designado para produzir melhores resumos de senten\xE7as(mais sobre isso no pr\xF3ximo cap\xEDtulo!)"),cr.forEach(o),Zl.forEach(o),Tt=p(T),lo=s(T,"LI",{});var an=t(lo);te=s(an,"P",{});var fr=t(te);no=s(fr,"STRONG",{});var on=t(no);$t=d(on,"Fevereiro de 2019"),on.forEach(o),Pt=d(fr,": "),Ie=s(fr,"A",{href:!0,rel:!0});var rn=t(Ie);kt=d(rn,"GPT-2"),rn.forEach(o),At=d(fr,", uma melhor (e maior) vers\xE3o da GPT que n\xE3o foi imediatamente publicizado o seu lan\xE7amento devido a preocupa\xE7\xF5es \xE9ticas [N.T.: n\xE3o apenas por isso]"),fr.forEach(o),an.forEach(o),wt=p(T),mo=s(T,"LI",{});var sn=t(mo);ie=s(sn,"P",{});var vr=t(ie);uo=s(vr,"STRONG",{});var tn=t(uo);xt=d(tn,"Outubro de 2019"),tn.forEach(o),zt=d(vr,": "),Ne=s(vr,"A",{href:!0,rel:!0});var dn=t(Ne);jt=d(dn,"DistilBERT"),dn.forEach(o),It=d(vr,", uma vers\xE3o destilada do BERT que \xE9 60% mais r\xE1pidam 40% mais leve em mem\xF3ria, e ainda ret\xE9m 97% da performance do BERT"),vr.forEach(o),sn.forEach(o),Nt=p(T),po=s(T,"LI",{});var ln=t(po);$=s(ln,"P",{});var fa=t($);co=s(fa,"STRONG",{});var nn=t(co);yt=d(nn,"Outubro de 2019"),nn.forEach(o),Ot=d(fa,": "),ye=s(fa,"A",{href:!0,rel:!0});var mn=t(ye);Gt=d(mn,"BART"),mn.forEach(o),Mt=d(fa," e "),Oe=s(fa,"A",{href:!0,rel:!0});var un=t(Oe);Rt=d(un,"T5"),un.forEach(o),St=d(fa,", dois grandes modelos pr\xE9-treinados usando a mesma arquitetura do modelo original Transformer (os primeiros a fazerem at\xE9 ent\xE3o)"),fa.forEach(o),ln.forEach(o),Lt=p(T),fo=s(T,"LI",{});var pn=t(fo);P=s(pn,"P",{});var va=t(P);vo=s(va,"STRONG",{});var cn=t(vo);Bt=d(cn,"Maio de 2020"),cn.forEach(o),Dt=d(va,", "),Ge=s(va,"A",{href:!0,rel:!0});var fn=t(Ge);Ct=d(fn,"GPT-3"),fn.forEach(o),Vt=d(va,", uma vers\xE3o ainda maior da GPT-2 que \xE9 capaz de performar bem em uma variedade de tarefas sem a necessidade de ajuste-fino (chamado de aprendizagem"),go=s(va,"EM",{});var vn=t(go);Ut=d(vn,"zero-shot"),vn.forEach(o),Ht=d(va,")"),va.forEach(o),pn.forEach(o),T.forEach(o),Ar=p(e),Ea=s(e,"P",{});var gn=t(Ea);Jt=d(gn,"Esta lista est\xE1 longe de ser abrangente e destina-se apenas a destacar alguns dos diferentes tipos de modelos de Transformers. Em linhas gerais, eles podem ser agrupados em tr\xEAs categorias:"),gn.forEach(o),wr=p(e),k=s(e,"UL",{});var Ja=t(k);Me=s(Ja,"LI",{});var ys=t(Me);Ft=d(ys,"GPT-like (tamb\xE9m chamados de modelos Transformers "),ho=s(ys,"EM",{});var hn=t(ho);Xt=d(hn,"auto-regressivos"),hn.forEach(o),Yt=d(ys,")"),ys.forEach(o),Qt=p(Ja),Re=s(Ja,"LI",{});var Os=t(Re);Wt=d(Os,"BERT-like (tamb\xE9m chamados de modelos Transformers "),qo=s(Os,"EM",{});var qn=t(qo);Zt=d(qn,"auto-codificadores"),qn.forEach(o),Kt=d(Os,")"),Os.forEach(o),ei=p(Ja),Se=s(Ja,"LI",{});var Gs=t(Se);ai=d(Gs,"BART/T5-like (tamb\xE9m chamados de modelos Transformers "),Eo=s(Gs,"EM",{});var En=t(Eo);oi=d(En,"sequence-to-sequence"),En.forEach(o),ri=d(Gs,")"),Gs.forEach(o),Ja.forEach(o),xr=p(e),_a=s(e,"P",{});var _n=t(_a);si=d(_n,"Vamos mergulhar nessas fam\xEDlias com mais profundidade mais adiante"),_n.forEach(o),zr=p(e),S=s(e,"H2",{class:!0});var Ms=t(S);de=s(Ms,"A",{id:!0,class:!0,href:!0});var bn=t(de);_o=s(bn,"SPAN",{});var Tn=t(_o);g(Le.$$.fragment,Tn),Tn.forEach(o),bn.forEach(o),ti=p(Ms),bo=s(Ms,"SPAN",{});var $n=t(bo);ii=d($n,"Transformers s\xE3o modelos de linguagem"),$n.forEach(o),Ms.forEach(o),jr=p(e),le=s(e,"P",{});var Rs=t(le);di=d(Rs,"Todos os modelos de Transformer mencionados acima (GPT, BERT, BART, T5, etc.) foram treinados como "),To=s(Rs,"EM",{});var Pn=t(To);li=d(Pn,"modelos de linguagem"),Pn.forEach(o),ni=d(Rs,". Isso significa que eles foram treinados em grandes quantidades de texto bruto de forma auto-supervisionada. O aprendizado autossupervisionado \xE9 um tipo de treinamento no qual o objetivo \xE9 calculado automaticamente a partir das entradas do modelo. Isso significa que os humanos n\xE3o s\xE3o necess\xE1rios para rotular os dados!"),Rs.forEach(o),Ir=p(e),ne=s(e,"P",{});var Ss=t(ne);mi=d(Ss,"Este tipo de modelo desenvolve uma compreens\xE3o estat\xEDstica da linguagem em que foi treinado, mas n\xE3o \xE9 muito \xFAtil para tarefas pr\xE1ticas espec\xEDficas. Por causa disso, o modelo geral pr\xE9-treinado passa por um processo chamado "),$o=s(Ss,"EM",{});var kn=t($o);ui=d(kn,"aprendizagem de transfer\xEAncia"),kn.forEach(o),pi=d(Ss,". Durante esse processo, o modelo \xE9 ajustado de maneira supervisionada - ou seja, usando r\xF3tulos anotados por humanos - em uma determinada tarefa."),Ss.forEach(o),Nr=p(e),A=s(e,"P",{});var Fa=t(A);ci=d(Fa,"Um exemplo de tarefa \xE9 prever a pr\xF3xima palavra em uma frase depois de ler as "),Po=s(Fa,"EM",{});var An=t(Po);fi=d(An,"n"),An.forEach(o),vi=d(Fa," palavras anteriores. Isso \xE9 chamado de "),ko=s(Fa,"EM",{});var wn=t(ko);gi=d(wn,"modelagem de linguagem causal"),wn.forEach(o),hi=d(Fa," porque a sa\xEDda depende das entradas passadas e presentes, mas n\xE3o das futuras."),Fa.forEach(o),yr=p(e),L=s(e,"DIV",{class:!0});var Ls=t(L);Be=s(Ls,"IMG",{class:!0,src:!0,alt:!0}),qi=p(Ls),De=s(Ls,"IMG",{class:!0,src:!0,alt:!0}),Ls.forEach(o),Or=p(e),me=s(e,"P",{});var Bs=t(me);Ei=d(Bs,"Outro exemplo \xE9 a "),Ao=s(Bs,"EM",{});var xn=t(Ao);_i=d(xn,"modelagem de linguagem mascarada"),xn.forEach(o),bi=d(Bs,", na qual o modelo prev\xEA uma palavra mascarada na frase."),Bs.forEach(o),Gr=p(e),B=s(e,"DIV",{class:!0});var Ds=t(B);Ce=s(Ds,"IMG",{class:!0,src:!0,alt:!0}),Ti=p(Ds),Ve=s(Ds,"IMG",{class:!0,src:!0,alt:!0}),Ds.forEach(o),Mr=p(e),D=s(e,"H2",{class:!0});var Cs=t(D);ue=s(Cs,"A",{id:!0,class:!0,href:!0});var zn=t(ue);wo=s(zn,"SPAN",{});var jn=t(wo);g(Ue.$$.fragment,jn),jn.forEach(o),zn.forEach(o),$i=p(Cs),xo=s(Cs,"SPAN",{});var In=t(xo);Pi=d(In,"Transformers s\xE3o modelos grandes"),In.forEach(o),Cs.forEach(o),Rr=p(e),ba=s(e,"P",{});var Nn=t(ba);ki=d(Nn,"Al\xE9m de alguns outliers (como o DistilBERT), a estrat\xE9gia geral para obter melhor desempenho \xE9 aumentar os tamanhos dos modelos, bem como a quantidade de dados em que s\xE3o pr\xE9-treinados."),Nn.forEach(o),Sr=p(e),He=s(e,"DIV",{class:!0});var yn=t(He);Je=s(yn,"IMG",{src:!0,alt:!0,width:!0}),yn.forEach(o),Lr=p(e),Ta=s(e,"P",{});var On=t(Ta);Ai=d(On,"Infelizmente, treinar um modelo, especialmente um grande, requer uma grande quantidade de dados. Isso se torna muito caro em termos de tempo e recursos de computa\xE7\xE3o. At\xE9 se traduz em impacto ambiental, como pode ser visto no gr\xE1fico a seguir."),On.forEach(o),Br=p(e),C=s(e,"DIV",{class:!0});var Vs=t(C);Fe=s(Vs,"IMG",{class:!0,src:!0,alt:!0}),wi=p(Vs),Xe=s(Vs,"IMG",{class:!0,src:!0,alt:!0}),Vs.forEach(o),Dr=p(e),g(Ye.$$.fragment,e),Cr=p(e),$a=s(e,"P",{});var Gn=t($a);xi=d(Gn,"E isso mostra um projeto para um modelo (muito grande) liderado por uma equipe que tenta conscientemente reduzir o impacto ambiental do pr\xE9-treinamento. Os gastos de executar muitos testes para obter os melhores hiperpar\xE2metros seria ainda maior."),Gn.forEach(o),Vr=p(e),Pa=s(e,"P",{});var Mn=t(Pa);zi=d(Mn,"Imagine se cada vez que uma equipe de pesquisa, uma organiza\xE7\xE3o estudantil ou uma empresa quisesse treinar um modelo, o fizesse do zero. Isso levaria a custos globais enormes e desnecess\xE1rios!"),Mn.forEach(o),Ur=p(e),ka=s(e,"P",{});var Rn=t(ka);ji=d(Rn,"\xC9 por isso que compartilhar modelos de linguagem \xE9 fundamental: compartilhar os pesos treinados e construir em cima dos pesos j\xE1 treinados reduz o custo geral de computa\xE7\xE3o e os gastos de carbono da comunidade."),Rn.forEach(o),Hr=p(e),V=s(e,"H2",{class:!0});var Us=t(V);pe=s(Us,"A",{id:!0,class:!0,href:!0});var Sn=t(pe);zo=s(Sn,"SPAN",{});var Ln=t(zo);g(Qe.$$.fragment,Ln),Ln.forEach(o),Sn.forEach(o),Ii=p(Us),jo=s(Us,"SPAN",{});var Bn=t(jo);Ni=d(Bn,"Transfer\xEAncia de Aprendizagem"),Bn.forEach(o),Us.forEach(o),Jr=p(e),g(We.$$.fragment,e),Fr=p(e),Ze=s(e,"P",{});var fl=t(Ze);Io=s(fl,"EM",{});var Dn=t(Io);yi=d(Dn,"Pr\xE9-treinamento"),Dn.forEach(o),Oi=d(fl," \xE9 o ato de treinar um modelo do zero: os pesos s\xE3o inicializados aleatoriamente e o treinamento come\xE7a sem nenhum conhecimento pr\xE9vio."),fl.forEach(o),Xr=p(e),U=s(e,"DIV",{class:!0});var Hs=t(U);Ke=s(Hs,"IMG",{class:!0,src:!0,alt:!0}),Gi=p(Hs),ea=s(Hs,"IMG",{class:!0,src:!0,alt:!0}),Hs.forEach(o),Yr=p(e),Aa=s(e,"P",{});var Cn=t(Aa);Mi=d(Cn,"Esse pr\xE9-treinamento geralmente \xE9 feito em grandes quantidades de dados. Portanto, requer um corpus de dados muito grande e o treinamento pode levar v\xE1rias semanas."),Cn.forEach(o),Qr=p(e),H=s(e,"P",{});var gr=t(H);No=s(gr,"EM",{});var Vn=t(No);Ri=d(Vn,"Ajuste fino"),Vn.forEach(o),Si=d(gr,", por outro lado, \xE9 o treinamento feito "),yo=s(gr,"STRONG",{});var Un=t(yo);Li=d(Un,"ap\xF3s"),Un.forEach(o),Bi=d(gr," um modelo ter sido pr\xE9-treinado. Para realizar o ajuste fino, primeiro voc\xEA adquire um modelo de linguagem pr\xE9-treinado e, em seguida, realiza treinamento adicional com um conjunto de dados espec\xEDfico para sua tarefa. Espere - por que n\xE3o simplesmente treinar diretamente para a tarefa final? Existem algumas raz\xF5es:"),gr.forEach(o),Wr=p(e),w=s(e,"UL",{});var Xa=t(w);Oo=s(Xa,"LI",{});var Hn=t(Oo);Di=d(Hn,"O modelo pr\xE9-treinado j\xE1 foi treinado em um conjunto de dados que possui algumas semelhan\xE7as com o conjunto de dados de ajuste fino. O processo de ajuste fino \xE9, portanto, capaz de aproveitar o conhecimento adquirido pelo modelo inicial durante o pr\xE9-treinamento (por exemplo, com problemas de NLP, o modelo pr\xE9-treinado ter\xE1 algum tipo de compreens\xE3o estat\xEDstica da linguagem que voc\xEA est\xE1 usando para sua tarefa)."),Hn.forEach(o),Ci=p(Xa),Go=s(Xa,"LI",{});var Jn=t(Go);Vi=d(Jn,"Como o modelo pr\xE9-treinado j\xE1 foi treinado com muitos dados, o ajuste fino requer muito menos dados para obter resultados decentes."),Jn.forEach(o),Ui=p(Xa),Mo=s(Xa,"LI",{});var Fn=t(Mo);Hi=d(Fn,"Pela mesma raz\xE3o, a quantidade de tempo e recursos necess\xE1rios para obter bons resultados s\xE3o muito menores."),Fn.forEach(o),Xa.forEach(o),Zr=p(e),ce=s(e,"P",{});var Js=t(ce);Ji=d(Js,"Por exemplo, pode-se alavancar um modelo pr\xE9-treinado treinado no idioma ingl\xEAs e depois ajust\xE1-lo em um corpus arXiv, resultando em um modelo baseado em ci\xEAncia/pesquisa. O ajuste fino exigir\xE1 apenas uma quantidade limitada de dados: o conhecimento que o modelo pr\xE9-treinado adquiriu \xE9 \u201Ctransferido\u201D, da\xED o termo "),Ro=s(Js,"EM",{});var Xn=t(Ro);Fi=d(Xn,"aprendizagem de transfer\xEAncia"),Xn.forEach(o),Xi=d(Js,"."),Js.forEach(o),Kr=p(e),J=s(e,"DIV",{class:!0});var Fs=t(J);aa=s(Fs,"IMG",{class:!0,src:!0,alt:!0}),Yi=p(Fs),oa=s(Fs,"IMG",{class:!0,src:!0,alt:!0}),Fs.forEach(o),es=p(e),wa=s(e,"P",{});var Yn=t(wa);Qi=d(Yn,"O ajuste fino de um modelo, portanto, tem menores custos de tempo, dados, financeiros e ambientais. Tamb\xE9m \xE9 mais r\xE1pido e f\xE1cil iterar em diferentes esquemas de ajuste fino, pois o treinamento \xE9 menos restritivo do que um pr\xE9-treinamento completo."),Yn.forEach(o),as=p(e),xa=s(e,"P",{});var Qn=t(xa);Wi=d(Qn,"Esse processo tamb\xE9m alcan\xE7ar\xE1 melhores resultados do que treinar do zero (a menos que voc\xEA tenha muitos dados), e \xE9 por isso que voc\xEA deve sempre tentar alavancar um modelo pr\xE9-treinado - um o mais pr\xF3ximo poss\xEDvel da tarefa que voc\xEA tem em m\xE3os - e  ent\xE3o fazer seu ajuste fino."),Qn.forEach(o),os=p(e),F=s(e,"H2",{class:!0});var Xs=t(F);fe=s(Xs,"A",{id:!0,class:!0,href:!0});var Wn=t(fe);So=s(Wn,"SPAN",{});var Zn=t(So);g(ra.$$.fragment,Zn),Zn.forEach(o),Wn.forEach(o),Zi=p(Xs),Lo=s(Xs,"SPAN",{});var Kn=t(Lo);Ki=d(Kn,"Arquitetura geral"),Kn.forEach(o),Xs.forEach(o),rs=p(e),za=s(e,"P",{});var em=t(za);ed=d(em,"Nesta se\xE7\xE3o, veremos a arquitetura geral do modelo Transformer. N\xE3o se preocupe se voc\xEA n\xE3o entender alguns dos conceitos; h\xE1 se\xE7\xF5es detalhadas posteriormente cobrindo cada um dos componentes."),em.forEach(o),ss=p(e),g(sa.$$.fragment,e),ts=p(e),X=s(e,"H2",{class:!0});var Ys=t(X);ve=s(Ys,"A",{id:!0,class:!0,href:!0});var am=t(ve);Bo=s(am,"SPAN",{});var om=t(Bo);g(ta.$$.fragment,om),om.forEach(o),am.forEach(o),ad=p(Ys),Do=s(Ys,"SPAN",{});var rm=t(Do);od=d(rm,"Introdu\xE7\xE3o"),rm.forEach(o),Ys.forEach(o),is=p(e),ja=s(e,"P",{});var sm=t(ja);rd=d(sm,"O modelo \xE9 principalmente composto por dois blocos:"),sm.forEach(o),ds=p(e),ge=s(e,"UL",{});var Qs=t(ge);Ia=s(Qs,"LI",{});var vl=t(Ia);Co=s(vl,"STRONG",{});var tm=t(Co);sd=d(tm,"Codificador (esquerda)"),tm.forEach(o),td=d(vl,": O codificador recebe uma entrada e constr\xF3i uma representa\xE7\xE3o dela (seus recursos). Isso significa que o modelo \xE9 otimizado para adquirir entendimento da entrada."),vl.forEach(o),id=p(Qs),Na=s(Qs,"LI",{});var gl=t(Na);Vo=s(gl,"STRONG",{});var im=t(Vo);dd=d(im,"Decodificador (\xE0 direita)"),im.forEach(o),ld=d(gl,": O decodificador usa a representa\xE7\xE3o do codificador (recursos) junto com outras entradas para gerar uma sequ\xEAncia de destino. Isso significa que o modelo \xE9 otimizado para gerar sa\xEDdas."),gl.forEach(o),Qs.forEach(o),ls=p(e),Y=s(e,"DIV",{class:!0});var Ws=t(Y);ia=s(Ws,"IMG",{class:!0,src:!0,alt:!0}),nd=p(Ws),da=s(Ws,"IMG",{class:!0,src:!0,alt:!0}),Ws.forEach(o),ns=p(e),ya=s(e,"P",{});var dm=t(ya);md=d(dm,"Cada uma dessas partes pode ser usada de forma independente, dependendo da tarefa:"),dm.forEach(o),ms=p(e),x=s(e,"UL",{});var Ya=t(x);Oa=s(Ya,"LI",{});var hl=t(Oa);Uo=s(hl,"STRONG",{});var lm=t(Uo);ud=d(lm,"Modelos somente de codificador"),lm.forEach(o),pd=d(hl,": bom para tarefas que exigem compreens\xE3o da entrada, como classifica\xE7\xE3o de senten\xE7a e reconhecimento de entidade nomeada."),hl.forEach(o),cd=p(Ya),Ga=s(Ya,"LI",{});var ql=t(Ga);Ho=s(ql,"STRONG",{});var nm=t(Ho);fd=d(nm,"Modelos somente decodificadores"),nm.forEach(o),vd=d(ql,": bom para tarefas generativas, como gera\xE7\xE3o de texto."),ql.forEach(o),gd=p(Ya),he=s(Ya,"LI",{});var hr=t(he);Jo=s(hr,"STRONG",{});var mm=t(Jo);hd=d(mm,"Modelos de codificador-decodificador"),mm.forEach(o),qd=d(hr," ou "),Fo=s(hr,"STRONG",{});var um=t(Fo);Ed=d(um,"modelos de sequ\xEAncia a sequ\xEAncia"),um.forEach(o),_d=d(hr,": bom para tarefas generativas que exigem uma entrada, como tradu\xE7\xE3o ou resumo. (corrigit sequence to sequence)"),hr.forEach(o),Ya.forEach(o),us=p(e),Ma=s(e,"P",{});var pm=t(Ma);bd=d(pm,"Vamos mergulhar nessas arquiteturas de forma independente em se\xE7\xF5es posteriores."),pm.forEach(o),ps=p(e),Q=s(e,"H2",{class:!0});var Zs=t(Q);qe=s(Zs,"A",{id:!0,class:!0,href:!0});var cm=t(qe);Xo=s(cm,"SPAN",{});var fm=t(Xo);g(la.$$.fragment,fm),fm.forEach(o),cm.forEach(o),Td=p(Zs),Yo=s(Zs,"SPAN",{});var vm=t(Yo);$d=d(vm,"Camadas de Aten\xE7\xE3o"),vm.forEach(o),Zs.forEach(o),cs=p(e),z=s(e,"P",{});var Qa=t(z);Pd=d(Qa,"Uma caracter\xEDstica chave dos modelos Transformer \xE9 que eles s\xE3o constru\xEDdos com camadas especiais chamadas "),Qo=s(Qa,"EM",{});var gm=t(Qo);kd=d(gm,"camadas de aten\xE7\xE3o"),gm.forEach(o),Ad=d(Qa,". Na verdade, o t\xEDtulo do artigo que apresenta a arquitetura do Transformer era "),na=s(Qa,"A",{href:!0,rel:!0});var hm=t(na);wd=d(hm,"\u201CAten\xE7\xE3o \xE9 tudo que voc\xEA precisa\u201D"),hm.forEach(o),xd=d(Qa,"! Exploraremos os detalhes das camadas de aten\xE7\xE3o posteriormente no curso; por enquanto, tudo o que voc\xEA precisa saber \xE9 que essa camada dir\xE1 ao modelo para prestar aten\xE7\xE3o espec\xEDfica a certas palavras na frase que voc\xEA passou (e mais ou menos ignorar as outras) ao lidar com a representa\xE7\xE3o de cada palavra."),Qa.forEach(o),fs=p(e),Ra=s(e,"P",{});var qm=t(Ra);zd=d(qm,"Para contextualizar, considere a tarefa de traduzir o texto do portugu\xEAs para o franc\xEAs. Dada a entrada \u201CVoc\xEA gosta deste curso\u201D, um modelo de tradu\xE7\xE3o precisar\xE1 atender tamb\xE9m \xE0 palavra adjacente \u201CVoc\xEA\u201D para obter a tradu\xE7\xE3o adequada para a palavra \u201Cgosta\u201D, pois em franc\xEAs o verbo \u201Cgostar\u201D \xE9 conjugado de forma diferente dependendo o sujeito. O resto da frase, no entanto, n\xE3o \xE9 \xFAtil para a tradu\xE7\xE3o dessa palavra. Na mesma linha, ao traduzir \u201Cdeste\u201D o modelo tamb\xE9m precisar\xE1 prestar aten\xE7\xE3o \xE0 palavra \u201Ccurso\u201D, pois \u201Cdeste\u201D traduz-se de forma diferente dependendo se o substantivo associado \xE9 masculino ou feminino. Novamente, as outras palavras na frase n\xE3o importar\xE3o para a tradu\xE7\xE3o de \u201Cdeste\u201D. Com frases mais complexas (e regras gramaticais mais complexas), o modelo precisaria prestar aten\xE7\xE3o especial \xE0s palavras que podem aparecer mais distantes na frase para traduzir adequadamente cada palavra."),qm.forEach(o),vs=p(e),Sa=s(e,"P",{});var Em=t(Sa);jd=d(Em,"O mesmo conceito se aplica a qualquer tarefa associada \xE0 linguagem natural: uma palavra por si s\xF3 tem um significado, mas esse significado \xE9 profundamente afetado pelo contexto, que pode ser qualquer outra palavra (ou palavras) antes ou depois da palavra que est\xE1 sendo estudada."),Em.forEach(o),gs=p(e),La=s(e,"P",{});var _m=t(La);Id=d(_m,"Agora que voc\xEA tem uma ideia do que s\xE3o as camadas de aten\xE7\xE3o, vamos dar uma olhada mais de perto na arquitetura do Transformer."),_m.forEach(o),hs=p(e),W=s(e,"H2",{class:!0});var Ks=t(W);Ee=s(Ks,"A",{id:!0,class:!0,href:!0});var bm=t(Ee);Wo=s(bm,"SPAN",{});var Tm=t(Wo);g(ma.$$.fragment,Tm),Tm.forEach(o),bm.forEach(o),Nd=p(Ks),Zo=s(Ks,"SPAN",{});var $m=t(Zo);yd=d($m,"A arquitetura original"),$m.forEach(o),Ks.forEach(o),qs=p(e),Ba=s(e,"P",{});var Pm=t(Ba);Od=d(Pm,"A arquitetura Transformer foi originalmente projetada para tradu\xE7\xE3o. Durante o treinamento, o codificador recebe entradas (frases) em um determinado idioma, enquanto o decodificador recebe as mesmas frases no idioma de destino desejado. No codificador, as camadas de aten\xE7\xE3o podem usar todas as palavras em uma frase (j\xE1 que, como acabamos de ver, a tradu\xE7\xE3o de uma determinada palavra pode ser dependente do que est\xE1 depois e antes dela na frase). O decodificador, no entanto, funciona sequencialmente e s\xF3 pode prestar aten\xE7\xE3o nas palavras da frase que ele j\xE1 traduziu (portanto, apenas as palavras anteriores \xE0 palavra que est\xE1 sendo gerada no momento). Por exemplo, quando previmos as tr\xEAs primeiras palavras do alvo traduzido, as entregamos ao decodificador que ent\xE3o usa todas as entradas do codificador para tentar prever a quarta palavra."),Pm.forEach(o),Es=p(e),Da=s(e,"P",{});var km=t(Da);Gd=d(km,"Para acelerar as coisas durante o treinamento (quando o modelo tem acesso \xE0s frases alvo), o decodificador \xE9 alimentado com todo o alvo, mas n\xE3o \xE9 permitido usar palavras futuras (se teve acesso \xE0 palavra na posi\xE7\xE3o 2 ao tentar prever a palavra na posi\xE7\xE3o 2, o problema n\xE3o seria muito dif\xEDcil!). Por exemplo, ao tentar prever a quarta palavra, a camada de aten\xE7\xE3o s\xF3 ter\xE1 acesso \xE0s palavras nas posi\xE7\xF5es 1 a 3."),km.forEach(o),_s=p(e),Ca=s(e,"P",{});var Am=t(Ca);Md=d(Am,"A arquitetura original do Transformer ficou assim, com o codificador \xE0 esquerda e o decodificador \xE0 direita:"),Am.forEach(o),bs=p(e),Z=s(e,"DIV",{class:!0});var et=t(Z);ua=s(et,"IMG",{class:!0,src:!0,alt:!0}),Rd=p(et),pa=s(et,"IMG",{class:!0,src:!0,alt:!0}),et.forEach(o),Ts=p(e),Va=s(e,"P",{});var wm=t(Va);Sd=d(wm,"Observe que a primeira camada de aten\xE7\xE3o em um bloco decodificador presta aten\xE7\xE3o a todas as entradas (passadas) do decodificador, mas a segunda camada de aten\xE7\xE3o usa a sa\xEDda do codificador. Ele pode, assim, acessar toda a frase de entrada para melhor prever a palavra atual. Isso \xE9 muito \xFAtil, pois diferentes idiomas podem ter regras gramaticais que colocam as palavras em ordens diferentes, ou algum contexto fornecido posteriormente na frase pode ser \xFAtil para determinar a melhor tradu\xE7\xE3o de uma determinada palavra."),wm.forEach(o),$s=p(e),_e=s(e,"P",{});var at=t(_e);Ld=d(at,"A "),Ko=s(at,"EM",{});var xm=t(Ko);Bd=d(xm,"m\xE1scara de aten\xE7\xE3o"),xm.forEach(o),Dd=d(at," tamb\xE9m pode ser usada no codificador/decodificador para evitar que o modelo preste aten\xE7\xE3o a algumas palavras especiais - por exemplo, a palavra de preenchimento especial usada para fazer com que todas as entradas tenham o mesmo comprimento ao agrupar frases."),at.forEach(o),Ps=p(e),K=s(e,"H2",{class:!0});var ot=t(K);be=s(ot,"A",{id:!0,class:!0,href:!0});var zm=t(be);er=s(zm,"SPAN",{});var jm=t(er);g(ca.$$.fragment,jm),jm.forEach(o),zm.forEach(o),Cd=p(ot),ar=s(ot,"SPAN",{});var Im=t(ar);Vd=d(Im,"Arquiteturas vs. checkpoints"),Im.forEach(o),ot.forEach(o),ks=p(e),b=s(e,"P",{});var Te=t(b);Ud=d(Te,"\xC0 medida que nos aprofundarmos nos modelos do Transformer neste curso, voc\xEA ver\xE1 men\xE7\xF5es a "),or=s(Te,"EM",{});var Nm=t(or);Hd=d(Nm,"arquiteturas"),Nm.forEach(o),Jd=d(Te," e "),rr=s(Te,"EM",{});var ym=t(rr);Fd=d(ym,"checkpoints"),ym.forEach(o),Xd=d(Te,", bem como "),sr=s(Te,"EM",{});var Om=t(sr);Yd=d(Om,"modelos"),Om.forEach(o),Qd=d(Te,". Todos esses termos t\xEAm significados ligeiramente diferentes:"),Te.forEach(o),As=p(e),j=s(e,"UL",{});var Wa=t(j);Ua=s(Wa,"LI",{});var El=t(Ua);tr=s(El,"STRONG",{});var Gm=t(tr);Wd=d(Gm,"Arquitetura"),Gm.forEach(o),Zd=d(El,": Este \xE9 o esqueleto do modelo \u2014 a defini\xE7\xE3o de cada camada e cada opera\xE7\xE3o que acontece dentro do modelo."),El.forEach(o),Kd=p(Wa),Ha=s(Wa,"LI",{});var _l=t(Ha);ir=s(_l,"STRONG",{});var Mm=t(ir);el=d(Mm,"Checkpoints"),Mm.forEach(o),al=d(_l,": Esses s\xE3o os pesos que ser\xE3o carregados em uma determinada arquitetura."),_l.forEach(o),ol=p(Wa),I=s(Wa,"LI",{});var ga=t(I);dr=s(ga,"STRONG",{});var Rm=t(dr);rl=d(Rm,"Modelos"),Rm.forEach(o),sl=d(ga,": Este \xE9 um termo abrangente que n\xE3o \xE9 t\xE3o preciso quanto \u201Carquitetura\u201D ou \u201Ccheckpoint\u201D: pode significar ambos. Este curso especificar\xE1 "),lr=s(ga,"EM",{});var Sm=t(lr);tl=d(Sm,"arquitetura"),Sm.forEach(o),il=d(ga," ou "),nr=s(ga,"EM",{});var Lm=t(nr);dl=d(Lm,"checkpoint"),Lm.forEach(o),ll=d(ga," quando for necess\xE1rio reduzir a ambiguidade."),ga.forEach(o),Wa.forEach(o),ws=p(e),N=s(e,"P",{});var Za=t(N);nl=d(Za,"Por exemplo, BERT \xE9 uma arquitetura enquanto "),mr=s(Za,"CODE",{});var Bm=t(mr);ml=d(Bm,"bert-base-cased"),Bm.forEach(o),ul=d(Za,", um conjunto de pesos treinados pela equipe do Google para a primeira vers\xE3o do BERT, \xE9 um checkpoint. No entanto, pode-se dizer \u201Co modelo BERT\u201D e \u201Co modelo "),ur=s(Za,"CODE",{});var Dm=t(ur);pl=d(Dm,"bert-base-cased"),Dm.forEach(o),cl=d(Za,"\u201C."),Za.forEach(o),this.h()},h(){m(O,"name","hf:doc:metadata"),m(O,"content",JSON.stringify(Qm)),m(ee,"id","como-os-transformers-trabalham"),m(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ee,"href","#como-os-transformers-trabalham"),m(G,"class","relative group"),m(ae,"id","um-pouco-da-histria-dos-transformers"),m(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ae,"href","#um-pouco-da-histria-dos-transformers"),m(M,"class","relative group"),m(Ae,"class","block dark:hidden"),c(Ae.src,$l="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||m(Ae,"src",$l),m(Ae,"alt","A brief chronology of Transformers models."),m(we,"class","hidden dark:block"),c(we.src,Pl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||m(we,"src",Pl),m(we,"alt","A brief chronology of Transformers models."),m(R,"class","flex justify-center"),m(xe,"href","https://arxiv.org/abs/1706.03762"),m(xe,"rel","nofollow"),m(ze,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),m(ze,"rel","nofollow"),m(je,"href","https://arxiv.org/abs/1810.04805"),m(je,"rel","nofollow"),m(Ie,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),m(Ie,"rel","nofollow"),m(Ne,"href","https://arxiv.org/abs/1910.01108"),m(Ne,"rel","nofollow"),m(ye,"href","https://arxiv.org/abs/1910.13461"),m(ye,"rel","nofollow"),m(Oe,"href","https://arxiv.org/abs/1910.10683"),m(Oe,"rel","nofollow"),m(Ge,"href","https://arxiv.org/abs/2005.14165"),m(Ge,"rel","nofollow"),m(de,"id","transformers-so-modelos-de-linguagem"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#transformers-so-modelos-de-linguagem"),m(S,"class","relative group"),m(Be,"class","block dark:hidden"),c(Be.src,kl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||m(Be,"src",kl),m(Be,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),m(De,"class","hidden dark:block"),c(De.src,Al="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||m(De,"src",Al),m(De,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),m(L,"class","flex justify-center"),m(Ce,"class","block dark:hidden"),c(Ce.src,wl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||m(Ce,"src",wl),m(Ce,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),m(Ve,"class","hidden dark:block"),c(Ve.src,xl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||m(Ve,"src",xl),m(Ve,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),m(B,"class","flex justify-center"),m(ue,"id","transformers-so-modelos-grandes"),m(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ue,"href","#transformers-so-modelos-grandes"),m(D,"class","relative group"),c(Je.src,zl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||m(Je,"src",zl),m(Je,"alt","Number of parameters of recent Transformers models"),m(Je,"width","90%"),m(He,"class","flex justify-center"),m(Fe,"class","block dark:hidden"),c(Fe.src,jl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||m(Fe,"src",jl),m(Fe,"alt","The carbon footprint of a large language model."),m(Xe,"class","hidden dark:block"),c(Xe.src,Il="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||m(Xe,"src",Il),m(Xe,"alt","The carbon footprint of a large language model."),m(C,"class","flex justify-center"),m(pe,"id","transferncia-de-aprendizagem"),m(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(pe,"href","#transferncia-de-aprendizagem"),m(V,"class","relative group"),m(Ke,"class","block dark:hidden"),c(Ke.src,Nl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||m(Ke,"src",Nl),m(Ke,"alt","The pretraining of a language model is costly in both time and money."),m(ea,"class","hidden dark:block"),c(ea.src,yl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||m(ea,"src",yl),m(ea,"alt","The pretraining of a language model is costly in both time and money."),m(U,"class","flex justify-center"),m(aa,"class","block dark:hidden"),c(aa.src,Ol="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||m(aa,"src",Ol),m(aa,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),m(oa,"class","hidden dark:block"),c(oa.src,Gl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||m(oa,"src",Gl),m(oa,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),m(J,"class","flex justify-center"),m(fe,"id","arquitetura-geral"),m(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(fe,"href","#arquitetura-geral"),m(F,"class","relative group"),m(ve,"id","introduo"),m(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ve,"href","#introduo"),m(X,"class","relative group"),m(ia,"class","block dark:hidden"),c(ia.src,Ml="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||m(ia,"src",Ml),m(ia,"alt","Architecture of a Transformers models"),m(da,"class","hidden dark:block"),c(da.src,Rl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||m(da,"src",Rl),m(da,"alt","Architecture of a Transformers models"),m(Y,"class","flex justify-center"),m(qe,"id","camadas-de-ateno"),m(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(qe,"href","#camadas-de-ateno"),m(Q,"class","relative group"),m(na,"href","https://arxiv.org/abs/1706.03762"),m(na,"rel","nofollow"),m(Ee,"id","a-arquitetura-original"),m(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ee,"href","#a-arquitetura-original"),m(W,"class","relative group"),m(ua,"class","block dark:hidden"),c(ua.src,Sl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||m(ua,"src",Sl),m(ua,"alt","Architecture of a Transformers models"),m(pa,"class","hidden dark:block"),c(pa.src,Ll="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||m(pa,"src",Ll),m(pa,"alt","Architecture of a Transformers models"),m(Z,"class","flex justify-center"),m(be,"id","arquiteturas-vs-checkpoints"),m(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(be,"href","#arquiteturas-vs-checkpoints"),m(K,"class","relative group")},m(e,l){a(document.head,O),n(e,qr,l),n(e,G,l),a(G,ee),a(ee,Ka),h($e,Ka,null),a(G,rt),a(G,eo),a(eo,st),n(e,Er,l),h(Pe,e,l),n(e,_r,l),n(e,ha,l),a(ha,tt),n(e,br,l),n(e,M,l),a(M,ae),a(ae,ao),h(ke,ao,null),a(M,it),a(M,oo),a(oo,dt),n(e,Tr,l),n(e,qa,l),a(qa,lt),n(e,$r,l),n(e,R,l),a(R,Ae),a(R,nt),a(R,we),n(e,Pr,l),n(e,oe,l),a(oe,mt),a(oe,xe),a(xe,ut),a(oe,pt),n(e,kr,l),n(e,f,l),a(f,ro),a(ro,re),a(re,so),a(so,ct),a(re,ft),a(re,ze),a(ze,vt),a(re,gt),a(f,ht),a(f,to),a(to,se),a(se,io),a(io,qt),a(se,Et),a(se,je),a(je,_t),a(se,bt),a(f,Tt),a(f,lo),a(lo,te),a(te,no),a(no,$t),a(te,Pt),a(te,Ie),a(Ie,kt),a(te,At),a(f,wt),a(f,mo),a(mo,ie),a(ie,uo),a(uo,xt),a(ie,zt),a(ie,Ne),a(Ne,jt),a(ie,It),a(f,Nt),a(f,po),a(po,$),a($,co),a(co,yt),a($,Ot),a($,ye),a(ye,Gt),a($,Mt),a($,Oe),a(Oe,Rt),a($,St),a(f,Lt),a(f,fo),a(fo,P),a(P,vo),a(vo,Bt),a(P,Dt),a(P,Ge),a(Ge,Ct),a(P,Vt),a(P,go),a(go,Ut),a(P,Ht),n(e,Ar,l),n(e,Ea,l),a(Ea,Jt),n(e,wr,l),n(e,k,l),a(k,Me),a(Me,Ft),a(Me,ho),a(ho,Xt),a(Me,Yt),a(k,Qt),a(k,Re),a(Re,Wt),a(Re,qo),a(qo,Zt),a(Re,Kt),a(k,ei),a(k,Se),a(Se,ai),a(Se,Eo),a(Eo,oi),a(Se,ri),n(e,xr,l),n(e,_a,l),a(_a,si),n(e,zr,l),n(e,S,l),a(S,de),a(de,_o),h(Le,_o,null),a(S,ti),a(S,bo),a(bo,ii),n(e,jr,l),n(e,le,l),a(le,di),a(le,To),a(To,li),a(le,ni),n(e,Ir,l),n(e,ne,l),a(ne,mi),a(ne,$o),a($o,ui),a(ne,pi),n(e,Nr,l),n(e,A,l),a(A,ci),a(A,Po),a(Po,fi),a(A,vi),a(A,ko),a(ko,gi),a(A,hi),n(e,yr,l),n(e,L,l),a(L,Be),a(L,qi),a(L,De),n(e,Or,l),n(e,me,l),a(me,Ei),a(me,Ao),a(Ao,_i),a(me,bi),n(e,Gr,l),n(e,B,l),a(B,Ce),a(B,Ti),a(B,Ve),n(e,Mr,l),n(e,D,l),a(D,ue),a(ue,wo),h(Ue,wo,null),a(D,$i),a(D,xo),a(xo,Pi),n(e,Rr,l),n(e,ba,l),a(ba,ki),n(e,Sr,l),n(e,He,l),a(He,Je),n(e,Lr,l),n(e,Ta,l),a(Ta,Ai),n(e,Br,l),n(e,C,l),a(C,Fe),a(C,wi),a(C,Xe),n(e,Dr,l),h(Ye,e,l),n(e,Cr,l),n(e,$a,l),a($a,xi),n(e,Vr,l),n(e,Pa,l),a(Pa,zi),n(e,Ur,l),n(e,ka,l),a(ka,ji),n(e,Hr,l),n(e,V,l),a(V,pe),a(pe,zo),h(Qe,zo,null),a(V,Ii),a(V,jo),a(jo,Ni),n(e,Jr,l),h(We,e,l),n(e,Fr,l),n(e,Ze,l),a(Ze,Io),a(Io,yi),a(Ze,Oi),n(e,Xr,l),n(e,U,l),a(U,Ke),a(U,Gi),a(U,ea),n(e,Yr,l),n(e,Aa,l),a(Aa,Mi),n(e,Qr,l),n(e,H,l),a(H,No),a(No,Ri),a(H,Si),a(H,yo),a(yo,Li),a(H,Bi),n(e,Wr,l),n(e,w,l),a(w,Oo),a(Oo,Di),a(w,Ci),a(w,Go),a(Go,Vi),a(w,Ui),a(w,Mo),a(Mo,Hi),n(e,Zr,l),n(e,ce,l),a(ce,Ji),a(ce,Ro),a(Ro,Fi),a(ce,Xi),n(e,Kr,l),n(e,J,l),a(J,aa),a(J,Yi),a(J,oa),n(e,es,l),n(e,wa,l),a(wa,Qi),n(e,as,l),n(e,xa,l),a(xa,Wi),n(e,os,l),n(e,F,l),a(F,fe),a(fe,So),h(ra,So,null),a(F,Zi),a(F,Lo),a(Lo,Ki),n(e,rs,l),n(e,za,l),a(za,ed),n(e,ss,l),h(sa,e,l),n(e,ts,l),n(e,X,l),a(X,ve),a(ve,Bo),h(ta,Bo,null),a(X,ad),a(X,Do),a(Do,od),n(e,is,l),n(e,ja,l),a(ja,rd),n(e,ds,l),n(e,ge,l),a(ge,Ia),a(Ia,Co),a(Co,sd),a(Ia,td),a(ge,id),a(ge,Na),a(Na,Vo),a(Vo,dd),a(Na,ld),n(e,ls,l),n(e,Y,l),a(Y,ia),a(Y,nd),a(Y,da),n(e,ns,l),n(e,ya,l),a(ya,md),n(e,ms,l),n(e,x,l),a(x,Oa),a(Oa,Uo),a(Uo,ud),a(Oa,pd),a(x,cd),a(x,Ga),a(Ga,Ho),a(Ho,fd),a(Ga,vd),a(x,gd),a(x,he),a(he,Jo),a(Jo,hd),a(he,qd),a(he,Fo),a(Fo,Ed),a(he,_d),n(e,us,l),n(e,Ma,l),a(Ma,bd),n(e,ps,l),n(e,Q,l),a(Q,qe),a(qe,Xo),h(la,Xo,null),a(Q,Td),a(Q,Yo),a(Yo,$d),n(e,cs,l),n(e,z,l),a(z,Pd),a(z,Qo),a(Qo,kd),a(z,Ad),a(z,na),a(na,wd),a(z,xd),n(e,fs,l),n(e,Ra,l),a(Ra,zd),n(e,vs,l),n(e,Sa,l),a(Sa,jd),n(e,gs,l),n(e,La,l),a(La,Id),n(e,hs,l),n(e,W,l),a(W,Ee),a(Ee,Wo),h(ma,Wo,null),a(W,Nd),a(W,Zo),a(Zo,yd),n(e,qs,l),n(e,Ba,l),a(Ba,Od),n(e,Es,l),n(e,Da,l),a(Da,Gd),n(e,_s,l),n(e,Ca,l),a(Ca,Md),n(e,bs,l),n(e,Z,l),a(Z,ua),a(Z,Rd),a(Z,pa),n(e,Ts,l),n(e,Va,l),a(Va,Sd),n(e,$s,l),n(e,_e,l),a(_e,Ld),a(_e,Ko),a(Ko,Bd),a(_e,Dd),n(e,Ps,l),n(e,K,l),a(K,be),a(be,er),h(ca,er,null),a(K,Cd),a(K,ar),a(ar,Vd),n(e,ks,l),n(e,b,l),a(b,Ud),a(b,or),a(or,Hd),a(b,Jd),a(b,rr),a(rr,Fd),a(b,Xd),a(b,sr),a(sr,Yd),a(b,Qd),n(e,As,l),n(e,j,l),a(j,Ua),a(Ua,tr),a(tr,Wd),a(Ua,Zd),a(j,Kd),a(j,Ha),a(Ha,ir),a(ir,el),a(Ha,al),a(j,ol),a(j,I),a(I,dr),a(dr,rl),a(I,sl),a(I,lr),a(lr,tl),a(I,il),a(I,nr),a(nr,dl),a(I,ll),n(e,ws,l),n(e,N,l),a(N,nl),a(N,mr),a(mr,ml),a(N,ul),a(N,ur),a(ur,pl),a(N,cl),xs=!0},p:Jm,i(e){xs||(q($e.$$.fragment,e),q(Pe.$$.fragment,e),q(ke.$$.fragment,e),q(Le.$$.fragment,e),q(Ue.$$.fragment,e),q(Ye.$$.fragment,e),q(Qe.$$.fragment,e),q(We.$$.fragment,e),q(ra.$$.fragment,e),q(sa.$$.fragment,e),q(ta.$$.fragment,e),q(la.$$.fragment,e),q(ma.$$.fragment,e),q(ca.$$.fragment,e),xs=!0)},o(e){E($e.$$.fragment,e),E(Pe.$$.fragment,e),E(ke.$$.fragment,e),E(Le.$$.fragment,e),E(Ue.$$.fragment,e),E(Ye.$$.fragment,e),E(Qe.$$.fragment,e),E(We.$$.fragment,e),E(ra.$$.fragment,e),E(sa.$$.fragment,e),E(ta.$$.fragment,e),E(la.$$.fragment,e),E(ma.$$.fragment,e),E(ca.$$.fragment,e),xs=!1},d(e){o(O),e&&o(qr),e&&o(G),_($e),e&&o(Er),_(Pe,e),e&&o(_r),e&&o(ha),e&&o(br),e&&o(M),_(ke),e&&o(Tr),e&&o(qa),e&&o($r),e&&o(R),e&&o(Pr),e&&o(oe),e&&o(kr),e&&o(f),e&&o(Ar),e&&o(Ea),e&&o(wr),e&&o(k),e&&o(xr),e&&o(_a),e&&o(zr),e&&o(S),_(Le),e&&o(jr),e&&o(le),e&&o(Ir),e&&o(ne),e&&o(Nr),e&&o(A),e&&o(yr),e&&o(L),e&&o(Or),e&&o(me),e&&o(Gr),e&&o(B),e&&o(Mr),e&&o(D),_(Ue),e&&o(Rr),e&&o(ba),e&&o(Sr),e&&o(He),e&&o(Lr),e&&o(Ta),e&&o(Br),e&&o(C),e&&o(Dr),_(Ye,e),e&&o(Cr),e&&o($a),e&&o(Vr),e&&o(Pa),e&&o(Ur),e&&o(ka),e&&o(Hr),e&&o(V),_(Qe),e&&o(Jr),_(We,e),e&&o(Fr),e&&o(Ze),e&&o(Xr),e&&o(U),e&&o(Yr),e&&o(Aa),e&&o(Qr),e&&o(H),e&&o(Wr),e&&o(w),e&&o(Zr),e&&o(ce),e&&o(Kr),e&&o(J),e&&o(es),e&&o(wa),e&&o(as),e&&o(xa),e&&o(os),e&&o(F),_(ra),e&&o(rs),e&&o(za),e&&o(ss),_(sa,e),e&&o(ts),e&&o(X),_(ta),e&&o(is),e&&o(ja),e&&o(ds),e&&o(ge),e&&o(ls),e&&o(Y),e&&o(ns),e&&o(ya),e&&o(ms),e&&o(x),e&&o(us),e&&o(Ma),e&&o(ps),e&&o(Q),_(la),e&&o(cs),e&&o(z),e&&o(fs),e&&o(Ra),e&&o(vs),e&&o(Sa),e&&o(gs),e&&o(La),e&&o(hs),e&&o(W),_(ma),e&&o(qs),e&&o(Ba),e&&o(Es),e&&o(Da),e&&o(_s),e&&o(Ca),e&&o(bs),e&&o(Z),e&&o(Ts),e&&o(Va),e&&o($s),e&&o(_e),e&&o(Ps),e&&o(K),_(ca),e&&o(ks),e&&o(b),e&&o(As),e&&o(j),e&&o(ws),e&&o(N)}}}const Qm={local:"como-os-transformers-trabalham",sections:[{local:"um-pouco-da-histria-dos-transformers",title:"Um pouco da hist\xF3ria dos Transformers"},{local:"transformers-so-modelos-de-linguagem",title:"Transformers s\xE3o modelos de linguagem"},{local:"transformers-so-modelos-grandes",title:"Transformers s\xE3o modelos grandes"},{local:"transferncia-de-aprendizagem",title:"Transfer\xEAncia de Aprendizagem"},{local:"arquitetura-geral",title:"Arquitetura geral"},{local:"introduo",title:"Introdu\xE7\xE3o"},{local:"camadas-de-ateno",title:"Camadas de Aten\xE7\xE3o"},{local:"a-arquitetura-original",title:"A arquitetura original"},{local:"arquiteturas-vs-checkpoints",title:"Arquiteturas vs. checkpoints"}],title:"Como os Transformers trabalham?"};function Wm(Tl){return Fm(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ou extends Cm{constructor(O){super();Vm(this,O,Wm,Ym,Um,{})}}export{ou as default,Qm as metadata};
