import{S as Yr,i as Kr,s as Vr,e as r,k as c,w as be,t as n,M as Xr,c as s,d as o,m as d,a as t,x as Ee,h as i,b as l,N as Wr,G as a,g as m,y as _e,L as Zr,q as we,o as Le,B as qe,v as es}from"../../chunks/vendor-hf-doc-builder.js";import{Y as as}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Na}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as os}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function rs(er){let b,We,E,A,Pe,M,Aa,ye,Ta,Ye,C,Ke,_,T,$e,F,Sa,Ne,za,Ve,I,Xe,p,Ha,D,ka,Ma,O,Ca,Fa,x,Ia,Da,G,Oa,xa,R,Ga,Ra,j,ja,Ba,Ze,w,S,Ae,B,Ua,Te,Ja,ea,ce,Qa,aa,L,U,ar,Wa,J,or,oa,z,Q,Ya,W,Ka,Va,Xa,Se,Za,ra,de,eo,sa,h,ze,ao,oo,v,ro,Y,so,to,K,no,io,V,lo,uo,q,mo,X,co,po,Z,fo,ho,ta,H,vo,ee,go,bo,na,P,k,He,ae,Eo,ke,_o,ia,pe,wo,la,oe,Me,Lo,qo,ua,re,Ce,Po,yo,ma,y,Fe,$o,No,Ie,se,Ao,To,ca,te,De,So,zo,da,ne,Oe,Ho,ko,pa,$,xe,Mo,Co,ie,Fo,Io,fa,N,Ge,Do,Oo,le,xo,Go,ha,fe,Ro,va,g,ue,jo,Re,Bo,Uo,Jo,je,Qo,Wo,Be,Yo,ga;return M=new Na({}),C=new os({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),F=new Na({}),I=new as({props:{id:"00GKzGyWFEs"}}),B=new Na({}),ae=new Na({}),{c(){b=r("meta"),We=c(),E=r("h1"),A=r("a"),Pe=r("span"),be(M.$$.fragment),Aa=c(),ye=r("span"),Ta=n("Introdu\xE7\xE3o"),Ye=c(),be(C.$$.fragment),Ke=c(),_=r("h2"),T=r("a"),$e=r("span"),be(F.$$.fragment),Sa=c(),Ne=r("span"),za=n("Bem-vindo(a) ao Curso \u{1F917}!"),Ve=c(),be(I.$$.fragment),Xe=c(),p=r("p"),Ha=n("Esse curso te ensinar\xE1 sobre processamento de linguagem natural (PLN, ou NLP em ingl\xEAs) usando as bibliotecas do ecossistema "),D=r("a"),ka=n("Hugging Face"),Ma=n(" \u2014 "),O=r("a"),Ca=n("\u{1F917} Transformers"),Fa=n(", "),x=r("a"),Ia=n("\u{1F917} Datasets"),Da=n(", "),G=r("a"),Oa=n("\u{1F917} Tokenizers"),xa=n(" e "),R=r("a"),Ga=n("\u{1F917} Accelerate"),Ra=n(" \u2014 assim como a "),j=r("a"),ja=n("Hugging Face Hub"),Ba=n(". \xC9 completamente gratuito e sem an\xFAncios!"),Ze=c(),w=r("h2"),S=r("a"),Ae=r("span"),be(B.$$.fragment),Ua=c(),Te=r("span"),Ja=n("O que esperar?"),ea=c(),ce=r("p"),Qa=n("Aqui uma vis\xE3o geral do curso:"),aa=c(),L=r("div"),U=r("img"),Wa=c(),J=r("img"),oa=c(),z=r("ul"),Q=r("li"),Ya=n("Cap\xEDtulos 1 ao 4 d\xE1 uma introdu\xE7\xE3o para os principais conceitos da biblioteca Transformers \u{1F917}. No final dessa parte do curso, voc\xEA se tornar\xE1 familiar sobre como os modelos Transformers funcionam, saber\xE1 como usar o modelo da "),W=r("a"),Ka=n("Hugging Face Hub"),Va=n(", fazer um ajuste fino em um dataset e compartilhar os resultados no Hub!"),Xa=c(),Se=r("li"),Za=n("Cap\xEDtulo 5 ao 8 ensina o b\xE1sico dos \u{1F917} Datasets e \u{1F917} Tokenizadores antes de mergulhar nas tarefas cl\xE1ssicas de NLP. Ao final dessa parte, voc\xEA ser\xE1 capaz de resolver por conta pr\xF3pria os problemas mais comuns de NLP. Ao longo do caminho, voc\xEA aprender\xE1 como construir e compartilhar demonstra\xE7\xF5es de seus modelos e otimiza-los para ambientes de produ\xE7\xE3o. No final dessa parte, voc\xEA ser\xE1 capaz de aplicar a \u{1F917} Transformers para (quase) qualquer problema de aprendizagem de m\xE1quina!"),ra=c(),de=r("p"),eo=n("Esse curso:"),sa=c(),h=r("ul"),ze=r("li"),ao=n("Requer um bom conhecimento de Python"),oo=c(),v=r("li"),ro=n("\xC9 melhor aproveitado depois de um curso de introdu\xE7\xE3o ao deep learning (aprendizagem profunda), como o da "),Y=r("a"),so=n("fast.ai"),to=c(),K=r("a"),no=n("Practical Deep Learning for Coders"),io=n("  ou um dos programas desenvolvidos pela "),V=r("a"),lo=n("DeepLearning.AI"),uo=c(),q=r("li"),mo=n("N\xE3o se espera nenhum conhecimento pr\xE9vio em "),X=r("a"),co=n("PyTorch"),po=n("  ou em "),Z=r("a"),fo=n("TensorFlow"),ho=n(" , ainda que certa familiaridade com ambas as ferramentas seja proveitoso"),ta=c(),H=r("p"),vo=n("Depois de voc\xEA ter completado esse curso, n\xF3s recomendamos dar uma olhada na especializa\xE7\xE3o da DeepLearning.AI de "),ee=r("a"),go=n("Processamento de Linguagem Natural"),bo=n(", que cobre uma grande gama de modelos de NLP como naive Bayes e LSTMs que valem bastante a pena ter conhecimento!"),na=c(),P=r("h2"),k=r("a"),He=r("span"),be(ae.$$.fragment),Eo=c(),ke=r("span"),_o=n("Quem n\xF3s somos?"),ia=c(),pe=r("p"),wo=n("Sobre os autores:"),la=c(),oe=r("p"),Me=r("strong"),Lo=n("Matthew Carrigan"),qo=n(" \xE9 Engenheiro de Machine Learning na Hugging Face. Ele mora em Dublin na Irlanda e anteriormente trabalhou como Engenheiro de ML na Parse.ly e antes disso como pesquisador p\xF3s-doc na Trinity College Dublin. Ele n\xE3o acredita que chegaremos a um rendimento anual bruto corrigido escalando as arquiteturas existentes, mas de qualquer forma ele tem grandes esperan\xE7as na imortalidade das m\xE1quinas."),ua=c(),re=r("p"),Ce=r("strong"),Po=n("Lysandre Debut"),yo=n(" \xE9 um Engenheiro de Machine Learning na Hugging Face e tem trabalhado para a biblioteca \u{1F917} Transformers desde seus est\xE1gios iniciais de desenvolvimento. Seu objetivo \xE9 fazer com que a \xE1rea de NLP se torne acess\xEDvel para qualquer pessoa atrav\xE9s do desenvolvimento de ferramentas com uma API simples."),ma=c(),y=r("p"),Fe=r("strong"),$o=n("Sylvain Gugger"),No=n(" \xE9 um Engenheiro Pesquisador na Hugging Face e um dos principais mantenedores da biblioteca \u{1F917} Transformers. Anteriormente ele foi Cientista Pesquisador na fast.ai, e co-escreveu "),Ie=r("em"),se=r("a"),Ao=n("Deep Learning for Coders with fastai and PyTorch"),To=n(" com Jeremy Howard. O principal foco de sua pesquisa est\xE1 em fazer o deep learning mais acess\xEDvel, atrav\xE9s de desenhos e t\xE9cnicas de aprimoramento que permitam que modelos sejam treinados mais r\xE1pidos e com recursos limitados."),ca=c(),te=r("p"),De=r("strong"),So=n("Merve Noyan"),zo=n(" \xE9 um desenvolvedor e evangelista na Hugging Face, trabalhando no desenvolvimento e constru\xE7\xE3o de conte\xFAdos envolta da tem\xE1tica de democratiza\xE7\xE3o do Machine Learning para todas as pessoas."),da=c(),ne=r("p"),Oe=r("strong"),Ho=n("Lucile Saulnier"),ko=n(" \xE9 uma Engenheira de Machine Learning na Hugging Face, desenvolvendo e apoiando o uso de ferramentas de c\xF3digo aberto. Ela tamb\xE9m \xE9 ativamente envolvida em muitos projetos de pesquisa no campo do Processamento de Linguagem natural assim como em treinamentos colaborativos e BigScience."),pa=c(),$=r("p"),xe=r("strong"),Mo=n("Lewis Tunstall"),Co=n("  \xE9 um Engenheiro de Machine Learning na Hugging Face, focado no desenvolvimento de ferramentas open-source e em faz\xEA-las amplamente acess\xEDveis pela comunidade. Ele tamb\xE9m \xE9 co-autor do livro que est\xE1 pra lan\xE7ar "),ie=r("a"),Fo=n("O\u2019Reilly book on Transformers"),Io=n("."),fa=c(),N=r("p"),Ge=r("strong"),Do=n("Leandro von Werra"),Oo=n("  \xE9 um Engenheiro de Machine Learning no time de open-source na Hugging Face e tamb\xE9m co-autor do livro "),le=r("a"),xo=n("O\u2019Reilly book on Transformers"),Go=n(". Ele tem muitos anos de experi\xEAncia na ind\xFAstria trazendo projetos de NLP para produ\xE7\xE3o trabalhando com v\xE1rias stacks de Machine Learning."),ha=c(),fe=r("p"),Ro=n("Est\xE1 pronto para seguir? Nesse cap\xEDtulo, voc\xEA aprender\xE1:"),va=c(),g=r("ul"),ue=r("li"),jo=n("Como usar a fun\xE7\xE3o "),Re=r("code"),Bo=n("pipeline()"),Uo=n("  para solucionar tarefas de NLP tais como gera\xE7\xE3o de texto e classifica\xE7\xE3o"),Jo=c(),je=r("li"),Qo=n("Sobre a arquitetura Transformer"),Wo=c(),Be=r("li"),Yo=n("Como distinguir entre as arquiteturas encoder, decoder, encoder-decoder e seus casos de uso"),this.h()},l(e){const u=Xr('[data-svelte="svelte-1phssyn"]',document.head);b=s(u,"META",{name:!0,content:!0}),u.forEach(o),We=d(e),E=s(e,"H1",{class:!0});var ba=t(E);A=s(ba,"A",{id:!0,class:!0,href:!0});var rr=t(A);Pe=s(rr,"SPAN",{});var sr=t(Pe);Ee(M.$$.fragment,sr),sr.forEach(o),rr.forEach(o),Aa=d(ba),ye=s(ba,"SPAN",{});var tr=t(ye);Ta=i(tr,"Introdu\xE7\xE3o"),tr.forEach(o),ba.forEach(o),Ye=d(e),Ee(C.$$.fragment,e),Ke=d(e),_=s(e,"H2",{class:!0});var Ea=t(_);T=s(Ea,"A",{id:!0,class:!0,href:!0});var nr=t(T);$e=s(nr,"SPAN",{});var ir=t($e);Ee(F.$$.fragment,ir),ir.forEach(o),nr.forEach(o),Sa=d(Ea),Ne=s(Ea,"SPAN",{});var lr=t(Ne);za=i(lr,"Bem-vindo(a) ao Curso \u{1F917}!"),lr.forEach(o),Ea.forEach(o),Ve=d(e),Ee(I.$$.fragment,e),Xe=d(e),p=s(e,"P",{});var f=t(p);Ha=i(f,"Esse curso te ensinar\xE1 sobre processamento de linguagem natural (PLN, ou NLP em ingl\xEAs) usando as bibliotecas do ecossistema "),D=s(f,"A",{href:!0,rel:!0});var ur=t(D);ka=i(ur,"Hugging Face"),ur.forEach(o),Ma=i(f," \u2014 "),O=s(f,"A",{href:!0,rel:!0});var mr=t(O);Ca=i(mr,"\u{1F917} Transformers"),mr.forEach(o),Fa=i(f,", "),x=s(f,"A",{href:!0,rel:!0});var cr=t(x);Ia=i(cr,"\u{1F917} Datasets"),cr.forEach(o),Da=i(f,", "),G=s(f,"A",{href:!0,rel:!0});var dr=t(G);Oa=i(dr,"\u{1F917} Tokenizers"),dr.forEach(o),xa=i(f," e "),R=s(f,"A",{href:!0,rel:!0});var pr=t(R);Ga=i(pr,"\u{1F917} Accelerate"),pr.forEach(o),Ra=i(f," \u2014 assim como a "),j=s(f,"A",{href:!0,rel:!0});var fr=t(j);ja=i(fr,"Hugging Face Hub"),fr.forEach(o),Ba=i(f,". \xC9 completamente gratuito e sem an\xFAncios!"),f.forEach(o),Ze=d(e),w=s(e,"H2",{class:!0});var _a=t(w);S=s(_a,"A",{id:!0,class:!0,href:!0});var hr=t(S);Ae=s(hr,"SPAN",{});var vr=t(Ae);Ee(B.$$.fragment,vr),vr.forEach(o),hr.forEach(o),Ua=d(_a),Te=s(_a,"SPAN",{});var gr=t(Te);Ja=i(gr,"O que esperar?"),gr.forEach(o),_a.forEach(o),ea=d(e),ce=s(e,"P",{});var br=t(ce);Qa=i(br,"Aqui uma vis\xE3o geral do curso:"),br.forEach(o),aa=d(e),L=s(e,"DIV",{class:!0});var wa=t(L);U=s(wa,"IMG",{class:!0,src:!0,alt:!0}),Wa=d(wa),J=s(wa,"IMG",{class:!0,src:!0,alt:!0}),wa.forEach(o),oa=d(e),z=s(e,"UL",{});var La=t(z);Q=s(La,"LI",{});var qa=t(Q);Ya=i(qa,"Cap\xEDtulos 1 ao 4 d\xE1 uma introdu\xE7\xE3o para os principais conceitos da biblioteca Transformers \u{1F917}. No final dessa parte do curso, voc\xEA se tornar\xE1 familiar sobre como os modelos Transformers funcionam, saber\xE1 como usar o modelo da "),W=s(qa,"A",{href:!0,rel:!0});var Er=t(W);Ka=i(Er,"Hugging Face Hub"),Er.forEach(o),Va=i(qa,", fazer um ajuste fino em um dataset e compartilhar os resultados no Hub!"),qa.forEach(o),Xa=d(La),Se=s(La,"LI",{});var _r=t(Se);Za=i(_r,"Cap\xEDtulo 5 ao 8 ensina o b\xE1sico dos \u{1F917} Datasets e \u{1F917} Tokenizadores antes de mergulhar nas tarefas cl\xE1ssicas de NLP. Ao final dessa parte, voc\xEA ser\xE1 capaz de resolver por conta pr\xF3pria os problemas mais comuns de NLP. Ao longo do caminho, voc\xEA aprender\xE1 como construir e compartilhar demonstra\xE7\xF5es de seus modelos e otimiza-los para ambientes de produ\xE7\xE3o. No final dessa parte, voc\xEA ser\xE1 capaz de aplicar a \u{1F917} Transformers para (quase) qualquer problema de aprendizagem de m\xE1quina!"),_r.forEach(o),La.forEach(o),ra=d(e),de=s(e,"P",{});var wr=t(de);eo=i(wr,"Esse curso:"),wr.forEach(o),sa=d(e),h=s(e,"UL",{});var he=t(h);ze=s(he,"LI",{});var Lr=t(ze);ao=i(Lr,"Requer um bom conhecimento de Python"),Lr.forEach(o),oo=d(he),v=s(he,"LI",{});var me=t(v);ro=i(me,"\xC9 melhor aproveitado depois de um curso de introdu\xE7\xE3o ao deep learning (aprendizagem profunda), como o da "),Y=s(me,"A",{href:!0,rel:!0});var qr=t(Y);so=i(qr,"fast.ai"),qr.forEach(o),to=d(me),K=s(me,"A",{href:!0,rel:!0});var Pr=t(K);no=i(Pr,"Practical Deep Learning for Coders"),Pr.forEach(o),io=i(me,"  ou um dos programas desenvolvidos pela "),V=s(me,"A",{href:!0,rel:!0});var yr=t(V);lo=i(yr,"DeepLearning.AI"),yr.forEach(o),me.forEach(o),uo=d(he),q=s(he,"LI",{});var ve=t(q);mo=i(ve,"N\xE3o se espera nenhum conhecimento pr\xE9vio em "),X=s(ve,"A",{href:!0,rel:!0});var $r=t(X);co=i($r,"PyTorch"),$r.forEach(o),po=i(ve,"  ou em "),Z=s(ve,"A",{href:!0,rel:!0});var Nr=t(Z);fo=i(Nr,"TensorFlow"),Nr.forEach(o),ho=i(ve," , ainda que certa familiaridade com ambas as ferramentas seja proveitoso"),ve.forEach(o),he.forEach(o),ta=d(e),H=s(e,"P",{});var Pa=t(H);vo=i(Pa,"Depois de voc\xEA ter completado esse curso, n\xF3s recomendamos dar uma olhada na especializa\xE7\xE3o da DeepLearning.AI de "),ee=s(Pa,"A",{href:!0,rel:!0});var Ar=t(ee);go=i(Ar,"Processamento de Linguagem Natural"),Ar.forEach(o),bo=i(Pa,", que cobre uma grande gama de modelos de NLP como naive Bayes e LSTMs que valem bastante a pena ter conhecimento!"),Pa.forEach(o),na=d(e),P=s(e,"H2",{class:!0});var ya=t(P);k=s(ya,"A",{id:!0,class:!0,href:!0});var Tr=t(k);He=s(Tr,"SPAN",{});var Sr=t(He);Ee(ae.$$.fragment,Sr),Sr.forEach(o),Tr.forEach(o),Eo=d(ya),ke=s(ya,"SPAN",{});var zr=t(ke);_o=i(zr,"Quem n\xF3s somos?"),zr.forEach(o),ya.forEach(o),ia=d(e),pe=s(e,"P",{});var Hr=t(pe);wo=i(Hr,"Sobre os autores:"),Hr.forEach(o),la=d(e),oe=s(e,"P",{});var Ko=t(oe);Me=s(Ko,"STRONG",{});var kr=t(Me);Lo=i(kr,"Matthew Carrigan"),kr.forEach(o),qo=i(Ko," \xE9 Engenheiro de Machine Learning na Hugging Face. Ele mora em Dublin na Irlanda e anteriormente trabalhou como Engenheiro de ML na Parse.ly e antes disso como pesquisador p\xF3s-doc na Trinity College Dublin. Ele n\xE3o acredita que chegaremos a um rendimento anual bruto corrigido escalando as arquiteturas existentes, mas de qualquer forma ele tem grandes esperan\xE7as na imortalidade das m\xE1quinas."),Ko.forEach(o),ua=d(e),re=s(e,"P",{});var Vo=t(re);Ce=s(Vo,"STRONG",{});var Mr=t(Ce);Po=i(Mr,"Lysandre Debut"),Mr.forEach(o),yo=i(Vo," \xE9 um Engenheiro de Machine Learning na Hugging Face e tem trabalhado para a biblioteca \u{1F917} Transformers desde seus est\xE1gios iniciais de desenvolvimento. Seu objetivo \xE9 fazer com que a \xE1rea de NLP se torne acess\xEDvel para qualquer pessoa atrav\xE9s do desenvolvimento de ferramentas com uma API simples."),Vo.forEach(o),ma=d(e),y=s(e,"P",{});var Ue=t(y);Fe=s(Ue,"STRONG",{});var Cr=t(Fe);$o=i(Cr,"Sylvain Gugger"),Cr.forEach(o),No=i(Ue," \xE9 um Engenheiro Pesquisador na Hugging Face e um dos principais mantenedores da biblioteca \u{1F917} Transformers. Anteriormente ele foi Cientista Pesquisador na fast.ai, e co-escreveu "),Ie=s(Ue,"EM",{});var Fr=t(Ie);se=s(Fr,"A",{href:!0,rel:!0});var Ir=t(se);Ao=i(Ir,"Deep Learning for Coders with fastai and PyTorch"),Ir.forEach(o),Fr.forEach(o),To=i(Ue," com Jeremy Howard. O principal foco de sua pesquisa est\xE1 em fazer o deep learning mais acess\xEDvel, atrav\xE9s de desenhos e t\xE9cnicas de aprimoramento que permitam que modelos sejam treinados mais r\xE1pidos e com recursos limitados."),Ue.forEach(o),ca=d(e),te=s(e,"P",{});var Xo=t(te);De=s(Xo,"STRONG",{});var Dr=t(De);So=i(Dr,"Merve Noyan"),Dr.forEach(o),zo=i(Xo," \xE9 um desenvolvedor e evangelista na Hugging Face, trabalhando no desenvolvimento e constru\xE7\xE3o de conte\xFAdos envolta da tem\xE1tica de democratiza\xE7\xE3o do Machine Learning para todas as pessoas."),Xo.forEach(o),da=d(e),ne=s(e,"P",{});var Zo=t(ne);Oe=s(Zo,"STRONG",{});var Or=t(Oe);Ho=i(Or,"Lucile Saulnier"),Or.forEach(o),ko=i(Zo," \xE9 uma Engenheira de Machine Learning na Hugging Face, desenvolvendo e apoiando o uso de ferramentas de c\xF3digo aberto. Ela tamb\xE9m \xE9 ativamente envolvida em muitos projetos de pesquisa no campo do Processamento de Linguagem natural assim como em treinamentos colaborativos e BigScience."),Zo.forEach(o),pa=d(e),$=s(e,"P",{});var Je=t($);xe=s(Je,"STRONG",{});var xr=t(xe);Mo=i(xr,"Lewis Tunstall"),xr.forEach(o),Co=i(Je,"  \xE9 um Engenheiro de Machine Learning na Hugging Face, focado no desenvolvimento de ferramentas open-source e em faz\xEA-las amplamente acess\xEDveis pela comunidade. Ele tamb\xE9m \xE9 co-autor do livro que est\xE1 pra lan\xE7ar "),ie=s(Je,"A",{href:!0,rel:!0});var Gr=t(ie);Fo=i(Gr,"O\u2019Reilly book on Transformers"),Gr.forEach(o),Io=i(Je,"."),Je.forEach(o),fa=d(e),N=s(e,"P",{});var Qe=t(N);Ge=s(Qe,"STRONG",{});var Rr=t(Ge);Do=i(Rr,"Leandro von Werra"),Rr.forEach(o),Oo=i(Qe,"  \xE9 um Engenheiro de Machine Learning no time de open-source na Hugging Face e tamb\xE9m co-autor do livro "),le=s(Qe,"A",{href:!0,rel:!0});var jr=t(le);xo=i(jr,"O\u2019Reilly book on Transformers"),jr.forEach(o),Go=i(Qe,". Ele tem muitos anos de experi\xEAncia na ind\xFAstria trazendo projetos de NLP para produ\xE7\xE3o trabalhando com v\xE1rias stacks de Machine Learning."),Qe.forEach(o),ha=d(e),fe=s(e,"P",{});var Br=t(fe);Ro=i(Br,"Est\xE1 pronto para seguir? Nesse cap\xEDtulo, voc\xEA aprender\xE1:"),Br.forEach(o),va=d(e),g=s(e,"UL",{});var ge=t(g);ue=s(ge,"LI",{});var $a=t(ue);jo=i($a,"Como usar a fun\xE7\xE3o "),Re=s($a,"CODE",{});var Ur=t(Re);Bo=i(Ur,"pipeline()"),Ur.forEach(o),Uo=i($a,"  para solucionar tarefas de NLP tais como gera\xE7\xE3o de texto e classifica\xE7\xE3o"),$a.forEach(o),Jo=d(ge),je=s(ge,"LI",{});var Jr=t(je);Qo=i(Jr,"Sobre a arquitetura Transformer"),Jr.forEach(o),Wo=d(ge),Be=s(ge,"LI",{});var Qr=t(Be);Yo=i(Qr,"Como distinguir entre as arquiteturas encoder, decoder, encoder-decoder e seus casos de uso"),Qr.forEach(o),ge.forEach(o),this.h()},h(){l(b,"name","hf:doc:metadata"),l(b,"content",JSON.stringify(ss)),l(A,"id","introduo"),l(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(A,"href","#introduo"),l(E,"class","relative group"),l(T,"id","bemvindoa-ao-curso"),l(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(T,"href","#bemvindoa-ao-curso"),l(_,"class","relative group"),l(D,"href","https://huggingface.co/"),l(D,"rel","nofollow"),l(O,"href","https://github.com/huggingface/transformers"),l(O,"rel","nofollow"),l(x,"href","https://github.com/huggingface/datasets"),l(x,"rel","nofollow"),l(G,"href","https://github.com/huggingface/tokenizers"),l(G,"rel","nofollow"),l(R,"href","https://github.com/huggingface/accelerate"),l(R,"rel","nofollow"),l(j,"href","https://huggingface.co/models"),l(j,"rel","nofollow"),l(S,"id","o-que-esperar"),l(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(S,"href","#o-que-esperar"),l(w,"class","relative group"),l(U,"class","block dark:hidden"),Wr(U.src,ar="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg")||l(U,"src",ar),l(U,"alt","Brief overview of the chapters of the course."),l(J,"class","hidden dark:block"),Wr(J.src,or="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg")||l(J,"src",or),l(J,"alt","Brief overview of the chapters of the course."),l(L,"class","flex justify-center"),l(W,"href","https://huggingface.co/models"),l(W,"rel","nofollow"),l(Y,"href","https://www.fast.ai/"),l(Y,"rel","nofollow"),l(K,"href","https://course.fast.ai/"),l(K,"rel","nofollow"),l(V,"href","https://www.deeplearning.ai/"),l(V,"rel","nofollow"),l(X,"href","https://pytorch.org/"),l(X,"rel","nofollow"),l(Z,"href","https://www.tensorflow.org/"),l(Z,"rel","nofollow"),l(ee,"href","https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh"),l(ee,"rel","nofollow"),l(k,"id","quem-ns-somos"),l(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(k,"href","#quem-ns-somos"),l(P,"class","relative group"),l(se,"href","https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/"),l(se,"rel","nofollow"),l(ie,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),l(ie,"rel","nofollow"),l(le,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),l(le,"rel","nofollow")},m(e,u){a(document.head,b),m(e,We,u),m(e,E,u),a(E,A),a(A,Pe),_e(M,Pe,null),a(E,Aa),a(E,ye),a(ye,Ta),m(e,Ye,u),_e(C,e,u),m(e,Ke,u),m(e,_,u),a(_,T),a(T,$e),_e(F,$e,null),a(_,Sa),a(_,Ne),a(Ne,za),m(e,Ve,u),_e(I,e,u),m(e,Xe,u),m(e,p,u),a(p,Ha),a(p,D),a(D,ka),a(p,Ma),a(p,O),a(O,Ca),a(p,Fa),a(p,x),a(x,Ia),a(p,Da),a(p,G),a(G,Oa),a(p,xa),a(p,R),a(R,Ga),a(p,Ra),a(p,j),a(j,ja),a(p,Ba),m(e,Ze,u),m(e,w,u),a(w,S),a(S,Ae),_e(B,Ae,null),a(w,Ua),a(w,Te),a(Te,Ja),m(e,ea,u),m(e,ce,u),a(ce,Qa),m(e,aa,u),m(e,L,u),a(L,U),a(L,Wa),a(L,J),m(e,oa,u),m(e,z,u),a(z,Q),a(Q,Ya),a(Q,W),a(W,Ka),a(Q,Va),a(z,Xa),a(z,Se),a(Se,Za),m(e,ra,u),m(e,de,u),a(de,eo),m(e,sa,u),m(e,h,u),a(h,ze),a(ze,ao),a(h,oo),a(h,v),a(v,ro),a(v,Y),a(Y,so),a(v,to),a(v,K),a(K,no),a(v,io),a(v,V),a(V,lo),a(h,uo),a(h,q),a(q,mo),a(q,X),a(X,co),a(q,po),a(q,Z),a(Z,fo),a(q,ho),m(e,ta,u),m(e,H,u),a(H,vo),a(H,ee),a(ee,go),a(H,bo),m(e,na,u),m(e,P,u),a(P,k),a(k,He),_e(ae,He,null),a(P,Eo),a(P,ke),a(ke,_o),m(e,ia,u),m(e,pe,u),a(pe,wo),m(e,la,u),m(e,oe,u),a(oe,Me),a(Me,Lo),a(oe,qo),m(e,ua,u),m(e,re,u),a(re,Ce),a(Ce,Po),a(re,yo),m(e,ma,u),m(e,y,u),a(y,Fe),a(Fe,$o),a(y,No),a(y,Ie),a(Ie,se),a(se,Ao),a(y,To),m(e,ca,u),m(e,te,u),a(te,De),a(De,So),a(te,zo),m(e,da,u),m(e,ne,u),a(ne,Oe),a(Oe,Ho),a(ne,ko),m(e,pa,u),m(e,$,u),a($,xe),a(xe,Mo),a($,Co),a($,ie),a(ie,Fo),a($,Io),m(e,fa,u),m(e,N,u),a(N,Ge),a(Ge,Do),a(N,Oo),a(N,le),a(le,xo),a(N,Go),m(e,ha,u),m(e,fe,u),a(fe,Ro),m(e,va,u),m(e,g,u),a(g,ue),a(ue,jo),a(ue,Re),a(Re,Bo),a(ue,Uo),a(g,Jo),a(g,je),a(je,Qo),a(g,Wo),a(g,Be),a(Be,Yo),ga=!0},p:Zr,i(e){ga||(we(M.$$.fragment,e),we(C.$$.fragment,e),we(F.$$.fragment,e),we(I.$$.fragment,e),we(B.$$.fragment,e),we(ae.$$.fragment,e),ga=!0)},o(e){Le(M.$$.fragment,e),Le(C.$$.fragment,e),Le(F.$$.fragment,e),Le(I.$$.fragment,e),Le(B.$$.fragment,e),Le(ae.$$.fragment,e),ga=!1},d(e){o(b),e&&o(We),e&&o(E),qe(M),e&&o(Ye),qe(C,e),e&&o(Ke),e&&o(_),qe(F),e&&o(Ve),qe(I,e),e&&o(Xe),e&&o(p),e&&o(Ze),e&&o(w),qe(B),e&&o(ea),e&&o(ce),e&&o(aa),e&&o(L),e&&o(oa),e&&o(z),e&&o(ra),e&&o(de),e&&o(sa),e&&o(h),e&&o(ta),e&&o(H),e&&o(na),e&&o(P),qe(ae),e&&o(ia),e&&o(pe),e&&o(la),e&&o(oe),e&&o(ua),e&&o(re),e&&o(ma),e&&o(y),e&&o(ca),e&&o(te),e&&o(da),e&&o(ne),e&&o(pa),e&&o($),e&&o(fa),e&&o(N),e&&o(ha),e&&o(fe),e&&o(va),e&&o(g)}}}const ss={local:"introduo",sections:[{local:"bemvindoa-ao-curso",title:"Bem-vindo(a) ao Curso \u{1F917}!"},{local:"o-que-esperar",title:"O que esperar?"},{local:"quem-ns-somos",title:"Quem n\xF3s somos?"}],title:"Introdu\xE7\xE3o"};function ts(er){return es(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ms extends Yr{constructor(b){super();Kr(this,b,ts,rs,Vr,{})}}export{ms as default,ss as metadata};
