import{S as at,i as ot,s as rt,e as o,k as n,w as Ye,t as d,M as st,c as r,d as a,m as l,a as s,x as Ke,h as i,b as g,G as e,g as E,y as We,L as nt,q as Ze,o as et,B as tt,v as dt}from"../../chunks/vendor-hf-doc-builder.js";import{I as lt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as it}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function ct(qe){let m,X,f,_,q,A,ae,x,oe,F,D,J,b,re,C,se,ne,Y,$,de,K,R,N,u,S,le,ie,G,ce,me,H,fe,ue,p,h,M,pe,he,k,ve,Te,z,Ee,_e,v,I,be,Re,O,Ae,De,U,$e,Be,T,j,Pe,ye,Q,Le,we,V,ge,W;return A=new lt({}),D=new it({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),{c(){m=o("meta"),X=n(),f=o("h1"),_=o("a"),q=o("span"),Ye(A.$$.fragment),ae=n(),x=o("span"),oe=d("Resumo"),F=n(),Ye(D.$$.fragment),J=n(),b=o("p"),re=d("Nesse cap\xEDtulo, voc\xEA viu como abordar diferentes tarefas de NLP usando a fun\xE7\xE3o de alto n\xEDvel "),C=o("code"),se=d("pipeline()"),ne=d(" da biblioteca \u{1F917} Transformers. Voc\xEA tamb\xE9m viu como pesquisar e usar modelos no Hub, bem como usar a API de infer\xEAncia para testar os modelos diretamente em seu navegador."),Y=n(),$=o("p"),de=d("Discutimos como os modelos Transformers funcionam em alto n\xEDvel e falamos sobre a import\xE2ncia do aprendizado de transfer\xEAncia (transfer learning) e do ajuste fino. Um aspecto chave \xE9 que voc\xEA pode usar a arquitetura completa ou apenas o codificador ou decodificador, dependendo do tipo de tarefa que voc\xEA pretende resolver. A tabela a seguir resume isso:"),K=n(),R=o("table"),N=o("thead"),u=o("tr"),S=o("th"),le=d("Modelo"),ie=n(),G=o("th"),ce=d("Exemplos"),me=n(),H=o("th"),fe=d("Tarefas"),ue=n(),p=o("tbody"),h=o("tr"),M=o("td"),pe=d("Encoder"),he=n(),k=o("td"),ve=d("ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),Te=n(),z=o("td"),Ee=d("Classifica\xE7\xE3o de senten\xE7as, reconhecimento de entidades nomeadas, Q&A"),_e=n(),v=o("tr"),I=o("td"),be=d("Decoder"),Re=n(),O=o("td"),Ae=d("CTRL, GPT, GPT-2, Transformer XL"),De=n(),U=o("td"),$e=d("Gera\xE7\xE3o de texto"),Be=n(),T=o("tr"),j=o("td"),Pe=d("Encoder-decoder"),ye=n(),Q=o("td"),Le=d("BART, T5, Marian, mBART"),we=n(),V=o("td"),ge=d("Sumariza\xE7\xE3o, tradu\xE7\xE3o, perguntas e respostas gerativas"),this.h()},l(t){const c=st('[data-svelte="svelte-1phssyn"]',document.head);m=r(c,"META",{name:!0,content:!0}),c.forEach(a),X=l(t),f=r(t,"H1",{class:!0});var Z=s(f);_=r(Z,"A",{id:!0,class:!0,href:!0});var xe=s(_);q=r(xe,"SPAN",{});var Ce=s(q);Ke(A.$$.fragment,Ce),Ce.forEach(a),xe.forEach(a),ae=l(Z),x=r(Z,"SPAN",{});var Ne=s(x);oe=i(Ne,"Resumo"),Ne.forEach(a),Z.forEach(a),F=l(t),Ke(D.$$.fragment,t),J=l(t),b=r(t,"P",{});var ee=s(b);re=i(ee,"Nesse cap\xEDtulo, voc\xEA viu como abordar diferentes tarefas de NLP usando a fun\xE7\xE3o de alto n\xEDvel "),C=r(ee,"CODE",{});var Se=s(C);se=i(Se,"pipeline()"),Se.forEach(a),ne=i(ee," da biblioteca \u{1F917} Transformers. Voc\xEA tamb\xE9m viu como pesquisar e usar modelos no Hub, bem como usar a API de infer\xEAncia para testar os modelos diretamente em seu navegador."),ee.forEach(a),Y=l(t),$=r(t,"P",{});var Ge=s($);de=i(Ge,"Discutimos como os modelos Transformers funcionam em alto n\xEDvel e falamos sobre a import\xE2ncia do aprendizado de transfer\xEAncia (transfer learning) e do ajuste fino. Um aspecto chave \xE9 que voc\xEA pode usar a arquitetura completa ou apenas o codificador ou decodificador, dependendo do tipo de tarefa que voc\xEA pretende resolver. A tabela a seguir resume isso:"),Ge.forEach(a),K=l(t),R=r(t,"TABLE",{});var te=s(R);N=r(te,"THEAD",{});var He=s(N);u=r(He,"TR",{});var B=s(u);S=r(B,"TH",{});var Me=s(S);le=i(Me,"Modelo"),Me.forEach(a),ie=l(B),G=r(B,"TH",{});var ke=s(G);ce=i(ke,"Exemplos"),ke.forEach(a),me=l(B),H=r(B,"TH",{});var ze=s(H);fe=i(ze,"Tarefas"),ze.forEach(a),B.forEach(a),He.forEach(a),ue=l(te),p=r(te,"TBODY",{});var P=s(p);h=r(P,"TR",{});var y=s(h);M=r(y,"TD",{});var Ie=s(M);pe=i(Ie,"Encoder"),Ie.forEach(a),he=l(y),k=r(y,"TD",{});var Oe=s(k);ve=i(Oe,"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),Oe.forEach(a),Te=l(y),z=r(y,"TD",{});var Ue=s(z);Ee=i(Ue,"Classifica\xE7\xE3o de senten\xE7as, reconhecimento de entidades nomeadas, Q&A"),Ue.forEach(a),y.forEach(a),_e=l(P),v=r(P,"TR",{});var L=s(v);I=r(L,"TD",{});var je=s(I);be=i(je,"Decoder"),je.forEach(a),Re=l(L),O=r(L,"TD",{});var Qe=s(O);Ae=i(Qe,"CTRL, GPT, GPT-2, Transformer XL"),Qe.forEach(a),De=l(L),U=r(L,"TD",{});var Ve=s(U);$e=i(Ve,"Gera\xE7\xE3o de texto"),Ve.forEach(a),L.forEach(a),Be=l(P),T=r(P,"TR",{});var w=s(T);j=r(w,"TD",{});var Xe=s(j);Pe=i(Xe,"Encoder-decoder"),Xe.forEach(a),ye=l(w),Q=r(w,"TD",{});var Fe=s(Q);Le=i(Fe,"BART, T5, Marian, mBART"),Fe.forEach(a),we=l(w),V=r(w,"TD",{});var Je=s(V);ge=i(Je,"Sumariza\xE7\xE3o, tradu\xE7\xE3o, perguntas e respostas gerativas"),Je.forEach(a),w.forEach(a),P.forEach(a),te.forEach(a),this.h()},h(){g(m,"name","hf:doc:metadata"),g(m,"content",JSON.stringify(mt)),g(_,"id","resumo"),g(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(_,"href","#resumo"),g(f,"class","relative group")},m(t,c){e(document.head,m),E(t,X,c),E(t,f,c),e(f,_),e(_,q),We(A,q,null),e(f,ae),e(f,x),e(x,oe),E(t,F,c),We(D,t,c),E(t,J,c),E(t,b,c),e(b,re),e(b,C),e(C,se),e(b,ne),E(t,Y,c),E(t,$,c),e($,de),E(t,K,c),E(t,R,c),e(R,N),e(N,u),e(u,S),e(S,le),e(u,ie),e(u,G),e(G,ce),e(u,me),e(u,H),e(H,fe),e(R,ue),e(R,p),e(p,h),e(h,M),e(M,pe),e(h,he),e(h,k),e(k,ve),e(h,Te),e(h,z),e(z,Ee),e(p,_e),e(p,v),e(v,I),e(I,be),e(v,Re),e(v,O),e(O,Ae),e(v,De),e(v,U),e(U,$e),e(p,Be),e(p,T),e(T,j),e(j,Pe),e(T,ye),e(T,Q),e(Q,Le),e(T,we),e(T,V),e(V,ge),W=!0},p:nt,i(t){W||(Ze(A.$$.fragment,t),Ze(D.$$.fragment,t),W=!0)},o(t){et(A.$$.fragment,t),et(D.$$.fragment,t),W=!1},d(t){a(m),t&&a(X),t&&a(f),tt(A),t&&a(F),tt(D,t),t&&a(J),t&&a(b),t&&a(Y),t&&a($),t&&a(K),t&&a(R)}}}const mt={local:"resumo",title:"Resumo"};function ft(qe){return dt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vt extends at{constructor(m){super();ot(this,m,ft,ct,rt,{})}}export{vt as default,mt as metadata};
