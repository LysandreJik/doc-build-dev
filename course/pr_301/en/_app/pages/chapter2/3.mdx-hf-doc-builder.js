import{S as Rr,i as Lr,s as Ur,e as f,k as m,w as C,t as n,M as Yr,c as h,d as o,m as _,x as q,a as d,h as i,b as j,G as r,g as c,y as P,o as b,p as xe,q as y,B as A,v as Gr,n as He}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Hr}from"../../chunks/Youtube-hf-doc-builder.js";import{I as _t}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as D}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as Wr}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{F as Jr}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Vr(k){let s,l;return s=new Wr({props:{chapter:2,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"}]}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function Kr(k){let s,l;return s=new Wr({props:{chapter:2,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"}]}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function Qr(k){let s,l;return s=new Hr({props:{id:"d3JVgghSOew"}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function Xr(k){let s,l;return s=new Hr({props:{id:"AhChOFRegn4"}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function Zr(k){let s,l,t,p,E,$,g,T,B,v,M;return{c(){s=f("p"),l=n("In this section we\u2019ll take a closer look at creating and using a model. We\u2019ll use the "),t=f("code"),p=n("TFAutoModel"),E=n(" class, which is handy when you want to instantiate any model from a checkpoint."),$=m(),g=f("p"),T=n("The "),B=f("code"),v=n("TFAutoModel"),M=n(" class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It\u2019s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.")},l(u){s=h(u,"P",{});var w=d(s);l=i(w,"In this section we\u2019ll take a closer look at creating and using a model. We\u2019ll use the "),t=h(w,"CODE",{});var z=d(t);p=i(z,"TFAutoModel"),z.forEach(o),E=i(w," class, which is handy when you want to instantiate any model from a checkpoint."),w.forEach(o),$=_(u),g=h(u,"P",{});var N=d(g);T=i(N,"The "),B=h(N,"CODE",{});var F=d(B);v=i(F,"TFAutoModel"),F.forEach(o),M=i(N," class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It\u2019s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture."),N.forEach(o)},m(u,w){c(u,s,w),r(s,l),r(s,t),r(t,p),r(s,E),c(u,$,w),c(u,g,w),r(g,T),r(g,B),r(B,v),r(g,M)},d(u){u&&o(s),u&&o($),u&&o(g)}}}function ea(k){let s,l,t,p,E,$,g,T,B,v,M;return{c(){s=f("p"),l=n("In this section we\u2019ll take a closer look at creating and using a model. We\u2019ll use the "),t=f("code"),p=n("AutoModel"),E=n(" class, which is handy when you want to instantiate any model from a checkpoint."),$=m(),g=f("p"),T=n("The "),B=f("code"),v=n("AutoModel"),M=n(" class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It\u2019s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.")},l(u){s=h(u,"P",{});var w=d(s);l=i(w,"In this section we\u2019ll take a closer look at creating and using a model. We\u2019ll use the "),t=h(w,"CODE",{});var z=d(t);p=i(z,"AutoModel"),z.forEach(o),E=i(w," class, which is handy when you want to instantiate any model from a checkpoint."),w.forEach(o),$=_(u),g=h(u,"P",{});var N=d(g);T=i(N,"The "),B=h(N,"CODE",{});var F=d(B);v=i(F,"AutoModel"),F.forEach(o),M=i(N," class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It\u2019s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture."),N.forEach(o)},m(u,w){c(u,s,w),r(s,l),r(s,t),r(t,p),r(s,E),c(u,$,w),c(u,g,w),r(g,T),r(g,B),r(B,v),r(g,M)},d(u){u&&o(s),u&&o($),u&&o(g)}}}function ta(k){let s,l;return s=new D({props:{code:`from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, TFBertModel

<span class="hljs-comment"># Building the config</span>
config = BertConfig()

<span class="hljs-comment"># Building the model from the config</span>
model = TFBertModel(config)`}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function oa(k){let s,l;return s=new D({props:{code:`from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, BertModel

<span class="hljs-comment"># Building the config</span>
config = BertConfig()

<span class="hljs-comment"># Building the model from the config</span>
model = BertModel(config)`}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function sa(k){let s,l;return s=new D({props:{code:`from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

<span class="hljs-comment"># Model is randomly initialized!</span>`}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function ra(k){let s,l;return s=new D({props:{code:`from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

<span class="hljs-comment"># Model is randomly initialized!</span>`}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function aa(k){let s,l,t,p,E,$,g,T,B,v,M;return s=new D({props:{code:`from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFBertModel

model = TFBertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),{c(){C(s.$$.fragment),l=m(),t=f("p"),p=n("As you saw earlier, we could replace "),E=f("code"),$=n("TFBertModel"),g=n(" with the equivalent "),T=f("code"),B=n("TFAutoModel"),v=n(" class. We\u2019ll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task).")},l(u){q(s.$$.fragment,u),l=_(u),t=h(u,"P",{});var w=d(t);p=i(w,"As you saw earlier, we could replace "),E=h(w,"CODE",{});var z=d(E);$=i(z,"TFBertModel"),z.forEach(o),g=i(w," with the equivalent "),T=h(w,"CODE",{});var N=d(T);B=i(N,"TFAutoModel"),N.forEach(o),v=i(w," class. We\u2019ll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task)."),w.forEach(o)},m(u,w){P(s,u,w),c(u,l,w),c(u,t,w),r(t,p),r(t,E),r(E,$),r(t,g),r(t,T),r(T,B),r(t,v),M=!0},i(u){M||(y(s.$$.fragment,u),M=!0)},o(u){b(s.$$.fragment,u),M=!1},d(u){A(s,u),u&&o(l),u&&o(t)}}}function la(k){let s,l,t,p,E,$,g,T,B,v,M;return s=new D({props:{code:`from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel

model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),{c(){C(s.$$.fragment),l=m(),t=f("p"),p=n("As you saw earlier, we could replace "),E=f("code"),$=n("BertModel"),g=n(" with the equivalent "),T=f("code"),B=n("AutoModel"),v=n(" class. We\u2019ll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task).")},l(u){q(s.$$.fragment,u),l=_(u),t=h(u,"P",{});var w=d(t);p=i(w,"As you saw earlier, we could replace "),E=h(w,"CODE",{});var z=d(E);$=i(z,"BertModel"),z.forEach(o),g=i(w," with the equivalent "),T=h(w,"CODE",{});var N=d(T);B=i(N,"AutoModel"),N.forEach(o),v=i(w," class. We\u2019ll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task)."),w.forEach(o)},m(u,w){P(s,u,w),c(u,l,w),c(u,t,w),r(t,p),r(t,E),r(E,$),r(t,g),r(t,T),r(T,B),r(t,v),M=!0},i(u){M||(y(s.$$.fragment,u),M=!0)},o(u){b(s.$$.fragment,u),M=!1},d(u){A(s,u),u&&o(l),u&&o(t)}}}function na(k){let s,l;return s=new D({props:{code:`ls directory_on_my_computer

config.json tf_model.h5`,highlighted:`ls <span class="hljs-keyword">directory_on_my_computer
</span>
<span class="hljs-built_in">config</span>.<span class="hljs-keyword">json </span>tf_model.h5`}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function ia(k){let s,l;return s=new D({props:{code:`ls directory_on_my_computer

config.json pytorch_model.bin`,highlighted:`ls <span class="hljs-keyword">directory_on_my_computer
</span>
<span class="hljs-built_in">config</span>.<span class="hljs-keyword">json </span>pytorch_model.<span class="hljs-keyword">bin</span>`}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function ca(k){let s,l,t,p,E,$,g,T;return{c(){s=f("p"),l=n("The "),t=f("em"),p=n("tf_model.h5"),E=n(" file is known as the "),$=f("em"),g=n("state dictionary"),T=n("; it contains all your model\u2019s weights. The two files go hand in hand; the configuration is necessary to know your model\u2019s architecture, while the model weights are your model\u2019s parameters.")},l(B){s=h(B,"P",{});var v=d(s);l=i(v,"The "),t=h(v,"EM",{});var M=d(t);p=i(M,"tf_model.h5"),M.forEach(o),E=i(v," file is known as the "),$=h(v,"EM",{});var u=d($);g=i(u,"state dictionary"),u.forEach(o),T=i(v,"; it contains all your model\u2019s weights. The two files go hand in hand; the configuration is necessary to know your model\u2019s architecture, while the model weights are your model\u2019s parameters."),v.forEach(o)},m(B,v){c(B,s,v),r(s,l),r(s,t),r(t,p),r(s,E),r(s,$),r($,g),r(s,T)},d(B){B&&o(s)}}}function fa(k){let s,l,t,p,E,$,g,T;return{c(){s=f("p"),l=n("The "),t=f("em"),p=n("pytorch_model.bin"),E=n(" file is known as the "),$=f("em"),g=n("state dictionary"),T=n("; it contains all your model\u2019s weights. The two files go hand in hand; the configuration is necessary to know your model\u2019s architecture, while the model weights are your model\u2019s parameters.")},l(B){s=h(B,"P",{});var v=d(s);l=i(v,"The "),t=h(v,"EM",{});var M=d(t);p=i(M,"pytorch_model.bin"),M.forEach(o),E=i(v," file is known as the "),$=h(v,"EM",{});var u=d($);g=i(u,"state dictionary"),u.forEach(o),T=i(v,"; it contains all your model\u2019s weights. The two files go hand in hand; the configuration is necessary to know your model\u2019s architecture, while the model weights are your model\u2019s parameters."),v.forEach(o)},m(B,v){c(B,s,v),r(s,l),r(s,t),r(t,p),r(s,E),r(s,$),r($,g),r(s,T)},d(B){B&&o(s)}}}function ha(k){let s,l;return s=new D({props:{code:`import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model_inputs = tf.constant(encoded_sequences)`}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function da(k){let s,l;return s=new D({props:{code:`import torch

model_inputs = torch.tensor(encoded_sequences)`,highlighted:`<span class="hljs-keyword">import</span> torch

model_inputs = torch.tensor(encoded_sequences)`}}),{c(){C(s.$$.fragment)},l(t){q(s.$$.fragment,t)},m(t,p){P(s,t,p),l=!0},i(t){l||(y(s.$$.fragment,t),l=!0)},o(t){b(s.$$.fragment,t),l=!1},d(t){A(s,t)}}}function ua(k){let s,l,t,p,E,$,g,T,B,v,M,u,w,z,N,F,x,We,Re,Le,qo,xt,X,ae,wt,ye,Po,bt,Ao,Ht,Ue,zo,Wt,H,W,Ye,Ge,No,Rt,ke,Lt,ge,Ut,S,Fo,yt,Do,So,kt,Oo,Io,gt,xo,Ho,Yt,Z,le,vt,ve,Wo,$t,Ro,Gt,Je,Lo,Jt,R,L,Ve,ne,Uo,Ke,Yo,Go,Vt,ie,Jo,Et,Vo,Ko,Kt,U,Y,Qe,O,Qo,Tt,Xo,Zo,Bt,es,ts,$e,os,ss,Qt,Xe,rs,Xt,I,as,jt,ls,ns,Mt,is,cs,Ct,fs,hs,Zt,ce,ds,Ee,us,ps,eo,ee,fe,qt,Te,ms,Pt,_s,to,Q,ws,At,bs,ys,zt,ks,gs,oo,Be,so,Ze,vs,ro,G,J,et,he,$s,Nt,Es,Ts,ao,tt,te,de,Ft,je,Bs,Dt,js,lo,ot,Ms,no,st,Cs,io,rt,qs,co,Me,fo,ue,Ps,St,As,zs,ho,Ce,uo,at,Ns,po,V,K,lt,oe,pe,Ot,qe,Fs,It,Ds,mo,nt,Ss,_o,Pe,wo,it,Os,bo;t=new Jr({props:{fw:k[0]}}),T=new _t({});const Is=[Kr,Vr],Ae=[];function xs(e,a){return e[0]==="pt"?0:1}w=xs(k),z=Ae[w]=Is[w](k);const Hs=[Xr,Qr],ze=[];function Ws(e,a){return e[0]==="pt"?0:1}F=Ws(k),x=ze[F]=Hs[F](k);function Rs(e,a){return e[0]==="pt"?ea:Zr}let yo=Rs(k),se=yo(k);ye=new _t({});const Ls=[oa,ta],Ne=[];function Us(e,a){return e[0]==="pt"?0:1}H=Us(k),W=Ne[H]=Ls[H](k),ke=new D({props:{code:"print(config)",highlighted:'<span class="hljs-built_in">print</span>(config)'}}),ge=new D({props:{code:`BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}`,highlighted:`BertConfig {
  [...]
  <span class="hljs-string">&quot;hidden_size&quot;</span>: <span class="hljs-number">768</span>,
  <span class="hljs-string">&quot;intermediate_size&quot;</span>: <span class="hljs-number">3072</span>,
  <span class="hljs-string">&quot;max_position_embeddings&quot;</span>: <span class="hljs-number">512</span>,
  <span class="hljs-string">&quot;num_attention_heads&quot;</span>: <span class="hljs-number">12</span>,
  <span class="hljs-string">&quot;num_hidden_layers&quot;</span>: <span class="hljs-number">12</span>,
  [...]
}`}}),ve=new _t({});const Ys=[ra,sa],Fe=[];function Gs(e,a){return e[0]==="pt"?0:1}R=Gs(k),L=Fe[R]=Ys[R](k);const Js=[la,aa],De=[];function Vs(e,a){return e[0]==="pt"?0:1}U=Vs(k),Y=De[U]=Js[U](k),Te=new _t({}),Be=new D({props:{code:'model.save_pretrained("directory_on_my_computer")',highlighted:'model.save_pretrained(<span class="hljs-string">&quot;directory_on_my_computer&quot;</span>)'}});const Ks=[ia,na],Se=[];function Qs(e,a){return e[0]==="pt"?0:1}G=Qs(k),J=Se[G]=Ks[G](k);function Xs(e,a){return e[0]==="pt"?fa:ca}let ko=Xs(k),re=ko(k);je=new _t({}),Me=new D({props:{code:'sequences = ["Hello!", "Cool.", "Nice!"]',highlighted:'sequences = [<span class="hljs-string">&quot;Hello!&quot;</span>, <span class="hljs-string">&quot;Cool.&quot;</span>, <span class="hljs-string">&quot;Nice!&quot;</span>]'}}),Ce=new D({props:{code:`encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]`,highlighted:`encoded_sequences = [
    [<span class="hljs-number">101</span>, <span class="hljs-number">7592</span>, <span class="hljs-number">999</span>, <span class="hljs-number">102</span>],
    [<span class="hljs-number">101</span>, <span class="hljs-number">4658</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
    [<span class="hljs-number">101</span>, <span class="hljs-number">3835</span>, <span class="hljs-number">999</span>, <span class="hljs-number">102</span>],
]`}});const Zs=[da,ha],Oe=[];function er(e,a){return e[0]==="pt"?0:1}return V=er(k),K=Oe[V]=Zs[V](k),qe=new _t({}),Pe=new D({props:{code:"output = model(model_inputs)",highlighted:"output = model(model_inputs)"}}),{c(){s=f("meta"),l=m(),C(t.$$.fragment),p=m(),E=f("h1"),$=f("a"),g=f("span"),C(T.$$.fragment),B=m(),v=f("span"),M=n("Models"),u=m(),z.c(),N=m(),x.c(),We=m(),se.c(),Re=m(),Le=f("p"),qo=n("However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Let\u2019s take a look at how this works with a BERT model."),xt=m(),X=f("h2"),ae=f("a"),wt=f("span"),C(ye.$$.fragment),Po=m(),bt=f("span"),Ao=n("Creating a Transformer"),Ht=m(),Ue=f("p"),zo=n("The first thing we\u2019ll need to do to initialize a BERT model is load a configuration object:"),Wt=m(),W.c(),Ye=m(),Ge=f("p"),No=n("The configuration contains many attributes that are used to build the model:"),Rt=m(),C(ke.$$.fragment),Lt=m(),C(ge.$$.fragment),Ut=m(),S=f("p"),Fo=n("While you haven\u2019t seen what all of these attributes do yet, you should recognize some of them: the "),yt=f("code"),Do=n("hidden_size"),So=n(" attribute defines the size of the "),kt=f("code"),Oo=n("hidden_states"),Io=n(" vector, and "),gt=f("code"),xo=n("num_hidden_layers"),Ho=n(" defines the number of layers the Transformer model has."),Yt=m(),Z=f("h3"),le=f("a"),vt=f("span"),C(ve.$$.fragment),Wo=m(),$t=f("span"),Ro=n("Different loading methods"),Gt=m(),Je=f("p"),Lo=n("Creating a model from the default configuration initializes it with random values:"),Jt=m(),L.c(),Ve=m(),ne=f("p"),Uo=n("The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but as you saw in "),Ke=f("a"),Yo=n("Chapter 1"),Go=n(", this would require a long time and a lot of data, and it would have a non-negligible environmental impact. To avoid unnecessary and duplicated effort, it\u2019s imperative to be able to share and reuse models that have already been trained."),Vt=m(),ie=f("p"),Jo=n("Loading a Transformer model that is already trained is simple \u2014 we can do this using the "),Et=f("code"),Vo=n("from_pretrained()"),Ko=n(" method:"),Kt=m(),Y.c(),Qe=m(),O=f("p"),Qo=n("In the code sample above we didn\u2019t use "),Tt=f("code"),Xo=n("BertConfig"),Zo=n(", and instead loaded a pretrained model via the "),Bt=f("code"),es=n("bert-base-cased"),ts=n(" identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its "),$e=f("a"),os=n("model card"),ss=n("."),Qt=m(),Xe=f("p"),rs=n("This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results."),Xt=m(),I=f("p"),as=n("The weights have been downloaded and cached (so future calls to the "),jt=f("code"),ls=n("from_pretrained()"),ns=n(" method won\u2019t re-download them) in the cache folder, which defaults to "),Mt=f("em"),is=n("~/.cache/huggingface/transformers"),cs=n(". You can customize your cache folder by setting the "),Ct=f("code"),fs=n("HF_HOME"),hs=n(" environment variable."),Zt=m(),ce=f("p"),ds=n("The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture. The entire list of available BERT checkpoints can be found "),Ee=f("a"),us=n("here"),ps=n("."),eo=m(),ee=f("h3"),fe=f("a"),qt=f("span"),C(Te.$$.fragment),ms=m(),Pt=f("span"),_s=n("Saving methods"),to=m(),Q=f("p"),ws=n("Saving a model is as easy as loading one \u2014 we use the "),At=f("code"),bs=n("save_pretrained()"),ys=n(" method, which is analogous to the "),zt=f("code"),ks=n("from_pretrained()"),gs=n(" method:"),oo=m(),C(Be.$$.fragment),so=m(),Ze=f("p"),vs=n("This saves two files to your disk:"),ro=m(),J.c(),et=m(),he=f("p"),$s=n("If you take a look at the "),Nt=f("em"),Es=n("config.json"),Ts=n(" file, you\u2019ll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what \u{1F917} Transformers version you were using when you last saved the checkpoint."),ao=m(),re.c(),tt=m(),te=f("h2"),de=f("a"),Ft=f("span"),C(je.$$.fragment),Bs=m(),Dt=f("span"),js=n("Using a Transformer model for inference"),lo=m(),ot=f("p"),Ms=n("Now that you know how to load and save a model, let\u2019s try using it to make some predictions. Transformer models can only process numbers \u2014 numbers that the tokenizer generates. But before we discuss tokenizers, let\u2019s explore what inputs the model accepts."),no=m(),st=f("p"),Cs=n("Tokenizers can take care of casting the inputs to the appropriate framework\u2019s tensors, but to help you understand what\u2019s going on, we\u2019ll take a quick look at what must be done before sending the inputs to the model."),io=m(),rt=f("p"),qs=n("Let\u2019s say we have a couple of sequences:"),co=m(),C(Me.$$.fragment),fo=m(),ue=f("p"),Ps=n("The tokenizer converts these to vocabulary indices which are typically called "),St=f("em"),As=n("input IDs"),zs=n(". Each sequence is now a list of numbers! The resulting output is:"),ho=m(),C(Ce.$$.fragment),uo=m(),at=f("p"),Ns=n("This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This \u201Carray\u201D is already of rectangular shape, so converting it to a tensor is easy:"),po=m(),K.c(),lt=m(),oe=f("h3"),pe=f("a"),Ot=f("span"),C(qe.$$.fragment),Fs=m(),It=f("span"),Ds=n("Using the tensors as inputs to the model"),mo=m(),nt=f("p"),Ss=n("Making use of the tensors with the model is extremely simple \u2014 we just call the model with the inputs:"),_o=m(),C(Pe.$$.fragment),wo=m(),it=f("p"),Os=n(`While the model accepts a lot of different arguments, only the input IDs are necessary. We\u2019ll explain what the other arguments do and when they are required later,
but first we need to take a closer look at the tokenizers that build the inputs that a Transformer model can understand.`),this.h()},l(e){const a=Yr('[data-svelte="svelte-1phssyn"]',document.head);s=h(a,"META",{name:!0,content:!0}),a.forEach(o),l=_(e),q(t.$$.fragment,e),p=_(e),E=h(e,"H1",{class:!0});var Ie=d(E);$=h(Ie,"A",{id:!0,class:!0,href:!0});var ct=d($);g=h(ct,"SPAN",{});var ft=d(g);q(T.$$.fragment,ft),ft.forEach(o),ct.forEach(o),B=_(Ie),v=h(Ie,"SPAN",{});var ht=d(v);M=i(ht,"Models"),ht.forEach(o),Ie.forEach(o),u=_(e),z.l(e),N=_(e),x.l(e),We=_(e),se.l(e),Re=_(e),Le=h(e,"P",{});var dt=d(Le);qo=i(dt,"However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Let\u2019s take a look at how this works with a BERT model."),dt.forEach(o),xt=_(e),X=h(e,"H2",{class:!0});var me=d(X);ae=h(me,"A",{id:!0,class:!0,href:!0});var ut=d(ae);wt=h(ut,"SPAN",{});var pt=d(wt);q(ye.$$.fragment,pt),pt.forEach(o),ut.forEach(o),Po=_(me),bt=h(me,"SPAN",{});var tr=d(bt);Ao=i(tr,"Creating a Transformer"),tr.forEach(o),me.forEach(o),Ht=_(e),Ue=h(e,"P",{});var or=d(Ue);zo=i(or,"The first thing we\u2019ll need to do to initialize a BERT model is load a configuration object:"),or.forEach(o),Wt=_(e),W.l(e),Ye=_(e),Ge=h(e,"P",{});var sr=d(Ge);No=i(sr,"The configuration contains many attributes that are used to build the model:"),sr.forEach(o),Rt=_(e),q(ke.$$.fragment,e),Lt=_(e),q(ge.$$.fragment,e),Ut=_(e),S=h(e,"P",{});var _e=d(S);Fo=i(_e,"While you haven\u2019t seen what all of these attributes do yet, you should recognize some of them: the "),yt=h(_e,"CODE",{});var rr=d(yt);Do=i(rr,"hidden_size"),rr.forEach(o),So=i(_e," attribute defines the size of the "),kt=h(_e,"CODE",{});var ar=d(kt);Oo=i(ar,"hidden_states"),ar.forEach(o),Io=i(_e," vector, and "),gt=h(_e,"CODE",{});var lr=d(gt);xo=i(lr,"num_hidden_layers"),lr.forEach(o),Ho=i(_e," defines the number of layers the Transformer model has."),_e.forEach(o),Yt=_(e),Z=h(e,"H3",{class:!0});var go=d(Z);le=h(go,"A",{id:!0,class:!0,href:!0});var nr=d(le);vt=h(nr,"SPAN",{});var ir=d(vt);q(ve.$$.fragment,ir),ir.forEach(o),nr.forEach(o),Wo=_(go),$t=h(go,"SPAN",{});var cr=d($t);Ro=i(cr,"Different loading methods"),cr.forEach(o),go.forEach(o),Gt=_(e),Je=h(e,"P",{});var fr=d(Je);Lo=i(fr,"Creating a model from the default configuration initializes it with random values:"),fr.forEach(o),Jt=_(e),L.l(e),Ve=_(e),ne=h(e,"P",{});var vo=d(ne);Uo=i(vo,"The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but as you saw in "),Ke=h(vo,"A",{href:!0});var hr=d(Ke);Yo=i(hr,"Chapter 1"),hr.forEach(o),Go=i(vo,", this would require a long time and a lot of data, and it would have a non-negligible environmental impact. To avoid unnecessary and duplicated effort, it\u2019s imperative to be able to share and reuse models that have already been trained."),vo.forEach(o),Vt=_(e),ie=h(e,"P",{});var $o=d(ie);Jo=i($o,"Loading a Transformer model that is already trained is simple \u2014 we can do this using the "),Et=h($o,"CODE",{});var dr=d(Et);Vo=i(dr,"from_pretrained()"),dr.forEach(o),Ko=i($o," method:"),$o.forEach(o),Kt=_(e),Y.l(e),Qe=_(e),O=h(e,"P",{});var we=d(O);Qo=i(we,"In the code sample above we didn\u2019t use "),Tt=h(we,"CODE",{});var ur=d(Tt);Xo=i(ur,"BertConfig"),ur.forEach(o),Zo=i(we,", and instead loaded a pretrained model via the "),Bt=h(we,"CODE",{});var pr=d(Bt);es=i(pr,"bert-base-cased"),pr.forEach(o),ts=i(we," identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its "),$e=h(we,"A",{href:!0,rel:!0});var mr=d($e);os=i(mr,"model card"),mr.forEach(o),ss=i(we,"."),we.forEach(o),Qt=_(e),Xe=h(e,"P",{});var _r=d(Xe);rs=i(_r,"This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results."),_r.forEach(o),Xt=_(e),I=h(e,"P",{});var be=d(I);as=i(be,"The weights have been downloaded and cached (so future calls to the "),jt=h(be,"CODE",{});var wr=d(jt);ls=i(wr,"from_pretrained()"),wr.forEach(o),ns=i(be," method won\u2019t re-download them) in the cache folder, which defaults to "),Mt=h(be,"EM",{});var br=d(Mt);is=i(br,"~/.cache/huggingface/transformers"),br.forEach(o),cs=i(be,". You can customize your cache folder by setting the "),Ct=h(be,"CODE",{});var yr=d(Ct);fs=i(yr,"HF_HOME"),yr.forEach(o),hs=i(be," environment variable."),be.forEach(o),Zt=_(e),ce=h(e,"P",{});var Eo=d(ce);ds=i(Eo,"The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture. The entire list of available BERT checkpoints can be found "),Ee=h(Eo,"A",{href:!0,rel:!0});var kr=d(Ee);us=i(kr,"here"),kr.forEach(o),ps=i(Eo,"."),Eo.forEach(o),eo=_(e),ee=h(e,"H3",{class:!0});var To=d(ee);fe=h(To,"A",{id:!0,class:!0,href:!0});var gr=d(fe);qt=h(gr,"SPAN",{});var vr=d(qt);q(Te.$$.fragment,vr),vr.forEach(o),gr.forEach(o),ms=_(To),Pt=h(To,"SPAN",{});var $r=d(Pt);_s=i($r,"Saving methods"),$r.forEach(o),To.forEach(o),to=_(e),Q=h(e,"P",{});var mt=d(Q);ws=i(mt,"Saving a model is as easy as loading one \u2014 we use the "),At=h(mt,"CODE",{});var Er=d(At);bs=i(Er,"save_pretrained()"),Er.forEach(o),ys=i(mt," method, which is analogous to the "),zt=h(mt,"CODE",{});var Tr=d(zt);ks=i(Tr,"from_pretrained()"),Tr.forEach(o),gs=i(mt," method:"),mt.forEach(o),oo=_(e),q(Be.$$.fragment,e),so=_(e),Ze=h(e,"P",{});var Br=d(Ze);vs=i(Br,"This saves two files to your disk:"),Br.forEach(o),ro=_(e),J.l(e),et=_(e),he=h(e,"P",{});var Bo=d(he);$s=i(Bo,"If you take a look at the "),Nt=h(Bo,"EM",{});var jr=d(Nt);Es=i(jr,"config.json"),jr.forEach(o),Ts=i(Bo," file, you\u2019ll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what \u{1F917} Transformers version you were using when you last saved the checkpoint."),Bo.forEach(o),ao=_(e),re.l(e),tt=_(e),te=h(e,"H2",{class:!0});var jo=d(te);de=h(jo,"A",{id:!0,class:!0,href:!0});var Mr=d(de);Ft=h(Mr,"SPAN",{});var Cr=d(Ft);q(je.$$.fragment,Cr),Cr.forEach(o),Mr.forEach(o),Bs=_(jo),Dt=h(jo,"SPAN",{});var qr=d(Dt);js=i(qr,"Using a Transformer model for inference"),qr.forEach(o),jo.forEach(o),lo=_(e),ot=h(e,"P",{});var Pr=d(ot);Ms=i(Pr,"Now that you know how to load and save a model, let\u2019s try using it to make some predictions. Transformer models can only process numbers \u2014 numbers that the tokenizer generates. But before we discuss tokenizers, let\u2019s explore what inputs the model accepts."),Pr.forEach(o),no=_(e),st=h(e,"P",{});var Ar=d(st);Cs=i(Ar,"Tokenizers can take care of casting the inputs to the appropriate framework\u2019s tensors, but to help you understand what\u2019s going on, we\u2019ll take a quick look at what must be done before sending the inputs to the model."),Ar.forEach(o),io=_(e),rt=h(e,"P",{});var zr=d(rt);qs=i(zr,"Let\u2019s say we have a couple of sequences:"),zr.forEach(o),co=_(e),q(Me.$$.fragment,e),fo=_(e),ue=h(e,"P",{});var Mo=d(ue);Ps=i(Mo,"The tokenizer converts these to vocabulary indices which are typically called "),St=h(Mo,"EM",{});var Nr=d(St);As=i(Nr,"input IDs"),Nr.forEach(o),zs=i(Mo,". Each sequence is now a list of numbers! The resulting output is:"),Mo.forEach(o),ho=_(e),q(Ce.$$.fragment,e),uo=_(e),at=h(e,"P",{});var Fr=d(at);Ns=i(Fr,"This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This \u201Carray\u201D is already of rectangular shape, so converting it to a tensor is easy:"),Fr.forEach(o),po=_(e),K.l(e),lt=_(e),oe=h(e,"H3",{class:!0});var Co=d(oe);pe=h(Co,"A",{id:!0,class:!0,href:!0});var Dr=d(pe);Ot=h(Dr,"SPAN",{});var Sr=d(Ot);q(qe.$$.fragment,Sr),Sr.forEach(o),Dr.forEach(o),Fs=_(Co),It=h(Co,"SPAN",{});var Or=d(It);Ds=i(Or,"Using the tensors as inputs to the model"),Or.forEach(o),Co.forEach(o),mo=_(e),nt=h(e,"P",{});var Ir=d(nt);Ss=i(Ir,"Making use of the tensors with the model is extremely simple \u2014 we just call the model with the inputs:"),Ir.forEach(o),_o=_(e),q(Pe.$$.fragment,e),wo=_(e),it=h(e,"P",{});var xr=d(it);Os=i(xr,`While the model accepts a lot of different arguments, only the input IDs are necessary. We\u2019ll explain what the other arguments do and when they are required later,
but first we need to take a closer look at the tokenizers that build the inputs that a Transformer model can understand.`),xr.forEach(o),this.h()},h(){j(s,"name","hf:doc:metadata"),j(s,"content",JSON.stringify(pa)),j($,"id","models"),j($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j($,"href","#models"),j(E,"class","relative group"),j(ae,"id","creating-a-transformer"),j(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(ae,"href","#creating-a-transformer"),j(X,"class","relative group"),j(le,"id","different-loading-methods"),j(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(le,"href","#different-loading-methods"),j(Z,"class","relative group"),j(Ke,"href","/course/chapter1"),j($e,"href","https://huggingface.co/bert-base-cased"),j($e,"rel","nofollow"),j(Ee,"href","https://huggingface.co/models?filter=bert"),j(Ee,"rel","nofollow"),j(fe,"id","saving-methods"),j(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(fe,"href","#saving-methods"),j(ee,"class","relative group"),j(de,"id","using-a-transformer-model-for-inference"),j(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(de,"href","#using-a-transformer-model-for-inference"),j(te,"class","relative group"),j(pe,"id","using-the-tensors-as-inputs-to-the-model"),j(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(pe,"href","#using-the-tensors-as-inputs-to-the-model"),j(oe,"class","relative group")},m(e,a){r(document.head,s),c(e,l,a),P(t,e,a),c(e,p,a),c(e,E,a),r(E,$),r($,g),P(T,g,null),r(E,B),r(E,v),r(v,M),c(e,u,a),Ae[w].m(e,a),c(e,N,a),ze[F].m(e,a),c(e,We,a),se.m(e,a),c(e,Re,a),c(e,Le,a),r(Le,qo),c(e,xt,a),c(e,X,a),r(X,ae),r(ae,wt),P(ye,wt,null),r(X,Po),r(X,bt),r(bt,Ao),c(e,Ht,a),c(e,Ue,a),r(Ue,zo),c(e,Wt,a),Ne[H].m(e,a),c(e,Ye,a),c(e,Ge,a),r(Ge,No),c(e,Rt,a),P(ke,e,a),c(e,Lt,a),P(ge,e,a),c(e,Ut,a),c(e,S,a),r(S,Fo),r(S,yt),r(yt,Do),r(S,So),r(S,kt),r(kt,Oo),r(S,Io),r(S,gt),r(gt,xo),r(S,Ho),c(e,Yt,a),c(e,Z,a),r(Z,le),r(le,vt),P(ve,vt,null),r(Z,Wo),r(Z,$t),r($t,Ro),c(e,Gt,a),c(e,Je,a),r(Je,Lo),c(e,Jt,a),Fe[R].m(e,a),c(e,Ve,a),c(e,ne,a),r(ne,Uo),r(ne,Ke),r(Ke,Yo),r(ne,Go),c(e,Vt,a),c(e,ie,a),r(ie,Jo),r(ie,Et),r(Et,Vo),r(ie,Ko),c(e,Kt,a),De[U].m(e,a),c(e,Qe,a),c(e,O,a),r(O,Qo),r(O,Tt),r(Tt,Xo),r(O,Zo),r(O,Bt),r(Bt,es),r(O,ts),r(O,$e),r($e,os),r(O,ss),c(e,Qt,a),c(e,Xe,a),r(Xe,rs),c(e,Xt,a),c(e,I,a),r(I,as),r(I,jt),r(jt,ls),r(I,ns),r(I,Mt),r(Mt,is),r(I,cs),r(I,Ct),r(Ct,fs),r(I,hs),c(e,Zt,a),c(e,ce,a),r(ce,ds),r(ce,Ee),r(Ee,us),r(ce,ps),c(e,eo,a),c(e,ee,a),r(ee,fe),r(fe,qt),P(Te,qt,null),r(ee,ms),r(ee,Pt),r(Pt,_s),c(e,to,a),c(e,Q,a),r(Q,ws),r(Q,At),r(At,bs),r(Q,ys),r(Q,zt),r(zt,ks),r(Q,gs),c(e,oo,a),P(Be,e,a),c(e,so,a),c(e,Ze,a),r(Ze,vs),c(e,ro,a),Se[G].m(e,a),c(e,et,a),c(e,he,a),r(he,$s),r(he,Nt),r(Nt,Es),r(he,Ts),c(e,ao,a),re.m(e,a),c(e,tt,a),c(e,te,a),r(te,de),r(de,Ft),P(je,Ft,null),r(te,Bs),r(te,Dt),r(Dt,js),c(e,lo,a),c(e,ot,a),r(ot,Ms),c(e,no,a),c(e,st,a),r(st,Cs),c(e,io,a),c(e,rt,a),r(rt,qs),c(e,co,a),P(Me,e,a),c(e,fo,a),c(e,ue,a),r(ue,Ps),r(ue,St),r(St,As),r(ue,zs),c(e,ho,a),P(Ce,e,a),c(e,uo,a),c(e,at,a),r(at,Ns),c(e,po,a),Oe[V].m(e,a),c(e,lt,a),c(e,oe,a),r(oe,pe),r(pe,Ot),P(qe,Ot,null),r(oe,Fs),r(oe,It),r(It,Ds),c(e,mo,a),c(e,nt,a),r(nt,Ss),c(e,_o,a),P(Pe,e,a),c(e,wo,a),c(e,it,a),r(it,Os),bo=!0},p(e,[a]){const Ie={};a&1&&(Ie.fw=e[0]),t.$set(Ie);let ct=w;w=xs(e),w!==ct&&(He(),b(Ae[ct],1,1,()=>{Ae[ct]=null}),xe(),z=Ae[w],z||(z=Ae[w]=Is[w](e),z.c()),y(z,1),z.m(N.parentNode,N));let ft=F;F=Ws(e),F!==ft&&(He(),b(ze[ft],1,1,()=>{ze[ft]=null}),xe(),x=ze[F],x||(x=ze[F]=Hs[F](e),x.c()),y(x,1),x.m(We.parentNode,We)),yo!==(yo=Rs(e))&&(se.d(1),se=yo(e),se&&(se.c(),se.m(Re.parentNode,Re)));let ht=H;H=Us(e),H!==ht&&(He(),b(Ne[ht],1,1,()=>{Ne[ht]=null}),xe(),W=Ne[H],W||(W=Ne[H]=Ls[H](e),W.c()),y(W,1),W.m(Ye.parentNode,Ye));let dt=R;R=Gs(e),R!==dt&&(He(),b(Fe[dt],1,1,()=>{Fe[dt]=null}),xe(),L=Fe[R],L||(L=Fe[R]=Ys[R](e),L.c()),y(L,1),L.m(Ve.parentNode,Ve));let me=U;U=Vs(e),U!==me&&(He(),b(De[me],1,1,()=>{De[me]=null}),xe(),Y=De[U],Y||(Y=De[U]=Js[U](e),Y.c()),y(Y,1),Y.m(Qe.parentNode,Qe));let ut=G;G=Qs(e),G!==ut&&(He(),b(Se[ut],1,1,()=>{Se[ut]=null}),xe(),J=Se[G],J||(J=Se[G]=Ks[G](e),J.c()),y(J,1),J.m(et.parentNode,et)),ko!==(ko=Xs(e))&&(re.d(1),re=ko(e),re&&(re.c(),re.m(tt.parentNode,tt)));let pt=V;V=er(e),V!==pt&&(He(),b(Oe[pt],1,1,()=>{Oe[pt]=null}),xe(),K=Oe[V],K||(K=Oe[V]=Zs[V](e),K.c()),y(K,1),K.m(lt.parentNode,lt))},i(e){bo||(y(t.$$.fragment,e),y(T.$$.fragment,e),y(z),y(x),y(ye.$$.fragment,e),y(W),y(ke.$$.fragment,e),y(ge.$$.fragment,e),y(ve.$$.fragment,e),y(L),y(Y),y(Te.$$.fragment,e),y(Be.$$.fragment,e),y(J),y(je.$$.fragment,e),y(Me.$$.fragment,e),y(Ce.$$.fragment,e),y(K),y(qe.$$.fragment,e),y(Pe.$$.fragment,e),bo=!0)},o(e){b(t.$$.fragment,e),b(T.$$.fragment,e),b(z),b(x),b(ye.$$.fragment,e),b(W),b(ke.$$.fragment,e),b(ge.$$.fragment,e),b(ve.$$.fragment,e),b(L),b(Y),b(Te.$$.fragment,e),b(Be.$$.fragment,e),b(J),b(je.$$.fragment,e),b(Me.$$.fragment,e),b(Ce.$$.fragment,e),b(K),b(qe.$$.fragment,e),b(Pe.$$.fragment,e),bo=!1},d(e){o(s),e&&o(l),A(t,e),e&&o(p),e&&o(E),A(T),e&&o(u),Ae[w].d(e),e&&o(N),ze[F].d(e),e&&o(We),se.d(e),e&&o(Re),e&&o(Le),e&&o(xt),e&&o(X),A(ye),e&&o(Ht),e&&o(Ue),e&&o(Wt),Ne[H].d(e),e&&o(Ye),e&&o(Ge),e&&o(Rt),A(ke,e),e&&o(Lt),A(ge,e),e&&o(Ut),e&&o(S),e&&o(Yt),e&&o(Z),A(ve),e&&o(Gt),e&&o(Je),e&&o(Jt),Fe[R].d(e),e&&o(Ve),e&&o(ne),e&&o(Vt),e&&o(ie),e&&o(Kt),De[U].d(e),e&&o(Qe),e&&o(O),e&&o(Qt),e&&o(Xe),e&&o(Xt),e&&o(I),e&&o(Zt),e&&o(ce),e&&o(eo),e&&o(ee),A(Te),e&&o(to),e&&o(Q),e&&o(oo),A(Be,e),e&&o(so),e&&o(Ze),e&&o(ro),Se[G].d(e),e&&o(et),e&&o(he),e&&o(ao),re.d(e),e&&o(tt),e&&o(te),A(je),e&&o(lo),e&&o(ot),e&&o(no),e&&o(st),e&&o(io),e&&o(rt),e&&o(co),A(Me,e),e&&o(fo),e&&o(ue),e&&o(ho),A(Ce,e),e&&o(uo),e&&o(at),e&&o(po),Oe[V].d(e),e&&o(lt),e&&o(oe),A(qe),e&&o(mo),e&&o(nt),e&&o(_o),A(Pe,e),e&&o(wo),e&&o(it)}}}const pa={local:"models",sections:[{local:"creating-a-transformer",sections:[{local:"different-loading-methods",title:"Different loading methods"},{local:"saving-methods",title:"Saving methods"}],title:"Creating a Transformer"},{local:"using-a-transformer-model-for-inference",sections:[{local:"using-the-tensors-as-inputs-to-the-model",title:"Using the tensors as inputs to the model"}],title:"Using a Transformer model for inference"}],title:"Models"};function ma(k,s,l){let t="pt";return Gr(()=>{const p=new URLSearchParams(window.location.search);l(0,t=p.get("fw")||"pt")}),[t]}class va extends Rr{constructor(s){super();Lr(this,s,ma,ua,Ur,{})}}export{va as default,pa as metadata};
