import{S as rt,i as lt,s as st,e as r,k as w,w as Oe,t as o,M as it,c as l,d as a,m as v,a as s,x as De,h as n,b as T,G as t,g as d,y as qe,q as He,o as Ge,B as Re,v as dt}from"../../chunks/vendor-hf-doc-builder.js";import{T as ht}from"../../chunks/Tip-hf-doc-builder.js";import{I as ct}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ft}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function mt(K){let c,p,m,u;return{c(){c=o("\u26A0\uFE0F In order to benefit from all features available with the Model Hub and \u{1F917} Transformers, we recommend "),p=r("a"),m=o("creating an account"),u=o("."),this.h()},l(h){c=n(h,"\u26A0\uFE0F In order to benefit from all features available with the Model Hub and \u{1F917} Transformers, we recommend "),p=l(h,"A",{href:!0});var f=s(p);m=n(f,"creating an account"),f.forEach(a),u=n(h,"."),this.h()},h(){T(p,"href","https://huggingface.co/join")},m(h,f){d(h,c,f),d(h,p,f),t(p,m),d(h,u,f)},d(h){h&&a(c),h&&a(p),h&&a(u)}}}function ut(K){let c,p,m,u,h,f,ne,D,re,Q,$,V,y,le,x,se,ie,q,de,he,X,A,ce,Y,b,M,H,fe,me,ue,g,G,pe,we,R,ve,ye,j,be,ge,_e,C,B,ke,Te,Z,L,Ee,ee,_,$e,U,Ie,Pe,N,xe,Ae,te,k,Me,W,Ce,Le,J,Ne,Se,ae,E,oe;return f=new ct({}),$=new ft({props:{chapter:2,classNames:"absolute z-10 right-0 top-0"}}),E=new ht({props:{$$slots:{default:[mt]},$$scope:{ctx:K}}}),{c(){c=r("meta"),p=w(),m=r("h1"),u=r("a"),h=r("span"),Oe(f.$$.fragment),ne=w(),D=r("span"),re=o("Introduction"),Q=w(),Oe($.$$.fragment),V=w(),y=r("p"),le=o("As you saw in "),x=r("a"),se=o("Chapter 1"),ie=o(", Transformer models are usually very large. With millions to tens of "),q=r("em"),de=o("billions"),he=o(" of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task."),X=w(),A=r("p"),ce=o("The \u{1F917} Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. The library\u2019s main features are:"),Y=w(),b=r("ul"),M=r("li"),H=r("strong"),fe=o("Ease of use"),me=o(": Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code."),ue=w(),g=r("li"),G=r("strong"),pe=o("Flexibility"),we=o(": At their core, all models are simple PyTorch "),R=r("code"),ve=o("nn.Module"),ye=o(" or TensorFlow "),j=r("code"),be=o("tf.keras.Model"),ge=o(" classes and can be handled like any other models in their respective machine learning (ML) frameworks."),_e=w(),C=r("li"),B=r("strong"),ke=o("Simplicity"),Te=o(": Hardly any abstractions are made across the library. The \u201CAll in one file\u201D is a core concept: a model\u2019s forward pass is entirely defined in a single file, so that the code itself is understandable and hackable."),Z=w(),L=r("p"),Ee=o(`This last feature makes \u{1F917} Transformers quite different from other ML libraries. The models are not built on modules
that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.`),ee=w(),_=r("p"),$e=o("This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the "),U=r("code"),Ie=o("pipeline()"),Pe=o(" function introduced in "),N=r("a"),xe=o("Chapter 1"),Ae=o(". Next, we\u2019ll discuss the model API: we\u2019ll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions."),te=w(),k=r("p"),Me=o("Then we\u2019ll look at the tokenizer API, which is the other main component of the "),W=r("code"),Ce=o("pipeline()"),Le=o(" function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we\u2019ll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level "),J=r("code"),Ne=o("tokenizer()"),Se=o(" function."),ae=w(),Oe(E.$$.fragment),this.h()},l(e){const i=it('[data-svelte="svelte-1phssyn"]',document.head);c=l(i,"META",{name:!0,content:!0}),i.forEach(a),p=v(e),m=l(e,"H1",{class:!0});var I=s(m);u=l(I,"A",{id:!0,class:!0,href:!0});var je=s(u);h=l(je,"SPAN",{});var Be=s(h);De(f.$$.fragment,Be),Be.forEach(a),je.forEach(a),ne=v(I),D=l(I,"SPAN",{});var Ue=s(D);re=n(Ue,"Introduction"),Ue.forEach(a),I.forEach(a),Q=v(e),De($.$$.fragment,e),V=v(e),y=l(e,"P",{});var S=s(y);le=n(S,"As you saw in "),x=l(S,"A",{href:!0});var We=s(x);se=n(We,"Chapter 1"),We.forEach(a),ie=n(S,", Transformer models are usually very large. With millions to tens of "),q=l(S,"EM",{});var Je=s(q);de=n(Je,"billions"),Je.forEach(a),he=n(S," of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task."),S.forEach(a),X=v(e),A=l(e,"P",{});var Ke=s(A);ce=n(Ke,"The \u{1F917} Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. The library\u2019s main features are:"),Ke.forEach(a),Y=v(e),b=l(e,"UL",{});var z=s(b);M=l(z,"LI",{});var ze=s(M);H=l(ze,"STRONG",{});var Qe=s(H);fe=n(Qe,"Ease of use"),Qe.forEach(a),me=n(ze,": Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code."),ze.forEach(a),ue=v(z),g=l(z,"LI",{});var P=s(g);G=l(P,"STRONG",{});var Ve=s(G);pe=n(Ve,"Flexibility"),Ve.forEach(a),we=n(P,": At their core, all models are simple PyTorch "),R=l(P,"CODE",{});var Xe=s(R);ve=n(Xe,"nn.Module"),Xe.forEach(a),ye=n(P," or TensorFlow "),j=l(P,"CODE",{});var Ye=s(j);be=n(Ye,"tf.keras.Model"),Ye.forEach(a),ge=n(P," classes and can be handled like any other models in their respective machine learning (ML) frameworks."),P.forEach(a),_e=v(z),C=l(z,"LI",{});var Fe=s(C);B=l(Fe,"STRONG",{});var Ze=s(B);ke=n(Ze,"Simplicity"),Ze.forEach(a),Te=n(Fe,": Hardly any abstractions are made across the library. The \u201CAll in one file\u201D is a core concept: a model\u2019s forward pass is entirely defined in a single file, so that the code itself is understandable and hackable."),Fe.forEach(a),z.forEach(a),Z=v(e),L=l(e,"P",{});var et=s(L);Ee=n(et,`This last feature makes \u{1F917} Transformers quite different from other ML libraries. The models are not built on modules
that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.`),et.forEach(a),ee=v(e),_=l(e,"P",{});var F=s(_);$e=n(F,"This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the "),U=l(F,"CODE",{});var tt=s(U);Ie=n(tt,"pipeline()"),tt.forEach(a),Pe=n(F," function introduced in "),N=l(F,"A",{href:!0});var at=s(N);xe=n(at,"Chapter 1"),at.forEach(a),Ae=n(F,". Next, we\u2019ll discuss the model API: we\u2019ll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions."),F.forEach(a),te=v(e),k=l(e,"P",{});var O=s(k);Me=n(O,"Then we\u2019ll look at the tokenizer API, which is the other main component of the "),W=l(O,"CODE",{});var ot=s(W);Ce=n(ot,"pipeline()"),ot.forEach(a),Le=n(O," function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we\u2019ll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level "),J=l(O,"CODE",{});var nt=s(J);Ne=n(nt,"tokenizer()"),nt.forEach(a),Se=n(O," function."),O.forEach(a),ae=v(e),De(E.$$.fragment,e),this.h()},h(){T(c,"name","hf:doc:metadata"),T(c,"content",JSON.stringify(pt)),T(u,"id","introduction"),T(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),T(u,"href","#introduction"),T(m,"class","relative group"),T(x,"href","/course/chapter1"),T(N,"href","/course/chapter1")},m(e,i){t(document.head,c),d(e,p,i),d(e,m,i),t(m,u),t(u,h),qe(f,h,null),t(m,ne),t(m,D),t(D,re),d(e,Q,i),qe($,e,i),d(e,V,i),d(e,y,i),t(y,le),t(y,x),t(x,se),t(y,ie),t(y,q),t(q,de),t(y,he),d(e,X,i),d(e,A,i),t(A,ce),d(e,Y,i),d(e,b,i),t(b,M),t(M,H),t(H,fe),t(M,me),t(b,ue),t(b,g),t(g,G),t(G,pe),t(g,we),t(g,R),t(R,ve),t(g,ye),t(g,j),t(j,be),t(g,ge),t(b,_e),t(b,C),t(C,B),t(B,ke),t(C,Te),d(e,Z,i),d(e,L,i),t(L,Ee),d(e,ee,i),d(e,_,i),t(_,$e),t(_,U),t(U,Ie),t(_,Pe),t(_,N),t(N,xe),t(_,Ae),d(e,te,i),d(e,k,i),t(k,Me),t(k,W),t(W,Ce),t(k,Le),t(k,J),t(J,Ne),t(k,Se),d(e,ae,i),qe(E,e,i),oe=!0},p(e,[i]){const I={};i&2&&(I.$$scope={dirty:i,ctx:e}),E.$set(I)},i(e){oe||(He(f.$$.fragment,e),He($.$$.fragment,e),He(E.$$.fragment,e),oe=!0)},o(e){Ge(f.$$.fragment,e),Ge($.$$.fragment,e),Ge(E.$$.fragment,e),oe=!1},d(e){a(c),e&&a(p),e&&a(m),Re(f),e&&a(Q),Re($,e),e&&a(V),e&&a(y),e&&a(X),e&&a(A),e&&a(Y),e&&a(b),e&&a(Z),e&&a(L),e&&a(ee),e&&a(_),e&&a(te),e&&a(k),e&&a(ae),Re(E,e)}}}const pt={local:"introduction",title:"Introduction"};function wt(K){return dt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _t extends rt{constructor(c){super();lt(this,c,wt,ut,st,{})}}export{_t as default,pt as metadata};
