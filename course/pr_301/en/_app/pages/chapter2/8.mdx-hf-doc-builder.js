import{S as po,i as mo,s as go,e as a,k as p,w as _,t as P,l as co,M as $o,c as s,d as t,m as d,x,a as h,h as N,b as i,G as o,g as f,y as A,o as w,p as uo,q as v,B as z,v as wo,n as fo}from"../../chunks/vendor-hf-doc-builder.js";import{I as M}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ht}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as vo}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{Q as W}from"../../chunks/Question-hf-doc-builder.js";import{F as ko}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function bo(j){let c,u,g,m,E,k,S,q,$,T;return m=new M({}),$=new W({props:{choices:[{text:"A model that automatically trains on your data",explain:"Incorrect. Are you mistaking this with our <a href='https://huggingface.co/autotrain'>AutoTrain</a> product?"},{text:"An object that returns the correct architecture based on the checkpoint",explain:"Exactly: the <code>TFAutoModel</code> only needs to know the checkpoint from which to initialize to return the correct architecture.",correct:!0},{text:"A model that automatically detects the language used for its inputs to load the correct weights",explain:"Incorrect; while some checkpoints and models are capable of handling multiple languages, there are no built-in tools for automatic checkpoint selection according to language. You should head over to the <a href='https://huggingface.co/models'>Model Hub</a> to find the best checkpoint for your task!"}]}}),{c(){c=a("h3"),u=a("a"),g=a("span"),_(m.$$.fragment),E=p(),k=a("span"),S=P("5. What is an TFAutoModel?"),q=p(),_($.$$.fragment),this.h()},l(n){c=s(n,"H3",{class:!0});var b=h(c);u=s(b,"A",{id:!0,class:!0,href:!0});var r=h(u);g=s(r,"SPAN",{});var y=h(g);x(m.$$.fragment,y),y.forEach(t),r.forEach(t),E=d(b),k=s(b,"SPAN",{});var I=h(k);S=N(I,"5. What is an TFAutoModel?"),I.forEach(t),b.forEach(t),q=d(n),x($.$$.fragment,n),this.h()},h(){i(u,"id","5.-what-is-an-tfautomodel?"),i(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(u,"href","#5.-what-is-an-tfautomodel?"),i(c,"class","relative group")},m(n,b){f(n,c,b),o(c,u),o(u,g),A(m,g,null),o(c,E),o(c,k),o(k,S),f(n,q,b),A($,n,b),T=!0},i(n){T||(v(m.$$.fragment,n),v($.$$.fragment,n),T=!0)},o(n){w(m.$$.fragment,n),w($.$$.fragment,n),T=!1},d(n){n&&t(c),z(m),n&&t(q),z($,n)}}}function yo(j){let c,u,g,m,E,k,S,q,$,T;return m=new M({}),$=new W({props:{choices:[{text:"A model that automatically trains on your data",explain:"Incorrect. Are you mistaking this with our <a href='https://huggingface.co/autotrain'>AutoTrain</a> product?"},{text:"An object that returns the correct architecture based on the checkpoint",explain:"Exactly: the <code>AutoModel</code> only needs to know the checkpoint from which to initialize to return the correct architecture.",correct:!0},{text:"A model that automatically detects the language used for its inputs to load the correct weights",explain:"Incorrect; while some checkpoints and models are capable of handling multiple languages, there are no built-in tools for automatic checkpoint selection according to language. You should head over to the <a href='https://huggingface.co/models'>Model Hub</a> to find the best checkpoint for your task!"}]}}),{c(){c=a("h3"),u=a("a"),g=a("span"),_(m.$$.fragment),E=p(),k=a("span"),S=P("5. What is an AutoModel?"),q=p(),_($.$$.fragment),this.h()},l(n){c=s(n,"H3",{class:!0});var b=h(c);u=s(b,"A",{id:!0,class:!0,href:!0});var r=h(u);g=s(r,"SPAN",{});var y=h(g);x(m.$$.fragment,y),y.forEach(t),r.forEach(t),E=d(b),k=s(b,"SPAN",{});var I=h(k);S=N(I,"5. What is an AutoModel?"),I.forEach(t),b.forEach(t),q=d(n),x($.$$.fragment,n),this.h()},h(){i(u,"id","5.-what-is-an-automodel?"),i(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(u,"href","#5.-what-is-an-automodel?"),i(c,"class","relative group")},m(n,b){f(n,c,b),o(c,u),o(u,g),A(m,g,null),o(c,E),o(c,k),o(k,S),f(n,q,b),A($,n,b),T=!0},i(n){T||(v(m.$$.fragment,n),v($.$$.fragment,n),T=!0)},o(n){w(m.$$.fragment,n),w($.$$.fragment,n),T=!1},d(n){n&&t(c),z(m),n&&t(q),z($,n)}}}function _o(j){let c,u,g,m,E,k,S,q,$,T,n,b;return m=new M({}),$=new Ht({props:{code:`from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`}}),n=new W({props:{choices:[{text:"No, it seems correct.",explain:"Unfortunately, coupling a model with a tokenizer that was trained with a different checkpoint is rarely a good idea. The model was not trained to make sense out of this tokenizer's output, so the model output (if it can even run!) will not make any sense."},{text:"The tokenizer and model should always be from the same checkpoint.",explain:"Right!",correct:!0},{text:"It's good practice to pad and truncate with the tokenizer as every input is a batch.",explain:"It's true that every model input needs to be a batch. However, truncating or padding this sequence wouldn't necessarily make sense as there is only one of it, and those are techniques to batch together a list of sentences."}]}}),{c(){c=a("h3"),u=a("a"),g=a("span"),_(m.$$.fragment),E=p(),k=a("span"),S=P("10. Is there something wrong with the following code?"),q=p(),_($.$$.fragment),T=p(),_(n.$$.fragment),this.h()},l(r){c=s(r,"H3",{class:!0});var y=h(c);u=s(y,"A",{id:!0,class:!0,href:!0});var I=h(u);g=s(I,"SPAN",{});var H=h(g);x(m.$$.fragment,H),H.forEach(t),I.forEach(t),E=d(y),k=s(y,"SPAN",{});var C=h(k);S=N(C,"10. Is there something wrong with the following code?"),C.forEach(t),y.forEach(t),q=d(r),x($.$$.fragment,r),T=d(r),x(n.$$.fragment,r),this.h()},h(){i(u,"id","10.-is-there-something-wrong-with-the-following-code?"),i(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(u,"href","#10.-is-there-something-wrong-with-the-following-code?"),i(c,"class","relative group")},m(r,y){f(r,c,y),o(c,u),o(u,g),A(m,g,null),o(c,E),o(c,k),o(k,S),f(r,q,y),A($,r,y),f(r,T,y),A(n,r,y),b=!0},i(r){b||(v(m.$$.fragment,r),v($.$$.fragment,r),v(n.$$.fragment,r),b=!0)},o(r){w(m.$$.fragment,r),w($.$$.fragment,r),w(n.$$.fragment,r),b=!1},d(r){r&&t(c),z(m),r&&t(q),z($,r),r&&t(T),z(n,r)}}}function xo(j){let c,u,g,m,E,k,S,q,$,T,n,b;return m=new M({}),$=new Ht({props:{code:`from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`}}),n=new W({props:{choices:[{text:"No, it seems correct.",explain:"Unfortunately, coupling a model with a tokenizer that was trained with a different checkpoint is rarely a good idea. The model was not trained to make sense out of this tokenizer's output, so the model output (if it can even run!) will not make any sense."},{text:"The tokenizer and model should always be from the same checkpoint.",explain:"Right!",correct:!0},{text:"It's good practice to pad and truncate with the tokenizer as every input is a batch.",explain:"It's true that every model input needs to be a batch. However, truncating or padding this sequence wouldn't necessarily make sense as there is only one of it, and those are techniques to batch together a list of sentences."}]}}),{c(){c=a("h3"),u=a("a"),g=a("span"),_(m.$$.fragment),E=p(),k=a("span"),S=P("10. Is there something wrong with the following code?"),q=p(),_($.$$.fragment),T=p(),_(n.$$.fragment),this.h()},l(r){c=s(r,"H3",{class:!0});var y=h(c);u=s(y,"A",{id:!0,class:!0,href:!0});var I=h(u);g=s(I,"SPAN",{});var H=h(g);x(m.$$.fragment,H),H.forEach(t),I.forEach(t),E=d(y),k=s(y,"SPAN",{});var C=h(k);S=N(C,"10. Is there something wrong with the following code?"),C.forEach(t),y.forEach(t),q=d(r),x($.$$.fragment,r),T=d(r),x(n.$$.fragment,r),this.h()},h(){i(u,"id","10.-is-there-something-wrong-with-the-following-code?"),i(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(u,"href","#10.-is-there-something-wrong-with-the-following-code?"),i(c,"class","relative group")},m(r,y){f(r,c,y),o(c,u),o(u,g),A(m,g,null),o(c,E),o(c,k),o(k,S),f(r,q,y),A($,r,y),f(r,T,y),A(n,r,y),b=!0},i(r){b||(v(m.$$.fragment,r),v($.$$.fragment,r),v(n.$$.fragment,r),b=!0)},o(r){w(m.$$.fragment,r),w($.$$.fragment,r),w(n.$$.fragment,r),b=!1},d(r){r&&t(c),z(m),r&&t(q),z($,r),r&&t(T),z(n,r)}}}function Ao(j){let c,u,g,m,E,k,S,q,$,T,n,b,r,y,I,H,C,re,gt,Ee,$t,Re,ne,Ue,R,K,Te,ae,wt,Se,vt,Le,se,Oe,U,V,Ie,ie,kt,Pe,bt,Qe,he,Ge,L,X,Ne,le,yt,He,_t,Je,ce,Ke,F,D,xe,O,Z,Me,ue,xt,We,At,Ve,fe,Xe,Q,ee,je,pe,zt,Ce,qt,Ze,de,et,G,te,Fe,me,Et,De,Tt,tt,ge,ot,J,oe,Ye,$e,St,we,It,Be,Pt,Nt,rt,ve,nt,ke,at,Y,B,Ae,st;g=new ko({props:{fw:j[0]}}),q=new M({}),r=new vo({props:{chapter:2,classNames:"absolute z-10 right-0 top-0"}}),re=new M({}),ne=new W({props:{choices:[{text:"First, the model, which handles text and returns raw predictions. The tokenizer then makes sense of these predictions and converts them back to text when needed.",explain:"The model cannot understand text! The tokenizer must first tokenize the text and convert it to IDs so that it is understandable by the model."},{text:"First, the tokenizer, which handles text and returns IDs. The model handles these IDs and outputs a prediction, which can be some text.",explain:"The model's prediction cannot be text straight away. The tokenizer has to be used in order to convert the prediction back to text!"},{text:"The tokenizer handles text and returns IDs. The model handles these IDs and outputs a prediction. The tokenizer can then be used once again to convert these predictions back to some text.",explain:"Correct! The tokenizer can be used for both tokenizing and de-tokenizing.",correct:!0}]}}),ae=new M({}),se=new W({props:{choices:[{text:"2: The sequence length and the batch size",explain:"False! The tensor output by the model has a third dimension: hidden size."},{text:"2: The sequence length and the hidden size",explain:"False! All Transformer models handle batches, even with a single sequence; that would be a batch size of 1!"},{text:"3: The sequence length, the batch size, and the hidden size",explain:"Correct!",correct:!0}]}}),ie=new M({}),he=new W({props:{choices:[{text:"WordPiece",explain:"Yes, that's one example of subword tokenization!",correct:!0},{text:"Character-based tokenization",explain:"Character-based tokenization is not a type of subword tokenization."},{text:"Splitting on whitespace and punctuation",explain:"That's a word-based tokenization scheme!"},{text:"BPE",explain:"Yes, that's one example of subword tokenization!",correct:!0},{text:"Unigram",explain:"Yes, that's one example of subword tokenization!",correct:!0},{text:"None of the above",explain:"Incorrect!"}]}}),le=new M({}),ce=new W({props:{choices:[{text:"A component of the base Transformer network that redirects tensors to their correct layers",explain:"Incorrect! There's no such component."},{text:"Also known as the self-attention mechanism, it adapts the representation of a token according to the other tokens of the sequence",explain:'Incorrect! The self-attention layer does contain attention "heads," but these are not adaptation heads.'},{text:"An additional component, usually made up of one or a few layers, to convert the transformer predictions to a task-specific output",explain:"That's right. Adaptation heads, also known simply as heads, come up in different forms: language modeling heads, question answering heads, sequence classification heads... ",correct:!0}]}});const Mt=[yo,bo],be=[];function Wt(e,l){return e[0]==="pt"?0:1}F=Wt(j),D=be[F]=Mt[F](j),ue=new M({}),fe=new W({props:{choices:[{text:"Truncating",explain:"Yes, truncation is a correct way of evening out sequences so that they fit in a rectangular shape. Is it the only one, though?",correct:!0},{text:"Returning tensors",explain:"While the other techniques allow you to return rectangular tensors, returning tensors isn't helpful when batching sequences together."},{text:"Padding",explain:"Yes, padding is a correct way of evening out sequences so that they fit in a rectangular shape. Is it the only one, though?",correct:!0},{text:"Attention masking",explain:"Absolutely! Attention masks are of prime importance when handling sequences of different lengths. That's not the only technique to be aware of, however.",correct:!0}]}}),pe=new M({}),de=new W({props:{choices:[{text:"It softens the logits so that they're more reliable.",explain:"No, the SoftMax function does not affect the reliability of results."},{text:"It applies a lower and upper bound so that they're understandable.",explain:"Correct! The resulting values are bound between 0 and 1. That's not the only reason we use a SoftMax function, though.",correct:!0},{text:"The total sum of the output is then 1, resulting in a possible probabilistic interpretation.",explain:"Correct! That's not the only reason we use a SoftMax function, though.",correct:!0}]}}),me=new M({}),ge=new W({props:{choices:[{text:"<code>encode</code>, as it can encode text into IDs and IDs into predictions",explain:"Wrong! While the <code>encode</code> method does exist on tokenizers, it does not exist on models."},{text:"Calling the tokenizer object directly.",explain:"Exactly! The <code>__call__</code> method of the tokenizer is a very powerful method which can handle pretty much anything. It is also the method used to retrieve predictions from a model.",correct:!0},{text:"<code>pad</code>",explain:"Wrong! Padding is very useful, but it's just one part of the tokenizer API."},{text:"<code>tokenize</code>",explain:"The <code>tokenize</code> method is arguably one of the most useful methods, but it isn't the core of the tokenizer API."}]}}),$e=new M({}),ve=new Ht({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
result = tokenizer.tokenize(<span class="hljs-string">&quot;Hello!&quot;</span>)`}}),ke=new W({props:{choices:[{text:"A list of strings, each string being a token",explain:"Absolutely! Convert this to IDs, and send them to a model!",correct:!0},{text:"A list of IDs",explain:"Incorrect; that's what the <code>__call__</code> or <code>convert_tokens_to_ids</code> method is for!"},{text:"A string containing all of the tokens",explain:"This would be suboptimal, as the goal is to split the string into multiple tokens."}]}});const jt=[xo,_o],ye=[];function Ct(e,l){return e[0]==="pt"?0:1}return Y=Ct(j),B=ye[Y]=jt[Y](j),{c(){c=a("meta"),u=p(),_(g.$$.fragment),m=p(),E=a("h1"),k=a("a"),S=a("span"),_(q.$$.fragment),$=p(),T=a("span"),n=P("End-of-chapter quiz"),b=p(),_(r.$$.fragment),y=p(),I=a("h3"),H=a("a"),C=a("span"),_(re.$$.fragment),gt=p(),Ee=a("span"),$t=P("1. What is the order of the language modeling pipeline?"),Re=p(),_(ne.$$.fragment),Ue=p(),R=a("h3"),K=a("a"),Te=a("span"),_(ae.$$.fragment),wt=p(),Se=a("span"),vt=P("2. How many dimensions does the tensor output by the base Transformer model have, and what are they?"),Le=p(),_(se.$$.fragment),Oe=p(),U=a("h3"),V=a("a"),Ie=a("span"),_(ie.$$.fragment),kt=p(),Pe=a("span"),bt=P("3. Which of the following is an example of subword tokenization?"),Qe=p(),_(he.$$.fragment),Ge=p(),L=a("h3"),X=a("a"),Ne=a("span"),_(le.$$.fragment),yt=p(),He=a("span"),_t=P("4. What is a model head?"),Je=p(),_(ce.$$.fragment),Ke=p(),D.c(),xe=p(),O=a("h3"),Z=a("a"),Me=a("span"),_(ue.$$.fragment),xt=p(),We=a("span"),At=P("6. What are the techniques to be aware of when batching sequences of different lengths together?"),Ve=p(),_(fe.$$.fragment),Xe=p(),Q=a("h3"),ee=a("a"),je=a("span"),_(pe.$$.fragment),zt=p(),Ce=a("span"),qt=P("7. What is the point of applying a SoftMax function to the logits output by a sequence classification model?"),Ze=p(),_(de.$$.fragment),et=p(),G=a("h3"),te=a("a"),Fe=a("span"),_(me.$$.fragment),Et=p(),De=a("span"),Tt=P("8. What method is most of the tokenizer API centered around?"),tt=p(),_(ge.$$.fragment),ot=p(),J=a("h3"),oe=a("a"),Ye=a("span"),_($e.$$.fragment),St=p(),we=a("span"),It=P("9. What does the "),Be=a("code"),Pt=P("result"),Nt=P(" variable contain in this code sample?"),rt=p(),_(ve.$$.fragment),nt=p(),_(ke.$$.fragment),at=p(),B.c(),Ae=co(),this.h()},l(e){const l=$o('[data-svelte="svelte-1phssyn"]',document.head);c=s(l,"META",{name:!0,content:!0}),l.forEach(t),u=d(e),x(g.$$.fragment,e),m=d(e),E=s(e,"H1",{class:!0});var _e=h(E);k=s(_e,"A",{id:!0,class:!0,href:!0});var ze=h(k);S=s(ze,"SPAN",{});var qe=h(S);x(q.$$.fragment,qe),qe.forEach(t),ze.forEach(t),$=d(_e),T=s(_e,"SPAN",{});var Ft=h(T);n=N(Ft,"End-of-chapter quiz"),Ft.forEach(t),_e.forEach(t),b=d(e),x(r.$$.fragment,e),y=d(e),I=s(e,"H3",{class:!0});var it=h(I);H=s(it,"A",{id:!0,class:!0,href:!0});var Dt=h(H);C=s(Dt,"SPAN",{});var Yt=h(C);x(re.$$.fragment,Yt),Yt.forEach(t),Dt.forEach(t),gt=d(it),Ee=s(it,"SPAN",{});var Bt=h(Ee);$t=N(Bt,"1. What is the order of the language modeling pipeline?"),Bt.forEach(t),it.forEach(t),Re=d(e),x(ne.$$.fragment,e),Ue=d(e),R=s(e,"H3",{class:!0});var ht=h(R);K=s(ht,"A",{id:!0,class:!0,href:!0});var Rt=h(K);Te=s(Rt,"SPAN",{});var Ut=h(Te);x(ae.$$.fragment,Ut),Ut.forEach(t),Rt.forEach(t),wt=d(ht),Se=s(ht,"SPAN",{});var Lt=h(Se);vt=N(Lt,"2. How many dimensions does the tensor output by the base Transformer model have, and what are they?"),Lt.forEach(t),ht.forEach(t),Le=d(e),x(se.$$.fragment,e),Oe=d(e),U=s(e,"H3",{class:!0});var lt=h(U);V=s(lt,"A",{id:!0,class:!0,href:!0});var Ot=h(V);Ie=s(Ot,"SPAN",{});var Qt=h(Ie);x(ie.$$.fragment,Qt),Qt.forEach(t),Ot.forEach(t),kt=d(lt),Pe=s(lt,"SPAN",{});var Gt=h(Pe);bt=N(Gt,"3. Which of the following is an example of subword tokenization?"),Gt.forEach(t),lt.forEach(t),Qe=d(e),x(he.$$.fragment,e),Ge=d(e),L=s(e,"H3",{class:!0});var ct=h(L);X=s(ct,"A",{id:!0,class:!0,href:!0});var Jt=h(X);Ne=s(Jt,"SPAN",{});var Kt=h(Ne);x(le.$$.fragment,Kt),Kt.forEach(t),Jt.forEach(t),yt=d(ct),He=s(ct,"SPAN",{});var Vt=h(He);_t=N(Vt,"4. What is a model head?"),Vt.forEach(t),ct.forEach(t),Je=d(e),x(ce.$$.fragment,e),Ke=d(e),D.l(e),xe=d(e),O=s(e,"H3",{class:!0});var ut=h(O);Z=s(ut,"A",{id:!0,class:!0,href:!0});var Xt=h(Z);Me=s(Xt,"SPAN",{});var Zt=h(Me);x(ue.$$.fragment,Zt),Zt.forEach(t),Xt.forEach(t),xt=d(ut),We=s(ut,"SPAN",{});var eo=h(We);At=N(eo,"6. What are the techniques to be aware of when batching sequences of different lengths together?"),eo.forEach(t),ut.forEach(t),Ve=d(e),x(fe.$$.fragment,e),Xe=d(e),Q=s(e,"H3",{class:!0});var ft=h(Q);ee=s(ft,"A",{id:!0,class:!0,href:!0});var to=h(ee);je=s(to,"SPAN",{});var oo=h(je);x(pe.$$.fragment,oo),oo.forEach(t),to.forEach(t),zt=d(ft),Ce=s(ft,"SPAN",{});var ro=h(Ce);qt=N(ro,"7. What is the point of applying a SoftMax function to the logits output by a sequence classification model?"),ro.forEach(t),ft.forEach(t),Ze=d(e),x(de.$$.fragment,e),et=d(e),G=s(e,"H3",{class:!0});var pt=h(G);te=s(pt,"A",{id:!0,class:!0,href:!0});var no=h(te);Fe=s(no,"SPAN",{});var ao=h(Fe);x(me.$$.fragment,ao),ao.forEach(t),no.forEach(t),Et=d(pt),De=s(pt,"SPAN",{});var so=h(De);Tt=N(so,"8. What method is most of the tokenizer API centered around?"),so.forEach(t),pt.forEach(t),tt=d(e),x(ge.$$.fragment,e),ot=d(e),J=s(e,"H3",{class:!0});var dt=h(J);oe=s(dt,"A",{id:!0,class:!0,href:!0});var io=h(oe);Ye=s(io,"SPAN",{});var ho=h(Ye);x($e.$$.fragment,ho),ho.forEach(t),io.forEach(t),St=d(dt),we=s(dt,"SPAN",{});var mt=h(we);It=N(mt,"9. What does the "),Be=s(mt,"CODE",{});var lo=h(Be);Pt=N(lo,"result"),lo.forEach(t),Nt=N(mt," variable contain in this code sample?"),mt.forEach(t),dt.forEach(t),rt=d(e),x(ve.$$.fragment,e),nt=d(e),x(ke.$$.fragment,e),at=d(e),B.l(e),Ae=co(),this.h()},h(){i(c,"name","hf:doc:metadata"),i(c,"content",JSON.stringify(zo)),i(k,"id","endofchapter-quiz"),i(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(k,"href","#endofchapter-quiz"),i(E,"class","relative group"),i(H,"id","1.-what-is-the-order-of-the-language-modeling-pipeline?"),i(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(H,"href","#1.-what-is-the-order-of-the-language-modeling-pipeline?"),i(I,"class","relative group"),i(K,"id","2.-how-many-dimensions-does-the-tensor-output-by-the-base-transformer-model-have,-and-what-are-they?"),i(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(K,"href","#2.-how-many-dimensions-does-the-tensor-output-by-the-base-transformer-model-have,-and-what-are-they?"),i(R,"class","relative group"),i(V,"id","3.-which-of-the-following-is-an-example-of-subword-tokenization?"),i(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(V,"href","#3.-which-of-the-following-is-an-example-of-subword-tokenization?"),i(U,"class","relative group"),i(X,"id","4.-what-is-a-model-head?"),i(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(X,"href","#4.-what-is-a-model-head?"),i(L,"class","relative group"),i(Z,"id","6.-what-are-the-techniques-to-be-aware-of-when-batching-sequences-of-different-lengths-together?"),i(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Z,"href","#6.-what-are-the-techniques-to-be-aware-of-when-batching-sequences-of-different-lengths-together?"),i(O,"class","relative group"),i(ee,"id","7.-what-is-the-point-of-applying-a-softmax-function-to-the-logits-output-by-a-sequence-classification-model?"),i(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ee,"href","#7.-what-is-the-point-of-applying-a-softmax-function-to-the-logits-output-by-a-sequence-classification-model?"),i(Q,"class","relative group"),i(te,"id","8.-what-method-is-most-of-the-tokenizer-api-centered-around?"),i(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(te,"href","#8.-what-method-is-most-of-the-tokenizer-api-centered-around?"),i(G,"class","relative group"),i(oe,"id","9.-what-does-the-<code>result</code>-variable-contain-in-this-code-sample?"),i(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(oe,"href","#9.-what-does-the-<code>result</code>-variable-contain-in-this-code-sample?"),i(J,"class","relative group")},m(e,l){o(document.head,c),f(e,u,l),A(g,e,l),f(e,m,l),f(e,E,l),o(E,k),o(k,S),A(q,S,null),o(E,$),o(E,T),o(T,n),f(e,b,l),A(r,e,l),f(e,y,l),f(e,I,l),o(I,H),o(H,C),A(re,C,null),o(I,gt),o(I,Ee),o(Ee,$t),f(e,Re,l),A(ne,e,l),f(e,Ue,l),f(e,R,l),o(R,K),o(K,Te),A(ae,Te,null),o(R,wt),o(R,Se),o(Se,vt),f(e,Le,l),A(se,e,l),f(e,Oe,l),f(e,U,l),o(U,V),o(V,Ie),A(ie,Ie,null),o(U,kt),o(U,Pe),o(Pe,bt),f(e,Qe,l),A(he,e,l),f(e,Ge,l),f(e,L,l),o(L,X),o(X,Ne),A(le,Ne,null),o(L,yt),o(L,He),o(He,_t),f(e,Je,l),A(ce,e,l),f(e,Ke,l),be[F].m(e,l),f(e,xe,l),f(e,O,l),o(O,Z),o(Z,Me),A(ue,Me,null),o(O,xt),o(O,We),o(We,At),f(e,Ve,l),A(fe,e,l),f(e,Xe,l),f(e,Q,l),o(Q,ee),o(ee,je),A(pe,je,null),o(Q,zt),o(Q,Ce),o(Ce,qt),f(e,Ze,l),A(de,e,l),f(e,et,l),f(e,G,l),o(G,te),o(te,Fe),A(me,Fe,null),o(G,Et),o(G,De),o(De,Tt),f(e,tt,l),A(ge,e,l),f(e,ot,l),f(e,J,l),o(J,oe),o(oe,Ye),A($e,Ye,null),o(J,St),o(J,we),o(we,It),o(we,Be),o(Be,Pt),o(we,Nt),f(e,rt,l),A(ve,e,l),f(e,nt,l),A(ke,e,l),f(e,at,l),ye[Y].m(e,l),f(e,Ae,l),st=!0},p(e,[l]){const _e={};l&1&&(_e.fw=e[0]),g.$set(_e);let ze=F;F=Wt(e),F!==ze&&(fo(),w(be[ze],1,1,()=>{be[ze]=null}),uo(),D=be[F],D||(D=be[F]=Mt[F](e),D.c()),v(D,1),D.m(xe.parentNode,xe));let qe=Y;Y=Ct(e),Y!==qe&&(fo(),w(ye[qe],1,1,()=>{ye[qe]=null}),uo(),B=ye[Y],B||(B=ye[Y]=jt[Y](e),B.c()),v(B,1),B.m(Ae.parentNode,Ae))},i(e){st||(v(g.$$.fragment,e),v(q.$$.fragment,e),v(r.$$.fragment,e),v(re.$$.fragment,e),v(ne.$$.fragment,e),v(ae.$$.fragment,e),v(se.$$.fragment,e),v(ie.$$.fragment,e),v(he.$$.fragment,e),v(le.$$.fragment,e),v(ce.$$.fragment,e),v(D),v(ue.$$.fragment,e),v(fe.$$.fragment,e),v(pe.$$.fragment,e),v(de.$$.fragment,e),v(me.$$.fragment,e),v(ge.$$.fragment,e),v($e.$$.fragment,e),v(ve.$$.fragment,e),v(ke.$$.fragment,e),v(B),st=!0)},o(e){w(g.$$.fragment,e),w(q.$$.fragment,e),w(r.$$.fragment,e),w(re.$$.fragment,e),w(ne.$$.fragment,e),w(ae.$$.fragment,e),w(se.$$.fragment,e),w(ie.$$.fragment,e),w(he.$$.fragment,e),w(le.$$.fragment,e),w(ce.$$.fragment,e),w(D),w(ue.$$.fragment,e),w(fe.$$.fragment,e),w(pe.$$.fragment,e),w(de.$$.fragment,e),w(me.$$.fragment,e),w(ge.$$.fragment,e),w($e.$$.fragment,e),w(ve.$$.fragment,e),w(ke.$$.fragment,e),w(B),st=!1},d(e){t(c),e&&t(u),z(g,e),e&&t(m),e&&t(E),z(q),e&&t(b),z(r,e),e&&t(y),e&&t(I),z(re),e&&t(Re),z(ne,e),e&&t(Ue),e&&t(R),z(ae),e&&t(Le),z(se,e),e&&t(Oe),e&&t(U),z(ie),e&&t(Qe),z(he,e),e&&t(Ge),e&&t(L),z(le),e&&t(Je),z(ce,e),e&&t(Ke),be[F].d(e),e&&t(xe),e&&t(O),z(ue),e&&t(Ve),z(fe,e),e&&t(Xe),e&&t(Q),z(pe),e&&t(Ze),z(de,e),e&&t(et),e&&t(G),z(me),e&&t(tt),z(ge,e),e&&t(ot),e&&t(J),z($e),e&&t(rt),z(ve,e),e&&t(nt),z(ke,e),e&&t(at),ye[Y].d(e),e&&t(Ae)}}}const zo={local:"endofchapter-quiz",title:"End-of-chapter quiz"};function qo(j,c,u){let g="pt";return wo(()=>{const m=new URLSearchParams(window.location.search);u(0,g=m.get("fw")||"pt")}),[g]}class Ho extends po{constructor(c){super();mo(this,c,qo,Ao,go,{})}}export{Ho as default,zo as metadata};
