import{S as Me,i as Ne,s as ze,e as r,k as c,w as de,t as d,M as Ue,c as o,d as t,m,a as n,x as he,h,b as l,G as a,g as f,y as ue,L as De,q as pe,o as ve,B as _e,v as Je}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ye}from"../../chunks/Youtube-hf-doc-builder.js";import{I as je}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Fe}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Ge(Ee){let p,z,v,_,P,g,K,x,V,U,w,D,y,J,E,W,q,X,Z,Y,L,ee,j,k,te,F,B,ae,G,i,I,$,re,oe,C,b,ne,se,S,T,le,ie,M,A,fe,ce,N,R,me,H;return g=new je({}),w=new Fe({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),y=new Ye({props:{id:"MUqNwgPjJvQ"}}),{c(){p=r("meta"),z=c(),v=r("h1"),_=r("a"),P=r("span"),de(g.$$.fragment),K=c(),x=r("span"),V=d("Encoder models"),U=c(),de(w.$$.fragment),D=c(),de(y.$$.fragment),J=c(),E=r("p"),W=d("Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \u201Cbi-directional\u201D attention, and are often called "),q=r("em"),X=d("auto-encoding models"),Z=d("."),Y=c(),L=r("p"),ee=d("The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence."),j=c(),k=r("p"),te=d("Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering."),F=c(),B=r("p"),ae=d("Representatives of this family of models include:"),G=c(),i=r("ul"),I=r("li"),$=r("a"),re=d("ALBERT"),oe=c(),C=r("li"),b=r("a"),ne=d("BERT"),se=c(),S=r("li"),T=r("a"),le=d("DistilBERT"),ie=c(),M=r("li"),A=r("a"),fe=d("ELECTRA"),ce=c(),N=r("li"),R=r("a"),me=d("RoBERTa"),this.h()},l(e){const s=Ue('[data-svelte="svelte-1phssyn"]',document.head);p=o(s,"META",{name:!0,content:!0}),s.forEach(t),z=m(e),v=o(e,"H1",{class:!0});var O=n(v);_=o(O,"A",{id:!0,class:!0,href:!0});var ge=n(_);P=o(ge,"SPAN",{});var we=n(P);he(g.$$.fragment,we),we.forEach(t),ge.forEach(t),K=m(O),x=o(O,"SPAN",{});var ye=n(x);V=h(ye,"Encoder models"),ye.forEach(t),O.forEach(t),U=m(e),he(w.$$.fragment,e),D=m(e),he(y.$$.fragment,e),J=m(e),E=o(e,"P",{});var Q=n(E);W=h(Q,"Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having \u201Cbi-directional\u201D attention, and are often called "),q=o(Q,"EM",{});var $e=n(q);X=h($e,"auto-encoding models"),$e.forEach(t),Z=h(Q,"."),Q.forEach(t),Y=m(e),L=o(e,"P",{});var be=n(L);ee=h(be,"The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence."),be.forEach(t),j=m(e),k=o(e,"P",{});var Te=n(k);te=h(Te,"Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering."),Te.forEach(t),F=m(e),B=o(e,"P",{});var Ae=n(B);ae=h(Ae,"Representatives of this family of models include:"),Ae.forEach(t),G=m(e),i=o(e,"UL",{});var u=n(i);I=o(u,"LI",{});var Re=n(I);$=o(Re,"A",{href:!0,rel:!0});var Le=n($);re=h(Le,"ALBERT"),Le.forEach(t),Re.forEach(t),oe=m(u),C=o(u,"LI",{});var ke=n(C);b=o(ke,"A",{href:!0,rel:!0});var Be=n(b);ne=h(Be,"BERT"),Be.forEach(t),ke.forEach(t),se=m(u),S=o(u,"LI",{});var Pe=n(S);T=o(Pe,"A",{href:!0,rel:!0});var xe=n(T);le=h(xe,"DistilBERT"),xe.forEach(t),Pe.forEach(t),ie=m(u),M=o(u,"LI",{});var qe=n(M);A=o(qe,"A",{href:!0,rel:!0});var Ie=n(A);fe=h(Ie,"ELECTRA"),Ie.forEach(t),qe.forEach(t),ce=m(u),N=o(u,"LI",{});var Ce=n(N);R=o(Ce,"A",{href:!0,rel:!0});var Se=n(R);me=h(Se,"RoBERTa"),Se.forEach(t),Ce.forEach(t),u.forEach(t),this.h()},h(){l(p,"name","hf:doc:metadata"),l(p,"content",JSON.stringify(He)),l(_,"id","encoder-models"),l(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_,"href","#encoder-models"),l(v,"class","relative group"),l($,"href","https://huggingface.co/transformers/model_doc/albert.html"),l($,"rel","nofollow"),l(b,"href","https://huggingface.co/transformers/model_doc/bert.html"),l(b,"rel","nofollow"),l(T,"href","https://huggingface.co/transformers/model_doc/distilbert.html"),l(T,"rel","nofollow"),l(A,"href","https://huggingface.co/transformers/model_doc/electra.html"),l(A,"rel","nofollow"),l(R,"href","https://huggingface.co/transformers/model_doc/roberta.html"),l(R,"rel","nofollow")},m(e,s){a(document.head,p),f(e,z,s),f(e,v,s),a(v,_),a(_,P),ue(g,P,null),a(v,K),a(v,x),a(x,V),f(e,U,s),ue(w,e,s),f(e,D,s),ue(y,e,s),f(e,J,s),f(e,E,s),a(E,W),a(E,q),a(q,X),a(E,Z),f(e,Y,s),f(e,L,s),a(L,ee),f(e,j,s),f(e,k,s),a(k,te),f(e,F,s),f(e,B,s),a(B,ae),f(e,G,s),f(e,i,s),a(i,I),a(I,$),a($,re),a(i,oe),a(i,C),a(C,b),a(b,ne),a(i,se),a(i,S),a(S,T),a(T,le),a(i,ie),a(i,M),a(M,A),a(A,fe),a(i,ce),a(i,N),a(N,R),a(R,me),H=!0},p:De,i(e){H||(pe(g.$$.fragment,e),pe(w.$$.fragment,e),pe(y.$$.fragment,e),H=!0)},o(e){ve(g.$$.fragment,e),ve(w.$$.fragment,e),ve(y.$$.fragment,e),H=!1},d(e){t(p),e&&t(z),e&&t(v),_e(g),e&&t(U),_e(w,e),e&&t(D),_e(y,e),e&&t(J),e&&t(E),e&&t(Y),e&&t(L),e&&t(j),e&&t(k),e&&t(F),e&&t(B),e&&t(G),e&&t(i)}}}const He={local:"encoder-models",title:"Encoder models"};function Oe(Ee){return Je(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xe extends Me{constructor(p){super();Ne(this,p,Oe,Ge,ze,{})}}export{Xe as default,He as metadata};
