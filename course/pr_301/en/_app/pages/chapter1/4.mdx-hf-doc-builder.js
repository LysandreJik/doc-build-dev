import{S as zf,i as Wf,s as Uf,e as a,k as c,w as p,t as i,M as Vf,c as o,d as r,m as d,a as s,x as g,h as n,b as f,N as u,G as t,g as h,y as v,L as Cf,q as w,o as y,B as _,v as Yf}from"../../chunks/vendor-hf-doc-builder.js";import{Y as El}from"../../chunks/Youtube-hf-doc-builder.js";import{I as M}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Jf}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Xf(Tl){let R,va,q,ee,Kt,Te,as,er,os,wa,ke,ya,vt,ss,_a,L,te,tr,$e,is,rr,ns,ba,wt,ls,Ea,B,Pe,kl,hs,Ae,$l,Ta,re,fs,Ie,cs,ds,ka,m,ar,ae,or,us,ms,xe,ps,gs,vs,sr,oe,ir,ws,ys,Ge,_s,bs,Es,nr,se,lr,Ts,ks,Ne,$s,Ps,As,hr,ie,fr,Is,xs,Se,Gs,Ns,Ss,cr,T,dr,Ms,Rs,Me,qs,Ls,Re,Bs,js,Ds,ur,k,mr,Os,Fs,qe,Hs,zs,pr,Ws,Us,$a,yt,Vs,Pa,$,Le,Cs,gr,Ys,Js,Xs,Be,Qs,vr,Zs,Ks,ei,je,ti,wr,ri,ai,Aa,_t,oi,Ia,j,ne,yr,De,si,_r,ii,xa,le,ni,br,li,hi,Ga,he,fi,Er,ci,di,Na,P,ui,Tr,mi,pi,kr,gi,vi,Sa,D,Oe,Pl,wi,Fe,Al,Ma,fe,yi,$r,_i,bi,Ra,O,He,Il,Ei,ze,xl,qa,F,ce,Pr,We,Ti,Ar,ki,La,bt,$i,Ba,Ue,Ve,Gl,ja,Et,Pi,Da,H,Ce,Nl,Ai,Ye,Sl,Oa,Je,Fa,Tt,Ii,Ha,kt,xi,za,$t,Gi,Wa,z,de,Ir,Xe,Ni,xr,Si,Ua,Qe,Va,Ze,Gr,Mi,Ri,Ca,W,Ke,Ml,qi,et,Rl,Ya,Pt,Li,Ja,U,Nr,Bi,ji,Sr,Di,Oi,Xa,A,Mr,Fi,Hi,Rr,zi,Wi,qr,Ui,Qa,ue,Vi,Lr,Ci,Yi,Za,V,tt,ql,Ji,rt,Ll,Ka,At,Xi,eo,It,Qi,to,C,me,Br,at,Zi,jr,Ki,ro,xt,en,ao,ot,oo,Y,pe,Dr,st,tn,Or,rn,so,Gt,an,io,ge,Nt,Fr,on,sn,nn,St,Hr,ln,hn,no,J,it,Bl,fn,nt,jl,lo,Mt,cn,ho,I,Rt,zr,dn,un,mn,qt,Wr,pn,gn,vn,ve,Ur,wn,yn,Vr,_n,bn,fo,Lt,En,co,X,we,Cr,lt,Tn,Yr,kn,uo,x,$n,Jr,Pn,An,ht,In,xn,mo,Bt,Gn,po,jt,Nn,go,Dt,Sn,vo,Q,ye,Xr,ft,Mn,Qr,Rn,wo,Ot,qn,yo,Ft,Ln,_o,Ht,Bn,bo,Z,ct,Dl,jn,dt,Ol,Eo,zt,Dn,To,_e,On,Zr,Fn,Hn,ko,K,be,Kr,ut,zn,ea,Wn,$o,b,Un,ta,Vn,Cn,ra,Yn,Jn,aa,Xn,Qn,Po,G,Wt,oa,Zn,Kn,el,Ut,sa,tl,rl,al,N,ia,ol,sl,na,il,nl,la,ll,hl,Ao,S,fl,ha,cl,dl,fa,ul,ml,Io;return Te=new M({}),ke=new Jf({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),$e=new M({}),De=new M({}),We=new M({}),Je=new El({props:{id:"ftWlj4FBHTg"}}),Xe=new M({}),Qe=new El({props:{id:"BqqfQnyjmgg"}}),at=new M({}),ot=new El({props:{id:"H39Z_720T5s"}}),st=new M({}),lt=new M({}),ft=new M({}),ut=new M({}),{c(){R=a("meta"),va=c(),q=a("h1"),ee=a("a"),Kt=a("span"),p(Te.$$.fragment),as=c(),er=a("span"),os=i("How do Transformers work?"),wa=c(),p(ke.$$.fragment),ya=c(),vt=a("p"),ss=i("In this section, we will take a high-level look at the architecture of Transformer models."),_a=c(),L=a("h2"),te=a("a"),tr=a("span"),p($e.$$.fragment),is=c(),rr=a("span"),ns=i("A bit of Transformer history"),ba=c(),wt=a("p"),ls=i("Here are some reference points in the (short) history of Transformer models:"),Ea=c(),B=a("div"),Pe=a("img"),hs=c(),Ae=a("img"),Ta=c(),re=a("p"),fs=i("The "),Ie=a("a"),cs=i("Transformer architecture"),ds=i(" was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:"),ka=c(),m=a("ul"),ar=a("li"),ae=a("p"),or=a("strong"),us=i("June 2018"),ms=i(": "),xe=a("a"),ps=i("GPT"),gs=i(", the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results"),vs=c(),sr=a("li"),oe=a("p"),ir=a("strong"),ws=i("October 2018"),ys=i(": "),Ge=a("a"),_s=i("BERT"),bs=i(", another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)"),Es=c(),nr=a("li"),se=a("p"),lr=a("strong"),Ts=i("February 2019"),ks=i(": "),Ne=a("a"),$s=i("GPT-2"),Ps=i(", an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns"),As=c(),hr=a("li"),ie=a("p"),fr=a("strong"),Is=i("October 2019"),xs=i(": "),Se=a("a"),Gs=i("DistilBERT"),Ns=i(", a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT\u2019s performance"),Ss=c(),cr=a("li"),T=a("p"),dr=a("strong"),Ms=i("October 2019"),Rs=i(": "),Me=a("a"),qs=i("BART"),Ls=i(" and "),Re=a("a"),Bs=i("T5"),js=i(", two large pretrained models using the same architecture as the original Transformer model (the first to do so)"),Ds=c(),ur=a("li"),k=a("p"),mr=a("strong"),Os=i("May 2020"),Fs=i(", "),qe=a("a"),Hs=i("GPT-3"),zs=i(", an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called "),pr=a("em"),Ws=i("zero-shot learning"),Us=i(")"),$a=c(),yt=a("p"),Vs=i("This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:"),Pa=c(),$=a("ul"),Le=a("li"),Cs=i("GPT-like (also called "),gr=a("em"),Ys=i("auto-regressive"),Js=i(" Transformer models)"),Xs=c(),Be=a("li"),Qs=i("BERT-like (also called "),vr=a("em"),Zs=i("auto-encoding"),Ks=i(" Transformer models)"),ei=c(),je=a("li"),ti=i("BART/T5-like (also called "),wr=a("em"),ri=i("sequence-to-sequence"),ai=i(" Transformer models)"),Aa=c(),_t=a("p"),oi=i("We will dive into these families in more depth later on."),Ia=c(),j=a("h2"),ne=a("a"),yr=a("span"),p(De.$$.fragment),si=c(),_r=a("span"),ii=i("Transformers are language models"),xa=c(),le=a("p"),ni=i("All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as "),br=a("em"),li=i("language models"),hi=i(". This means they have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!"),Ga=c(),he=a("p"),fi=i("This type of model develops a statistical understanding of the language it has been trained on, but it\u2019s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called "),Er=a("em"),ci=i("transfer learning"),di=i(". During this process, the model is fine-tuned in a supervised way \u2014 that is, using human-annotated labels \u2014 on a given task."),Na=c(),P=a("p"),ui=i("An example of a task is predicting the next word in a sentence having read the "),Tr=a("em"),mi=i("n"),pi=i(" previous words. This is called "),kr=a("em"),gi=i("causal language modeling"),vi=i(" because the output depends on the past and present inputs, but not the future ones."),Sa=c(),D=a("div"),Oe=a("img"),wi=c(),Fe=a("img"),Ma=c(),fe=a("p"),yi=i("Another example is "),$r=a("em"),_i=i("masked language modeling"),bi=i(", in which the model predicts a masked word in the sentence."),Ra=c(),O=a("div"),He=a("img"),Ei=c(),ze=a("img"),qa=c(),F=a("h2"),ce=a("a"),Pr=a("span"),p(We.$$.fragment),Ti=c(),Ar=a("span"),ki=i("Transformers are big models"),La=c(),bt=a("p"),$i=i("Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models\u2019 sizes as well as the amount of data they are pretrained on."),Ba=c(),Ue=a("div"),Ve=a("img"),ja=c(),Et=a("p"),Pi=i("Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources. It even translates to environmental impact, as can be seen in the following graph."),Da=c(),H=a("div"),Ce=a("img"),Ai=c(),Ye=a("img"),Oa=c(),p(Je.$$.fragment),Fa=c(),Tt=a("p"),Ii=i("And this is showing a project for a (very big) model led by a team consciously trying to reduce the environmental impact of pretraining. The footprint of running lots of trials to get the best hyperparameters would be even higher."),Ha=c(),kt=a("p"),xi=i("Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs!"),za=c(),$t=a("p"),Gi=i("This is why sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community."),Wa=c(),z=a("h2"),de=a("a"),Ir=a("span"),p(Xe.$$.fragment),Ni=c(),xr=a("span"),Si=i("Transfer Learning"),Ua=c(),p(Qe.$$.fragment),Va=c(),Ze=a("p"),Gr=a("em"),Mi=i("Pretraining"),Ri=i(" is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge."),Ca=c(),W=a("div"),Ke=a("img"),qi=c(),et=a("img"),Ya=c(),Pt=a("p"),Li=i("This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks."),Ja=c(),U=a("p"),Nr=a("em"),Bi=i("Fine-tuning"),ji=i(", on the other hand, is the training done "),Sr=a("strong"),Di=i("after"),Oi=i(" a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task. Wait \u2014 why not simply train directly for the final task? There are a couple of reasons:"),Xa=c(),A=a("ul"),Mr=a("li"),Fi=i("The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task)."),Hi=c(),Rr=a("li"),zi=i("Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results."),Wi=c(),qr=a("li"),Ui=i("For the same reason, the amount of time and resources needed to get good results are much lower."),Qa=c(),ue=a("p"),Vi=i("For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is \u201Ctransferred,\u201D hence the term "),Lr=a("em"),Ci=i("transfer learning"),Yi=i("."),Za=c(),V=a("div"),tt=a("img"),Ji=c(),rt=a("img"),Ka=c(),At=a("p"),Xi=i("Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining."),eo=c(),It=a("p"),Qi=i("This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model \u2014 one as close as possible to the task you have at hand \u2014 and fine-tune it."),to=c(),C=a("h2"),me=a("a"),Br=a("span"),p(at.$$.fragment),Zi=c(),jr=a("span"),Ki=i("General architecture"),ro=c(),xt=a("p"),en=i("In this section, we\u2019ll go over the general architecture of the Transformer model. Don\u2019t worry if you don\u2019t understand some of the concepts; there are detailed sections later covering each of the components."),ao=c(),p(ot.$$.fragment),oo=c(),Y=a("h2"),pe=a("a"),Dr=a("span"),p(st.$$.fragment),tn=c(),Or=a("span"),rn=i("Introduction"),so=c(),Gt=a("p"),an=i("The model is primarily composed of two blocks:"),io=c(),ge=a("ul"),Nt=a("li"),Fr=a("strong"),on=i("Encoder (left)"),sn=i(": The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input."),nn=c(),St=a("li"),Hr=a("strong"),ln=i("Decoder (right)"),hn=i(": The decoder uses the encoder\u2019s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs."),no=c(),J=a("div"),it=a("img"),fn=c(),nt=a("img"),lo=c(),Mt=a("p"),cn=i("Each of these parts can be used independently, depending on the task:"),ho=c(),I=a("ul"),Rt=a("li"),zr=a("strong"),dn=i("Encoder-only models"),un=i(": Good for tasks that require understanding of the input, such as sentence classification and named entity recognition."),mn=c(),qt=a("li"),Wr=a("strong"),pn=i("Decoder-only models"),gn=i(": Good for generative tasks such as text generation."),vn=c(),ve=a("li"),Ur=a("strong"),wn=i("Encoder-decoder models"),yn=i(" or "),Vr=a("strong"),_n=i("sequence-to-sequence models"),bn=i(": Good for generative tasks that require an input, such as translation or summarization."),fo=c(),Lt=a("p"),En=i("We will dive into those architectures independently in later sections."),co=c(),X=a("h2"),we=a("a"),Cr=a("span"),p(lt.$$.fragment),Tn=c(),Yr=a("span"),kn=i("Attention layers"),uo=c(),x=a("p"),$n=i("A key feature of Transformer models is that they are built with special layers called "),Jr=a("em"),Pn=i("attention layers"),An=i(". In fact, the title of the paper introducing the Transformer architecture was "),ht=a("a"),In=i("\u201CAttention Is All You Need\u201D"),xn=i("! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word."),mo=c(),Bt=a("p"),Gn=i("To put this into context, consider the task of translating text from English to French. Given the input \u201CYou like this course\u201D, a translation model will need to also attend to the adjacent word \u201CYou\u201D to get the proper translation for the word \u201Clike\u201D, because in French the verb \u201Clike\u201D is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating \u201Cthis\u201D the model will also need to pay attention to the word \u201Ccourse\u201D, because \u201Cthis\u201D translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of \u201Cthis\u201D. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word."),po=c(),jt=a("p"),Nn=i("The same concept applies to any task associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied."),go=c(),Dt=a("p"),Sn=i("Now that you have an idea of what attention layers are all about, let\u2019s take a closer look at the Transformer architecture."),vo=c(),Q=a("h2"),ye=a("a"),Xr=a("span"),p(ft.$$.fragment),Mn=c(),Qr=a("span"),Rn=i("The original architecture"),wo=c(),Ot=a("p"),qn=i("The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder  which then uses all the inputs of the encoder to try to predict the fourth word."),yo=c(),Ft=a("p"),Ln=i("To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3."),_o=c(),Ht=a("p"),Bn=i("The original Transformer architecture looked like this, with the encoder on the left and the decoder on the right:"),bo=c(),Z=a("div"),ct=a("img"),jn=c(),dt=a("img"),Eo=c(),zt=a("p"),Dn=i("Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word."),To=c(),_e=a("p"),On=i("The "),Zr=a("em"),Fn=i("attention mask"),Hn=i(" can also be used in the encoder/decoder to prevent the model from paying attention to some special words \u2014 for instance, the special padding word used to make all the inputs the same length when batching together sentences."),ko=c(),K=a("h2"),be=a("a"),Kr=a("span"),p(ut.$$.fragment),zn=c(),ea=a("span"),Wn=i("Architectures vs. checkpoints"),$o=c(),b=a("p"),Un=i("As we dive into Transformer models in this course, you\u2019ll see mentions of "),ta=a("em"),Vn=i("architectures"),Cn=i(" and "),ra=a("em"),Yn=i("checkpoints"),Jn=i(" as well as "),aa=a("em"),Xn=i("models"),Qn=i(". These terms all have slightly different meanings:"),Po=c(),G=a("ul"),Wt=a("li"),oa=a("strong"),Zn=i("Architecture"),Kn=i(": This is the skeleton of the model \u2014 the definition of each layer and each operation that happens within the model."),el=c(),Ut=a("li"),sa=a("strong"),tl=i("Checkpoints"),rl=i(": These are the weights that will be loaded in a given architecture."),al=c(),N=a("li"),ia=a("strong"),ol=i("Model"),sl=i(": This is an umbrella term that isn\u2019t as precise as \u201Carchitecture\u201D or \u201Ccheckpoint\u201D: it can mean both. This course will specify "),na=a("em"),il=i("architecture"),nl=i(" or "),la=a("em"),ll=i("checkpoint"),hl=i(" when it matters to reduce ambiguity."),Ao=c(),S=a("p"),fl=i("For example, BERT is an architecture while "),ha=a("code"),cl=i("bert-base-cased"),dl=i(", a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say \u201Cthe BERT model\u201D and \u201Cthe "),fa=a("code"),ul=i("bert-base-cased"),ml=i(" model.\u201D"),this.h()},l(e){const l=Vf('[data-svelte="svelte-1phssyn"]',document.head);R=o(l,"META",{name:!0,content:!0}),l.forEach(r),va=d(e),q=o(e,"H1",{class:!0});var xo=s(q);ee=o(xo,"A",{id:!0,class:!0,href:!0});var Fl=s(ee);Kt=o(Fl,"SPAN",{});var Hl=s(Kt);g(Te.$$.fragment,Hl),Hl.forEach(r),Fl.forEach(r),as=d(xo),er=o(xo,"SPAN",{});var zl=s(er);os=n(zl,"How do Transformers work?"),zl.forEach(r),xo.forEach(r),wa=d(e),g(ke.$$.fragment,e),ya=d(e),vt=o(e,"P",{});var Wl=s(vt);ss=n(Wl,"In this section, we will take a high-level look at the architecture of Transformer models."),Wl.forEach(r),_a=d(e),L=o(e,"H2",{class:!0});var Go=s(L);te=o(Go,"A",{id:!0,class:!0,href:!0});var Ul=s(te);tr=o(Ul,"SPAN",{});var Vl=s(tr);g($e.$$.fragment,Vl),Vl.forEach(r),Ul.forEach(r),is=d(Go),rr=o(Go,"SPAN",{});var Cl=s(rr);ns=n(Cl,"A bit of Transformer history"),Cl.forEach(r),Go.forEach(r),ba=d(e),wt=o(e,"P",{});var Yl=s(wt);ls=n(Yl,"Here are some reference points in the (short) history of Transformer models:"),Yl.forEach(r),Ea=d(e),B=o(e,"DIV",{class:!0});var No=s(B);Pe=o(No,"IMG",{class:!0,src:!0,alt:!0}),hs=d(No),Ae=o(No,"IMG",{class:!0,src:!0,alt:!0}),No.forEach(r),Ta=d(e),re=o(e,"P",{});var So=s(re);fs=n(So,"The "),Ie=o(So,"A",{href:!0,rel:!0});var Jl=s(Ie);cs=n(Jl,"Transformer architecture"),Jl.forEach(r),ds=n(So," was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:"),So.forEach(r),ka=d(e),m=o(e,"UL",{});var E=s(m);ar=o(E,"LI",{});var Xl=s(ar);ae=o(Xl,"P",{});var ca=s(ae);or=o(ca,"STRONG",{});var Ql=s(or);us=n(Ql,"June 2018"),Ql.forEach(r),ms=n(ca,": "),xe=o(ca,"A",{href:!0,rel:!0});var Zl=s(xe);ps=n(Zl,"GPT"),Zl.forEach(r),gs=n(ca,", the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results"),ca.forEach(r),Xl.forEach(r),vs=d(E),sr=o(E,"LI",{});var Kl=s(sr);oe=o(Kl,"P",{});var da=s(oe);ir=o(da,"STRONG",{});var eh=s(ir);ws=n(eh,"October 2018"),eh.forEach(r),ys=n(da,": "),Ge=o(da,"A",{href:!0,rel:!0});var th=s(Ge);_s=n(th,"BERT"),th.forEach(r),bs=n(da,", another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)"),da.forEach(r),Kl.forEach(r),Es=d(E),nr=o(E,"LI",{});var rh=s(nr);se=o(rh,"P",{});var ua=s(se);lr=o(ua,"STRONG",{});var ah=s(lr);Ts=n(ah,"February 2019"),ah.forEach(r),ks=n(ua,": "),Ne=o(ua,"A",{href:!0,rel:!0});var oh=s(Ne);$s=n(oh,"GPT-2"),oh.forEach(r),Ps=n(ua,", an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns"),ua.forEach(r),rh.forEach(r),As=d(E),hr=o(E,"LI",{});var sh=s(hr);ie=o(sh,"P",{});var ma=s(ie);fr=o(ma,"STRONG",{});var ih=s(fr);Is=n(ih,"October 2019"),ih.forEach(r),xs=n(ma,": "),Se=o(ma,"A",{href:!0,rel:!0});var nh=s(Se);Gs=n(nh,"DistilBERT"),nh.forEach(r),Ns=n(ma,", a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT\u2019s performance"),ma.forEach(r),sh.forEach(r),Ss=d(E),cr=o(E,"LI",{});var lh=s(cr);T=o(lh,"P",{});var mt=s(T);dr=o(mt,"STRONG",{});var hh=s(dr);Ms=n(hh,"October 2019"),hh.forEach(r),Rs=n(mt,": "),Me=o(mt,"A",{href:!0,rel:!0});var fh=s(Me);qs=n(fh,"BART"),fh.forEach(r),Ls=n(mt," and "),Re=o(mt,"A",{href:!0,rel:!0});var ch=s(Re);Bs=n(ch,"T5"),ch.forEach(r),js=n(mt,", two large pretrained models using the same architecture as the original Transformer model (the first to do so)"),mt.forEach(r),lh.forEach(r),Ds=d(E),ur=o(E,"LI",{});var dh=s(ur);k=o(dh,"P",{});var pt=s(k);mr=o(pt,"STRONG",{});var uh=s(mr);Os=n(uh,"May 2020"),uh.forEach(r),Fs=n(pt,", "),qe=o(pt,"A",{href:!0,rel:!0});var mh=s(qe);Hs=n(mh,"GPT-3"),mh.forEach(r),zs=n(pt,", an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called "),pr=o(pt,"EM",{});var ph=s(pr);Ws=n(ph,"zero-shot learning"),ph.forEach(r),Us=n(pt,")"),pt.forEach(r),dh.forEach(r),E.forEach(r),$a=d(e),yt=o(e,"P",{});var gh=s(yt);Vs=n(gh,"This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:"),gh.forEach(r),Pa=d(e),$=o(e,"UL",{});var Vt=s($);Le=o(Vt,"LI",{});var Mo=s(Le);Cs=n(Mo,"GPT-like (also called "),gr=o(Mo,"EM",{});var vh=s(gr);Ys=n(vh,"auto-regressive"),vh.forEach(r),Js=n(Mo," Transformer models)"),Mo.forEach(r),Xs=d(Vt),Be=o(Vt,"LI",{});var Ro=s(Be);Qs=n(Ro,"BERT-like (also called "),vr=o(Ro,"EM",{});var wh=s(vr);Zs=n(wh,"auto-encoding"),wh.forEach(r),Ks=n(Ro," Transformer models)"),Ro.forEach(r),ei=d(Vt),je=o(Vt,"LI",{});var qo=s(je);ti=n(qo,"BART/T5-like (also called "),wr=o(qo,"EM",{});var yh=s(wr);ri=n(yh,"sequence-to-sequence"),yh.forEach(r),ai=n(qo," Transformer models)"),qo.forEach(r),Vt.forEach(r),Aa=d(e),_t=o(e,"P",{});var _h=s(_t);oi=n(_h,"We will dive into these families in more depth later on."),_h.forEach(r),Ia=d(e),j=o(e,"H2",{class:!0});var Lo=s(j);ne=o(Lo,"A",{id:!0,class:!0,href:!0});var bh=s(ne);yr=o(bh,"SPAN",{});var Eh=s(yr);g(De.$$.fragment,Eh),Eh.forEach(r),bh.forEach(r),si=d(Lo),_r=o(Lo,"SPAN",{});var Th=s(_r);ii=n(Th,"Transformers are language models"),Th.forEach(r),Lo.forEach(r),xa=d(e),le=o(e,"P",{});var Bo=s(le);ni=n(Bo,"All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as "),br=o(Bo,"EM",{});var kh=s(br);li=n(kh,"language models"),kh.forEach(r),hi=n(Bo,". This means they have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!"),Bo.forEach(r),Ga=d(e),he=o(e,"P",{});var jo=s(he);fi=n(jo,"This type of model develops a statistical understanding of the language it has been trained on, but it\u2019s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called "),Er=o(jo,"EM",{});var $h=s(Er);ci=n($h,"transfer learning"),$h.forEach(r),di=n(jo,". During this process, the model is fine-tuned in a supervised way \u2014 that is, using human-annotated labels \u2014 on a given task."),jo.forEach(r),Na=d(e),P=o(e,"P",{});var Ct=s(P);ui=n(Ct,"An example of a task is predicting the next word in a sentence having read the "),Tr=o(Ct,"EM",{});var Ph=s(Tr);mi=n(Ph,"n"),Ph.forEach(r),pi=n(Ct," previous words. This is called "),kr=o(Ct,"EM",{});var Ah=s(kr);gi=n(Ah,"causal language modeling"),Ah.forEach(r),vi=n(Ct," because the output depends on the past and present inputs, but not the future ones."),Ct.forEach(r),Sa=d(e),D=o(e,"DIV",{class:!0});var Do=s(D);Oe=o(Do,"IMG",{class:!0,src:!0,alt:!0}),wi=d(Do),Fe=o(Do,"IMG",{class:!0,src:!0,alt:!0}),Do.forEach(r),Ma=d(e),fe=o(e,"P",{});var Oo=s(fe);yi=n(Oo,"Another example is "),$r=o(Oo,"EM",{});var Ih=s($r);_i=n(Ih,"masked language modeling"),Ih.forEach(r),bi=n(Oo,", in which the model predicts a masked word in the sentence."),Oo.forEach(r),Ra=d(e),O=o(e,"DIV",{class:!0});var Fo=s(O);He=o(Fo,"IMG",{class:!0,src:!0,alt:!0}),Ei=d(Fo),ze=o(Fo,"IMG",{class:!0,src:!0,alt:!0}),Fo.forEach(r),qa=d(e),F=o(e,"H2",{class:!0});var Ho=s(F);ce=o(Ho,"A",{id:!0,class:!0,href:!0});var xh=s(ce);Pr=o(xh,"SPAN",{});var Gh=s(Pr);g(We.$$.fragment,Gh),Gh.forEach(r),xh.forEach(r),Ti=d(Ho),Ar=o(Ho,"SPAN",{});var Nh=s(Ar);ki=n(Nh,"Transformers are big models"),Nh.forEach(r),Ho.forEach(r),La=d(e),bt=o(e,"P",{});var Sh=s(bt);$i=n(Sh,"Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models\u2019 sizes as well as the amount of data they are pretrained on."),Sh.forEach(r),Ba=d(e),Ue=o(e,"DIV",{class:!0});var Mh=s(Ue);Ve=o(Mh,"IMG",{src:!0,alt:!0,width:!0}),Mh.forEach(r),ja=d(e),Et=o(e,"P",{});var Rh=s(Et);Pi=n(Rh,"Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources. It even translates to environmental impact, as can be seen in the following graph."),Rh.forEach(r),Da=d(e),H=o(e,"DIV",{class:!0});var zo=s(H);Ce=o(zo,"IMG",{class:!0,src:!0,alt:!0}),Ai=d(zo),Ye=o(zo,"IMG",{class:!0,src:!0,alt:!0}),zo.forEach(r),Oa=d(e),g(Je.$$.fragment,e),Fa=d(e),Tt=o(e,"P",{});var qh=s(Tt);Ii=n(qh,"And this is showing a project for a (very big) model led by a team consciously trying to reduce the environmental impact of pretraining. The footprint of running lots of trials to get the best hyperparameters would be even higher."),qh.forEach(r),Ha=d(e),kt=o(e,"P",{});var Lh=s(kt);xi=n(Lh,"Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs!"),Lh.forEach(r),za=d(e),$t=o(e,"P",{});var Bh=s($t);Gi=n(Bh,"This is why sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community."),Bh.forEach(r),Wa=d(e),z=o(e,"H2",{class:!0});var Wo=s(z);de=o(Wo,"A",{id:!0,class:!0,href:!0});var jh=s(de);Ir=o(jh,"SPAN",{});var Dh=s(Ir);g(Xe.$$.fragment,Dh),Dh.forEach(r),jh.forEach(r),Ni=d(Wo),xr=o(Wo,"SPAN",{});var Oh=s(xr);Si=n(Oh,"Transfer Learning"),Oh.forEach(r),Wo.forEach(r),Ua=d(e),g(Qe.$$.fragment,e),Va=d(e),Ze=o(e,"P",{});var pl=s(Ze);Gr=o(pl,"EM",{});var Fh=s(Gr);Mi=n(Fh,"Pretraining"),Fh.forEach(r),Ri=n(pl," is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge."),pl.forEach(r),Ca=d(e),W=o(e,"DIV",{class:!0});var Uo=s(W);Ke=o(Uo,"IMG",{class:!0,src:!0,alt:!0}),qi=d(Uo),et=o(Uo,"IMG",{class:!0,src:!0,alt:!0}),Uo.forEach(r),Ya=d(e),Pt=o(e,"P",{});var Hh=s(Pt);Li=n(Hh,"This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks."),Hh.forEach(r),Ja=d(e),U=o(e,"P",{});var pa=s(U);Nr=o(pa,"EM",{});var zh=s(Nr);Bi=n(zh,"Fine-tuning"),zh.forEach(r),ji=n(pa,", on the other hand, is the training done "),Sr=o(pa,"STRONG",{});var Wh=s(Sr);Di=n(Wh,"after"),Wh.forEach(r),Oi=n(pa," a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task. Wait \u2014 why not simply train directly for the final task? There are a couple of reasons:"),pa.forEach(r),Xa=d(e),A=o(e,"UL",{});var Yt=s(A);Mr=o(Yt,"LI",{});var Uh=s(Mr);Fi=n(Uh,"The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task)."),Uh.forEach(r),Hi=d(Yt),Rr=o(Yt,"LI",{});var Vh=s(Rr);zi=n(Vh,"Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results."),Vh.forEach(r),Wi=d(Yt),qr=o(Yt,"LI",{});var Ch=s(qr);Ui=n(Ch,"For the same reason, the amount of time and resources needed to get good results are much lower."),Ch.forEach(r),Yt.forEach(r),Qa=d(e),ue=o(e,"P",{});var Vo=s(ue);Vi=n(Vo,"For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is \u201Ctransferred,\u201D hence the term "),Lr=o(Vo,"EM",{});var Yh=s(Lr);Ci=n(Yh,"transfer learning"),Yh.forEach(r),Yi=n(Vo,"."),Vo.forEach(r),Za=d(e),V=o(e,"DIV",{class:!0});var Co=s(V);tt=o(Co,"IMG",{class:!0,src:!0,alt:!0}),Ji=d(Co),rt=o(Co,"IMG",{class:!0,src:!0,alt:!0}),Co.forEach(r),Ka=d(e),At=o(e,"P",{});var Jh=s(At);Xi=n(Jh,"Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining."),Jh.forEach(r),eo=d(e),It=o(e,"P",{});var Xh=s(It);Qi=n(Xh,"This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model \u2014 one as close as possible to the task you have at hand \u2014 and fine-tune it."),Xh.forEach(r),to=d(e),C=o(e,"H2",{class:!0});var Yo=s(C);me=o(Yo,"A",{id:!0,class:!0,href:!0});var Qh=s(me);Br=o(Qh,"SPAN",{});var Zh=s(Br);g(at.$$.fragment,Zh),Zh.forEach(r),Qh.forEach(r),Zi=d(Yo),jr=o(Yo,"SPAN",{});var Kh=s(jr);Ki=n(Kh,"General architecture"),Kh.forEach(r),Yo.forEach(r),ro=d(e),xt=o(e,"P",{});var ef=s(xt);en=n(ef,"In this section, we\u2019ll go over the general architecture of the Transformer model. Don\u2019t worry if you don\u2019t understand some of the concepts; there are detailed sections later covering each of the components."),ef.forEach(r),ao=d(e),g(ot.$$.fragment,e),oo=d(e),Y=o(e,"H2",{class:!0});var Jo=s(Y);pe=o(Jo,"A",{id:!0,class:!0,href:!0});var tf=s(pe);Dr=o(tf,"SPAN",{});var rf=s(Dr);g(st.$$.fragment,rf),rf.forEach(r),tf.forEach(r),tn=d(Jo),Or=o(Jo,"SPAN",{});var af=s(Or);rn=n(af,"Introduction"),af.forEach(r),Jo.forEach(r),so=d(e),Gt=o(e,"P",{});var of=s(Gt);an=n(of,"The model is primarily composed of two blocks:"),of.forEach(r),io=d(e),ge=o(e,"UL",{});var Xo=s(ge);Nt=o(Xo,"LI",{});var gl=s(Nt);Fr=o(gl,"STRONG",{});var sf=s(Fr);on=n(sf,"Encoder (left)"),sf.forEach(r),sn=n(gl,": The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input."),gl.forEach(r),nn=d(Xo),St=o(Xo,"LI",{});var vl=s(St);Hr=o(vl,"STRONG",{});var nf=s(Hr);ln=n(nf,"Decoder (right)"),nf.forEach(r),hn=n(vl,": The decoder uses the encoder\u2019s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs."),vl.forEach(r),Xo.forEach(r),no=d(e),J=o(e,"DIV",{class:!0});var Qo=s(J);it=o(Qo,"IMG",{class:!0,src:!0,alt:!0}),fn=d(Qo),nt=o(Qo,"IMG",{class:!0,src:!0,alt:!0}),Qo.forEach(r),lo=d(e),Mt=o(e,"P",{});var lf=s(Mt);cn=n(lf,"Each of these parts can be used independently, depending on the task:"),lf.forEach(r),ho=d(e),I=o(e,"UL",{});var Jt=s(I);Rt=o(Jt,"LI",{});var wl=s(Rt);zr=o(wl,"STRONG",{});var hf=s(zr);dn=n(hf,"Encoder-only models"),hf.forEach(r),un=n(wl,": Good for tasks that require understanding of the input, such as sentence classification and named entity recognition."),wl.forEach(r),mn=d(Jt),qt=o(Jt,"LI",{});var yl=s(qt);Wr=o(yl,"STRONG",{});var ff=s(Wr);pn=n(ff,"Decoder-only models"),ff.forEach(r),gn=n(yl,": Good for generative tasks such as text generation."),yl.forEach(r),vn=d(Jt),ve=o(Jt,"LI",{});var ga=s(ve);Ur=o(ga,"STRONG",{});var cf=s(Ur);wn=n(cf,"Encoder-decoder models"),cf.forEach(r),yn=n(ga," or "),Vr=o(ga,"STRONG",{});var df=s(Vr);_n=n(df,"sequence-to-sequence models"),df.forEach(r),bn=n(ga,": Good for generative tasks that require an input, such as translation or summarization."),ga.forEach(r),Jt.forEach(r),fo=d(e),Lt=o(e,"P",{});var uf=s(Lt);En=n(uf,"We will dive into those architectures independently in later sections."),uf.forEach(r),co=d(e),X=o(e,"H2",{class:!0});var Zo=s(X);we=o(Zo,"A",{id:!0,class:!0,href:!0});var mf=s(we);Cr=o(mf,"SPAN",{});var pf=s(Cr);g(lt.$$.fragment,pf),pf.forEach(r),mf.forEach(r),Tn=d(Zo),Yr=o(Zo,"SPAN",{});var gf=s(Yr);kn=n(gf,"Attention layers"),gf.forEach(r),Zo.forEach(r),uo=d(e),x=o(e,"P",{});var Xt=s(x);$n=n(Xt,"A key feature of Transformer models is that they are built with special layers called "),Jr=o(Xt,"EM",{});var vf=s(Jr);Pn=n(vf,"attention layers"),vf.forEach(r),An=n(Xt,". In fact, the title of the paper introducing the Transformer architecture was "),ht=o(Xt,"A",{href:!0,rel:!0});var wf=s(ht);In=n(wf,"\u201CAttention Is All You Need\u201D"),wf.forEach(r),xn=n(Xt,"! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word."),Xt.forEach(r),mo=d(e),Bt=o(e,"P",{});var yf=s(Bt);Gn=n(yf,"To put this into context, consider the task of translating text from English to French. Given the input \u201CYou like this course\u201D, a translation model will need to also attend to the adjacent word \u201CYou\u201D to get the proper translation for the word \u201Clike\u201D, because in French the verb \u201Clike\u201D is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating \u201Cthis\u201D the model will also need to pay attention to the word \u201Ccourse\u201D, because \u201Cthis\u201D translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of \u201Cthis\u201D. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word."),yf.forEach(r),po=d(e),jt=o(e,"P",{});var _f=s(jt);Nn=n(_f,"The same concept applies to any task associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied."),_f.forEach(r),go=d(e),Dt=o(e,"P",{});var bf=s(Dt);Sn=n(bf,"Now that you have an idea of what attention layers are all about, let\u2019s take a closer look at the Transformer architecture."),bf.forEach(r),vo=d(e),Q=o(e,"H2",{class:!0});var Ko=s(Q);ye=o(Ko,"A",{id:!0,class:!0,href:!0});var Ef=s(ye);Xr=o(Ef,"SPAN",{});var Tf=s(Xr);g(ft.$$.fragment,Tf),Tf.forEach(r),Ef.forEach(r),Mn=d(Ko),Qr=o(Ko,"SPAN",{});var kf=s(Qr);Rn=n(kf,"The original architecture"),kf.forEach(r),Ko.forEach(r),wo=d(e),Ot=o(e,"P",{});var $f=s(Ot);qn=n($f,"The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder  which then uses all the inputs of the encoder to try to predict the fourth word."),$f.forEach(r),yo=d(e),Ft=o(e,"P",{});var Pf=s(Ft);Ln=n(Pf,"To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3."),Pf.forEach(r),_o=d(e),Ht=o(e,"P",{});var Af=s(Ht);Bn=n(Af,"The original Transformer architecture looked like this, with the encoder on the left and the decoder on the right:"),Af.forEach(r),bo=d(e),Z=o(e,"DIV",{class:!0});var es=s(Z);ct=o(es,"IMG",{class:!0,src:!0,alt:!0}),jn=d(es),dt=o(es,"IMG",{class:!0,src:!0,alt:!0}),es.forEach(r),Eo=d(e),zt=o(e,"P",{});var If=s(zt);Dn=n(If,"Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word."),If.forEach(r),To=d(e),_e=o(e,"P",{});var ts=s(_e);On=n(ts,"The "),Zr=o(ts,"EM",{});var xf=s(Zr);Fn=n(xf,"attention mask"),xf.forEach(r),Hn=n(ts," can also be used in the encoder/decoder to prevent the model from paying attention to some special words \u2014 for instance, the special padding word used to make all the inputs the same length when batching together sentences."),ts.forEach(r),ko=d(e),K=o(e,"H2",{class:!0});var rs=s(K);be=o(rs,"A",{id:!0,class:!0,href:!0});var Gf=s(be);Kr=o(Gf,"SPAN",{});var Nf=s(Kr);g(ut.$$.fragment,Nf),Nf.forEach(r),Gf.forEach(r),zn=d(rs),ea=o(rs,"SPAN",{});var Sf=s(ea);Wn=n(Sf,"Architectures vs. checkpoints"),Sf.forEach(r),rs.forEach(r),$o=d(e),b=o(e,"P",{});var Ee=s(b);Un=n(Ee,"As we dive into Transformer models in this course, you\u2019ll see mentions of "),ta=o(Ee,"EM",{});var Mf=s(ta);Vn=n(Mf,"architectures"),Mf.forEach(r),Cn=n(Ee," and "),ra=o(Ee,"EM",{});var Rf=s(ra);Yn=n(Rf,"checkpoints"),Rf.forEach(r),Jn=n(Ee," as well as "),aa=o(Ee,"EM",{});var qf=s(aa);Xn=n(qf,"models"),qf.forEach(r),Qn=n(Ee,". These terms all have slightly different meanings:"),Ee.forEach(r),Po=d(e),G=o(e,"UL",{});var Qt=s(G);Wt=o(Qt,"LI",{});var _l=s(Wt);oa=o(_l,"STRONG",{});var Lf=s(oa);Zn=n(Lf,"Architecture"),Lf.forEach(r),Kn=n(_l,": This is the skeleton of the model \u2014 the definition of each layer and each operation that happens within the model."),_l.forEach(r),el=d(Qt),Ut=o(Qt,"LI",{});var bl=s(Ut);sa=o(bl,"STRONG",{});var Bf=s(sa);tl=n(Bf,"Checkpoints"),Bf.forEach(r),rl=n(bl,": These are the weights that will be loaded in a given architecture."),bl.forEach(r),al=d(Qt),N=o(Qt,"LI",{});var gt=s(N);ia=o(gt,"STRONG",{});var jf=s(ia);ol=n(jf,"Model"),jf.forEach(r),sl=n(gt,": This is an umbrella term that isn\u2019t as precise as \u201Carchitecture\u201D or \u201Ccheckpoint\u201D: it can mean both. This course will specify "),na=o(gt,"EM",{});var Df=s(na);il=n(Df,"architecture"),Df.forEach(r),nl=n(gt," or "),la=o(gt,"EM",{});var Of=s(la);ll=n(Of,"checkpoint"),Of.forEach(r),hl=n(gt," when it matters to reduce ambiguity."),gt.forEach(r),Qt.forEach(r),Ao=d(e),S=o(e,"P",{});var Zt=s(S);fl=n(Zt,"For example, BERT is an architecture while "),ha=o(Zt,"CODE",{});var Ff=s(ha);cl=n(Ff,"bert-base-cased"),Ff.forEach(r),dl=n(Zt,", a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say \u201Cthe BERT model\u201D and \u201Cthe "),fa=o(Zt,"CODE",{});var Hf=s(fa);ul=n(Hf,"bert-base-cased"),Hf.forEach(r),ml=n(Zt," model.\u201D"),Zt.forEach(r),this.h()},h(){f(R,"name","hf:doc:metadata"),f(R,"content",JSON.stringify(Qf)),f(ee,"id","how-do-transformers-work"),f(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ee,"href","#how-do-transformers-work"),f(q,"class","relative group"),f(te,"id","a-bit-of-transformer-history"),f(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(te,"href","#a-bit-of-transformer-history"),f(L,"class","relative group"),f(Pe,"class","block dark:hidden"),u(Pe.src,kl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||f(Pe,"src",kl),f(Pe,"alt","A brief chronology of Transformers models."),f(Ae,"class","hidden dark:block"),u(Ae.src,$l="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||f(Ae,"src",$l),f(Ae,"alt","A brief chronology of Transformers models."),f(B,"class","flex justify-center"),f(Ie,"href","https://arxiv.org/abs/1706.03762"),f(Ie,"rel","nofollow"),f(xe,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),f(xe,"rel","nofollow"),f(Ge,"href","https://arxiv.org/abs/1810.04805"),f(Ge,"rel","nofollow"),f(Ne,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),f(Ne,"rel","nofollow"),f(Se,"href","https://arxiv.org/abs/1910.01108"),f(Se,"rel","nofollow"),f(Me,"href","https://arxiv.org/abs/1910.13461"),f(Me,"rel","nofollow"),f(Re,"href","https://arxiv.org/abs/1910.10683"),f(Re,"rel","nofollow"),f(qe,"href","https://arxiv.org/abs/2005.14165"),f(qe,"rel","nofollow"),f(ne,"id","transformers-are-language-models"),f(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ne,"href","#transformers-are-language-models"),f(j,"class","relative group"),f(Oe,"class","block dark:hidden"),u(Oe.src,Pl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||f(Oe,"src",Pl),f(Oe,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),f(Fe,"class","hidden dark:block"),u(Fe.src,Al="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||f(Fe,"src",Al),f(Fe,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),f(D,"class","flex justify-center"),f(He,"class","block dark:hidden"),u(He.src,Il="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||f(He,"src",Il),f(He,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),f(ze,"class","hidden dark:block"),u(ze.src,xl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||f(ze,"src",xl),f(ze,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),f(O,"class","flex justify-center"),f(ce,"id","transformers-are-big-models"),f(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ce,"href","#transformers-are-big-models"),f(F,"class","relative group"),u(Ve.src,Gl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||f(Ve,"src",Gl),f(Ve,"alt","Number of parameters of recent Transformers models"),f(Ve,"width","90%"),f(Ue,"class","flex justify-center"),f(Ce,"class","block dark:hidden"),u(Ce.src,Nl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||f(Ce,"src",Nl),f(Ce,"alt","The carbon footprint of a large language model."),f(Ye,"class","hidden dark:block"),u(Ye.src,Sl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||f(Ye,"src",Sl),f(Ye,"alt","The carbon footprint of a large language model."),f(H,"class","flex justify-center"),f(de,"id","transfer-learning"),f(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(de,"href","#transfer-learning"),f(z,"class","relative group"),f(Ke,"class","block dark:hidden"),u(Ke.src,Ml="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||f(Ke,"src",Ml),f(Ke,"alt","The pretraining of a language model is costly in both time and money."),f(et,"class","hidden dark:block"),u(et.src,Rl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||f(et,"src",Rl),f(et,"alt","The pretraining of a language model is costly in both time and money."),f(W,"class","flex justify-center"),f(tt,"class","block dark:hidden"),u(tt.src,ql="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||f(tt,"src",ql),f(tt,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),f(rt,"class","hidden dark:block"),u(rt.src,Ll="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||f(rt,"src",Ll),f(rt,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),f(V,"class","flex justify-center"),f(me,"id","general-architecture"),f(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(me,"href","#general-architecture"),f(C,"class","relative group"),f(pe,"id","introduction"),f(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(pe,"href","#introduction"),f(Y,"class","relative group"),f(it,"class","block dark:hidden"),u(it.src,Bl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||f(it,"src",Bl),f(it,"alt","Architecture of a Transformers models"),f(nt,"class","hidden dark:block"),u(nt.src,jl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||f(nt,"src",jl),f(nt,"alt","Architecture of a Transformers models"),f(J,"class","flex justify-center"),f(we,"id","attention-layers"),f(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(we,"href","#attention-layers"),f(X,"class","relative group"),f(ht,"href","https://arxiv.org/abs/1706.03762"),f(ht,"rel","nofollow"),f(ye,"id","the-original-architecture"),f(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ye,"href","#the-original-architecture"),f(Q,"class","relative group"),f(ct,"class","block dark:hidden"),u(ct.src,Dl="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||f(ct,"src",Dl),f(ct,"alt","Architecture of a Transformers models"),f(dt,"class","hidden dark:block"),u(dt.src,Ol="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||f(dt,"src",Ol),f(dt,"alt","Architecture of a Transformers models"),f(Z,"class","flex justify-center"),f(be,"id","architectures-vs-checkpoints"),f(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(be,"href","#architectures-vs-checkpoints"),f(K,"class","relative group")},m(e,l){t(document.head,R),h(e,va,l),h(e,q,l),t(q,ee),t(ee,Kt),v(Te,Kt,null),t(q,as),t(q,er),t(er,os),h(e,wa,l),v(ke,e,l),h(e,ya,l),h(e,vt,l),t(vt,ss),h(e,_a,l),h(e,L,l),t(L,te),t(te,tr),v($e,tr,null),t(L,is),t(L,rr),t(rr,ns),h(e,ba,l),h(e,wt,l),t(wt,ls),h(e,Ea,l),h(e,B,l),t(B,Pe),t(B,hs),t(B,Ae),h(e,Ta,l),h(e,re,l),t(re,fs),t(re,Ie),t(Ie,cs),t(re,ds),h(e,ka,l),h(e,m,l),t(m,ar),t(ar,ae),t(ae,or),t(or,us),t(ae,ms),t(ae,xe),t(xe,ps),t(ae,gs),t(m,vs),t(m,sr),t(sr,oe),t(oe,ir),t(ir,ws),t(oe,ys),t(oe,Ge),t(Ge,_s),t(oe,bs),t(m,Es),t(m,nr),t(nr,se),t(se,lr),t(lr,Ts),t(se,ks),t(se,Ne),t(Ne,$s),t(se,Ps),t(m,As),t(m,hr),t(hr,ie),t(ie,fr),t(fr,Is),t(ie,xs),t(ie,Se),t(Se,Gs),t(ie,Ns),t(m,Ss),t(m,cr),t(cr,T),t(T,dr),t(dr,Ms),t(T,Rs),t(T,Me),t(Me,qs),t(T,Ls),t(T,Re),t(Re,Bs),t(T,js),t(m,Ds),t(m,ur),t(ur,k),t(k,mr),t(mr,Os),t(k,Fs),t(k,qe),t(qe,Hs),t(k,zs),t(k,pr),t(pr,Ws),t(k,Us),h(e,$a,l),h(e,yt,l),t(yt,Vs),h(e,Pa,l),h(e,$,l),t($,Le),t(Le,Cs),t(Le,gr),t(gr,Ys),t(Le,Js),t($,Xs),t($,Be),t(Be,Qs),t(Be,vr),t(vr,Zs),t(Be,Ks),t($,ei),t($,je),t(je,ti),t(je,wr),t(wr,ri),t(je,ai),h(e,Aa,l),h(e,_t,l),t(_t,oi),h(e,Ia,l),h(e,j,l),t(j,ne),t(ne,yr),v(De,yr,null),t(j,si),t(j,_r),t(_r,ii),h(e,xa,l),h(e,le,l),t(le,ni),t(le,br),t(br,li),t(le,hi),h(e,Ga,l),h(e,he,l),t(he,fi),t(he,Er),t(Er,ci),t(he,di),h(e,Na,l),h(e,P,l),t(P,ui),t(P,Tr),t(Tr,mi),t(P,pi),t(P,kr),t(kr,gi),t(P,vi),h(e,Sa,l),h(e,D,l),t(D,Oe),t(D,wi),t(D,Fe),h(e,Ma,l),h(e,fe,l),t(fe,yi),t(fe,$r),t($r,_i),t(fe,bi),h(e,Ra,l),h(e,O,l),t(O,He),t(O,Ei),t(O,ze),h(e,qa,l),h(e,F,l),t(F,ce),t(ce,Pr),v(We,Pr,null),t(F,Ti),t(F,Ar),t(Ar,ki),h(e,La,l),h(e,bt,l),t(bt,$i),h(e,Ba,l),h(e,Ue,l),t(Ue,Ve),h(e,ja,l),h(e,Et,l),t(Et,Pi),h(e,Da,l),h(e,H,l),t(H,Ce),t(H,Ai),t(H,Ye),h(e,Oa,l),v(Je,e,l),h(e,Fa,l),h(e,Tt,l),t(Tt,Ii),h(e,Ha,l),h(e,kt,l),t(kt,xi),h(e,za,l),h(e,$t,l),t($t,Gi),h(e,Wa,l),h(e,z,l),t(z,de),t(de,Ir),v(Xe,Ir,null),t(z,Ni),t(z,xr),t(xr,Si),h(e,Ua,l),v(Qe,e,l),h(e,Va,l),h(e,Ze,l),t(Ze,Gr),t(Gr,Mi),t(Ze,Ri),h(e,Ca,l),h(e,W,l),t(W,Ke),t(W,qi),t(W,et),h(e,Ya,l),h(e,Pt,l),t(Pt,Li),h(e,Ja,l),h(e,U,l),t(U,Nr),t(Nr,Bi),t(U,ji),t(U,Sr),t(Sr,Di),t(U,Oi),h(e,Xa,l),h(e,A,l),t(A,Mr),t(Mr,Fi),t(A,Hi),t(A,Rr),t(Rr,zi),t(A,Wi),t(A,qr),t(qr,Ui),h(e,Qa,l),h(e,ue,l),t(ue,Vi),t(ue,Lr),t(Lr,Ci),t(ue,Yi),h(e,Za,l),h(e,V,l),t(V,tt),t(V,Ji),t(V,rt),h(e,Ka,l),h(e,At,l),t(At,Xi),h(e,eo,l),h(e,It,l),t(It,Qi),h(e,to,l),h(e,C,l),t(C,me),t(me,Br),v(at,Br,null),t(C,Zi),t(C,jr),t(jr,Ki),h(e,ro,l),h(e,xt,l),t(xt,en),h(e,ao,l),v(ot,e,l),h(e,oo,l),h(e,Y,l),t(Y,pe),t(pe,Dr),v(st,Dr,null),t(Y,tn),t(Y,Or),t(Or,rn),h(e,so,l),h(e,Gt,l),t(Gt,an),h(e,io,l),h(e,ge,l),t(ge,Nt),t(Nt,Fr),t(Fr,on),t(Nt,sn),t(ge,nn),t(ge,St),t(St,Hr),t(Hr,ln),t(St,hn),h(e,no,l),h(e,J,l),t(J,it),t(J,fn),t(J,nt),h(e,lo,l),h(e,Mt,l),t(Mt,cn),h(e,ho,l),h(e,I,l),t(I,Rt),t(Rt,zr),t(zr,dn),t(Rt,un),t(I,mn),t(I,qt),t(qt,Wr),t(Wr,pn),t(qt,gn),t(I,vn),t(I,ve),t(ve,Ur),t(Ur,wn),t(ve,yn),t(ve,Vr),t(Vr,_n),t(ve,bn),h(e,fo,l),h(e,Lt,l),t(Lt,En),h(e,co,l),h(e,X,l),t(X,we),t(we,Cr),v(lt,Cr,null),t(X,Tn),t(X,Yr),t(Yr,kn),h(e,uo,l),h(e,x,l),t(x,$n),t(x,Jr),t(Jr,Pn),t(x,An),t(x,ht),t(ht,In),t(x,xn),h(e,mo,l),h(e,Bt,l),t(Bt,Gn),h(e,po,l),h(e,jt,l),t(jt,Nn),h(e,go,l),h(e,Dt,l),t(Dt,Sn),h(e,vo,l),h(e,Q,l),t(Q,ye),t(ye,Xr),v(ft,Xr,null),t(Q,Mn),t(Q,Qr),t(Qr,Rn),h(e,wo,l),h(e,Ot,l),t(Ot,qn),h(e,yo,l),h(e,Ft,l),t(Ft,Ln),h(e,_o,l),h(e,Ht,l),t(Ht,Bn),h(e,bo,l),h(e,Z,l),t(Z,ct),t(Z,jn),t(Z,dt),h(e,Eo,l),h(e,zt,l),t(zt,Dn),h(e,To,l),h(e,_e,l),t(_e,On),t(_e,Zr),t(Zr,Fn),t(_e,Hn),h(e,ko,l),h(e,K,l),t(K,be),t(be,Kr),v(ut,Kr,null),t(K,zn),t(K,ea),t(ea,Wn),h(e,$o,l),h(e,b,l),t(b,Un),t(b,ta),t(ta,Vn),t(b,Cn),t(b,ra),t(ra,Yn),t(b,Jn),t(b,aa),t(aa,Xn),t(b,Qn),h(e,Po,l),h(e,G,l),t(G,Wt),t(Wt,oa),t(oa,Zn),t(Wt,Kn),t(G,el),t(G,Ut),t(Ut,sa),t(sa,tl),t(Ut,rl),t(G,al),t(G,N),t(N,ia),t(ia,ol),t(N,sl),t(N,na),t(na,il),t(N,nl),t(N,la),t(la,ll),t(N,hl),h(e,Ao,l),h(e,S,l),t(S,fl),t(S,ha),t(ha,cl),t(S,dl),t(S,fa),t(fa,ul),t(S,ml),Io=!0},p:Cf,i(e){Io||(w(Te.$$.fragment,e),w(ke.$$.fragment,e),w($e.$$.fragment,e),w(De.$$.fragment,e),w(We.$$.fragment,e),w(Je.$$.fragment,e),w(Xe.$$.fragment,e),w(Qe.$$.fragment,e),w(at.$$.fragment,e),w(ot.$$.fragment,e),w(st.$$.fragment,e),w(lt.$$.fragment,e),w(ft.$$.fragment,e),w(ut.$$.fragment,e),Io=!0)},o(e){y(Te.$$.fragment,e),y(ke.$$.fragment,e),y($e.$$.fragment,e),y(De.$$.fragment,e),y(We.$$.fragment,e),y(Je.$$.fragment,e),y(Xe.$$.fragment,e),y(Qe.$$.fragment,e),y(at.$$.fragment,e),y(ot.$$.fragment,e),y(st.$$.fragment,e),y(lt.$$.fragment,e),y(ft.$$.fragment,e),y(ut.$$.fragment,e),Io=!1},d(e){r(R),e&&r(va),e&&r(q),_(Te),e&&r(wa),_(ke,e),e&&r(ya),e&&r(vt),e&&r(_a),e&&r(L),_($e),e&&r(ba),e&&r(wt),e&&r(Ea),e&&r(B),e&&r(Ta),e&&r(re),e&&r(ka),e&&r(m),e&&r($a),e&&r(yt),e&&r(Pa),e&&r($),e&&r(Aa),e&&r(_t),e&&r(Ia),e&&r(j),_(De),e&&r(xa),e&&r(le),e&&r(Ga),e&&r(he),e&&r(Na),e&&r(P),e&&r(Sa),e&&r(D),e&&r(Ma),e&&r(fe),e&&r(Ra),e&&r(O),e&&r(qa),e&&r(F),_(We),e&&r(La),e&&r(bt),e&&r(Ba),e&&r(Ue),e&&r(ja),e&&r(Et),e&&r(Da),e&&r(H),e&&r(Oa),_(Je,e),e&&r(Fa),e&&r(Tt),e&&r(Ha),e&&r(kt),e&&r(za),e&&r($t),e&&r(Wa),e&&r(z),_(Xe),e&&r(Ua),_(Qe,e),e&&r(Va),e&&r(Ze),e&&r(Ca),e&&r(W),e&&r(Ya),e&&r(Pt),e&&r(Ja),e&&r(U),e&&r(Xa),e&&r(A),e&&r(Qa),e&&r(ue),e&&r(Za),e&&r(V),e&&r(Ka),e&&r(At),e&&r(eo),e&&r(It),e&&r(to),e&&r(C),_(at),e&&r(ro),e&&r(xt),e&&r(ao),_(ot,e),e&&r(oo),e&&r(Y),_(st),e&&r(so),e&&r(Gt),e&&r(io),e&&r(ge),e&&r(no),e&&r(J),e&&r(lo),e&&r(Mt),e&&r(ho),e&&r(I),e&&r(fo),e&&r(Lt),e&&r(co),e&&r(X),_(lt),e&&r(uo),e&&r(x),e&&r(mo),e&&r(Bt),e&&r(po),e&&r(jt),e&&r(go),e&&r(Dt),e&&r(vo),e&&r(Q),_(ft),e&&r(wo),e&&r(Ot),e&&r(yo),e&&r(Ft),e&&r(_o),e&&r(Ht),e&&r(bo),e&&r(Z),e&&r(Eo),e&&r(zt),e&&r(To),e&&r(_e),e&&r(ko),e&&r(K),_(ut),e&&r($o),e&&r(b),e&&r(Po),e&&r(G),e&&r(Ao),e&&r(S)}}}const Qf={local:"how-do-transformers-work",sections:[{local:"a-bit-of-transformer-history",title:"A bit of Transformer history"},{local:"transformers-are-language-models",title:"Transformers are language models"},{local:"transformers-are-big-models",title:"Transformers are big models"},{local:"transfer-learning",title:"Transfer Learning"},{local:"general-architecture",title:"General architecture"},{local:"introduction",title:"Introduction"},{local:"attention-layers",title:"Attention layers"},{local:"the-original-architecture",title:"The original architecture"},{local:"architectures-vs-checkpoints",title:"Architectures vs. checkpoints"}],title:"How do Transformers work?"};function Zf(Tl){return Yf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ac extends zf{constructor(R){super();Wf(this,R,Zf,Xf,Uf,{})}}export{ac as default,Qf as metadata};
