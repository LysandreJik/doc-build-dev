import{S as Ce,i as Ie,s as Se,e as a,k as m,w as ie,t as c,M as De,c as l,d as t,m as d,a as s,x as fe,h,b as n,G as o,g as i,y as me,L as Ge,q as de,o as ce,B as he,v as Re}from"../../chunks/vendor-hf-doc-builder.js";import{Y as qe}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Me}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ne}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Be(ue){let u,q,p,v,k,w,O,C,Q,M,y,N,g,B,_,j,I,K,V,U,b,W,X,L,Z,Y,A,ee,z,f,S,E,te,oe,D,T,re,ae,G,x,le,se,R,P,ne,F;return w=new Me({}),y=new Ne({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),g=new qe({props:{id:"d_ixlCubqQw"}}),{c(){u=a("meta"),q=m(),p=a("h1"),v=a("a"),k=a("span"),ie(w.$$.fragment),O=m(),C=a("span"),Q=c("Decoder models"),M=m(),ie(y.$$.fragment),N=m(),ie(g.$$.fragment),B=m(),_=a("p"),j=c("Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called "),I=a("em"),K=c("auto-regressive models"),V=c("."),U=m(),b=a("p"),W=c("The pretraining of decoder models usually revolves around predicting the next word in the sentence."),X=m(),L=a("p"),Z=c("These models are best suited for tasks involving text generation."),Y=m(),A=a("p"),ee=c("Representatives of this family of models include:"),z=m(),f=a("ul"),S=a("li"),E=a("a"),te=c("CTRL"),oe=m(),D=a("li"),T=a("a"),re=c("GPT"),ae=m(),G=a("li"),x=a("a"),le=c("GPT-2"),se=m(),R=a("li"),P=a("a"),ne=c("Transformer XL"),this.h()},l(e){const r=De('[data-svelte="svelte-1phssyn"]',document.head);u=l(r,"META",{name:!0,content:!0}),r.forEach(t),q=d(e),p=l(e,"H1",{class:!0});var H=s(p);v=l(H,"A",{id:!0,class:!0,href:!0});var pe=s(v);k=l(pe,"SPAN",{});var ve=s(k);fe(w.$$.fragment,ve),ve.forEach(t),pe.forEach(t),O=d(H),C=l(H,"SPAN",{});var _e=s(C);Q=h(_e,"Decoder models"),_e.forEach(t),H.forEach(t),M=d(e),fe(y.$$.fragment,e),N=d(e),fe(g.$$.fragment,e),B=d(e),_=l(e,"P",{});var J=s(_);j=h(J,"Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called "),I=l(J,"EM",{});var $e=s(I);K=h($e,"auto-regressive models"),$e.forEach(t),V=h(J,"."),J.forEach(t),U=d(e),b=l(e,"P",{});var we=s(b);W=h(we,"The pretraining of decoder models usually revolves around predicting the next word in the sentence."),we.forEach(t),X=d(e),L=l(e,"P",{});var ye=s(L);Z=h(ye,"These models are best suited for tasks involving text generation."),ye.forEach(t),Y=d(e),A=l(e,"P",{});var ge=s(A);ee=h(ge,"Representatives of this family of models include:"),ge.forEach(t),z=d(e),f=l(e,"UL",{});var $=s(f);S=l($,"LI",{});var Ee=s(S);E=l(Ee,"A",{href:!0,rel:!0});var Te=s(E);te=h(Te,"CTRL"),Te.forEach(t),Ee.forEach(t),oe=d($),D=l($,"LI",{});var xe=s(D);T=l(xe,"A",{href:!0,rel:!0});var Pe=s(T);re=h(Pe,"GPT"),Pe.forEach(t),xe.forEach(t),ae=d($),G=l($,"LI",{});var be=s(G);x=l(be,"A",{href:!0,rel:!0});var Le=s(x);le=h(Le,"GPT-2"),Le.forEach(t),be.forEach(t),se=d($),R=l($,"LI",{});var Ae=s(R);P=l(Ae,"A",{href:!0,rel:!0});var ke=s(P);ne=h(ke,"Transformer XL"),ke.forEach(t),Ae.forEach(t),$.forEach(t),this.h()},h(){n(u,"name","hf:doc:metadata"),n(u,"content",JSON.stringify(Ue)),n(v,"id","decoder-models"),n(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(v,"href","#decoder-models"),n(p,"class","relative group"),n(E,"href","https://huggingface.co/transformers/model_doc/ctrl.html"),n(E,"rel","nofollow"),n(T,"href","https://huggingface.co/transformers/model_doc/gpt.html"),n(T,"rel","nofollow"),n(x,"href","https://huggingface.co/transformers/model_doc/gpt2.html"),n(x,"rel","nofollow"),n(P,"href","https://huggingface.co/transformers/model_doc/transfo-xl.html"),n(P,"rel","nofollow")},m(e,r){o(document.head,u),i(e,q,r),i(e,p,r),o(p,v),o(v,k),me(w,k,null),o(p,O),o(p,C),o(C,Q),i(e,M,r),me(y,e,r),i(e,N,r),me(g,e,r),i(e,B,r),i(e,_,r),o(_,j),o(_,I),o(I,K),o(_,V),i(e,U,r),i(e,b,r),o(b,W),i(e,X,r),i(e,L,r),o(L,Z),i(e,Y,r),i(e,A,r),o(A,ee),i(e,z,r),i(e,f,r),o(f,S),o(S,E),o(E,te),o(f,oe),o(f,D),o(D,T),o(T,re),o(f,ae),o(f,G),o(G,x),o(x,le),o(f,se),o(f,R),o(R,P),o(P,ne),F=!0},p:Ge,i(e){F||(de(w.$$.fragment,e),de(y.$$.fragment,e),de(g.$$.fragment,e),F=!0)},o(e){ce(w.$$.fragment,e),ce(y.$$.fragment,e),ce(g.$$.fragment,e),F=!1},d(e){t(u),e&&t(q),e&&t(p),he(w),e&&t(M),he(y,e),e&&t(N),he(g,e),e&&t(B),e&&t(_),e&&t(U),e&&t(b),e&&t(X),e&&t(L),e&&t(Y),e&&t(A),e&&t(z),e&&t(f)}}}const Ue={local:"decoder-models",title:"Decoder models"};function Xe(ue){return Re(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Je extends Ce{constructor(u){super();Ie(this,u,Xe,Be,Se,{})}}export{Je as default,Ue as metadata};
