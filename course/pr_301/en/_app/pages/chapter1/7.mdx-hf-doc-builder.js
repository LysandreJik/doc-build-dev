import{S as Ie,i as Me,s as je,e as o,k as m,w as me,t as c,M as Ce,c as s,d as t,m as u,a as n,x as ue,h as f,b as l,G as a,g as i,y as de,L as Ne,q as pe,o as ve,B as _e,v as ze}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Fe}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ue}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ye}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Ge(we){let d,C,p,v,P,b,Q,L,V,N,E,z,$,F,_,W,R,X,Z,U,w,ee,q,te,ae,Y,k,oe,G,S,se,H,h,B,y,re,ne,I,A,le,ie,M,x,ce,fe,j,T,he,J;return b=new Ue({}),E=new Ye({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),$=new Fe({props:{id:"0_4KEb08xrE"}}),{c(){d=o("meta"),C=m(),p=o("h1"),v=o("a"),P=o("span"),me(b.$$.fragment),Q=m(),L=o("span"),V=c("Sequence-to-sequence models"),N=m(),me(E.$$.fragment),z=m(),me($.$$.fragment),F=m(),_=o("p"),W=c("Encoder-decoder models (also called "),R=o("em"),X=c("sequence-to-sequence models"),Z=c(") use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input."),U=m(),w=o("p"),ee=c("The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, "),q=o("a"),te=c("T5"),ae=c(" is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces."),Y=m(),k=o("p"),oe=c("Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering."),G=m(),S=o("p"),se=c("Representatives of this family of models include:"),H=m(),h=o("ul"),B=o("li"),y=o("a"),re=c("BART"),ne=m(),I=o("li"),A=o("a"),le=c("mBART"),ie=m(),M=o("li"),x=o("a"),ce=c("Marian"),fe=m(),j=o("li"),T=o("a"),he=c("T5"),this.h()},l(e){const r=Ce('[data-svelte="svelte-1phssyn"]',document.head);d=s(r,"META",{name:!0,content:!0}),r.forEach(t),C=u(e),p=s(e,"H1",{class:!0});var K=n(p);v=s(K,"A",{id:!0,class:!0,href:!0});var ge=n(v);P=s(ge,"SPAN",{});var be=n(P);ue(b.$$.fragment,be),be.forEach(t),ge.forEach(t),Q=u(K),L=s(K,"SPAN",{});var Ee=n(L);V=f(Ee,"Sequence-to-sequence models"),Ee.forEach(t),K.forEach(t),N=u(e),ue(E.$$.fragment,e),z=u(e),ue($.$$.fragment,e),F=u(e),_=s(e,"P",{});var O=n(_);W=f(O,"Encoder-decoder models (also called "),R=s(O,"EM",{});var $e=n(R);X=f($e,"sequence-to-sequence models"),$e.forEach(t),Z=f(O,") use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input."),O.forEach(t),U=u(e),w=s(e,"P",{});var D=n(w);ee=f(D,"The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, "),q=s(D,"A",{href:!0,rel:!0});var qe=n(q);te=f(qe,"T5"),qe.forEach(t),ae=f(D," is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces."),D.forEach(t),Y=u(e),k=s(e,"P",{});var ye=n(k);oe=f(ye,"Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering."),ye.forEach(t),G=u(e),S=s(e,"P",{});var Ae=n(S);se=f(Ae,"Representatives of this family of models include:"),Ae.forEach(t),H=u(e),h=s(e,"UL",{});var g=n(h);B=s(g,"LI",{});var xe=n(B);y=s(xe,"A",{href:!0,rel:!0});var Te=n(y);re=f(Te,"BART"),Te.forEach(t),xe.forEach(t),ne=u(g),I=s(g,"LI",{});var ke=n(I);A=s(ke,"A",{href:!0,rel:!0});var Se=n(A);le=f(Se,"mBART"),Se.forEach(t),ke.forEach(t),ie=u(g),M=s(g,"LI",{});var Pe=n(M);x=s(Pe,"A",{href:!0,rel:!0});var Le=n(x);ce=f(Le,"Marian"),Le.forEach(t),Pe.forEach(t),fe=u(g),j=s(g,"LI",{});var Re=n(j);T=s(Re,"A",{href:!0,rel:!0});var Be=n(T);he=f(Be,"T5"),Be.forEach(t),Re.forEach(t),g.forEach(t),this.h()},h(){l(d,"name","hf:doc:metadata"),l(d,"content",JSON.stringify(He)),l(v,"id","sequencetosequence-models"),l(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(v,"href","#sequencetosequence-models"),l(p,"class","relative group"),l(q,"href","https://huggingface.co/t5-base"),l(q,"rel","nofollow"),l(y,"href","https://huggingface.co/transformers/model_doc/bart.html"),l(y,"rel","nofollow"),l(A,"href","https://huggingface.co/transformers/model_doc/mbart.html"),l(A,"rel","nofollow"),l(x,"href","https://huggingface.co/transformers/model_doc/marian.html"),l(x,"rel","nofollow"),l(T,"href","https://huggingface.co/transformers/model_doc/t5.html"),l(T,"rel","nofollow")},m(e,r){a(document.head,d),i(e,C,r),i(e,p,r),a(p,v),a(v,P),de(b,P,null),a(p,Q),a(p,L),a(L,V),i(e,N,r),de(E,e,r),i(e,z,r),de($,e,r),i(e,F,r),i(e,_,r),a(_,W),a(_,R),a(R,X),a(_,Z),i(e,U,r),i(e,w,r),a(w,ee),a(w,q),a(q,te),a(w,ae),i(e,Y,r),i(e,k,r),a(k,oe),i(e,G,r),i(e,S,r),a(S,se),i(e,H,r),i(e,h,r),a(h,B),a(B,y),a(y,re),a(h,ne),a(h,I),a(I,A),a(A,le),a(h,ie),a(h,M),a(M,x),a(x,ce),a(h,fe),a(h,j),a(j,T),a(T,he),J=!0},p:Ne,i(e){J||(pe(b.$$.fragment,e),pe(E.$$.fragment,e),pe($.$$.fragment,e),J=!0)},o(e){ve(b.$$.fragment,e),ve(E.$$.fragment,e),ve($.$$.fragment,e),J=!1},d(e){t(d),e&&t(C),e&&t(p),_e(b),e&&t(N),_e(E,e),e&&t(z),_e($,e),e&&t(F),e&&t(_),e&&t(U),e&&t(w),e&&t(Y),e&&t(k),e&&t(G),e&&t(S),e&&t(H),e&&t(h)}}}const He={local:"sequencetosequence-models",title:"Sequence-to-sequence models"};function Je(we){return ze(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ve extends Ie{constructor(d){super();Me(this,d,Je,Ge,je,{})}}export{Ve as default,He as metadata};
