import{S as at,i as ot,s as rt,e as o,k as s,w as Ke,t as l,M as nt,c as r,d as a,m as i,a as n,x as Qe,h as d,b as P,G as e,g as v,y as Ve,L as st,q as Ze,o as et,B as tt,v as lt}from"../../chunks/vendor-hf-doc-builder.js";import{I as it}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as dt}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function ht(Le){let c,J,f,_,L,g,ae,x,oe,U,R,j,w,re,S,ne,se,K,b,le,Q,y,C,m,I,ie,de,q,he,ce,H,fe,me,u,p,M,ue,pe,N,Te,Ee,z,ve,_e,T,G,we,ye,O,ge,Re,Y,be,ke,E,W,Ae,$e,X,Be,De,F,Pe,V;return g=new it({}),R=new dt({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),{c(){c=o("meta"),J=s(),f=o("h1"),_=o("a"),L=o("span"),Ke(g.$$.fragment),ae=s(),x=o("span"),oe=l("Summary"),U=s(),Ke(R.$$.fragment),j=s(),w=o("p"),re=l("In this chapter, you saw how to approach different NLP tasks using the high-level "),S=o("code"),ne=l("pipeline()"),se=l(" function from \u{1F917} Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser."),K=s(),b=o("p"),le=l("We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. The following table summarizes this:"),Q=s(),y=o("table"),C=o("thead"),m=o("tr"),I=o("th"),ie=l("Model"),de=s(),q=o("th"),he=l("Examples"),ce=s(),H=o("th"),fe=l("Tasks"),me=s(),u=o("tbody"),p=o("tr"),M=o("td"),ue=l("Encoder"),pe=s(),N=o("td"),Te=l("ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),Ee=s(),z=o("td"),ve=l("Sentence classification, named entity recognition, extractive question answering"),_e=s(),T=o("tr"),G=o("td"),we=l("Decoder"),ye=s(),O=o("td"),ge=l("CTRL, GPT, GPT-2, Transformer XL"),Re=s(),Y=o("td"),be=l("Text generation"),ke=s(),E=o("tr"),W=o("td"),Ae=l("Encoder-decoder"),$e=s(),X=o("td"),Be=l("BART, T5, Marian, mBART"),De=s(),F=o("td"),Pe=l("Summarization, translation, generative question answering"),this.h()},l(t){const h=nt('[data-svelte="svelte-1phssyn"]',document.head);c=r(h,"META",{name:!0,content:!0}),h.forEach(a),J=i(t),f=r(t,"H1",{class:!0});var Z=n(f);_=r(Z,"A",{id:!0,class:!0,href:!0});var xe=n(_);L=r(xe,"SPAN",{});var Se=n(L);Qe(g.$$.fragment,Se),Se.forEach(a),xe.forEach(a),ae=i(Z),x=r(Z,"SPAN",{});var Ce=n(x);oe=d(Ce,"Summary"),Ce.forEach(a),Z.forEach(a),U=i(t),Qe(R.$$.fragment,t),j=i(t),w=r(t,"P",{});var ee=n(w);re=d(ee,"In this chapter, you saw how to approach different NLP tasks using the high-level "),S=r(ee,"CODE",{});var Ie=n(S);ne=d(Ie,"pipeline()"),Ie.forEach(a),se=d(ee," function from \u{1F917} Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser."),ee.forEach(a),K=i(t),b=r(t,"P",{});var qe=n(b);le=d(qe,"We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. The following table summarizes this:"),qe.forEach(a),Q=i(t),y=r(t,"TABLE",{});var te=n(y);C=r(te,"THEAD",{});var He=n(C);m=r(He,"TR",{});var k=n(m);I=r(k,"TH",{});var Me=n(I);ie=d(Me,"Model"),Me.forEach(a),de=i(k),q=r(k,"TH",{});var Ne=n(q);he=d(Ne,"Examples"),Ne.forEach(a),ce=i(k),H=r(k,"TH",{});var ze=n(H);fe=d(ze,"Tasks"),ze.forEach(a),k.forEach(a),He.forEach(a),me=i(te),u=r(te,"TBODY",{});var A=n(u);p=r(A,"TR",{});var $=n(p);M=r($,"TD",{});var Ge=n(M);ue=d(Ge,"Encoder"),Ge.forEach(a),pe=i($),N=r($,"TD",{});var Oe=n(N);Te=d(Oe,"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),Oe.forEach(a),Ee=i($),z=r($,"TD",{});var Ye=n(z);ve=d(Ye,"Sentence classification, named entity recognition, extractive question answering"),Ye.forEach(a),$.forEach(a),_e=i(A),T=r(A,"TR",{});var B=n(T);G=r(B,"TD",{});var We=n(G);we=d(We,"Decoder"),We.forEach(a),ye=i(B),O=r(B,"TD",{});var Xe=n(O);ge=d(Xe,"CTRL, GPT, GPT-2, Transformer XL"),Xe.forEach(a),Re=i(B),Y=r(B,"TD",{});var Fe=n(Y);be=d(Fe,"Text generation"),Fe.forEach(a),B.forEach(a),ke=i(A),E=r(A,"TR",{});var D=n(E);W=r(D,"TD",{});var Je=n(W);Ae=d(Je,"Encoder-decoder"),Je.forEach(a),$e=i(D),X=r(D,"TD",{});var Ue=n(X);Be=d(Ue,"BART, T5, Marian, mBART"),Ue.forEach(a),De=i(D),F=r(D,"TD",{});var je=n(F);Pe=d(je,"Summarization, translation, generative question answering"),je.forEach(a),D.forEach(a),A.forEach(a),te.forEach(a),this.h()},h(){P(c,"name","hf:doc:metadata"),P(c,"content",JSON.stringify(ct)),P(_,"id","summary"),P(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(_,"href","#summary"),P(f,"class","relative group")},m(t,h){e(document.head,c),v(t,J,h),v(t,f,h),e(f,_),e(_,L),Ve(g,L,null),e(f,ae),e(f,x),e(x,oe),v(t,U,h),Ve(R,t,h),v(t,j,h),v(t,w,h),e(w,re),e(w,S),e(S,ne),e(w,se),v(t,K,h),v(t,b,h),e(b,le),v(t,Q,h),v(t,y,h),e(y,C),e(C,m),e(m,I),e(I,ie),e(m,de),e(m,q),e(q,he),e(m,ce),e(m,H),e(H,fe),e(y,me),e(y,u),e(u,p),e(p,M),e(M,ue),e(p,pe),e(p,N),e(N,Te),e(p,Ee),e(p,z),e(z,ve),e(u,_e),e(u,T),e(T,G),e(G,we),e(T,ye),e(T,O),e(O,ge),e(T,Re),e(T,Y),e(Y,be),e(u,ke),e(u,E),e(E,W),e(W,Ae),e(E,$e),e(E,X),e(X,Be),e(E,De),e(E,F),e(F,Pe),V=!0},p:st,i(t){V||(Ze(g.$$.fragment,t),Ze(R.$$.fragment,t),V=!0)},o(t){et(g.$$.fragment,t),et(R.$$.fragment,t),V=!1},d(t){a(c),t&&a(J),t&&a(f),tt(g),t&&a(U),tt(R,t),t&&a(j),t&&a(w),t&&a(K),t&&a(b),t&&a(Q),t&&a(y)}}}const ct={local:"summary",title:"Summary"};function ft(Le){return lt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Tt extends at{constructor(c){super();ot(this,c,ft,ht,rt,{})}}export{Tt as default,ct as metadata};
