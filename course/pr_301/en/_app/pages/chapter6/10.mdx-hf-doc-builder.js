import{S as qa,i as Wa,s as Ia,e as r,k as l,w as f,t as g,M as Ha,c as n,d as t,m as p,a as i,x as u,h as v,b as s,G as a,g as h,y as c,L as Ba,q as m,o as d,B as w,v as Ca}from"../../chunks/vendor-hf-doc-builder.js";import{I as $}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ua}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{Q as y}from"../../chunks/Question-hf-doc-builder.js";function La(Vt){let x,Ue,k,W,we,F,kt,ge,bt,Le,Q,De,de,zt,Oe,b,I,ve,Y,_t,$e,Et,Me,j,Re,z,H,ye,G,Pt,J,Nt,xe,At,Tt,Fe,K,Qe,_,B,ke,V,St,be,qt,Ye,X,je,E,C,ze,Z,Wt,ee,It,_e,Ht,Bt,Ge,te,Je,P,U,Ee,ae,Ct,oe,Ut,Pe,Lt,Dt,Ke,re,Ve,N,L,Ne,ne,Ot,Ae,Mt,Xe,ie,Ze,A,D,Te,se,Rt,Se,Ft,et,he,tt,T,O,qe,le,Qt,We,Yt,at,pe,ot,S,M,Ie,fe,jt,He,Gt,rt,ue,nt,q,R,Be,ce,Jt,Ce,Kt,it,me,st;return F=new $({}),Q=new Ua({props:{chapter:6,classNames:"absolute z-10 right-0 top-0"}}),Y=new $({}),j=new y({props:{choices:[{text:"When your dataset is similar to that used by an existing pretrained model, and you want to pretrain a new model",explain:"In this case, to save time and compute resources, a better choice would be to use the same tokenizer as the pretrained model and fine-tune that model instead."},{text:"When your dataset is similar to that used by an existing pretrained model, and you want to fine-tune a new model using this pretrained model",explain:"To fine-tune a model from a pretrained model, you should always use the same tokenizer."},{text:"When your dataset is different from the one used by an existing pretrained model, and you want to pretrain a new model",explain:"Correct! In this case there's no advantage to using the same tokenizer.",correct:!0},{text:"When your dataset is different from the one used by an existing pretrained model, but you want to fine-tune a new model using this pretrained model",explain:"To fine-tune a model from a pretrained model, you should always use the same tokenizer."}]}}),G=new $({}),K=new y({props:{choices:[{text:"That's the only type the method <code>train_new_from_iterator()</code> accepts.",explain:"A list of lists of texts is a particular kind of generator of lists of texts, so the method will accept this too. Try again!"},{text:"You will avoid loading the whole dataset into memory at once.",explain:"Right! Each batch of texts will be released from memory when you iterate, and the gain will be especially visible if you use \u{1F917} Datasets to store your texts.",correct:!0},{text:"This will allow the \u{1F917} Tokenizers library to use multiprocessing.",explain:"No, it will use multiprocessing either way."},{text:"The tokenizer you train will generate better texts.",explain:"The tokenizer does not generate text -- are you confusing it with a language model?"}]}}),V=new $({}),X=new y({props:{choices:[{text:"It can process inputs faster than a slow tokenizer when you batch lots of inputs together.",explain:"Correct! Thanks to parallelism implemented in Rust, it will be faster on batches of inputs. What other benefit can you think of?",correct:!0},{text:"Fast tokenizers always tokenize faster than their slow counterparts.",explain:"A fast tokenizer can actually be slower when you only give it one or very few texts, since it can't use parallelism."},{text:"It can apply padding and truncation.",explain:"True, but slow tokenizers also do that."},{text:"It has some additional features allowing you to map tokens to the span of text that created them.",explain:"Indeed -- those are called offset mappings. That's not the only advantage, though.",correct:!0}]}}),Z=new $({}),te=new y({props:{choices:[{text:"The entities with the same label are merged into one entity.",explain:"That's oversimplifying things a little. Try again!"},{text:"There is a label for the beginning of an entity and a label for the continuation of an entity.",explain:"Correct!",correct:!0},{text:"In a given word, as long as the first token has the label of the entity, the whole word is considered labeled with that entity.",explain:"That's one strategy to handle entities. What other answers here apply?",correct:!0},{text:"When a token has the label of a given entity, any other following token with the same label is considered part of the same entity, unless it's labeled as the start of a new entity.",explain:"That's the most common way to group entities together -- it's not the only right answer, though.",correct:!0}]}}),ae=new $({}),re=new y({props:{choices:[{text:"It doesn't really, as it truncates the long context at the maximum length accepted by the model.",explain:"There is a trick you can use to handle long contexts. Do you remember what it is?"},{text:"It splits the context into several parts and averages the results obtained.",explain:"No, it wouldn't make sense to average the results, as some parts of the context won't include the answer."},{text:"It splits the context into several parts (with overlap) and finds the maximum score for an answer in each part.",explain:"That's the correct answer!",correct:!0},{text:"It splits the context into several parts (without overlap, for efficiency) and finds the maximum score for an answer in each part.",explain:"No, it includes some overlap between the parts to avoid a situation where the answer would be split across two parts."}]}}),ne=new $({}),ie=new y({props:{choices:[{text:"It's any cleanup the tokenizer performs on the texts in the initial stages.",explain:"That's correct -- for instance, it might involve removing accents or whitespace, or lowercasing the inputs.",correct:!0},{text:"It's a data augmentation technique that involves making the text more normal by removing rare words.",explain:"That's incorrect! Try again."},{text:"It's the final post-processing step where the tokenizer adds the special tokens.",explain:"That stage is simply called post-processing."},{text:"It's when the embeddings are made with mean 0 and standard deviation 1, by subtracting the mean and dividing by the std.",explain:"That process is commonly called normalization when applied to pixel values in computer vision, but it's not what normalization means in NLP."}]}}),se=new $({}),he=new y({props:{choices:[{text:"It's the step before the tokenization, where data augmentation (like random masking) is applied.",explain:"No, that step is part of the preprocessing."},{text:"It's the step before the tokenization, where the desired cleanup operations are applied to the text.",explain:"No, that's the normalization step."},{text:"It's the step before the tokenizer model is applied, to split the input into words.",explain:"That's the correct answer!",correct:!0},{text:"It's the step before the tokenizer model is applied, to split the input into tokens.",explain:"No, splitting into tokens is the job of the tokenizer model."}]}}),le=new $({}),pe=new y({props:{choices:[{text:"BPE is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.",explain:"That's the case indeed!",correct:!0},{text:"BPE is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.",explain:"No, that's the approach taken by a different tokenization algorithm."},{text:"BPE tokenizers learn merge rules by merging the pair of tokens that is the most frequent.",explain:"That's correct!",correct:!0},{text:"A BPE tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.",explain:"No, that's the strategy applied by another tokenization algorithm."},{text:"BPE tokenizes words into subwords by splitting them into characters and then applying the merge rules.",explain:"That's correct!",correct:!0},{text:"BPE tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.",explain:"No, that's another tokenization algorithm's way of doing things."}]}}),fe=new $({}),ue=new y({props:{choices:[{text:"WordPiece is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.",explain:"That's the case indeed!",correct:!0},{text:"WordPiece is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.",explain:"No, that's the approach taken by a different tokenization algorithm."},{text:"WordPiece tokenizers learn merge rules by merging the pair of tokens that is the most frequent.",explain:"No, that's the strategy applied by another tokenization algorithm."},{text:"A WordPiece tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts.",explain:"That's correct!",correct:!0},{text:"WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.",explain:"No, that's how another tokenization algorithm works."},{text:"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.",explain:"Yes, this is how WordPiece proceeds for the encoding.",correct:!0}]}}),ce=new $({}),me=new y({props:{choices:[{text:"Unigram is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.",explain:"No, that's the approach taken by a different tokenization algorithm."},{text:"Unigram is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.",explain:"That's correct!",correct:!0},{text:"Unigram adapts its vocabulary by minimizing a loss computed over the whole corpus.",explain:"That's correct!",correct:!0},{text:"Unigram adapts its vocabulary by keeping the most frequent subwords.",explain:"No, this incorrect."},{text:"Unigram tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.",explain:"That's correct!",correct:!0},{text:"Unigram tokenizes words into subwords by splitting them into characters, then applying the merge rules.",explain:"No, that's how another tokenization algorithm works."}]}}),{c(){x=r("meta"),Ue=l(),k=r("h1"),W=r("a"),we=r("span"),f(F.$$.fragment),kt=l(),ge=r("span"),bt=g("End-of-chapter quiz"),Le=l(),f(Q.$$.fragment),De=l(),de=r("p"),zt=g("Let\u2019s test what you learned in this chapter!"),Oe=l(),b=r("h3"),I=r("a"),ve=r("span"),f(Y.$$.fragment),_t=l(),$e=r("span"),Et=g("1. When should you train a new tokenizer?"),Me=l(),f(j.$$.fragment),Re=l(),z=r("h3"),H=r("a"),ye=r("span"),f(G.$$.fragment),Pt=l(),J=r("span"),Nt=g("2. What is the advantage of using a generator of lists of texts compared to a list of lists of texts when using "),xe=r("code"),At=g("train_new_from_iterator()"),Tt=g("?"),Fe=l(),f(K.$$.fragment),Qe=l(),_=r("h3"),B=r("a"),ke=r("span"),f(V.$$.fragment),St=l(),be=r("span"),qt=g("3. What are the advantages of using a \u201Cfast\u201D tokenizer?"),Ye=l(),f(X.$$.fragment),je=l(),E=r("h3"),C=r("a"),ze=r("span"),f(Z.$$.fragment),Wt=l(),ee=r("span"),It=g("4. How does the "),_e=r("code"),Ht=g("token-classification"),Bt=g(" pipeline handle entities that span over several tokens?"),Ge=l(),f(te.$$.fragment),Je=l(),P=r("h3"),U=r("a"),Ee=r("span"),f(ae.$$.fragment),Ct=l(),oe=r("span"),Ut=g("5. How does the "),Pe=r("code"),Lt=g("question-answering"),Dt=g(" pipeline handle long contexts?"),Ke=l(),f(re.$$.fragment),Ve=l(),N=r("h3"),L=r("a"),Ne=r("span"),f(ne.$$.fragment),Ot=l(),Ae=r("span"),Mt=g("6. What is normalization?"),Xe=l(),f(ie.$$.fragment),Ze=l(),A=r("h3"),D=r("a"),Te=r("span"),f(se.$$.fragment),Rt=l(),Se=r("span"),Ft=g("7. What is pre-tokenization for a subword tokenizer?"),et=l(),f(he.$$.fragment),tt=l(),T=r("h3"),O=r("a"),qe=r("span"),f(le.$$.fragment),Qt=l(),We=r("span"),Yt=g("8. Select the sentences that apply to the BPE model of tokenization."),at=l(),f(pe.$$.fragment),ot=l(),S=r("h3"),M=r("a"),Ie=r("span"),f(fe.$$.fragment),jt=l(),He=r("span"),Gt=g("9. Select the sentences that apply to the WordPiece model of tokenization."),rt=l(),f(ue.$$.fragment),nt=l(),q=r("h3"),R=r("a"),Be=r("span"),f(ce.$$.fragment),Jt=l(),Ce=r("span"),Kt=g("10. Select the sentences that apply to the Unigram model of tokenization."),it=l(),f(me.$$.fragment),this.h()},l(e){const o=Ha('[data-svelte="svelte-1phssyn"]',document.head);x=n(o,"META",{name:!0,content:!0}),o.forEach(t),Ue=p(e),k=n(e,"H1",{class:!0});var ht=i(k);W=n(ht,"A",{id:!0,class:!0,href:!0});var Xt=i(W);we=n(Xt,"SPAN",{});var Zt=i(we);u(F.$$.fragment,Zt),Zt.forEach(t),Xt.forEach(t),kt=p(ht),ge=n(ht,"SPAN",{});var ea=i(ge);bt=v(ea,"End-of-chapter quiz"),ea.forEach(t),ht.forEach(t),Le=p(e),u(Q.$$.fragment,e),De=p(e),de=n(e,"P",{});var ta=i(de);zt=v(ta,"Let\u2019s test what you learned in this chapter!"),ta.forEach(t),Oe=p(e),b=n(e,"H3",{class:!0});var lt=i(b);I=n(lt,"A",{id:!0,class:!0,href:!0});var aa=i(I);ve=n(aa,"SPAN",{});var oa=i(ve);u(Y.$$.fragment,oa),oa.forEach(t),aa.forEach(t),_t=p(lt),$e=n(lt,"SPAN",{});var ra=i($e);Et=v(ra,"1. When should you train a new tokenizer?"),ra.forEach(t),lt.forEach(t),Me=p(e),u(j.$$.fragment,e),Re=p(e),z=n(e,"H3",{class:!0});var pt=i(z);H=n(pt,"A",{id:!0,class:!0,href:!0});var na=i(H);ye=n(na,"SPAN",{});var ia=i(ye);u(G.$$.fragment,ia),ia.forEach(t),na.forEach(t),Pt=p(pt),J=n(pt,"SPAN",{});var ft=i(J);Nt=v(ft,"2. What is the advantage of using a generator of lists of texts compared to a list of lists of texts when using "),xe=n(ft,"CODE",{});var sa=i(xe);At=v(sa,"train_new_from_iterator()"),sa.forEach(t),Tt=v(ft,"?"),ft.forEach(t),pt.forEach(t),Fe=p(e),u(K.$$.fragment,e),Qe=p(e),_=n(e,"H3",{class:!0});var ut=i(_);B=n(ut,"A",{id:!0,class:!0,href:!0});var ha=i(B);ke=n(ha,"SPAN",{});var la=i(ke);u(V.$$.fragment,la),la.forEach(t),ha.forEach(t),St=p(ut),be=n(ut,"SPAN",{});var pa=i(be);qt=v(pa,"3. What are the advantages of using a \u201Cfast\u201D tokenizer?"),pa.forEach(t),ut.forEach(t),Ye=p(e),u(X.$$.fragment,e),je=p(e),E=n(e,"H3",{class:!0});var ct=i(E);C=n(ct,"A",{id:!0,class:!0,href:!0});var fa=i(C);ze=n(fa,"SPAN",{});var ua=i(ze);u(Z.$$.fragment,ua),ua.forEach(t),fa.forEach(t),Wt=p(ct),ee=n(ct,"SPAN",{});var mt=i(ee);It=v(mt,"4. How does the "),_e=n(mt,"CODE",{});var ca=i(_e);Ht=v(ca,"token-classification"),ca.forEach(t),Bt=v(mt," pipeline handle entities that span over several tokens?"),mt.forEach(t),ct.forEach(t),Ge=p(e),u(te.$$.fragment,e),Je=p(e),P=n(e,"H3",{class:!0});var dt=i(P);U=n(dt,"A",{id:!0,class:!0,href:!0});var ma=i(U);Ee=n(ma,"SPAN",{});var da=i(Ee);u(ae.$$.fragment,da),da.forEach(t),ma.forEach(t),Ct=p(dt),oe=n(dt,"SPAN",{});var wt=i(oe);Ut=v(wt,"5. How does the "),Pe=n(wt,"CODE",{});var wa=i(Pe);Lt=v(wa,"question-answering"),wa.forEach(t),Dt=v(wt," pipeline handle long contexts?"),wt.forEach(t),dt.forEach(t),Ke=p(e),u(re.$$.fragment,e),Ve=p(e),N=n(e,"H3",{class:!0});var gt=i(N);L=n(gt,"A",{id:!0,class:!0,href:!0});var ga=i(L);Ne=n(ga,"SPAN",{});var va=i(Ne);u(ne.$$.fragment,va),va.forEach(t),ga.forEach(t),Ot=p(gt),Ae=n(gt,"SPAN",{});var $a=i(Ae);Mt=v($a,"6. What is normalization?"),$a.forEach(t),gt.forEach(t),Xe=p(e),u(ie.$$.fragment,e),Ze=p(e),A=n(e,"H3",{class:!0});var vt=i(A);D=n(vt,"A",{id:!0,class:!0,href:!0});var ya=i(D);Te=n(ya,"SPAN",{});var xa=i(Te);u(se.$$.fragment,xa),xa.forEach(t),ya.forEach(t),Rt=p(vt),Se=n(vt,"SPAN",{});var ka=i(Se);Ft=v(ka,"7. What is pre-tokenization for a subword tokenizer?"),ka.forEach(t),vt.forEach(t),et=p(e),u(he.$$.fragment,e),tt=p(e),T=n(e,"H3",{class:!0});var $t=i(T);O=n($t,"A",{id:!0,class:!0,href:!0});var ba=i(O);qe=n(ba,"SPAN",{});var za=i(qe);u(le.$$.fragment,za),za.forEach(t),ba.forEach(t),Qt=p($t),We=n($t,"SPAN",{});var _a=i(We);Yt=v(_a,"8. Select the sentences that apply to the BPE model of tokenization."),_a.forEach(t),$t.forEach(t),at=p(e),u(pe.$$.fragment,e),ot=p(e),S=n(e,"H3",{class:!0});var yt=i(S);M=n(yt,"A",{id:!0,class:!0,href:!0});var Ea=i(M);Ie=n(Ea,"SPAN",{});var Pa=i(Ie);u(fe.$$.fragment,Pa),Pa.forEach(t),Ea.forEach(t),jt=p(yt),He=n(yt,"SPAN",{});var Na=i(He);Gt=v(Na,"9. Select the sentences that apply to the WordPiece model of tokenization."),Na.forEach(t),yt.forEach(t),rt=p(e),u(ue.$$.fragment,e),nt=p(e),q=n(e,"H3",{class:!0});var xt=i(q);R=n(xt,"A",{id:!0,class:!0,href:!0});var Aa=i(R);Be=n(Aa,"SPAN",{});var Ta=i(Be);u(ce.$$.fragment,Ta),Ta.forEach(t),Aa.forEach(t),Jt=p(xt),Ce=n(xt,"SPAN",{});var Sa=i(Ce);Kt=v(Sa,"10. Select the sentences that apply to the Unigram model of tokenization."),Sa.forEach(t),xt.forEach(t),it=p(e),u(me.$$.fragment,e),this.h()},h(){s(x,"name","hf:doc:metadata"),s(x,"content",JSON.stringify(Da)),s(W,"id","endofchapter-quiz"),s(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(W,"href","#endofchapter-quiz"),s(k,"class","relative group"),s(I,"id","1.-when-should-you-train-a-new-tokenizer?"),s(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(I,"href","#1.-when-should-you-train-a-new-tokenizer?"),s(b,"class","relative group"),s(H,"id","2.-what-is-the-advantage-of-using-a-generator-of-lists-of-texts-compared-to-a-list-of-lists-of-texts-when-using-<code>train_new_from_iterator()</code>?"),s(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(H,"href","#2.-what-is-the-advantage-of-using-a-generator-of-lists-of-texts-compared-to-a-list-of-lists-of-texts-when-using-<code>train_new_from_iterator()</code>?"),s(z,"class","relative group"),s(B,"id","3.-what-are-the-advantages-of-using-a-\u201Cfast\u201D-tokenizer?"),s(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(B,"href","#3.-what-are-the-advantages-of-using-a-\u201Cfast\u201D-tokenizer?"),s(_,"class","relative group"),s(C,"id","4.-how-does-the-<code>token-classification</code>-pipeline-handle-entities-that-span-over-several-tokens?"),s(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(C,"href","#4.-how-does-the-<code>token-classification</code>-pipeline-handle-entities-that-span-over-several-tokens?"),s(E,"class","relative group"),s(U,"id","5.-how-does-the-<code>question-answering</code>-pipeline-handle-long-contexts?"),s(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(U,"href","#5.-how-does-the-<code>question-answering</code>-pipeline-handle-long-contexts?"),s(P,"class","relative group"),s(L,"id","6.-what-is-normalization?"),s(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(L,"href","#6.-what-is-normalization?"),s(N,"class","relative group"),s(D,"id","7.-what-is-pre-tokenization-for-a-subword-tokenizer?"),s(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(D,"href","#7.-what-is-pre-tokenization-for-a-subword-tokenizer?"),s(A,"class","relative group"),s(O,"id","8.-select-the-sentences-that-apply-to-the-bpe-model-of-tokenization."),s(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(O,"href","#8.-select-the-sentences-that-apply-to-the-bpe-model-of-tokenization."),s(T,"class","relative group"),s(M,"id","9.-select-the-sentences-that-apply-to-the-wordpiece-model-of-tokenization."),s(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(M,"href","#9.-select-the-sentences-that-apply-to-the-wordpiece-model-of-tokenization."),s(S,"class","relative group"),s(R,"id","10.-select-the-sentences-that-apply-to-the-unigram-model-of-tokenization."),s(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(R,"href","#10.-select-the-sentences-that-apply-to-the-unigram-model-of-tokenization."),s(q,"class","relative group")},m(e,o){a(document.head,x),h(e,Ue,o),h(e,k,o),a(k,W),a(W,we),c(F,we,null),a(k,kt),a(k,ge),a(ge,bt),h(e,Le,o),c(Q,e,o),h(e,De,o),h(e,de,o),a(de,zt),h(e,Oe,o),h(e,b,o),a(b,I),a(I,ve),c(Y,ve,null),a(b,_t),a(b,$e),a($e,Et),h(e,Me,o),c(j,e,o),h(e,Re,o),h(e,z,o),a(z,H),a(H,ye),c(G,ye,null),a(z,Pt),a(z,J),a(J,Nt),a(J,xe),a(xe,At),a(J,Tt),h(e,Fe,o),c(K,e,o),h(e,Qe,o),h(e,_,o),a(_,B),a(B,ke),c(V,ke,null),a(_,St),a(_,be),a(be,qt),h(e,Ye,o),c(X,e,o),h(e,je,o),h(e,E,o),a(E,C),a(C,ze),c(Z,ze,null),a(E,Wt),a(E,ee),a(ee,It),a(ee,_e),a(_e,Ht),a(ee,Bt),h(e,Ge,o),c(te,e,o),h(e,Je,o),h(e,P,o),a(P,U),a(U,Ee),c(ae,Ee,null),a(P,Ct),a(P,oe),a(oe,Ut),a(oe,Pe),a(Pe,Lt),a(oe,Dt),h(e,Ke,o),c(re,e,o),h(e,Ve,o),h(e,N,o),a(N,L),a(L,Ne),c(ne,Ne,null),a(N,Ot),a(N,Ae),a(Ae,Mt),h(e,Xe,o),c(ie,e,o),h(e,Ze,o),h(e,A,o),a(A,D),a(D,Te),c(se,Te,null),a(A,Rt),a(A,Se),a(Se,Ft),h(e,et,o),c(he,e,o),h(e,tt,o),h(e,T,o),a(T,O),a(O,qe),c(le,qe,null),a(T,Qt),a(T,We),a(We,Yt),h(e,at,o),c(pe,e,o),h(e,ot,o),h(e,S,o),a(S,M),a(M,Ie),c(fe,Ie,null),a(S,jt),a(S,He),a(He,Gt),h(e,rt,o),c(ue,e,o),h(e,nt,o),h(e,q,o),a(q,R),a(R,Be),c(ce,Be,null),a(q,Jt),a(q,Ce),a(Ce,Kt),h(e,it,o),c(me,e,o),st=!0},p:Ba,i(e){st||(m(F.$$.fragment,e),m(Q.$$.fragment,e),m(Y.$$.fragment,e),m(j.$$.fragment,e),m(G.$$.fragment,e),m(K.$$.fragment,e),m(V.$$.fragment,e),m(X.$$.fragment,e),m(Z.$$.fragment,e),m(te.$$.fragment,e),m(ae.$$.fragment,e),m(re.$$.fragment,e),m(ne.$$.fragment,e),m(ie.$$.fragment,e),m(se.$$.fragment,e),m(he.$$.fragment,e),m(le.$$.fragment,e),m(pe.$$.fragment,e),m(fe.$$.fragment,e),m(ue.$$.fragment,e),m(ce.$$.fragment,e),m(me.$$.fragment,e),st=!0)},o(e){d(F.$$.fragment,e),d(Q.$$.fragment,e),d(Y.$$.fragment,e),d(j.$$.fragment,e),d(G.$$.fragment,e),d(K.$$.fragment,e),d(V.$$.fragment,e),d(X.$$.fragment,e),d(Z.$$.fragment,e),d(te.$$.fragment,e),d(ae.$$.fragment,e),d(re.$$.fragment,e),d(ne.$$.fragment,e),d(ie.$$.fragment,e),d(se.$$.fragment,e),d(he.$$.fragment,e),d(le.$$.fragment,e),d(pe.$$.fragment,e),d(fe.$$.fragment,e),d(ue.$$.fragment,e),d(ce.$$.fragment,e),d(me.$$.fragment,e),st=!1},d(e){t(x),e&&t(Ue),e&&t(k),w(F),e&&t(Le),w(Q,e),e&&t(De),e&&t(de),e&&t(Oe),e&&t(b),w(Y),e&&t(Me),w(j,e),e&&t(Re),e&&t(z),w(G),e&&t(Fe),w(K,e),e&&t(Qe),e&&t(_),w(V),e&&t(Ye),w(X,e),e&&t(je),e&&t(E),w(Z),e&&t(Ge),w(te,e),e&&t(Je),e&&t(P),w(ae),e&&t(Ke),w(re,e),e&&t(Ve),e&&t(N),w(ne),e&&t(Xe),w(ie,e),e&&t(Ze),e&&t(A),w(se),e&&t(et),w(he,e),e&&t(tt),e&&t(T),w(le),e&&t(at),w(pe,e),e&&t(ot),e&&t(S),w(fe),e&&t(rt),w(ue,e),e&&t(nt),e&&t(q),w(ce),e&&t(it),w(me,e)}}}const Da={local:"endofchapter-quiz",title:"End-of-chapter quiz"};function Oa(Vt){return Ca(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ya extends qa{constructor(x){super();Wa(this,x,Oa,La,Ia,{})}}export{Ya as default,Da as metadata};
