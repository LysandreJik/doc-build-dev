import{S as xe,i as Ae,s as Ue,e as r,k as u,w as ue,t as s,M as Fe,c as n,d as t,m as c,a as i,x as ce,h,b as L,G as o,g as f,y as de,L as He,q as me,o as pe,B as we,v as Ke}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ce}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Me}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Se}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function qe(ge){let m,H,p,w,I,g,W,N,z,K,y,C,b,J,M,$,j,S,v,q,E,Q,O,l,T,V,X,x,Z,ee,_,te,A,oe,ae,re,U,ne,ie,k,le,F,se,he,B,P,fe,D;return g=new Me({}),y=new Se({props:{chapter:7,classNames:"absolute z-10 right-0 top-0"}}),v=new Ce({props:{id:"-RPeakdlHYo"}}),{c(){m=r("meta"),H=u(),p=r("h1"),w=r("a"),I=r("span"),ue(g.$$.fragment),W=u(),N=r("span"),z=s("Mastering NLP"),K=u(),ue(y.$$.fragment),C=u(),b=r("p"),J=s("If you\u2019ve made it this far in the course, congratulations \u2014 you now have all the knowledge and tools you need to tackle (almost) any NLP task with \u{1F917} Transformers and the Hugging Face ecosystem!"),M=u(),$=r("p"),j=s("We have seen a lot of different data collators, so we made this little video to help you find which one to use for each task:"),S=u(),ue(v.$$.fragment),q=u(),E=r("p"),Q=s("After completing this lightning tour through the core NLP tasks, you should:"),O=u(),l=r("ul"),T=r("li"),V=s("Know which architectures (encoder, decoder, or encoder-decoder) are best suited for each task"),X=u(),x=r("li"),Z=s("Understand the difference between pretraining and fine-tuning a language model"),ee=u(),_=r("li"),te=s("Know how to train Transformer models using either the "),A=r("code"),oe=s("Trainer"),ae=s(" API and distributed training features of \u{1F917} Accelerate or TensorFlow and Keras, depending on which track you\u2019ve been following"),re=u(),U=r("li"),ne=s("Understand the meaning and limitations of metrics like ROUGE and BLEU for text generation tasks"),ie=u(),k=r("li"),le=s("Know how to interact with your fine-tuned models, both on the Hub and using the "),F=r("code"),se=s("pipeline"),he=s(" from \u{1F917} Transformers"),B=u(),P=r("p"),fe=s("Despite all this knowledge, there will come a time when you\u2019ll either encounter a difficult bug in your code or have a question about how to solve a particular NLP problem. Fortunately, the Hugging Face community is here to help you! In the final chapter of this part of the course, we\u2019ll explore how you can debug your Transformer models and ask for help effectively."),this.h()},l(e){const a=Fe('[data-svelte="svelte-1phssyn"]',document.head);m=n(a,"META",{name:!0,content:!0}),a.forEach(t),H=c(e),p=n(e,"H1",{class:!0});var R=i(p);w=n(R,"A",{id:!0,class:!0,href:!0});var ye=i(w);I=n(ye,"SPAN",{});var ve=i(I);ce(g.$$.fragment,ve),ve.forEach(t),ye.forEach(t),W=c(R),N=n(R,"SPAN",{});var _e=i(N);z=h(_e,"Mastering NLP"),_e.forEach(t),R.forEach(t),K=c(e),ce(y.$$.fragment,e),C=c(e),b=n(e,"P",{});var ke=i(b);J=h(ke,"If you\u2019ve made it this far in the course, congratulations \u2014 you now have all the knowledge and tools you need to tackle (almost) any NLP task with \u{1F917} Transformers and the Hugging Face ecosystem!"),ke.forEach(t),M=c(e),$=n(e,"P",{});var be=i($);j=h(be,"We have seen a lot of different data collators, so we made this little video to help you find which one to use for each task:"),be.forEach(t),S=c(e),ce(v.$$.fragment,e),q=c(e),E=n(e,"P",{});var $e=i(E);Q=h($e,"After completing this lightning tour through the core NLP tasks, you should:"),$e.forEach(t),O=c(e),l=n(e,"UL",{});var d=i(l);T=n(d,"LI",{});var Ee=i(T);V=h(Ee,"Know which architectures (encoder, decoder, or encoder-decoder) are best suited for each task"),Ee.forEach(t),X=c(d),x=n(d,"LI",{});var Pe=i(x);Z=h(Pe,"Understand the difference between pretraining and fine-tuning a language model"),Pe.forEach(t),ee=c(d),_=n(d,"LI",{});var G=i(_);te=h(G,"Know how to train Transformer models using either the "),A=n(G,"CODE",{});var Le=i(A);oe=h(Le,"Trainer"),Le.forEach(t),ae=h(G," API and distributed training features of \u{1F917} Accelerate or TensorFlow and Keras, depending on which track you\u2019ve been following"),G.forEach(t),re=c(d),U=n(d,"LI",{});var Ie=i(U);ne=h(Ie,"Understand the meaning and limitations of metrics like ROUGE and BLEU for text generation tasks"),Ie.forEach(t),ie=c(d),k=n(d,"LI",{});var Y=i(k);le=h(Y,"Know how to interact with your fine-tuned models, both on the Hub and using the "),F=n(Y,"CODE",{});var Ne=i(F);se=h(Ne,"pipeline"),Ne.forEach(t),he=h(Y," from \u{1F917} Transformers"),Y.forEach(t),d.forEach(t),B=c(e),P=n(e,"P",{});var Te=i(P);fe=h(Te,"Despite all this knowledge, there will come a time when you\u2019ll either encounter a difficult bug in your code or have a question about how to solve a particular NLP problem. Fortunately, the Hugging Face community is here to help you! In the final chapter of this part of the course, we\u2019ll explore how you can debug your Transformer models and ask for help effectively."),Te.forEach(t),this.h()},h(){L(m,"name","hf:doc:metadata"),L(m,"content",JSON.stringify(Oe)),L(w,"id","mastering-nlp"),L(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),L(w,"href","#mastering-nlp"),L(p,"class","relative group")},m(e,a){o(document.head,m),f(e,H,a),f(e,p,a),o(p,w),o(w,I),de(g,I,null),o(p,W),o(p,N),o(N,z),f(e,K,a),de(y,e,a),f(e,C,a),f(e,b,a),o(b,J),f(e,M,a),f(e,$,a),o($,j),f(e,S,a),de(v,e,a),f(e,q,a),f(e,E,a),o(E,Q),f(e,O,a),f(e,l,a),o(l,T),o(T,V),o(l,X),o(l,x),o(x,Z),o(l,ee),o(l,_),o(_,te),o(_,A),o(A,oe),o(_,ae),o(l,re),o(l,U),o(U,ne),o(l,ie),o(l,k),o(k,le),o(k,F),o(F,se),o(k,he),f(e,B,a),f(e,P,a),o(P,fe),D=!0},p:He,i(e){D||(me(g.$$.fragment,e),me(y.$$.fragment,e),me(v.$$.fragment,e),D=!0)},o(e){pe(g.$$.fragment,e),pe(y.$$.fragment,e),pe(v.$$.fragment,e),D=!1},d(e){t(m),e&&t(H),e&&t(p),we(g),e&&t(K),we(y,e),e&&t(C),e&&t(b),e&&t(M),e&&t($),e&&t(S),we(v,e),e&&t(q),e&&t(E),e&&t(O),e&&t(l),e&&t(B),e&&t(P)}}}const Oe={local:"mastering-nlp",title:"Mastering NLP"};function Be(ge){return Ke(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class We extends xe{constructor(m){super();Ae(this,m,Be,qe,Ue,{})}}export{We as default,Oe as metadata};
