import{S as Be,i as Se,s as Re,e as o,k as u,w as ue,t as d,M as Ce,c as l,d as t,m as p,a as i,x as pe,h as c,b as n,G as a,g as s,y as fe,L as Ne,q as he,o as _e,B as ve,v as Ue}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ye}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Fe}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ge}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function He(ge){let f,C,h,_,M,z,Q,P,V,N,$,U,E,Y,v,W,x,X,Z,F,g,ee,w,te,ae,G,I,oe,H,k,le,J,m,L,b,re,ie,B,A,ne,se,S,y,de,ce,R,T,me,K;return z=new Fe({}),$=new Ge({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),E=new Ye({props:{id:"0_4KEb08xrE"}}),{c(){f=o("meta"),C=u(),h=o("h1"),_=o("a"),M=o("span"),ue(z.$$.fragment),Q=u(),P=o("span"),V=d("Modelli sequence-to-sequence"),N=u(),ue($.$$.fragment),U=u(),ue(E.$$.fragment),Y=u(),v=o("p"),W=d("I modelli encoder-decoder (detti anche modelli "),x=o("em"),X=d("sequence-to-sequence"),Z=d(") utilizzano entrambi i componenti dell\u2019architettura Transformer. Ad ogni passaggio, gli attention layer dell\u2019encoder hanno accesso a tutte le parole della frase iniziale, mentre gli attention layer del decoder possono solo accedere alle parole che precedono linearmente una data parola nell\u2019input."),F=u(),g=o("p"),ee=d("Il pre-addestramento di questi modelli pu\xF2 essere fatto utilizzando gli obiettivi dei modelli encoder o decoder, anche se solitamente include un livello di complessit\xE0 maggiore. Ad esempio, "),w=o("a"),te=d("T5"),ae=d(" \xE8 pre-addestrato rimpiazzando porzioni random di testo (che possono contenere pi\xF9 di una parola) con una speciale mask word, con l\u2019obiettivo di predirre il testo rimpiazzato dalla mask word stessa."),G=u(),I=o("p"),oe=d("I modelli sequence-to-sequence sono pi\xF9 adatti ai compiti che hanno a che fare con la generazione di nuove frasi sulla base di un input preciso, come il riassunto, la traduzione, o la generazione di risposte a domande."),H=u(),k=o("p"),le=d("Tra i rappresentanti di questa famiglia di modelli ci sono:"),J=u(),m=o("ul"),L=o("li"),b=o("a"),re=d("BART"),ie=u(),B=o("li"),A=o("a"),ne=d("mBART"),se=u(),S=o("li"),y=o("a"),de=d("Marian"),ce=u(),R=o("li"),T=o("a"),me=d("T5"),this.h()},l(e){const r=Ce('[data-svelte="svelte-1phssyn"]',document.head);f=l(r,"META",{name:!0,content:!0}),r.forEach(t),C=p(e),h=l(e,"H1",{class:!0});var O=i(h);_=l(O,"A",{id:!0,class:!0,href:!0});var qe=i(_);M=l(qe,"SPAN",{});var ze=i(M);pe(z.$$.fragment,ze),ze.forEach(t),qe.forEach(t),Q=p(O),P=l(O,"SPAN",{});var $e=i(P);V=c($e,"Modelli sequence-to-sequence"),$e.forEach(t),O.forEach(t),N=p(e),pe($.$$.fragment,e),U=p(e),pe(E.$$.fragment,e),Y=p(e),v=l(e,"P",{});var j=i(v);W=c(j,"I modelli encoder-decoder (detti anche modelli "),x=l(j,"EM",{});var Ee=i(x);X=c(Ee,"sequence-to-sequence"),Ee.forEach(t),Z=c(j,") utilizzano entrambi i componenti dell\u2019architettura Transformer. Ad ogni passaggio, gli attention layer dell\u2019encoder hanno accesso a tutte le parole della frase iniziale, mentre gli attention layer del decoder possono solo accedere alle parole che precedono linearmente una data parola nell\u2019input."),j.forEach(t),F=p(e),g=l(e,"P",{});var D=i(g);ee=c(D,"Il pre-addestramento di questi modelli pu\xF2 essere fatto utilizzando gli obiettivi dei modelli encoder o decoder, anche se solitamente include un livello di complessit\xE0 maggiore. Ad esempio, "),w=l(D,"A",{href:!0,rel:!0});var we=i(w);te=c(we,"T5"),we.forEach(t),ae=c(D," \xE8 pre-addestrato rimpiazzando porzioni random di testo (che possono contenere pi\xF9 di una parola) con una speciale mask word, con l\u2019obiettivo di predirre il testo rimpiazzato dalla mask word stessa."),D.forEach(t),G=p(e),I=l(e,"P",{});var be=i(I);oe=c(be,"I modelli sequence-to-sequence sono pi\xF9 adatti ai compiti che hanno a che fare con la generazione di nuove frasi sulla base di un input preciso, come il riassunto, la traduzione, o la generazione di risposte a domande."),be.forEach(t),H=p(e),k=l(e,"P",{});var Ae=i(k);le=c(Ae,"Tra i rappresentanti di questa famiglia di modelli ci sono:"),Ae.forEach(t),J=p(e),m=l(e,"UL",{});var q=i(m);L=l(q,"LI",{});var ye=i(L);b=l(ye,"A",{href:!0,rel:!0});var Te=i(b);re=c(Te,"BART"),Te.forEach(t),ye.forEach(t),ie=p(q),B=l(q,"LI",{});var Ie=i(B);A=l(Ie,"A",{href:!0,rel:!0});var ke=i(A);ne=c(ke,"mBART"),ke.forEach(t),Ie.forEach(t),se=p(q),S=l(q,"LI",{});var Me=i(S);y=l(Me,"A",{href:!0,rel:!0});var Pe=i(y);de=c(Pe,"Marian"),Pe.forEach(t),Me.forEach(t),ce=p(q),R=l(q,"LI",{});var xe=i(R);T=l(xe,"A",{href:!0,rel:!0});var Le=i(T);me=c(Le,"T5"),Le.forEach(t),xe.forEach(t),q.forEach(t),this.h()},h(){n(f,"name","hf:doc:metadata"),n(f,"content",JSON.stringify(Je)),n(_,"id","modelli-sequencetosequence"),n(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(_,"href","#modelli-sequencetosequence"),n(h,"class","relative group"),n(w,"href","https://huggingface.co/t5-base"),n(w,"rel","nofollow"),n(b,"href","https://huggingface.co/transformers/model_doc/bart.html"),n(b,"rel","nofollow"),n(A,"href","https://huggingface.co/transformers/model_doc/mbart.html"),n(A,"rel","nofollow"),n(y,"href","https://huggingface.co/transformers/model_doc/marian.html"),n(y,"rel","nofollow"),n(T,"href","https://huggingface.co/transformers/model_doc/t5.html"),n(T,"rel","nofollow")},m(e,r){a(document.head,f),s(e,C,r),s(e,h,r),a(h,_),a(_,M),fe(z,M,null),a(h,Q),a(h,P),a(P,V),s(e,N,r),fe($,e,r),s(e,U,r),fe(E,e,r),s(e,Y,r),s(e,v,r),a(v,W),a(v,x),a(x,X),a(v,Z),s(e,F,r),s(e,g,r),a(g,ee),a(g,w),a(w,te),a(g,ae),s(e,G,r),s(e,I,r),a(I,oe),s(e,H,r),s(e,k,r),a(k,le),s(e,J,r),s(e,m,r),a(m,L),a(L,b),a(b,re),a(m,ie),a(m,B),a(B,A),a(A,ne),a(m,se),a(m,S),a(S,y),a(y,de),a(m,ce),a(m,R),a(R,T),a(T,me),K=!0},p:Ne,i(e){K||(he(z.$$.fragment,e),he($.$$.fragment,e),he(E.$$.fragment,e),K=!0)},o(e){_e(z.$$.fragment,e),_e($.$$.fragment,e),_e(E.$$.fragment,e),K=!1},d(e){t(f),e&&t(C),e&&t(h),ve(z),e&&t(N),ve($,e),e&&t(U),ve(E,e),e&&t(Y),e&&t(v),e&&t(F),e&&t(g),e&&t(G),e&&t(I),e&&t(H),e&&t(k),e&&t(J),e&&t(m)}}}const Je={local:"modelli-sequencetosequence",title:"Modelli sequence-to-sequence"};function Ke(ge){return Ue(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ve extends Be{constructor(f){super();Se(this,f,Ke,He,Re,{})}}export{Ve as default,Je as metadata};
