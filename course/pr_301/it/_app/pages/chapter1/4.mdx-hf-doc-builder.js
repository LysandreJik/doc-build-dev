import{S as Zc,i as Kc,s as eu,e as a,k as u,w as v,t as l,M as iu,c as r,d as t,m as p,a as o,x as h,h as n,b as c,N as m,G as i,g as d,y as g,L as tu,q as _,o as E,B as z,v as au}from"../../chunks/vendor-hf-doc-builder.js";import{Y as As}from"../../chunks/Youtube-hf-doc-builder.js";import{I as S}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ru}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function ou(Ls){let R,za,x,ie,it,qe,oo,tt,lo,ba,Ie,Ta,_i,no,$a,D,te,at,we,so,rt,co,ka,Ei,uo,qa,B,Pe,Ms,po,ye,Ns,Ia,ae,mo,Ae,fo,vo,wa,f,ot,re,lt,ho,go,Le,_o,Eo,zo,nt,oe,st,bo,To,Me,$o,ko,qo,dt,le,ct,Io,wo,Ne,Po,yo,Ao,ut,ne,pt,Lo,Mo,Ge,No,Go,So,mt,k,ft,Ro,xo,Se,Do,Bo,Re,Oo,Co,Uo,vt,q,ht,Qo,Ho,xe,jo,Vo,gt,Yo,Fo,Pa,zi,Xo,ya,I,De,Jo,_t,Wo,Zo,Ko,Be,el,Et,il,tl,al,Oe,rl,zt,ol,ll,Aa,bi,nl,La,O,se,bt,Ce,sl,Tt,dl,Ma,w,cl,$t,ul,pl,kt,ml,fl,Na,de,vl,qt,hl,gl,Ga,P,_l,It,El,zl,wt,bl,Tl,Sa,C,Ue,Gs,$l,Qe,Ss,Ra,ce,kl,Pt,ql,Il,xa,U,He,Rs,wl,je,xs,Da,Q,ue,yt,Ve,Pl,At,yl,Ba,Ti,Al,Oa,Ye,Fe,Ds,Ca,$i,Ll,Ua,H,Xe,Bs,Ml,Je,Os,Qa,We,Ha,ki,Nl,ja,qi,Gl,Va,Ii,Sl,Ya,j,pe,Lt,Ze,Rl,Mt,xl,Fa,Ke,Xa,wi,Dl,Ja,V,ei,Cs,Bl,ii,Us,Wa,Pi,Ol,Za,y,Cl,Nt,Ul,Ql,Gt,Hl,jl,Ka,A,St,Vl,Yl,Rt,Fl,Xl,xt,Jl,er,me,Wl,Dt,Zl,Kl,ir,Y,ti,Qs,en,ai,Hs,tr,yi,tn,ar,Ai,an,rr,F,fe,Bt,ri,rn,Ot,on,or,Li,ln,lr,oi,nr,X,ve,Ct,li,nn,Ut,sn,sr,Mi,dn,dr,he,Ni,Qt,cn,un,pn,Gi,Ht,mn,fn,cr,J,ni,js,vn,si,Vs,ur,Si,hn,pr,L,Ri,jt,gn,_n,En,xi,Vt,zn,bn,Tn,ge,Yt,$n,kn,Ft,qn,In,mr,Di,wn,fr,W,_e,Xt,di,Pn,Jt,yn,vr,M,An,Wt,Ln,Mn,ci,Nn,Gn,hr,Bi,Sn,gr,Oi,Rn,_r,Ci,xn,Er,Z,Ee,Zt,ui,Dn,Kt,Bn,zr,Ui,On,br,Qi,Cn,Tr,Hi,Un,$r,K,pi,Ys,Qn,mi,Fs,kr,ze,Hn,ea,jn,Vn,qr,be,Yn,ia,Fn,Xn,Ir,ee,Te,ta,fi,Jn,aa,Wn,wr,b,Zn,ra,Kn,es,oa,is,ts,la,as,rs,Pr,N,ji,na,os,ls,ns,Vi,sa,ss,ds,cs,G,da,us,ps,ca,ms,fs,ua,vs,hs,yr,T,gs,pa,_s,Es,ma,zs,bs,fa,Ts,$s,Ar;return qe=new S({}),Ie=new ru({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),we=new S({}),Ce=new S({}),Ve=new S({}),We=new As({props:{id:"ftWlj4FBHTg"}}),Ze=new S({}),Ke=new As({props:{id:"BqqfQnyjmgg"}}),ri=new S({}),oi=new As({props:{id:"H39Z_720T5s"}}),li=new S({}),di=new S({}),ui=new S({}),fi=new S({}),{c(){R=a("meta"),za=u(),x=a("h1"),ie=a("a"),it=a("span"),v(qe.$$.fragment),oo=u(),tt=a("span"),lo=l("Come funzionano i Transformer?"),ba=u(),v(Ie.$$.fragment),Ta=u(),_i=a("p"),no=l("In questa sezione, vedremo in maniera approfondita l\u2019architettura dei modelli Transformer."),$a=u(),D=a("h2"),te=a("a"),at=a("span"),v(we.$$.fragment),so=u(),rt=a("span"),co=l("Un po' di storia dei Transformer"),ka=u(),Ei=a("p"),uo=l("Ecco alcuni punti di riferimento nella (breve) storia dei modelli Transformer:"),qa=u(),B=a("div"),Pe=a("img"),po=u(),ye=a("img"),Ia=u(),ae=a("p"),mo=l("L\u2019"),Ae=a("a"),fo=l("architettura Transformer"),vo=l(" \xE8 stata introdotta in giugno 2017. Il focus della ricerca di partenza era sui compiti di traduzione. A questa segu\xEC l\u2019introduzione di numerosi modelli influenti, tra cui figurano:"),wa=u(),f=a("ul"),ot=a("li"),re=a("p"),lt=a("strong"),ho=l("giugno 2018"),go=l(": "),Le=a("a"),_o=l("GPT"),Eo=l(", il primo modello Transformer pre-addestrato, viene usato per affinare diversi compiti di NLP e ottiene risultati all\u2019avanguardia"),zo=u(),nt=a("li"),oe=a("p"),st=a("strong"),bo=l("ottobre 2018"),To=l(": "),Me=a("a"),$o=l("BERT"),ko=l(", un altro ampio modello pre-addestrato, questa volta progettato per produrre riassunti di frasi migliori (ne scopriremo di pi\xF9 nel prossimo capitolo!)"),qo=u(),dt=a("li"),le=a("p"),ct=a("strong"),Io=l("febbraio 2019"),wo=l(": "),Ne=a("a"),Po=l("GPT-2"),yo=l(", una versione (migliorata e ingrandita) di GPT che non fu distribuita immediatamente al pubblico a causa di preoccupazioni etiche"),Ao=u(),ut=a("li"),ne=a("p"),pt=a("strong"),Lo=l("ottobre 2019"),Mo=l(": "),Ge=a("a"),No=l("DistilBERT"),Go=l(", una versione distillata di BERT che \xE8 il 60% pi\xF9 rapida e il 40% pi\xF9 leggera in memoria, pur conservando il 97% della performance di BERT"),So=u(),mt=a("li"),k=a("p"),ft=a("strong"),Ro=l("ottobre 2019"),xo=l(": "),Se=a("a"),Do=l("BART"),Bo=l(" e "),Re=a("a"),Oo=l("T5"),Co=l(", due grossi modelli pre-addestrati che utilizzano la stessa architettura del modello Transformer originale (nonch\xE9 i primi a farlo)"),Uo=u(),vt=a("li"),q=a("p"),ht=a("strong"),Qo=l("maggio 2020"),Ho=l(", "),xe=a("a"),jo=l("GPT-3"),Vo=l(", una versione ancora pi\xF9 ampia di GPT-2, con buone prestazioni in vari compiti e nessun bisogno di fine-tuning (il cosiddetto "),gt=a("em"),Yo=l("zero-shot learning"),Fo=l(")"),Pa=u(),zi=a("p"),Xo=l("La lista \xE8 tutto fuorch\xE9 esaustiva ed \xE8 volta solo a mettere in evidenza alcuni dei diversi tipi di modelli Transformer. In genere, questi possono essere raggruppati in tre categorie:"),ya=u(),I=a("ul"),De=a("li"),Jo=l("Modelli in stile GPT (detti anche modelli Transformer "),_t=a("em"),Wo=l("auto-regressive"),Zo=l(")"),Ko=u(),Be=a("li"),el=l("Modelli in stile BERT (detti anche modelli Transformer "),Et=a("em"),il=l("auto-encoding"),tl=l(")"),al=u(),Oe=a("li"),rl=l("Modelli in stile BART/T5 (detti anche modelli Transformer "),zt=a("em"),ol=l("sequence-to-sequence"),ll=l(")"),Aa=u(),bi=a("p"),nl=l("Studieremo queste famiglie pi\xF9 nel dettaglio in seguito."),La=u(),O=a("h2"),se=a("a"),bt=a("span"),v(Ce.$$.fragment),sl=u(),Tt=a("span"),dl=l("I Transformer sono modelli linguistici"),Ma=u(),w=a("p"),cl=l("Tutti i modelli Transformer menzionati qui sopra (GPT, BERT, BART, T5, ecc.) sono stati addestrati come modelli linguistici ("),$t=a("em"),ul=l("language models"),pl=l("). Ci\xF2 significa che sono stati addestrati su grandi quantit\xE0 di testo grezzo in stile auto-supervisionato ("),kt=a("em"),ml=l("self-supervising"),fl=l("). L\u2019apprendimento auto-supervisionato \xE8 un tipo di apprendimento il cui obbiettivo viene computato direttamente dagli input del modello. Ci\xF2 significa che non \xE8 richiesto alcun intervento umano per etichettare i dati!"),Na=u(),de=a("p"),vl=l("Un modello di questo tipo sviluppa una comprensione statistica della lingua alla quale \xE8 stato addestrato, ma non \xE8 molto utile in compiti pratici e precisi. Per questa ragione, il modello pre-addestrato generale viene in seguito sottoposto a un processo detto "),qt=a("em"),hl=l("transfer learning"),gl=l(". Durante questo processo, il modello viene affinato per un determinato compito in maniera supervisionata (ossia utilizzando etichette generate da umani)."),Ga=u(),P=a("p"),_l=l("Un esempio di compito \xE8 la previsione della parola seguente in una frase di cui sono state lette "),It=a("em"),El=l("n"),zl=l(" parole precedenti. Quest\u2019operazione si chiama "),wt=a("em"),bl=l("causal language modeling"),Tl=l(" perch\xE9 il suo output dipende dagli input presenti e passati, ma non da quelli futuri."),Sa=u(),C=a("div"),Ue=a("img"),$l=u(),Qe=a("img"),Ra=u(),ce=a("p"),kl=l("Un altro esempio \xE8 il "),Pt=a("em"),ql=l("masked language modeling"),Il=l(", in cui il modello prevede una parola occultata della frase."),xa=u(),U=a("div"),He=a("img"),wl=u(),je=a("img"),Da=u(),Q=a("h2"),ue=a("a"),yt=a("span"),v(Ve.$$.fragment),Pl=u(),At=a("span"),yl=l("I Transformers sono modelli enormi"),Ba=u(),Ti=a("p"),Al=l("A parte per alcune eccezioni (come DistilBERT), la strategia generale per ottenere performance migliori consiste nell\u2019aumentare la taglia dei modelli, nonch\xE9 la quantit\xE0 di dati utilizzati per il pre-addestramento."),Oa=u(),Ye=a("div"),Fe=a("img"),Ca=u(),$i=a("p"),Ll=l("Sfortunatamente, l\u2019addestramento di un modello, e specialmente di un modello grosso, richiede grandi quantit\xE0 di dati. Ci\xF2 si rivela molto costoso in termini di tempo, risorse informatiche e impatto ambientale, come mostrano i grafici qui sotto."),Ua=u(),H=a("div"),Xe=a("img"),Ml=u(),Je=a("img"),Qa=u(),v(We.$$.fragment),Ha=u(),ki=a("p"),Nl=l("Questi dati si riferiscono a un progetto per un modello (molto grande) condotto da un team che provava consciamente a ridurre l\u2019impatto ambientale del pre-addestramento. L\u2019impronta di trials volti a ottenere i miglior iperparamenti possibili sarebbe ancora pi\xF9 importante."),ja=u(),qi=a("p"),Gl=l("Immagina cosa succederebbe se ogni volta che un gruppo di ricerca, un\u2019organizzazione studentesca o un\u2019azienda vuole addestrare un modello lo facesse da zero! I costi globali sarebbero inutilmente enormi!"),Va=u(),Ii=a("p"),Sl=l("Questo \xE8 il motivo per cui la condivisione di modelli linguistici \xE8 fondamentale: lavorare a partire da modelli gi\xE0 addestrati riduce i costi informatici complessivi e l\u2019impatto ambientale della comunit\xE0."),Ya=u(),j=a("h2"),pe=a("a"),Lt=a("span"),v(Ze.$$.fragment),Rl=u(),Mt=a("span"),xl=l("Transfer Learning"),Fa=u(),v(Ke.$$.fragment),Xa=u(),wi=a("p"),Dl=l("Il pre-addestramento \xE8 l\u2019atto di addestrare un modello da zero: i pesi sono inizializzati in maniera casuale, e l\u2019addestramento inizia senza alcuna conoscenza pregressa."),Ja=u(),V=a("div"),ei=a("img"),Bl=u(),ii=a("img"),Wa=u(),Pi=a("p"),Ol=l("Questo pre-addestramento \xE8 solitamente fatto su enormi quantit\xE0 di dati. Di conseguenza, l\u2019addestramento richiede un corpus di dati molto ampio e pu\xF2 prendere diverse settimane."),Za=u(),y=a("p"),Cl=l("L\u2019affinamento ("),Nt=a("em"),Ul=l("fine-tuning"),Ql=l("), al contrario, \xE8 un addestramento che ha luogo "),Gt=a("strong"),Hl=l("dopo"),jl=l(" che il modello \xE8 stato pre-addestrato. Per poter effettuare un fine-tuning, \xE8 necessario acquisire un modello linguistico pre-addestrato e addestrarlo ulteriormente con una base dati adatta al compito in questione. Ma perch\xE9 non addestrare direttamente al compito finale? Esistono alcune ragioni:"),Ka=u(),A=a("ul"),St=a("li"),Vl=l("Il modello pre-addestrato \xE8 gi\xE0 addestrato su basi dati che contengono similarit\xE0 con la base dati usata per il fine-tuning. Il processo di fine-tuning riesce quindi ad beneficiare della conoscenza acquisita dal modello iniziale durante il pre-addestramento (ad esempio, nei problemi di NLP, il modello pre-addestrato avr\xE0 gi\xE0 conoscenze statistiche della lingua utilizzata nel compito)."),Yl=u(),Rt=a("li"),Fl=l("Siccome il modello pre-addestrato \xE8 stato addestrato usando moltissimi dati, il fine-tuning richiede molto meno dati per ottenere buoni risultati."),Xl=u(),xt=a("li"),Jl=l("Per la stessa ragione, occorrono molto meno tempo e risorse per ottenere buoni risultati."),er=u(),me=a("p"),Wl=l("Ad esempio, \xE8 possibile approfittare di un modello pre-addestrato per la lingua inglese e poi affinarlo usando un corpus arXiv, ottenendo cos\xEC un modello specifico per la scienza/ricerca. L\u2019affinamento non richieder\xE0 che una quantit\xE0 limitata di dati: le conoscenze acquisite dal modello pre-addestrato sono \u201Ctrasferite\u201D, come riflette il nome "),Dt=a("em"),Zl=l("transfer learning"),Kl=l("."),ir=u(),Y=a("div"),ti=a("img"),en=u(),ai=a("img"),tr=u(),yi=a("p"),tn=l("Il fine-tuning di un modello ha quindi costi ridotti in termini di dati, finanze e impatto ambientale. Iterare su diversi schemi di fine-tuning \xE8 anche pi\xF9 rapido e semplice, in quanto l\u2019addestramento \xE8 meno restrittivo di un pre-addestramento completo."),ar=u(),Ai=a("p"),an=l("Questo processo permette anche di ottenere risultati migliori di un addestramento da zero (a meno di non essere in possesso di moltissimi dati), motivo per cui bisognerebbe sempre partire da un modello pre-addestrato (quanto possibile compatibile con il compito da eseguire) e affinarlo."),rr=u(),F=a("h2"),fe=a("a"),Bt=a("span"),v(ri.$$.fragment),rn=u(),Ot=a("span"),on=l("Architettura generale"),or=u(),Li=a("p"),ln=l("In questa sezione, vedremo l\u2019architettura generale del modello Transformer. Non preoccuparti se non capisci tutti i concetti: pi\xF9 avanti, troverai sezioni dettagliate per ogni componente."),lr=u(),v(oi.$$.fragment),nr=u(),X=a("h2"),ve=a("a"),Ct=a("span"),v(li.$$.fragment),nn=u(),Ut=a("span"),sn=l("Introduzione"),sr=u(),Mi=a("p"),dn=l("Il modello si compone principalmente di due blocchi:"),dr=u(),he=a("ul"),Ni=a("li"),Qt=a("strong"),cn=l("Encoder (sinistra)"),un=l(": L\u2019encoder riceve un input e ne costruisce una rappresentazione, le features. Ci\xF2 significa che il modello \xE8 ottimizzato per la comprensione dell\u2019input."),pn=u(),Gi=a("li"),Ht=a("strong"),mn=l("Decoder (destra)"),fn=l(": Il decoder utilizza la rappresentazione dell\u2019encoder (le features) assieme ad ulteriori input per generare la sequenza target. Ci\xF2 significa che il modello \xE8 ottimizzato per la generazione di output."),cr=u(),J=a("div"),ni=a("img"),vn=u(),si=a("img"),ur=u(),Si=a("p"),hn=l("Ognuna di queste parti pu\xF2 essere utilizzata indipendentemente, in base al compito:"),pr=u(),L=a("ul"),Ri=a("li"),jt=a("strong"),gn=l("Modelli Encoder-only"),_n=l(": Ottimi per compiti che richiedono una comprensione dell\u2019input, come la classificazione frasale e il riconoscimento delle entit\xE0 nominate."),En=u(),xi=a("li"),Vt=a("strong"),zn=l("Modelli Decoder-only"),bn=l(": Ottimi per compiti generativi come la generazione testuale."),Tn=u(),ge=a("li"),Yt=a("strong"),$n=l("Modelli Encoder-decoder"),kn=l(" o "),Ft=a("strong"),qn=l("modelli sequence-to-sequence"),In=l(": Ottimi per compiti generativi che richiedono un input, come la traduzione o il riassunto."),mr=u(),Di=a("p"),wn=l("Analizzeremo ciascuna di queste architetture indipendentemente pi\xF9 tardi nel corso."),fr=u(),W=a("h2"),_e=a("a"),Xt=a("span"),v(di.$$.fragment),Pn=u(),Jt=a("span"),yn=l("Attention layers"),vr=u(),M=a("p"),An=l("Una caratteristica chiave dei modelli Transformer \xE8 che sono basati su strati speciali detti "),Wt=a("em"),Ln=l("attention layers"),Mn=l(". Non a caso, il titolo del paper che introdusse l\u2019architettura Transformer era "),ci=a("a"),Nn=l("\u201CAttention Is All You Need\u201D"),Gn=l("! Esploreremo gli attention layer nel dettaglio pi\xF9 avanti in questo corso; per ora, tutto ci\xF2 che hai bisogno di sapere \xE8 che un layer dir\xE0 al modello di prestare particolare attenzione a certe parole nella frase input (ignorando praticamente le altre) quando si occupa della rappresentazione delle singole parole."),hr=u(),Bi=a("p"),Sn=l("Come esempio concreto, pensa ad un compito di traduzione testuale dall\u2019inglese al francese. Dato l\u2019input \u201CYou like this course\u201D, un modello di traduzione dovr\xE0 fare riferimento alla parola adiacente \u201CYou\u201D per fornire la traduzione corretta della parola \u201Clike\u201D, perch\xE9 in francese la coniugazione del verbo \u201Clike\u201D cambia in base al soggetto. Diversamente, il resto della frase non \xE8 utile alla sua traduzione di quella precisa parola. In maniera simile, durante la traduzione di \u201Cthis\u201D il modello dovr\xE0 prestare attenzione alla parola \u201Ccourse\u201D, in quanto \u201Cthis\u201D ha traduzioni diverse se associato con nomi femminili o maschili. Di nuovo, il resto delle parole della frase non contribuiscono alla corretta traduzione di \u201Cthis\u201D. Con frasi pi\xF9 complesse (e regole grammaticali pi\xF9 complesse), il modello potrebbe aver bisogno di prestare particolare attenzione a parole ben pi\xF9 lontane nella frase per tradurre correttamente ogni parola."),gr=u(),Oi=a("p"),Rn=l("Lo stesso concetto si applica a qualsiasi compito che ha a che fare con il linguaggio naturale: una parola ha un senso a s\xE9 stante, ma tale senso \xE8 profondamente influenzato dal contesto, il quale \xE8 costituito da una qualsiasi parola (o parole) che precede o segue la parola sotto osservazione."),_r=u(),Ci=a("p"),xn=l("Ora che sai cosa sono gli attention layer, guardiamo un po\u2019 pi\xF9 nel dettaglio all\u2019architettura Transformer."),Er=u(),Z=a("h2"),Ee=a("a"),Zt=a("span"),v(ui.$$.fragment),Dn=u(),Kt=a("span"),Bn=l("L'architettura originale"),zr=u(),Ui=a("p"),On=l("All\u2019origine, l\u2019architettura Transformer fu creata per la traduzione. In fase di addestramento, l\u2019encoder riceve degli input (frasi) in una certa lingua, mentre il decoder riceve le stesse frasi nella lingua target d\u2019elezione. Nell\u2019encoder, gli attention layer sono in grado di utilizzare qualsiasi parola in una data frase (dato che, come abbiamo appena visto, la traduzione di una determinata parola pu\xF2 dipendere da ci\xF2 che la precede o segue nella frase). Diversamente, decoder procede in maniera sequenziale ed \xE8 capace di prestare attenzione solo alle parole della frase che ha gi\xE0 tradotto (ossia, solo le parole che precedono la parola che sta generando). Ad esempio, una volta predette le prime tre parole della frase target, le passiamo al decoder  che utilizza tutti gli input dell\u2019encoder per provare a predirre la quarta parola."),br=u(),Qi=a("p"),Cn=l("Per accelerare il processo di addestramento (quando il modello ha accesso alle frasi target), l\u2019intero target viene fornito al decoder, che per\xF2 non \xE8 in grado di accedere alle parole future (se avesse accesso alla parola in seconda posizione mentre cerca di predirre la parola in seconda posizione, il problema cesserebbe di essere complesso). Ad esempio, mentre prova a predirre la quarta parola, l\u2019attention layer avr\xE0 accesso solo alle posizioni tra la prima e la terza."),Tr=u(),Hi=a("p"),Un=l("L\u2019architettura Transformer originale aveva la struttura qui sotto, con l\u2019encoder a sinistra e il decoder a destra:"),$r=u(),K=a("div"),pi=a("img"),Qn=u(),mi=a("img"),kr=u(),ze=a("p"),Hn=l("Nota che il primo attention layer in un "),ea=a("em"),jn=l("decoder block"),Vn=l(" presta attenzione a tutti gli input (passati) al decoder, mentre il secondo attention layer utilizza l\u2019output del encoder. Gli \xE8 perci\xF2 possibile avere accesso a tutta la frase input per meglio prevedere la parola corrente. Questa caratteristica \xE8 molto utile in quanto lingue diverse possono avere regole grammaticali diverse piazzano le parole in ordini diversi, oppure perch\xE9 il contesto che compare pi\xF9 tardi nella frase potrebbe essere utile nella determinazione della migliore traduzione di una data parola."),qr=u(),be=a("p"),Yn=l("L\u2019"),ia=a("em"),Fn=l("attention mask"),Xn=l(" pu\xF2 essere utilizzato anche nell\u2019encoder/decoder per evitare che il modello presti attenzione a certe parole speciali, come ad esempio parole riempitive utilizzate per rendere tutti gli input della stessa lunghezza."),Ir=u(),ee=a("h2"),Te=a("a"),ta=a("span"),v(fi.$$.fragment),Jn=u(),aa=a("span"),Wn=l("Architetture vs. checkpoint"),wr=u(),b=a("p"),Zn=l("Durante questo viaggio nel mondo dei modelli Transformer, incontrerai menzioni di "),ra=a("em"),Kn=l("architetture"),es=l(" e "),oa=a("em"),is=l("checkpoint"),ts=l(", nonch\xE9 di "),la=a("em"),as=l("modelli"),rs=l(". Questi termini hanno significati leggermente diversi:"),Pr=u(),N=a("ul"),ji=a("li"),na=a("strong"),os=l("Architettura"),ls=l(": Lo scheletro del modello, ossia la definizione di ogni livello e operazione che compare nel modello."),ns=u(),Vi=a("li"),sa=a("strong"),ss=l("Checkpoint"),ds=l(": I pesi che verranno caricati in una determinata architettura."),cs=u(),G=a("li"),da=a("strong"),us=l("Modello"),ps=l(": Un termine generico meno preciso di \u201Carchitettura\u201D o \u201Ccheckpoint\u201D, in quanto pu\xF2 significare entrambi. In questo corso faremo la distinzione tra "),ca=a("em"),ms=l("architettura"),fs=l(" e "),ua=a("em"),vs=l("checkpoint"),hs=l(" quando sar\xE0 necessario ridurre le ambiguit\xE0."),yr=u(),T=a("p"),gs=l("Ad esempio, BERT \xE8 un\u2019architettura, mentre "),pa=a("code"),_s=l("bert-base-cased"),Es=l(", un set di pesi ("),ma=a("em"),zs=l("weights"),bs=l(") addestrati dal team di Google per la prima versione di BERT, \xE8 un checkpoint. Ciononostante, \xE8 possibile dire \u201Cil modello BERT\u201D e \u201Cil modello "),fa=a("code"),Ts=l("bert-base-cased"),$s=l(".\u201D"),this.h()},l(e){const s=iu('[data-svelte="svelte-1phssyn"]',document.head);R=r(s,"META",{name:!0,content:!0}),s.forEach(t),za=p(e),x=r(e,"H1",{class:!0});var Lr=o(x);ie=r(Lr,"A",{id:!0,class:!0,href:!0});var Xs=o(ie);it=r(Xs,"SPAN",{});var Js=o(it);h(qe.$$.fragment,Js),Js.forEach(t),Xs.forEach(t),oo=p(Lr),tt=r(Lr,"SPAN",{});var Ws=o(tt);lo=n(Ws,"Come funzionano i Transformer?"),Ws.forEach(t),Lr.forEach(t),ba=p(e),h(Ie.$$.fragment,e),Ta=p(e),_i=r(e,"P",{});var Zs=o(_i);no=n(Zs,"In questa sezione, vedremo in maniera approfondita l\u2019architettura dei modelli Transformer."),Zs.forEach(t),$a=p(e),D=r(e,"H2",{class:!0});var Mr=o(D);te=r(Mr,"A",{id:!0,class:!0,href:!0});var Ks=o(te);at=r(Ks,"SPAN",{});var ed=o(at);h(we.$$.fragment,ed),ed.forEach(t),Ks.forEach(t),so=p(Mr),rt=r(Mr,"SPAN",{});var id=o(rt);co=n(id,"Un po' di storia dei Transformer"),id.forEach(t),Mr.forEach(t),ka=p(e),Ei=r(e,"P",{});var td=o(Ei);uo=n(td,"Ecco alcuni punti di riferimento nella (breve) storia dei modelli Transformer:"),td.forEach(t),qa=p(e),B=r(e,"DIV",{class:!0});var Nr=o(B);Pe=r(Nr,"IMG",{class:!0,src:!0,alt:!0}),po=p(Nr),ye=r(Nr,"IMG",{class:!0,src:!0,alt:!0}),Nr.forEach(t),Ia=p(e),ae=r(e,"P",{});var Gr=o(ae);mo=n(Gr,"L\u2019"),Ae=r(Gr,"A",{href:!0,rel:!0});var ad=o(Ae);fo=n(ad,"architettura Transformer"),ad.forEach(t),vo=n(Gr," \xE8 stata introdotta in giugno 2017. Il focus della ricerca di partenza era sui compiti di traduzione. A questa segu\xEC l\u2019introduzione di numerosi modelli influenti, tra cui figurano:"),Gr.forEach(t),wa=p(e),f=r(e,"UL",{});var $=o(f);ot=r($,"LI",{});var rd=o(ot);re=r(rd,"P",{});var va=o(re);lt=r(va,"STRONG",{});var od=o(lt);ho=n(od,"giugno 2018"),od.forEach(t),go=n(va,": "),Le=r(va,"A",{href:!0,rel:!0});var ld=o(Le);_o=n(ld,"GPT"),ld.forEach(t),Eo=n(va,", il primo modello Transformer pre-addestrato, viene usato per affinare diversi compiti di NLP e ottiene risultati all\u2019avanguardia"),va.forEach(t),rd.forEach(t),zo=p($),nt=r($,"LI",{});var nd=o(nt);oe=r(nd,"P",{});var ha=o(oe);st=r(ha,"STRONG",{});var sd=o(st);bo=n(sd,"ottobre 2018"),sd.forEach(t),To=n(ha,": "),Me=r(ha,"A",{href:!0,rel:!0});var dd=o(Me);$o=n(dd,"BERT"),dd.forEach(t),ko=n(ha,", un altro ampio modello pre-addestrato, questa volta progettato per produrre riassunti di frasi migliori (ne scopriremo di pi\xF9 nel prossimo capitolo!)"),ha.forEach(t),nd.forEach(t),qo=p($),dt=r($,"LI",{});var cd=o(dt);le=r(cd,"P",{});var ga=o(le);ct=r(ga,"STRONG",{});var ud=o(ct);Io=n(ud,"febbraio 2019"),ud.forEach(t),wo=n(ga,": "),Ne=r(ga,"A",{href:!0,rel:!0});var pd=o(Ne);Po=n(pd,"GPT-2"),pd.forEach(t),yo=n(ga,", una versione (migliorata e ingrandita) di GPT che non fu distribuita immediatamente al pubblico a causa di preoccupazioni etiche"),ga.forEach(t),cd.forEach(t),Ao=p($),ut=r($,"LI",{});var md=o(ut);ne=r(md,"P",{});var _a=o(ne);pt=r(_a,"STRONG",{});var fd=o(pt);Lo=n(fd,"ottobre 2019"),fd.forEach(t),Mo=n(_a,": "),Ge=r(_a,"A",{href:!0,rel:!0});var vd=o(Ge);No=n(vd,"DistilBERT"),vd.forEach(t),Go=n(_a,", una versione distillata di BERT che \xE8 il 60% pi\xF9 rapida e il 40% pi\xF9 leggera in memoria, pur conservando il 97% della performance di BERT"),_a.forEach(t),md.forEach(t),So=p($),mt=r($,"LI",{});var hd=o(mt);k=r(hd,"P",{});var vi=o(k);ft=r(vi,"STRONG",{});var gd=o(ft);Ro=n(gd,"ottobre 2019"),gd.forEach(t),xo=n(vi,": "),Se=r(vi,"A",{href:!0,rel:!0});var _d=o(Se);Do=n(_d,"BART"),_d.forEach(t),Bo=n(vi," e "),Re=r(vi,"A",{href:!0,rel:!0});var Ed=o(Re);Oo=n(Ed,"T5"),Ed.forEach(t),Co=n(vi,", due grossi modelli pre-addestrati che utilizzano la stessa architettura del modello Transformer originale (nonch\xE9 i primi a farlo)"),vi.forEach(t),hd.forEach(t),Uo=p($),vt=r($,"LI",{});var zd=o(vt);q=r(zd,"P",{});var hi=o(q);ht=r(hi,"STRONG",{});var bd=o(ht);Qo=n(bd,"maggio 2020"),bd.forEach(t),Ho=n(hi,", "),xe=r(hi,"A",{href:!0,rel:!0});var Td=o(xe);jo=n(Td,"GPT-3"),Td.forEach(t),Vo=n(hi,", una versione ancora pi\xF9 ampia di GPT-2, con buone prestazioni in vari compiti e nessun bisogno di fine-tuning (il cosiddetto "),gt=r(hi,"EM",{});var $d=o(gt);Yo=n($d,"zero-shot learning"),$d.forEach(t),Fo=n(hi,")"),hi.forEach(t),zd.forEach(t),$.forEach(t),Pa=p(e),zi=r(e,"P",{});var kd=o(zi);Xo=n(kd,"La lista \xE8 tutto fuorch\xE9 esaustiva ed \xE8 volta solo a mettere in evidenza alcuni dei diversi tipi di modelli Transformer. In genere, questi possono essere raggruppati in tre categorie:"),kd.forEach(t),ya=p(e),I=r(e,"UL",{});var Yi=o(I);De=r(Yi,"LI",{});var Sr=o(De);Jo=n(Sr,"Modelli in stile GPT (detti anche modelli Transformer "),_t=r(Sr,"EM",{});var qd=o(_t);Wo=n(qd,"auto-regressive"),qd.forEach(t),Zo=n(Sr,")"),Sr.forEach(t),Ko=p(Yi),Be=r(Yi,"LI",{});var Rr=o(Be);el=n(Rr,"Modelli in stile BERT (detti anche modelli Transformer "),Et=r(Rr,"EM",{});var Id=o(Et);il=n(Id,"auto-encoding"),Id.forEach(t),tl=n(Rr,")"),Rr.forEach(t),al=p(Yi),Oe=r(Yi,"LI",{});var xr=o(Oe);rl=n(xr,"Modelli in stile BART/T5 (detti anche modelli Transformer "),zt=r(xr,"EM",{});var wd=o(zt);ol=n(wd,"sequence-to-sequence"),wd.forEach(t),ll=n(xr,")"),xr.forEach(t),Yi.forEach(t),Aa=p(e),bi=r(e,"P",{});var Pd=o(bi);nl=n(Pd,"Studieremo queste famiglie pi\xF9 nel dettaglio in seguito."),Pd.forEach(t),La=p(e),O=r(e,"H2",{class:!0});var Dr=o(O);se=r(Dr,"A",{id:!0,class:!0,href:!0});var yd=o(se);bt=r(yd,"SPAN",{});var Ad=o(bt);h(Ce.$$.fragment,Ad),Ad.forEach(t),yd.forEach(t),sl=p(Dr),Tt=r(Dr,"SPAN",{});var Ld=o(Tt);dl=n(Ld,"I Transformer sono modelli linguistici"),Ld.forEach(t),Dr.forEach(t),Ma=p(e),w=r(e,"P",{});var Fi=o(w);cl=n(Fi,"Tutti i modelli Transformer menzionati qui sopra (GPT, BERT, BART, T5, ecc.) sono stati addestrati come modelli linguistici ("),$t=r(Fi,"EM",{});var Md=o($t);ul=n(Md,"language models"),Md.forEach(t),pl=n(Fi,"). Ci\xF2 significa che sono stati addestrati su grandi quantit\xE0 di testo grezzo in stile auto-supervisionato ("),kt=r(Fi,"EM",{});var Nd=o(kt);ml=n(Nd,"self-supervising"),Nd.forEach(t),fl=n(Fi,"). L\u2019apprendimento auto-supervisionato \xE8 un tipo di apprendimento il cui obbiettivo viene computato direttamente dagli input del modello. Ci\xF2 significa che non \xE8 richiesto alcun intervento umano per etichettare i dati!"),Fi.forEach(t),Na=p(e),de=r(e,"P",{});var Br=o(de);vl=n(Br,"Un modello di questo tipo sviluppa una comprensione statistica della lingua alla quale \xE8 stato addestrato, ma non \xE8 molto utile in compiti pratici e precisi. Per questa ragione, il modello pre-addestrato generale viene in seguito sottoposto a un processo detto "),qt=r(Br,"EM",{});var Gd=o(qt);hl=n(Gd,"transfer learning"),Gd.forEach(t),gl=n(Br,". Durante questo processo, il modello viene affinato per un determinato compito in maniera supervisionata (ossia utilizzando etichette generate da umani)."),Br.forEach(t),Ga=p(e),P=r(e,"P",{});var Xi=o(P);_l=n(Xi,"Un esempio di compito \xE8 la previsione della parola seguente in una frase di cui sono state lette "),It=r(Xi,"EM",{});var Sd=o(It);El=n(Sd,"n"),Sd.forEach(t),zl=n(Xi," parole precedenti. Quest\u2019operazione si chiama "),wt=r(Xi,"EM",{});var Rd=o(wt);bl=n(Rd,"causal language modeling"),Rd.forEach(t),Tl=n(Xi," perch\xE9 il suo output dipende dagli input presenti e passati, ma non da quelli futuri."),Xi.forEach(t),Sa=p(e),C=r(e,"DIV",{class:!0});var Or=o(C);Ue=r(Or,"IMG",{class:!0,src:!0,alt:!0}),$l=p(Or),Qe=r(Or,"IMG",{class:!0,src:!0,alt:!0}),Or.forEach(t),Ra=p(e),ce=r(e,"P",{});var Cr=o(ce);kl=n(Cr,"Un altro esempio \xE8 il "),Pt=r(Cr,"EM",{});var xd=o(Pt);ql=n(xd,"masked language modeling"),xd.forEach(t),Il=n(Cr,", in cui il modello prevede una parola occultata della frase."),Cr.forEach(t),xa=p(e),U=r(e,"DIV",{class:!0});var Ur=o(U);He=r(Ur,"IMG",{class:!0,src:!0,alt:!0}),wl=p(Ur),je=r(Ur,"IMG",{class:!0,src:!0,alt:!0}),Ur.forEach(t),Da=p(e),Q=r(e,"H2",{class:!0});var Qr=o(Q);ue=r(Qr,"A",{id:!0,class:!0,href:!0});var Dd=o(ue);yt=r(Dd,"SPAN",{});var Bd=o(yt);h(Ve.$$.fragment,Bd),Bd.forEach(t),Dd.forEach(t),Pl=p(Qr),At=r(Qr,"SPAN",{});var Od=o(At);yl=n(Od,"I Transformers sono modelli enormi"),Od.forEach(t),Qr.forEach(t),Ba=p(e),Ti=r(e,"P",{});var Cd=o(Ti);Al=n(Cd,"A parte per alcune eccezioni (come DistilBERT), la strategia generale per ottenere performance migliori consiste nell\u2019aumentare la taglia dei modelli, nonch\xE9 la quantit\xE0 di dati utilizzati per il pre-addestramento."),Cd.forEach(t),Oa=p(e),Ye=r(e,"DIV",{class:!0});var Ud=o(Ye);Fe=r(Ud,"IMG",{src:!0,alt:!0,width:!0}),Ud.forEach(t),Ca=p(e),$i=r(e,"P",{});var Qd=o($i);Ll=n(Qd,"Sfortunatamente, l\u2019addestramento di un modello, e specialmente di un modello grosso, richiede grandi quantit\xE0 di dati. Ci\xF2 si rivela molto costoso in termini di tempo, risorse informatiche e impatto ambientale, come mostrano i grafici qui sotto."),Qd.forEach(t),Ua=p(e),H=r(e,"DIV",{class:!0});var Hr=o(H);Xe=r(Hr,"IMG",{class:!0,src:!0,alt:!0}),Ml=p(Hr),Je=r(Hr,"IMG",{class:!0,src:!0,alt:!0}),Hr.forEach(t),Qa=p(e),h(We.$$.fragment,e),Ha=p(e),ki=r(e,"P",{});var Hd=o(ki);Nl=n(Hd,"Questi dati si riferiscono a un progetto per un modello (molto grande) condotto da un team che provava consciamente a ridurre l\u2019impatto ambientale del pre-addestramento. L\u2019impronta di trials volti a ottenere i miglior iperparamenti possibili sarebbe ancora pi\xF9 importante."),Hd.forEach(t),ja=p(e),qi=r(e,"P",{});var jd=o(qi);Gl=n(jd,"Immagina cosa succederebbe se ogni volta che un gruppo di ricerca, un\u2019organizzazione studentesca o un\u2019azienda vuole addestrare un modello lo facesse da zero! I costi globali sarebbero inutilmente enormi!"),jd.forEach(t),Va=p(e),Ii=r(e,"P",{});var Vd=o(Ii);Sl=n(Vd,"Questo \xE8 il motivo per cui la condivisione di modelli linguistici \xE8 fondamentale: lavorare a partire da modelli gi\xE0 addestrati riduce i costi informatici complessivi e l\u2019impatto ambientale della comunit\xE0."),Vd.forEach(t),Ya=p(e),j=r(e,"H2",{class:!0});var jr=o(j);pe=r(jr,"A",{id:!0,class:!0,href:!0});var Yd=o(pe);Lt=r(Yd,"SPAN",{});var Fd=o(Lt);h(Ze.$$.fragment,Fd),Fd.forEach(t),Yd.forEach(t),Rl=p(jr),Mt=r(jr,"SPAN",{});var Xd=o(Mt);xl=n(Xd,"Transfer Learning"),Xd.forEach(t),jr.forEach(t),Fa=p(e),h(Ke.$$.fragment,e),Xa=p(e),wi=r(e,"P",{});var Jd=o(wi);Dl=n(Jd,"Il pre-addestramento \xE8 l\u2019atto di addestrare un modello da zero: i pesi sono inizializzati in maniera casuale, e l\u2019addestramento inizia senza alcuna conoscenza pregressa."),Jd.forEach(t),Ja=p(e),V=r(e,"DIV",{class:!0});var Vr=o(V);ei=r(Vr,"IMG",{class:!0,src:!0,alt:!0}),Bl=p(Vr),ii=r(Vr,"IMG",{class:!0,src:!0,alt:!0}),Vr.forEach(t),Wa=p(e),Pi=r(e,"P",{});var Wd=o(Pi);Ol=n(Wd,"Questo pre-addestramento \xE8 solitamente fatto su enormi quantit\xE0 di dati. Di conseguenza, l\u2019addestramento richiede un corpus di dati molto ampio e pu\xF2 prendere diverse settimane."),Wd.forEach(t),Za=p(e),y=r(e,"P",{});var Ji=o(y);Cl=n(Ji,"L\u2019affinamento ("),Nt=r(Ji,"EM",{});var Zd=o(Nt);Ul=n(Zd,"fine-tuning"),Zd.forEach(t),Ql=n(Ji,"), al contrario, \xE8 un addestramento che ha luogo "),Gt=r(Ji,"STRONG",{});var Kd=o(Gt);Hl=n(Kd,"dopo"),Kd.forEach(t),jl=n(Ji," che il modello \xE8 stato pre-addestrato. Per poter effettuare un fine-tuning, \xE8 necessario acquisire un modello linguistico pre-addestrato e addestrarlo ulteriormente con una base dati adatta al compito in questione. Ma perch\xE9 non addestrare direttamente al compito finale? Esistono alcune ragioni:"),Ji.forEach(t),Ka=p(e),A=r(e,"UL",{});var Wi=o(A);St=r(Wi,"LI",{});var ec=o(St);Vl=n(ec,"Il modello pre-addestrato \xE8 gi\xE0 addestrato su basi dati che contengono similarit\xE0 con la base dati usata per il fine-tuning. Il processo di fine-tuning riesce quindi ad beneficiare della conoscenza acquisita dal modello iniziale durante il pre-addestramento (ad esempio, nei problemi di NLP, il modello pre-addestrato avr\xE0 gi\xE0 conoscenze statistiche della lingua utilizzata nel compito)."),ec.forEach(t),Yl=p(Wi),Rt=r(Wi,"LI",{});var ic=o(Rt);Fl=n(ic,"Siccome il modello pre-addestrato \xE8 stato addestrato usando moltissimi dati, il fine-tuning richiede molto meno dati per ottenere buoni risultati."),ic.forEach(t),Xl=p(Wi),xt=r(Wi,"LI",{});var tc=o(xt);Jl=n(tc,"Per la stessa ragione, occorrono molto meno tempo e risorse per ottenere buoni risultati."),tc.forEach(t),Wi.forEach(t),er=p(e),me=r(e,"P",{});var Yr=o(me);Wl=n(Yr,"Ad esempio, \xE8 possibile approfittare di un modello pre-addestrato per la lingua inglese e poi affinarlo usando un corpus arXiv, ottenendo cos\xEC un modello specifico per la scienza/ricerca. L\u2019affinamento non richieder\xE0 che una quantit\xE0 limitata di dati: le conoscenze acquisite dal modello pre-addestrato sono \u201Ctrasferite\u201D, come riflette il nome "),Dt=r(Yr,"EM",{});var ac=o(Dt);Zl=n(ac,"transfer learning"),ac.forEach(t),Kl=n(Yr,"."),Yr.forEach(t),ir=p(e),Y=r(e,"DIV",{class:!0});var Fr=o(Y);ti=r(Fr,"IMG",{class:!0,src:!0,alt:!0}),en=p(Fr),ai=r(Fr,"IMG",{class:!0,src:!0,alt:!0}),Fr.forEach(t),tr=p(e),yi=r(e,"P",{});var rc=o(yi);tn=n(rc,"Il fine-tuning di un modello ha quindi costi ridotti in termini di dati, finanze e impatto ambientale. Iterare su diversi schemi di fine-tuning \xE8 anche pi\xF9 rapido e semplice, in quanto l\u2019addestramento \xE8 meno restrittivo di un pre-addestramento completo."),rc.forEach(t),ar=p(e),Ai=r(e,"P",{});var oc=o(Ai);an=n(oc,"Questo processo permette anche di ottenere risultati migliori di un addestramento da zero (a meno di non essere in possesso di moltissimi dati), motivo per cui bisognerebbe sempre partire da un modello pre-addestrato (quanto possibile compatibile con il compito da eseguire) e affinarlo."),oc.forEach(t),rr=p(e),F=r(e,"H2",{class:!0});var Xr=o(F);fe=r(Xr,"A",{id:!0,class:!0,href:!0});var lc=o(fe);Bt=r(lc,"SPAN",{});var nc=o(Bt);h(ri.$$.fragment,nc),nc.forEach(t),lc.forEach(t),rn=p(Xr),Ot=r(Xr,"SPAN",{});var sc=o(Ot);on=n(sc,"Architettura generale"),sc.forEach(t),Xr.forEach(t),or=p(e),Li=r(e,"P",{});var dc=o(Li);ln=n(dc,"In questa sezione, vedremo l\u2019architettura generale del modello Transformer. Non preoccuparti se non capisci tutti i concetti: pi\xF9 avanti, troverai sezioni dettagliate per ogni componente."),dc.forEach(t),lr=p(e),h(oi.$$.fragment,e),nr=p(e),X=r(e,"H2",{class:!0});var Jr=o(X);ve=r(Jr,"A",{id:!0,class:!0,href:!0});var cc=o(ve);Ct=r(cc,"SPAN",{});var uc=o(Ct);h(li.$$.fragment,uc),uc.forEach(t),cc.forEach(t),nn=p(Jr),Ut=r(Jr,"SPAN",{});var pc=o(Ut);sn=n(pc,"Introduzione"),pc.forEach(t),Jr.forEach(t),sr=p(e),Mi=r(e,"P",{});var mc=o(Mi);dn=n(mc,"Il modello si compone principalmente di due blocchi:"),mc.forEach(t),dr=p(e),he=r(e,"UL",{});var Wr=o(he);Ni=r(Wr,"LI",{});var ks=o(Ni);Qt=r(ks,"STRONG",{});var fc=o(Qt);cn=n(fc,"Encoder (sinistra)"),fc.forEach(t),un=n(ks,": L\u2019encoder riceve un input e ne costruisce una rappresentazione, le features. Ci\xF2 significa che il modello \xE8 ottimizzato per la comprensione dell\u2019input."),ks.forEach(t),pn=p(Wr),Gi=r(Wr,"LI",{});var qs=o(Gi);Ht=r(qs,"STRONG",{});var vc=o(Ht);mn=n(vc,"Decoder (destra)"),vc.forEach(t),fn=n(qs,": Il decoder utilizza la rappresentazione dell\u2019encoder (le features) assieme ad ulteriori input per generare la sequenza target. Ci\xF2 significa che il modello \xE8 ottimizzato per la generazione di output."),qs.forEach(t),Wr.forEach(t),cr=p(e),J=r(e,"DIV",{class:!0});var Zr=o(J);ni=r(Zr,"IMG",{class:!0,src:!0,alt:!0}),vn=p(Zr),si=r(Zr,"IMG",{class:!0,src:!0,alt:!0}),Zr.forEach(t),ur=p(e),Si=r(e,"P",{});var hc=o(Si);hn=n(hc,"Ognuna di queste parti pu\xF2 essere utilizzata indipendentemente, in base al compito:"),hc.forEach(t),pr=p(e),L=r(e,"UL",{});var Zi=o(L);Ri=r(Zi,"LI",{});var Is=o(Ri);jt=r(Is,"STRONG",{});var gc=o(jt);gn=n(gc,"Modelli Encoder-only"),gc.forEach(t),_n=n(Is,": Ottimi per compiti che richiedono una comprensione dell\u2019input, come la classificazione frasale e il riconoscimento delle entit\xE0 nominate."),Is.forEach(t),En=p(Zi),xi=r(Zi,"LI",{});var ws=o(xi);Vt=r(ws,"STRONG",{});var _c=o(Vt);zn=n(_c,"Modelli Decoder-only"),_c.forEach(t),bn=n(ws,": Ottimi per compiti generativi come la generazione testuale."),ws.forEach(t),Tn=p(Zi),ge=r(Zi,"LI",{});var Ea=o(ge);Yt=r(Ea,"STRONG",{});var Ec=o(Yt);$n=n(Ec,"Modelli Encoder-decoder"),Ec.forEach(t),kn=n(Ea," o "),Ft=r(Ea,"STRONG",{});var zc=o(Ft);qn=n(zc,"modelli sequence-to-sequence"),zc.forEach(t),In=n(Ea,": Ottimi per compiti generativi che richiedono un input, come la traduzione o il riassunto."),Ea.forEach(t),Zi.forEach(t),mr=p(e),Di=r(e,"P",{});var bc=o(Di);wn=n(bc,"Analizzeremo ciascuna di queste architetture indipendentemente pi\xF9 tardi nel corso."),bc.forEach(t),fr=p(e),W=r(e,"H2",{class:!0});var Kr=o(W);_e=r(Kr,"A",{id:!0,class:!0,href:!0});var Tc=o(_e);Xt=r(Tc,"SPAN",{});var $c=o(Xt);h(di.$$.fragment,$c),$c.forEach(t),Tc.forEach(t),Pn=p(Kr),Jt=r(Kr,"SPAN",{});var kc=o(Jt);yn=n(kc,"Attention layers"),kc.forEach(t),Kr.forEach(t),vr=p(e),M=r(e,"P",{});var Ki=o(M);An=n(Ki,"Una caratteristica chiave dei modelli Transformer \xE8 che sono basati su strati speciali detti "),Wt=r(Ki,"EM",{});var qc=o(Wt);Ln=n(qc,"attention layers"),qc.forEach(t),Mn=n(Ki,". Non a caso, il titolo del paper che introdusse l\u2019architettura Transformer era "),ci=r(Ki,"A",{href:!0,rel:!0});var Ic=o(ci);Nn=n(Ic,"\u201CAttention Is All You Need\u201D"),Ic.forEach(t),Gn=n(Ki,"! Esploreremo gli attention layer nel dettaglio pi\xF9 avanti in questo corso; per ora, tutto ci\xF2 che hai bisogno di sapere \xE8 che un layer dir\xE0 al modello di prestare particolare attenzione a certe parole nella frase input (ignorando praticamente le altre) quando si occupa della rappresentazione delle singole parole."),Ki.forEach(t),hr=p(e),Bi=r(e,"P",{});var wc=o(Bi);Sn=n(wc,"Come esempio concreto, pensa ad un compito di traduzione testuale dall\u2019inglese al francese. Dato l\u2019input \u201CYou like this course\u201D, un modello di traduzione dovr\xE0 fare riferimento alla parola adiacente \u201CYou\u201D per fornire la traduzione corretta della parola \u201Clike\u201D, perch\xE9 in francese la coniugazione del verbo \u201Clike\u201D cambia in base al soggetto. Diversamente, il resto della frase non \xE8 utile alla sua traduzione di quella precisa parola. In maniera simile, durante la traduzione di \u201Cthis\u201D il modello dovr\xE0 prestare attenzione alla parola \u201Ccourse\u201D, in quanto \u201Cthis\u201D ha traduzioni diverse se associato con nomi femminili o maschili. Di nuovo, il resto delle parole della frase non contribuiscono alla corretta traduzione di \u201Cthis\u201D. Con frasi pi\xF9 complesse (e regole grammaticali pi\xF9 complesse), il modello potrebbe aver bisogno di prestare particolare attenzione a parole ben pi\xF9 lontane nella frase per tradurre correttamente ogni parola."),wc.forEach(t),gr=p(e),Oi=r(e,"P",{});var Pc=o(Oi);Rn=n(Pc,"Lo stesso concetto si applica a qualsiasi compito che ha a che fare con il linguaggio naturale: una parola ha un senso a s\xE9 stante, ma tale senso \xE8 profondamente influenzato dal contesto, il quale \xE8 costituito da una qualsiasi parola (o parole) che precede o segue la parola sotto osservazione."),Pc.forEach(t),_r=p(e),Ci=r(e,"P",{});var yc=o(Ci);xn=n(yc,"Ora che sai cosa sono gli attention layer, guardiamo un po\u2019 pi\xF9 nel dettaglio all\u2019architettura Transformer."),yc.forEach(t),Er=p(e),Z=r(e,"H2",{class:!0});var eo=o(Z);Ee=r(eo,"A",{id:!0,class:!0,href:!0});var Ac=o(Ee);Zt=r(Ac,"SPAN",{});var Lc=o(Zt);h(ui.$$.fragment,Lc),Lc.forEach(t),Ac.forEach(t),Dn=p(eo),Kt=r(eo,"SPAN",{});var Mc=o(Kt);Bn=n(Mc,"L'architettura originale"),Mc.forEach(t),eo.forEach(t),zr=p(e),Ui=r(e,"P",{});var Nc=o(Ui);On=n(Nc,"All\u2019origine, l\u2019architettura Transformer fu creata per la traduzione. In fase di addestramento, l\u2019encoder riceve degli input (frasi) in una certa lingua, mentre il decoder riceve le stesse frasi nella lingua target d\u2019elezione. Nell\u2019encoder, gli attention layer sono in grado di utilizzare qualsiasi parola in una data frase (dato che, come abbiamo appena visto, la traduzione di una determinata parola pu\xF2 dipendere da ci\xF2 che la precede o segue nella frase). Diversamente, decoder procede in maniera sequenziale ed \xE8 capace di prestare attenzione solo alle parole della frase che ha gi\xE0 tradotto (ossia, solo le parole che precedono la parola che sta generando). Ad esempio, una volta predette le prime tre parole della frase target, le passiamo al decoder  che utilizza tutti gli input dell\u2019encoder per provare a predirre la quarta parola."),Nc.forEach(t),br=p(e),Qi=r(e,"P",{});var Gc=o(Qi);Cn=n(Gc,"Per accelerare il processo di addestramento (quando il modello ha accesso alle frasi target), l\u2019intero target viene fornito al decoder, che per\xF2 non \xE8 in grado di accedere alle parole future (se avesse accesso alla parola in seconda posizione mentre cerca di predirre la parola in seconda posizione, il problema cesserebbe di essere complesso). Ad esempio, mentre prova a predirre la quarta parola, l\u2019attention layer avr\xE0 accesso solo alle posizioni tra la prima e la terza."),Gc.forEach(t),Tr=p(e),Hi=r(e,"P",{});var Sc=o(Hi);Un=n(Sc,"L\u2019architettura Transformer originale aveva la struttura qui sotto, con l\u2019encoder a sinistra e il decoder a destra:"),Sc.forEach(t),$r=p(e),K=r(e,"DIV",{class:!0});var io=o(K);pi=r(io,"IMG",{class:!0,src:!0,alt:!0}),Qn=p(io),mi=r(io,"IMG",{class:!0,src:!0,alt:!0}),io.forEach(t),kr=p(e),ze=r(e,"P",{});var to=o(ze);Hn=n(to,"Nota che il primo attention layer in un "),ea=r(to,"EM",{});var Rc=o(ea);jn=n(Rc,"decoder block"),Rc.forEach(t),Vn=n(to," presta attenzione a tutti gli input (passati) al decoder, mentre il secondo attention layer utilizza l\u2019output del encoder. Gli \xE8 perci\xF2 possibile avere accesso a tutta la frase input per meglio prevedere la parola corrente. Questa caratteristica \xE8 molto utile in quanto lingue diverse possono avere regole grammaticali diverse piazzano le parole in ordini diversi, oppure perch\xE9 il contesto che compare pi\xF9 tardi nella frase potrebbe essere utile nella determinazione della migliore traduzione di una data parola."),to.forEach(t),qr=p(e),be=r(e,"P",{});var ao=o(be);Yn=n(ao,"L\u2019"),ia=r(ao,"EM",{});var xc=o(ia);Fn=n(xc,"attention mask"),xc.forEach(t),Xn=n(ao," pu\xF2 essere utilizzato anche nell\u2019encoder/decoder per evitare che il modello presti attenzione a certe parole speciali, come ad esempio parole riempitive utilizzate per rendere tutti gli input della stessa lunghezza."),ao.forEach(t),Ir=p(e),ee=r(e,"H2",{class:!0});var ro=o(ee);Te=r(ro,"A",{id:!0,class:!0,href:!0});var Dc=o(Te);ta=r(Dc,"SPAN",{});var Bc=o(ta);h(fi.$$.fragment,Bc),Bc.forEach(t),Dc.forEach(t),Jn=p(ro),aa=r(ro,"SPAN",{});var Oc=o(aa);Wn=n(Oc,"Architetture vs. checkpoint"),Oc.forEach(t),ro.forEach(t),wr=p(e),b=r(e,"P",{});var $e=o(b);Zn=n($e,"Durante questo viaggio nel mondo dei modelli Transformer, incontrerai menzioni di "),ra=r($e,"EM",{});var Cc=o(ra);Kn=n(Cc,"architetture"),Cc.forEach(t),es=n($e," e "),oa=r($e,"EM",{});var Uc=o(oa);is=n(Uc,"checkpoint"),Uc.forEach(t),ts=n($e,", nonch\xE9 di "),la=r($e,"EM",{});var Qc=o(la);as=n(Qc,"modelli"),Qc.forEach(t),rs=n($e,". Questi termini hanno significati leggermente diversi:"),$e.forEach(t),Pr=p(e),N=r(e,"UL",{});var et=o(N);ji=r(et,"LI",{});var Ps=o(ji);na=r(Ps,"STRONG",{});var Hc=o(na);os=n(Hc,"Architettura"),Hc.forEach(t),ls=n(Ps,": Lo scheletro del modello, ossia la definizione di ogni livello e operazione che compare nel modello."),Ps.forEach(t),ns=p(et),Vi=r(et,"LI",{});var ys=o(Vi);sa=r(ys,"STRONG",{});var jc=o(sa);ss=n(jc,"Checkpoint"),jc.forEach(t),ds=n(ys,": I pesi che verranno caricati in una determinata architettura."),ys.forEach(t),cs=p(et),G=r(et,"LI",{});var gi=o(G);da=r(gi,"STRONG",{});var Vc=o(da);us=n(Vc,"Modello"),Vc.forEach(t),ps=n(gi,": Un termine generico meno preciso di \u201Carchitettura\u201D o \u201Ccheckpoint\u201D, in quanto pu\xF2 significare entrambi. In questo corso faremo la distinzione tra "),ca=r(gi,"EM",{});var Yc=o(ca);ms=n(Yc,"architettura"),Yc.forEach(t),fs=n(gi," e "),ua=r(gi,"EM",{});var Fc=o(ua);vs=n(Fc,"checkpoint"),Fc.forEach(t),hs=n(gi," quando sar\xE0 necessario ridurre le ambiguit\xE0."),gi.forEach(t),et.forEach(t),yr=p(e),T=r(e,"P",{});var ke=o(T);gs=n(ke,"Ad esempio, BERT \xE8 un\u2019architettura, mentre "),pa=r(ke,"CODE",{});var Xc=o(pa);_s=n(Xc,"bert-base-cased"),Xc.forEach(t),Es=n(ke,", un set di pesi ("),ma=r(ke,"EM",{});var Jc=o(ma);zs=n(Jc,"weights"),Jc.forEach(t),bs=n(ke,") addestrati dal team di Google per la prima versione di BERT, \xE8 un checkpoint. Ciononostante, \xE8 possibile dire \u201Cil modello BERT\u201D e \u201Cil modello "),fa=r(ke,"CODE",{});var Wc=o(fa);Ts=n(Wc,"bert-base-cased"),Wc.forEach(t),$s=n(ke,".\u201D"),ke.forEach(t),this.h()},h(){c(R,"name","hf:doc:metadata"),c(R,"content",JSON.stringify(lu)),c(ie,"id","come-funzionano-i-transformer"),c(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ie,"href","#come-funzionano-i-transformer"),c(x,"class","relative group"),c(te,"id","un-po-di-storia-dei-transformer"),c(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(te,"href","#un-po-di-storia-dei-transformer"),c(D,"class","relative group"),c(Pe,"class","block dark:hidden"),m(Pe.src,Ms="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||c(Pe,"src",Ms),c(Pe,"alt","A brief chronology of Transformers models."),c(ye,"class","hidden dark:block"),m(ye.src,Ns="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||c(ye,"src",Ns),c(ye,"alt","A brief chronology of Transformers models."),c(B,"class","flex justify-center"),c(Ae,"href","https://arxiv.org/abs/1706.03762"),c(Ae,"rel","nofollow"),c(Le,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),c(Le,"rel","nofollow"),c(Me,"href","https://arxiv.org/abs/1810.04805"),c(Me,"rel","nofollow"),c(Ne,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),c(Ne,"rel","nofollow"),c(Ge,"href","https://arxiv.org/abs/1910.01108"),c(Ge,"rel","nofollow"),c(Se,"href","https://arxiv.org/abs/1910.13461"),c(Se,"rel","nofollow"),c(Re,"href","https://arxiv.org/abs/1910.10683"),c(Re,"rel","nofollow"),c(xe,"href","https://arxiv.org/abs/2005.14165"),c(xe,"rel","nofollow"),c(se,"id","i-transformer-sono-modelli-linguistici"),c(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(se,"href","#i-transformer-sono-modelli-linguistici"),c(O,"class","relative group"),c(Ue,"class","block dark:hidden"),m(Ue.src,Gs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||c(Ue,"src",Gs),c(Ue,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(Qe,"class","hidden dark:block"),m(Qe.src,Ss="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||c(Qe,"src",Ss),c(Qe,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),c(C,"class","flex justify-center"),c(He,"class","block dark:hidden"),m(He.src,Rs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||c(He,"src",Rs),c(He,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(je,"class","hidden dark:block"),m(je.src,xs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||c(je,"src",xs),c(je,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),c(U,"class","flex justify-center"),c(ue,"id","i-transformers-sono-modelli-enormi"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#i-transformers-sono-modelli-enormi"),c(Q,"class","relative group"),m(Fe.src,Ds="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||c(Fe,"src",Ds),c(Fe,"alt","Number of parameters of recent Transformers models"),c(Fe,"width","90%"),c(Ye,"class","flex justify-center"),c(Xe,"class","block dark:hidden"),m(Xe.src,Bs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||c(Xe,"src",Bs),c(Xe,"alt","The carbon footprint of a large language model."),c(Je,"class","hidden dark:block"),m(Je.src,Os="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||c(Je,"src",Os),c(Je,"alt","The carbon footprint of a large language model."),c(H,"class","flex justify-center"),c(pe,"id","transfer-learning"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#transfer-learning"),c(j,"class","relative group"),c(ei,"class","block dark:hidden"),m(ei.src,Cs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||c(ei,"src",Cs),c(ei,"alt","The pretraining of a language model is costly in both time and money."),c(ii,"class","hidden dark:block"),m(ii.src,Us="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||c(ii,"src",Us),c(ii,"alt","The pretraining of a language model is costly in both time and money."),c(V,"class","flex justify-center"),c(ti,"class","block dark:hidden"),m(ti.src,Qs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||c(ti,"src",Qs),c(ti,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(ai,"class","hidden dark:block"),m(ai.src,Hs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||c(ai,"src",Hs),c(ai,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),c(Y,"class","flex justify-center"),c(fe,"id","architettura-generale"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#architettura-generale"),c(F,"class","relative group"),c(ve,"id","introduzione"),c(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ve,"href","#introduzione"),c(X,"class","relative group"),c(ni,"class","block dark:hidden"),m(ni.src,js="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||c(ni,"src",js),c(ni,"alt","Architecture of a Transformers models"),c(si,"class","hidden dark:block"),m(si.src,Vs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||c(si,"src",Vs),c(si,"alt","Architecture of a Transformers models"),c(J,"class","flex justify-center"),c(_e,"id","attention-layers"),c(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_e,"href","#attention-layers"),c(W,"class","relative group"),c(ci,"href","https://arxiv.org/abs/1706.03762"),c(ci,"rel","nofollow"),c(Ee,"id","larchitettura-originale"),c(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ee,"href","#larchitettura-originale"),c(Z,"class","relative group"),c(pi,"class","block dark:hidden"),m(pi.src,Ys="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||c(pi,"src",Ys),c(pi,"alt","Architecture of a Transformers models"),c(mi,"class","hidden dark:block"),m(mi.src,Fs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||c(mi,"src",Fs),c(mi,"alt","Architecture of a Transformers models"),c(K,"class","flex justify-center"),c(Te,"id","architetture-vs-checkpoint"),c(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Te,"href","#architetture-vs-checkpoint"),c(ee,"class","relative group")},m(e,s){i(document.head,R),d(e,za,s),d(e,x,s),i(x,ie),i(ie,it),g(qe,it,null),i(x,oo),i(x,tt),i(tt,lo),d(e,ba,s),g(Ie,e,s),d(e,Ta,s),d(e,_i,s),i(_i,no),d(e,$a,s),d(e,D,s),i(D,te),i(te,at),g(we,at,null),i(D,so),i(D,rt),i(rt,co),d(e,ka,s),d(e,Ei,s),i(Ei,uo),d(e,qa,s),d(e,B,s),i(B,Pe),i(B,po),i(B,ye),d(e,Ia,s),d(e,ae,s),i(ae,mo),i(ae,Ae),i(Ae,fo),i(ae,vo),d(e,wa,s),d(e,f,s),i(f,ot),i(ot,re),i(re,lt),i(lt,ho),i(re,go),i(re,Le),i(Le,_o),i(re,Eo),i(f,zo),i(f,nt),i(nt,oe),i(oe,st),i(st,bo),i(oe,To),i(oe,Me),i(Me,$o),i(oe,ko),i(f,qo),i(f,dt),i(dt,le),i(le,ct),i(ct,Io),i(le,wo),i(le,Ne),i(Ne,Po),i(le,yo),i(f,Ao),i(f,ut),i(ut,ne),i(ne,pt),i(pt,Lo),i(ne,Mo),i(ne,Ge),i(Ge,No),i(ne,Go),i(f,So),i(f,mt),i(mt,k),i(k,ft),i(ft,Ro),i(k,xo),i(k,Se),i(Se,Do),i(k,Bo),i(k,Re),i(Re,Oo),i(k,Co),i(f,Uo),i(f,vt),i(vt,q),i(q,ht),i(ht,Qo),i(q,Ho),i(q,xe),i(xe,jo),i(q,Vo),i(q,gt),i(gt,Yo),i(q,Fo),d(e,Pa,s),d(e,zi,s),i(zi,Xo),d(e,ya,s),d(e,I,s),i(I,De),i(De,Jo),i(De,_t),i(_t,Wo),i(De,Zo),i(I,Ko),i(I,Be),i(Be,el),i(Be,Et),i(Et,il),i(Be,tl),i(I,al),i(I,Oe),i(Oe,rl),i(Oe,zt),i(zt,ol),i(Oe,ll),d(e,Aa,s),d(e,bi,s),i(bi,nl),d(e,La,s),d(e,O,s),i(O,se),i(se,bt),g(Ce,bt,null),i(O,sl),i(O,Tt),i(Tt,dl),d(e,Ma,s),d(e,w,s),i(w,cl),i(w,$t),i($t,ul),i(w,pl),i(w,kt),i(kt,ml),i(w,fl),d(e,Na,s),d(e,de,s),i(de,vl),i(de,qt),i(qt,hl),i(de,gl),d(e,Ga,s),d(e,P,s),i(P,_l),i(P,It),i(It,El),i(P,zl),i(P,wt),i(wt,bl),i(P,Tl),d(e,Sa,s),d(e,C,s),i(C,Ue),i(C,$l),i(C,Qe),d(e,Ra,s),d(e,ce,s),i(ce,kl),i(ce,Pt),i(Pt,ql),i(ce,Il),d(e,xa,s),d(e,U,s),i(U,He),i(U,wl),i(U,je),d(e,Da,s),d(e,Q,s),i(Q,ue),i(ue,yt),g(Ve,yt,null),i(Q,Pl),i(Q,At),i(At,yl),d(e,Ba,s),d(e,Ti,s),i(Ti,Al),d(e,Oa,s),d(e,Ye,s),i(Ye,Fe),d(e,Ca,s),d(e,$i,s),i($i,Ll),d(e,Ua,s),d(e,H,s),i(H,Xe),i(H,Ml),i(H,Je),d(e,Qa,s),g(We,e,s),d(e,Ha,s),d(e,ki,s),i(ki,Nl),d(e,ja,s),d(e,qi,s),i(qi,Gl),d(e,Va,s),d(e,Ii,s),i(Ii,Sl),d(e,Ya,s),d(e,j,s),i(j,pe),i(pe,Lt),g(Ze,Lt,null),i(j,Rl),i(j,Mt),i(Mt,xl),d(e,Fa,s),g(Ke,e,s),d(e,Xa,s),d(e,wi,s),i(wi,Dl),d(e,Ja,s),d(e,V,s),i(V,ei),i(V,Bl),i(V,ii),d(e,Wa,s),d(e,Pi,s),i(Pi,Ol),d(e,Za,s),d(e,y,s),i(y,Cl),i(y,Nt),i(Nt,Ul),i(y,Ql),i(y,Gt),i(Gt,Hl),i(y,jl),d(e,Ka,s),d(e,A,s),i(A,St),i(St,Vl),i(A,Yl),i(A,Rt),i(Rt,Fl),i(A,Xl),i(A,xt),i(xt,Jl),d(e,er,s),d(e,me,s),i(me,Wl),i(me,Dt),i(Dt,Zl),i(me,Kl),d(e,ir,s),d(e,Y,s),i(Y,ti),i(Y,en),i(Y,ai),d(e,tr,s),d(e,yi,s),i(yi,tn),d(e,ar,s),d(e,Ai,s),i(Ai,an),d(e,rr,s),d(e,F,s),i(F,fe),i(fe,Bt),g(ri,Bt,null),i(F,rn),i(F,Ot),i(Ot,on),d(e,or,s),d(e,Li,s),i(Li,ln),d(e,lr,s),g(oi,e,s),d(e,nr,s),d(e,X,s),i(X,ve),i(ve,Ct),g(li,Ct,null),i(X,nn),i(X,Ut),i(Ut,sn),d(e,sr,s),d(e,Mi,s),i(Mi,dn),d(e,dr,s),d(e,he,s),i(he,Ni),i(Ni,Qt),i(Qt,cn),i(Ni,un),i(he,pn),i(he,Gi),i(Gi,Ht),i(Ht,mn),i(Gi,fn),d(e,cr,s),d(e,J,s),i(J,ni),i(J,vn),i(J,si),d(e,ur,s),d(e,Si,s),i(Si,hn),d(e,pr,s),d(e,L,s),i(L,Ri),i(Ri,jt),i(jt,gn),i(Ri,_n),i(L,En),i(L,xi),i(xi,Vt),i(Vt,zn),i(xi,bn),i(L,Tn),i(L,ge),i(ge,Yt),i(Yt,$n),i(ge,kn),i(ge,Ft),i(Ft,qn),i(ge,In),d(e,mr,s),d(e,Di,s),i(Di,wn),d(e,fr,s),d(e,W,s),i(W,_e),i(_e,Xt),g(di,Xt,null),i(W,Pn),i(W,Jt),i(Jt,yn),d(e,vr,s),d(e,M,s),i(M,An),i(M,Wt),i(Wt,Ln),i(M,Mn),i(M,ci),i(ci,Nn),i(M,Gn),d(e,hr,s),d(e,Bi,s),i(Bi,Sn),d(e,gr,s),d(e,Oi,s),i(Oi,Rn),d(e,_r,s),d(e,Ci,s),i(Ci,xn),d(e,Er,s),d(e,Z,s),i(Z,Ee),i(Ee,Zt),g(ui,Zt,null),i(Z,Dn),i(Z,Kt),i(Kt,Bn),d(e,zr,s),d(e,Ui,s),i(Ui,On),d(e,br,s),d(e,Qi,s),i(Qi,Cn),d(e,Tr,s),d(e,Hi,s),i(Hi,Un),d(e,$r,s),d(e,K,s),i(K,pi),i(K,Qn),i(K,mi),d(e,kr,s),d(e,ze,s),i(ze,Hn),i(ze,ea),i(ea,jn),i(ze,Vn),d(e,qr,s),d(e,be,s),i(be,Yn),i(be,ia),i(ia,Fn),i(be,Xn),d(e,Ir,s),d(e,ee,s),i(ee,Te),i(Te,ta),g(fi,ta,null),i(ee,Jn),i(ee,aa),i(aa,Wn),d(e,wr,s),d(e,b,s),i(b,Zn),i(b,ra),i(ra,Kn),i(b,es),i(b,oa),i(oa,is),i(b,ts),i(b,la),i(la,as),i(b,rs),d(e,Pr,s),d(e,N,s),i(N,ji),i(ji,na),i(na,os),i(ji,ls),i(N,ns),i(N,Vi),i(Vi,sa),i(sa,ss),i(Vi,ds),i(N,cs),i(N,G),i(G,da),i(da,us),i(G,ps),i(G,ca),i(ca,ms),i(G,fs),i(G,ua),i(ua,vs),i(G,hs),d(e,yr,s),d(e,T,s),i(T,gs),i(T,pa),i(pa,_s),i(T,Es),i(T,ma),i(ma,zs),i(T,bs),i(T,fa),i(fa,Ts),i(T,$s),Ar=!0},p:tu,i(e){Ar||(_(qe.$$.fragment,e),_(Ie.$$.fragment,e),_(we.$$.fragment,e),_(Ce.$$.fragment,e),_(Ve.$$.fragment,e),_(We.$$.fragment,e),_(Ze.$$.fragment,e),_(Ke.$$.fragment,e),_(ri.$$.fragment,e),_(oi.$$.fragment,e),_(li.$$.fragment,e),_(di.$$.fragment,e),_(ui.$$.fragment,e),_(fi.$$.fragment,e),Ar=!0)},o(e){E(qe.$$.fragment,e),E(Ie.$$.fragment,e),E(we.$$.fragment,e),E(Ce.$$.fragment,e),E(Ve.$$.fragment,e),E(We.$$.fragment,e),E(Ze.$$.fragment,e),E(Ke.$$.fragment,e),E(ri.$$.fragment,e),E(oi.$$.fragment,e),E(li.$$.fragment,e),E(di.$$.fragment,e),E(ui.$$.fragment,e),E(fi.$$.fragment,e),Ar=!1},d(e){t(R),e&&t(za),e&&t(x),z(qe),e&&t(ba),z(Ie,e),e&&t(Ta),e&&t(_i),e&&t($a),e&&t(D),z(we),e&&t(ka),e&&t(Ei),e&&t(qa),e&&t(B),e&&t(Ia),e&&t(ae),e&&t(wa),e&&t(f),e&&t(Pa),e&&t(zi),e&&t(ya),e&&t(I),e&&t(Aa),e&&t(bi),e&&t(La),e&&t(O),z(Ce),e&&t(Ma),e&&t(w),e&&t(Na),e&&t(de),e&&t(Ga),e&&t(P),e&&t(Sa),e&&t(C),e&&t(Ra),e&&t(ce),e&&t(xa),e&&t(U),e&&t(Da),e&&t(Q),z(Ve),e&&t(Ba),e&&t(Ti),e&&t(Oa),e&&t(Ye),e&&t(Ca),e&&t($i),e&&t(Ua),e&&t(H),e&&t(Qa),z(We,e),e&&t(Ha),e&&t(ki),e&&t(ja),e&&t(qi),e&&t(Va),e&&t(Ii),e&&t(Ya),e&&t(j),z(Ze),e&&t(Fa),z(Ke,e),e&&t(Xa),e&&t(wi),e&&t(Ja),e&&t(V),e&&t(Wa),e&&t(Pi),e&&t(Za),e&&t(y),e&&t(Ka),e&&t(A),e&&t(er),e&&t(me),e&&t(ir),e&&t(Y),e&&t(tr),e&&t(yi),e&&t(ar),e&&t(Ai),e&&t(rr),e&&t(F),z(ri),e&&t(or),e&&t(Li),e&&t(lr),z(oi,e),e&&t(nr),e&&t(X),z(li),e&&t(sr),e&&t(Mi),e&&t(dr),e&&t(he),e&&t(cr),e&&t(J),e&&t(ur),e&&t(Si),e&&t(pr),e&&t(L),e&&t(mr),e&&t(Di),e&&t(fr),e&&t(W),z(di),e&&t(vr),e&&t(M),e&&t(hr),e&&t(Bi),e&&t(gr),e&&t(Oi),e&&t(_r),e&&t(Ci),e&&t(Er),e&&t(Z),z(ui),e&&t(zr),e&&t(Ui),e&&t(br),e&&t(Qi),e&&t(Tr),e&&t(Hi),e&&t($r),e&&t(K),e&&t(kr),e&&t(ze),e&&t(qr),e&&t(be),e&&t(Ir),e&&t(ee),z(fi),e&&t(wr),e&&t(b),e&&t(Pr),e&&t(N),e&&t(yr),e&&t(T)}}}const lu={local:"come-funzionano-i-transformer",sections:[{local:"un-po-di-storia-dei-transformer",title:"Un po' di storia dei Transformer"},{local:"i-transformer-sono-modelli-linguistici",title:"I Transformer sono modelli linguistici"},{local:"i-transformers-sono-modelli-enormi",title:"I Transformers sono modelli enormi"},{local:"transfer-learning",title:"Transfer Learning"},{local:"architettura-generale",title:"Architettura generale"},{local:"introduzione",title:"Introduzione"},{local:"attention-layers",title:"Attention layers"},{local:"larchitettura-originale",title:"L'architettura originale"},{local:"architetture-vs-checkpoint",title:"Architetture vs. checkpoint"}],title:"Come funzionano i Transformer?"};function nu(Ls){return au(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pu extends Zc{constructor(R){super();Kc(this,R,nu,ou,eu,{})}}export{pu as default,lu as metadata};
