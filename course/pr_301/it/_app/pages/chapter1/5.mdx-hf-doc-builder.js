import{S as Ce,i as ke,s as Ne,e as a,k as m,w as fe,t as f,M as Qe,c as l,d as t,m as d,a as r,x as pe,h as p,b as n,G as o,g as c,y as ue,L as Ue,q as he,o as _e,B as ve,v as De}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Je}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ye}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as je}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function Fe(Ee){let h,N,_,v,q,$,K,B,V,Q,z,U,w,D,E,W,P,X,Z,J,L,ee,Y,R,te,j,T,oe,F,s,M,g,ae,le,S,A,re,ie,x,y,ne,se,C,b,ce,me,k,I,de,G;return $=new Ye({}),z=new je({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),w=new Je({props:{id:"MUqNwgPjJvQ"}}),{c(){h=a("meta"),N=m(),_=a("h1"),v=a("a"),q=a("span"),fe($.$$.fragment),K=m(),B=a("span"),V=f("Modelli encoder"),Q=m(),fe(z.$$.fragment),U=m(),fe(w.$$.fragment),D=m(),E=a("p"),W=f("I modelli encoder utilizzano solo l\u2019encoder di un modello Transformer. In ogni fase, gli attention layer hanno accesso a tutte le parole della frase di partenza. Questi modelli sono spesso caratterizzati come aventi attenzione \u201Cbi-direzionale\u201D e chiamati "),P=a("em"),X=f("auto-encoding models"),Z=f("."),J=m(),L=a("p"),ee=f("Solitamente, il pre-addestramento di questi modelli consiste nel corrompere una determinata frase (ad esempio, nascondendone casualmente alcune parole) e incaricare il modello di ritrovare o ricostruire la frase di partenza."),Y=m(),R=a("p"),te=f("I modelli encoder sono particolarmente appropriati per compiti che richiedono la comprensione di frasi intere, quali la classificazione di frasi, riconoscimento delle entit\xE0 nominate (e in senso pi\xF9 ampio, la classificazione di parole), e l\u2019estrazione di risposte da un contesto."),j=m(),T=a("p"),oe=f("Alcuni esempi di modelli di questo tipo includono:"),F=m(),s=a("ul"),M=a("li"),g=a("a"),ae=f("ALBERT"),le=m(),S=a("li"),A=a("a"),re=f("BERT"),ie=m(),x=a("li"),y=a("a"),ne=f("DistilBERT"),se=m(),C=a("li"),b=a("a"),ce=f("ELECTRA"),me=m(),k=a("li"),I=a("a"),de=f("RoBERTa"),this.h()},l(e){const i=Qe('[data-svelte="svelte-1phssyn"]',document.head);h=l(i,"META",{name:!0,content:!0}),i.forEach(t),N=d(e),_=l(e,"H1",{class:!0});var H=r(_);v=l(H,"A",{id:!0,class:!0,href:!0});var $e=r(v);q=l($e,"SPAN",{});var ze=r(q);pe($.$$.fragment,ze),ze.forEach(t),$e.forEach(t),K=d(H),B=l(H,"SPAN",{});var we=r(B);V=p(we,"Modelli encoder"),we.forEach(t),H.forEach(t),Q=d(e),pe(z.$$.fragment,e),U=d(e),pe(w.$$.fragment,e),D=d(e),E=l(e,"P",{});var O=r(E);W=p(O,"I modelli encoder utilizzano solo l\u2019encoder di un modello Transformer. In ogni fase, gli attention layer hanno accesso a tutte le parole della frase di partenza. Questi modelli sono spesso caratterizzati come aventi attenzione \u201Cbi-direzionale\u201D e chiamati "),P=l(O,"EM",{});var ge=r(P);X=p(ge,"auto-encoding models"),ge.forEach(t),Z=p(O,"."),O.forEach(t),J=d(e),L=l(e,"P",{});var Ae=r(L);ee=p(Ae,"Solitamente, il pre-addestramento di questi modelli consiste nel corrompere una determinata frase (ad esempio, nascondendone casualmente alcune parole) e incaricare il modello di ritrovare o ricostruire la frase di partenza."),Ae.forEach(t),Y=d(e),R=l(e,"P",{});var ye=r(R);te=p(ye,"I modelli encoder sono particolarmente appropriati per compiti che richiedono la comprensione di frasi intere, quali la classificazione di frasi, riconoscimento delle entit\xE0 nominate (e in senso pi\xF9 ampio, la classificazione di parole), e l\u2019estrazione di risposte da un contesto."),ye.forEach(t),j=d(e),T=l(e,"P",{});var be=r(T);oe=p(be,"Alcuni esempi di modelli di questo tipo includono:"),be.forEach(t),F=d(e),s=l(e,"UL",{});var u=r(s);M=l(u,"LI",{});var Ie=r(M);g=l(Ie,"A",{href:!0,rel:!0});var Le=r(g);ae=p(Le,"ALBERT"),Le.forEach(t),Ie.forEach(t),le=d(u),S=l(u,"LI",{});var Re=r(S);A=l(Re,"A",{href:!0,rel:!0});var Te=r(A);re=p(Te,"BERT"),Te.forEach(t),Re.forEach(t),ie=d(u),x=l(u,"LI",{});var qe=r(x);y=l(qe,"A",{href:!0,rel:!0});var Be=r(y);ne=p(Be,"DistilBERT"),Be.forEach(t),qe.forEach(t),se=d(u),C=l(u,"LI",{});var Pe=r(C);b=l(Pe,"A",{href:!0,rel:!0});var Me=r(b);ce=p(Me,"ELECTRA"),Me.forEach(t),Pe.forEach(t),me=d(u),k=l(u,"LI",{});var Se=r(k);I=l(Se,"A",{href:!0,rel:!0});var xe=r(I);de=p(xe,"RoBERTa"),xe.forEach(t),Se.forEach(t),u.forEach(t),this.h()},h(){n(h,"name","hf:doc:metadata"),n(h,"content",JSON.stringify(Ge)),n(v,"id","modelli-encoder"),n(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(v,"href","#modelli-encoder"),n(_,"class","relative group"),n(g,"href","https://huggingface.co/transformers/model_doc/albert.html"),n(g,"rel","nofollow"),n(A,"href","https://huggingface.co/transformers/model_doc/bert.html"),n(A,"rel","nofollow"),n(y,"href","https://huggingface.co/transformers/model_doc/distilbert.html"),n(y,"rel","nofollow"),n(b,"href","https://huggingface.co/transformers/model_doc/electra.html"),n(b,"rel","nofollow"),n(I,"href","https://huggingface.co/transformers/model_doc/roberta.html"),n(I,"rel","nofollow")},m(e,i){o(document.head,h),c(e,N,i),c(e,_,i),o(_,v),o(v,q),ue($,q,null),o(_,K),o(_,B),o(B,V),c(e,Q,i),ue(z,e,i),c(e,U,i),ue(w,e,i),c(e,D,i),c(e,E,i),o(E,W),o(E,P),o(P,X),o(E,Z),c(e,J,i),c(e,L,i),o(L,ee),c(e,Y,i),c(e,R,i),o(R,te),c(e,j,i),c(e,T,i),o(T,oe),c(e,F,i),c(e,s,i),o(s,M),o(M,g),o(g,ae),o(s,le),o(s,S),o(S,A),o(A,re),o(s,ie),o(s,x),o(x,y),o(y,ne),o(s,se),o(s,C),o(C,b),o(b,ce),o(s,me),o(s,k),o(k,I),o(I,de),G=!0},p:Ue,i(e){G||(he($.$$.fragment,e),he(z.$$.fragment,e),he(w.$$.fragment,e),G=!0)},o(e){_e($.$$.fragment,e),_e(z.$$.fragment,e),_e(w.$$.fragment,e),G=!1},d(e){t(h),e&&t(N),e&&t(_),ve($),e&&t(Q),ve(z,e),e&&t(U),ve(w,e),e&&t(D),e&&t(E),e&&t(J),e&&t(L),e&&t(Y),e&&t(R),e&&t(j),e&&t(T),e&&t(F),e&&t(s)}}}const Ge={local:"modelli-encoder",title:"Modelli encoder"};function He(Ee){return De(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xe extends Ce{constructor(h){super();ke(this,h,He,Fe,Ne,{})}}export{Xe as default,Ge as metadata};
