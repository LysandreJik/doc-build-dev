import{S as _v,i as gv,s as $v,e as l,k as c,w as u,t as s,M as kv,c as n,d as a,m as d,x as m,a as p,h as o,b as g,G as i,g as r,y as f,q as h,o as b,B as v,v as qv}from"../../chunks/vendor-hf-doc-builder.js";import{T as Li}from"../../chunks/Tip-hf-doc-builder.js";import{Y as zv}from"../../chunks/Youtube-hf-doc-builder.js";import{I as L}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as $}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as wv}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Ev}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function jv(A){let _,z,k,q,j;return{c(){_=l("p"),z=s("\u270F\uFE0F "),k=l("strong"),q=s("Prova tu!"),j=s(" Controlla che tutto sia corretto nel secondo elemento del training set.")},l(w){_=n(w,"P",{});var E=p(_);z=o(E,"\u270F\uFE0F "),k=n(E,"STRONG",{});var y=p(k);q=o(y,"Prova tu!"),y.forEach(a),j=o(E," Controlla che tutto sia corretto nel secondo elemento del training set."),E.forEach(a)},m(w,E){r(w,_,E),i(_,z),i(_,k),i(k,q),i(_,j)},d(w){w&&a(_)}}}function yv(A){let _,z;return{c(){_=l("p"),z=s("Nella prossima parte del corso, esamineremo tecniche pi\xF9 avanzate che possono aiutare a ridurre l\u2019impatto sulla memoria e ad affinare i modelli pi\xF9 grandi.")},l(k){_=n(k,"P",{});var q=p(_);z=o(q,"Nella prossima parte del corso, esamineremo tecniche pi\xF9 avanzate che possono aiutare a ridurre l\u2019impatto sulla memoria e ad affinare i modelli pi\xF9 grandi."),q.forEach(a)},m(k,q){r(k,_,q),i(_,z)},d(k){k&&a(_)}}}function Pv(A){let _,z,k,q,j,w,E,y;return{c(){_=l("p"),z=s("\u{1F4A1} Bisogna sempre assicurarsi di poter eseguire "),k=l("code"),q=s("trainer.evaluate()"),j=s(" prima di lanciare "),w=l("code"),E=s("trainer.train()"),y=s(", per evitare di sprecare molte risorse di calcolo prima di incorrere in un errore.")},l(T){_=n(T,"P",{});var P=p(_);z=o(P,"\u{1F4A1} Bisogna sempre assicurarsi di poter eseguire "),k=n(P,"CODE",{});var K=p(k);q=o(K,"trainer.evaluate()"),K.forEach(a),j=o(P," prima di lanciare "),w=n(P,"CODE",{});var U=p(w);E=o(U,"trainer.train()"),U.forEach(a),y=o(P,", per evitare di sprecare molte risorse di calcolo prima di incorrere in un errore."),P.forEach(a)},m(T,P){r(T,_,P),i(_,z),i(_,k),i(k,q),i(_,j),i(_,w),i(w,E),i(_,y)},d(T){T&&a(_)}}}function Cv(A){let _,z,k,q,j,w,E,y,T,P,K;return{c(){_=l("p"),z=s("\u{1F4A1} Se si utilizza un ciclo di addestramento manuale, per il debug della pipeline di addestramento valgono gli stessi passaggi, ma \xE8 pi\xF9 facile separarli. Assicurati per\xF2 di non aver dimenticato il "),k=l("code"),q=s("model.eval()"),j=s(" o il "),w=l("code"),E=s("model.train()"),y=s(" nei punti giusti, o lo "),T=l("code"),P=s("zero_grad()"),K=s(" a ogni passo!")},l(U){_=n(U,"P",{});var C=p(_);z=o(C,"\u{1F4A1} Se si utilizza un ciclo di addestramento manuale, per il debug della pipeline di addestramento valgono gli stessi passaggi, ma \xE8 pi\xF9 facile separarli. Assicurati per\xF2 di non aver dimenticato il "),k=n(C,"CODE",{});var ha=p(k);q=o(ha,"model.eval()"),ha.forEach(a),j=o(C," o il "),w=n(C,"CODE",{});var I=p(w);E=o(I,"model.train()"),I.forEach(a),y=o(C," nei punti giusti, o lo "),T=n(C,"CODE",{});var Qi=p(T);P=o(Qi,"zero_grad()"),Qi.forEach(a),K=o(C," a ogni passo!"),C.forEach(a)},m(U,C){r(U,_,C),i(_,z),i(_,k),i(k,q),i(_,j),i(_,w),i(w,E),i(_,y),i(_,T),i(T,P),i(_,K)},d(U){U&&a(_)}}}function Av(A){let _,z;return{c(){_=l("p"),z=s("\u26A0\uFE0F Se effettui un addestramento in modo distribuito, stampa campioni del set di dati in ogni processo e controlla molto attentamente che ottieni la stessa cosa. Un bug comune \xE8 la presenza di una qualche fonte di casualit\xE0 nella creazione dei dati che fa s\xEC che ogni processo abbia una versione diversa del set di dati.")},l(k){_=n(k,"P",{});var q=p(_);z=o(q,"\u26A0\uFE0F Se effettui un addestramento in modo distribuito, stampa campioni del set di dati in ogni processo e controlla molto attentamente che ottieni la stessa cosa. Un bug comune \xE8 la presenza di una qualche fonte di casualit\xE0 nella creazione dei dati che fa s\xEC che ogni processo abbia una versione diversa del set di dati."),q.forEach(a)},m(k,q){r(k,_,q),i(_,z)},d(k){k&&a(_)}}}function Dv(A){let _,z;return{c(){_=l("p"),z=s("\u{1F4A1} Se i dati di addestramento sono sbilanciati, assicurati di creare un batch di dati di addestramento contenente tutte le label.")},l(k){_=n(k,"P",{});var q=p(_);z=o(q,"\u{1F4A1} Se i dati di addestramento sono sbilanciati, assicurati di creare un batch di dati di addestramento contenente tutte le label."),q.forEach(a)},m(k,q){r(k,_,q),i(_,z)},d(k){k&&a(_)}}}function Tv(A){let _,z,k,q,j;return{c(){_=l("p"),z=s("\u26A0\uFE0F Sar\xE0 necessario ricreare il modello e il "),k=l("code"),q=s("Trainer"),j=s(" dopo questo test, poich\xE9 il modello ottenuto probabilmente non sar\xE0 in grado di recuperare e imparare qualcosa di utile sul set di dati completo.")},l(w){_=n(w,"P",{});var E=p(_);z=o(E,"\u26A0\uFE0F Sar\xE0 necessario ricreare il modello e il "),k=n(E,"CODE",{});var y=p(k);q=o(y,"Trainer"),y.forEach(a),j=o(E," dopo questo test, poich\xE9 il modello ottenuto probabilmente non sar\xE0 in grado di recuperare e imparare qualcosa di utile sul set di dati completo."),E.forEach(a)},m(w,E){r(w,_,E),i(_,z),i(_,k),i(k,q),i(_,j)},d(w){w&&a(_)}}}function Sv(A){let _,z,k,q,j,w,E,y,T,P,K,U,C,ha,I,Qi,Mi,dp,up,It,mp,fp,Io,ne,ke,Lt,ba,hp,Qt,bp,Lo,va,Qo,S,vp,Mt,_p,gp,Ft,$p,kp,Gt,qp,zp,Ht,wp,Ep,Mo,qe,jp,Rt,yp,Pp,Fo,ze,Cp,_a,Ap,Dp,Go,ga,Ho,Fi,Tp,Ro,$a,Wo,pe,we,Wt,ka,Sp,Bt,xp,Bo,Y,Op,Vt,Np,Up,Kt,Ip,Lp,Vo,Ee,Qp,Yt,Mp,Fp,Ko,qa,Yo,za,Jo,Q,Gp,Jt,Hp,Rp,Zt,Wp,Bp,Xt,Vp,Kp,Zo,D,Yp,es,Jp,Zp,as,Xp,ec,is,ac,ic,ts,tc,sc,ss,oc,rc,Xo,wa,er,Gi,lc,ar,Ea,ir,Hi,nc,tr,ja,sr,Ri,pc,or,Wi,cc,rr,ya,lr,Pa,nr,Bi,dc,pr,Ca,cr,Aa,dr,x,uc,os,mc,fc,rs,hc,bc,ls,vc,_c,ns,gc,$c,ur,Da,mr,Ta,fr,J,kc,Sa,qc,zc,ps,wc,Ec,hr,je,jc,cs,yc,Pc,br,xa,vr,Oa,_r,ye,Cc,ds,Ac,Dc,gr,Na,$r,Ua,kr,Vi,Tc,qr,Ia,zr,La,wr,Z,Sc,us,xc,Oc,ms,Nc,Uc,Er,Qa,jr,Ma,yr,M,Ic,fs,Lc,Qc,hs,Mc,Fc,bs,Gc,Hc,Pr,Pe,Rc,vs,Wc,Bc,Cr,Ce,Ar,Ki,Vc,Dr,Yi,Kc,Tr,ce,Ae,_s,Fa,Yc,gs,Jc,Sr,O,Zc,$s,Xc,ed,ks,ad,id,qs,td,sd,zs,od,rd,xr,Ga,Or,De,ld,ws,nd,pd,Nr,Ha,Ur,X,cd,Es,dd,ud,js,md,fd,Ir,Ra,Lr,Wa,Qr,F,hd,ys,bd,vd,Ps,_d,gd,Cs,$d,kd,Mr,G,qd,As,zd,wd,Ds,Ed,jd,Ts,yd,Pd,Fr,Ba,Gr,Ji,Cd,Hr,Va,Rr,Zi,Ad,Wr,Xi,Dd,Br,Ka,Vr,N,Td,Ss,Sd,xd,xs,Od,Nd,Os,Ud,Id,Ns,Ld,Qd,Kr,Ya,Yr,et,Md,Jr,at,Fd,Zr,de,Te,Us,Ja,Gd,Is,Hd,Xr,it,Rd,el,Za,al,Se,Wd,Ls,Bd,Vd,il,ee,Kd,Qs,Yd,Jd,Ms,Zd,Xd,tl,tt,eu,sl,xe,au,Fs,iu,tu,ol,Xa,rl,ei,ll,ae,su,Gs,ou,ru,Hs,lu,nu,nl,ai,pl,ii,cl,st,pu,dl,ti,ul,Oe,cu,Rs,du,uu,ml,si,fl,ot,mu,hl,oi,bl,rt,fu,vl,ue,Ne,Ws,ri,hu,Bs,bu,_l,lt,vu,gl,Ue,_u,Vs,gu,$u,$l,li,kl,nt,ku,ql,ie,qu,Ks,zu,wu,Ys,Eu,ju,zl,ni,wl,Ie,yu,Js,Pu,Cu,El,me,Le,Zs,pi,Au,Xs,Du,jl,Qe,Tu,eo,Su,xu,yl,pt,Ou,Pl,Me,Cl,fe,Fe,ao,ci,Nu,io,Uu,Al,Ge,Iu,to,Lu,Qu,Dl,di,Tl,ui,Sl,ct,Mu,xl,He,Fu,so,Gu,Hu,Ol,mi,Nl,fi,Ul,Re,Il,dt,Ru,Ll,hi,Ql,ut,Wu,Ml,bi,Fl,te,Bu,oo,Vu,Ku,ro,Yu,Ju,Gl,vi,Hl,_i,Rl,H,Zu,lo,Xu,em,no,am,im,po,tm,sm,Wl,gi,Bl,$i,Vl,We,om,co,rm,lm,Kl,ki,Yl,qi,Jl,mt,nm,Zl,ft,pm,Xl,zi,en,ht,cm,an,Be,tn,he,Ve,uo,wi,dm,mo,um,sn,bt,mm,on,be,Ke,fo,Ei,fm,ho,hm,rn,vt,bm,ln,R,bo,vm,_m,vo,gm,$m,_o,km,qm,go,zm,nn,Ye,pn,Je,wm,$o,Em,jm,cn,_t,ym,dn,gt,Pm,un,ve,Ze,ko,ji,Cm,qo,Am,mn,Xe,Dm,zo,Tm,Sm,fn,ea,xm,wo,Om,Nm,hn,yi,bn,aa,vn,ia,Um,Eo,Im,Lm,_n,Pi,gn,Ci,$n,$t,Qm,kn,kt,Mm,qn,ta,zn,_e,sa,jo,Ai,Fm,yo,Gm,wn,W,Hm,Po,Rm,Wm,Co,Bm,Vm,Ao,Km,Ym,En,qt,Jm,jn,zt,Zm,yn,ge,oa,Do,Di,Xm,To,ef,Pn,ra,af,Ti,tf,sf,Cn,wt,of,An,B,Et,Si,rf,lf,nf,jt,xi,pf,cf,df,yt,Oi,uf,mf,ff,Pt,Ni,hf,bf,Dn,Ct,vf,Tn;return k=new Ev({props:{fw:A[0]}}),y=new L({}),C=new wv({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section4_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section4_pt.ipynb"}]}}),ba=new L({}),va=new zv({props:{id:"L-WSwUWde1U"}}),ga=new $({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=raw_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
)
trainer.train()`}}),$a=new $({props:{code:"'ValueError: You have to specify either input_ids or inputs_embeds'",highlighted:'<span class="hljs-string">&#x27;ValueError: You have to specify either input_ids or inputs_embeds&#x27;</span>'}}),ka=new L({}),qa=new $({props:{code:"trainer.train_dataset[0]",highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>]'}}),za=new $({props:{code:`{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}`,highlighted:`{<span class="hljs-string">&#x27;hypothesis&#x27;</span>: <span class="hljs-string">&#x27;Product and geography are what make cream skimming work. &#x27;</span>,
 <span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;premise&#x27;</span>: <span class="hljs-string">&#x27;Conceptually cream skimming has two basic dimensions - product and geography.&#x27;</span>}`}}),wa=new $({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
)
trainer.train()`}}),Ea=new $({props:{code:"'ValueError: expected sequence of length 43 at dim 1 (got 37)'",highlighted:'<span class="hljs-string">&#x27;ValueError: expected sequence of length 43 at dim 1 (got 37)&#x27;</span>'}}),ja=new $({props:{code:`~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch`,highlighted:`~/git/transformers/src/transformers/data/data_collator.py <span class="hljs-keyword">in</span> torch_default_data_collator(features)
    <span class="hljs-number">105</span>                 batch[k] = torch.stack([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">106</span>             <span class="hljs-keyword">else</span>:
--&gt; <span class="hljs-number">107</span>                 batch[k] = torch.tensor([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">108</span> 
    <span class="hljs-number">109</span>     <span class="hljs-keyword">return</span> batch`}}),ya=new $({props:{code:'tokenizer.decode(trainer.train_dataset[0]["input_ids"])',highlighted:'tokenizer.decode(trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),Pa=new $({props:{code:"'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'",highlighted:'<span class="hljs-string">&#x27;[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]&#x27;</span>'}}),Ca=new $({props:{code:"trainer.train_dataset[0].keys()",highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>].keys()'}}),Aa=new $({props:{code:"dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])",highlighted:'dict_keys([<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;hypothesis&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;premise&#x27;</span>])'}}),Da=new $({props:{code:"type(trainer.model)",highlighted:'<span class="hljs-built_in">type</span>(trainer.model)'}}),Ta=new $({props:{code:"transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification",highlighted:"transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"}}),xa=new $({props:{code:'trainer.train_dataset[0]["attention_mask"]',highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;attention_mask&quot;</span>]'}}),Oa=new $({props:{code:"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]",highlighted:'[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]'}}),Na=new $({props:{code:`len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)`,highlighted:`<span class="hljs-built_in">len</span>(trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;attention_mask&quot;</span>]) == <span class="hljs-built_in">len</span>(
    trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;input_ids&quot;</span>]
)`}}),Ua=new $({props:{code:"True",highlighted:'<span class="hljs-literal">True</span>'}}),Ia=new $({props:{code:'trainer.train_dataset[0]["label"]',highlighted:'trainer.train_dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;label&quot;</span>]'}}),La=new $({props:{code:"1",highlighted:'<span class="hljs-number">1</span>'}}),Qa=new $({props:{code:'trainer.train_dataset.features["label"].names',highlighted:'trainer.train_dataset.features[<span class="hljs-string">&quot;label&quot;</span>].names'}}),Ma=new $({props:{code:"['entailment', 'neutral', 'contradiction']",highlighted:'[<span class="hljs-string">&#x27;entailment&#x27;</span>, <span class="hljs-string">&#x27;neutral&#x27;</span>, <span class="hljs-string">&#x27;contradiction&#x27;</span>]'}}),Ce=new Li({props:{$$slots:{default:[jv]},$$scope:{ctx:A}}}),Fa=new L({}),Ga=new $({props:{code:`for batch in trainer.get_train_dataloader():
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>`}}),Ha=new $({props:{code:`~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)`,highlighted:`~/git/transformers/src/transformers/data/data_collator.py <span class="hljs-keyword">in</span> torch_default_data_collator(features)
    <span class="hljs-number">105</span>                 batch[k] = torch.stack([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">106</span>             <span class="hljs-keyword">else</span>:
--&gt; <span class="hljs-number">107</span>                 batch[k] = torch.tensor([f[k] <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> features])
    <span class="hljs-number">108</span> 
    <span class="hljs-number">109</span>     <span class="hljs-keyword">return</span> batch

ValueError: expected sequence of length <span class="hljs-number">45</span> at dim <span class="hljs-number">1</span> (got <span class="hljs-number">76</span>)`}}),Ra=new $({props:{code:`data_collator = trainer.get_train_dataloader().collate_fn
data_collator`,highlighted:`data_collator = trainer.get_train_dataloader().collate_fn
data_collator`}}),Wa=new $({props:{code:"<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>",highlighted:'&lt;function transformers.data.data_collator.default_data_collator(features: <span class="hljs-type">List</span>[InputDataClass], return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]&gt;'}}),Ba=new $({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`}}),Va=new $({props:{code:"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",highlighted:"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"}}),Ka=new $({props:{code:`data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])`,highlighted:`data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)])`}}),Ya=new $({props:{code:`data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])`,highlighted:`data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>)])`}}),Ja=new L({}),Za=new $({props:{code:`for batch in trainer.get_train_dataloader():
    break`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>`}}),Xa=new $({props:{code:"outputs = trainer.model.cpu()(**batch)",highlighted:"outputs = trainer.model.cpu()(**batch)"}}),ei=new $({props:{code:`~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.`,highlighted:`~/.pyenv/versions/<span class="hljs-number">3.7</span><span class="hljs-number">.9</span>/envs/base/lib/python3<span class="hljs-number">.7</span>/site-packages/torch/nn/functional.py <span class="hljs-keyword">in</span> nll_loss(<span class="hljs-built_in">input</span>, target, weight, size_average, ignore_index, reduce, reduction)
   <span class="hljs-number">2386</span>         )
   <span class="hljs-number">2387</span>     <span class="hljs-keyword">if</span> dim == <span class="hljs-number">2</span>:
-&gt; <span class="hljs-number">2388</span>         ret = torch._C._nn.nll_loss(<span class="hljs-built_in">input</span>, target, weight, _Reduction.get_enum(reduction), ignore_index)
   <span class="hljs-number">2389</span>     <span class="hljs-keyword">elif</span> dim == <span class="hljs-number">4</span>:
   <span class="hljs-number">2390</span>         ret = torch._C._nn.nll_loss2d(<span class="hljs-built_in">input</span>, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target <span class="hljs-number">2</span> <span class="hljs-keyword">is</span> out of bounds.`}}),ai=new $({props:{code:"trainer.model.config.num_labels",highlighted:"trainer.model.config.num_labels"}}),ii=new $({props:{code:"2",highlighted:'<span class="hljs-number">2</span>'}}),ti=new $({props:{code:`from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=<span class="hljs-number">3</span>)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)`}}),si=new $({props:{code:`for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>

outputs = trainer.model.cpu()(**batch)`}}),oi=new $({props:{code:`import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)`,highlighted:`<span class="hljs-keyword">import</span> torch

device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}

outputs = trainer.model.to(device)(**batch)`}}),ri=new L({}),li=new $({props:{code:`loss = outputs.loss
loss.backward()`,highlighted:`loss = outputs.loss
loss.backward()`}}),ni=new $({props:{code:`trainer.create_optimizer()
trainer.optimizer.step()`,highlighted:`trainer.create_optimizer()
trainer.optimizer.step()`}}),pi=new L({}),Me=new Li({props:{$$slots:{default:[yv]},$$scope:{ctx:A}}}),ci=new L({}),di=new $({props:{code:`# This will take a long time and error out, so you shouldn't run this cell
trainer.train()`,highlighted:`<span class="hljs-comment"># This will take a long time and error out, so you shouldn&#x27;t run this cell</span>
trainer.train()`}}),ui=new $({props:{code:"TypeError: only size-1 arrays can be converted to Python scalars",highlighted:'TypeError: only size-<span class="hljs-number">1</span> arrays can be converted to Python scalars'}}),mi=new $({props:{code:"trainer.evaluate()",highlighted:"trainer.evaluate()"}}),fi=new $({props:{code:"TypeError: only size-1 arrays can be converted to Python scalars",highlighted:'TypeError: only size-<span class="hljs-number">1</span> arrays can be converted to Python scalars'}}),Re=new Li({props:{$$slots:{default:[Pv]},$$scope:{ctx:A}}}),hi=new $({props:{code:`for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_eval_dataloader():
    <span class="hljs-keyword">break</span>

batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = trainer.model(**batch)`}}),bi=new $({props:{code:`~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         """
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()`,highlighted:`~/git/datasets/src/datasets/metric.py <span class="hljs-keyword">in</span> add_batch(self, predictions, references)
    <span class="hljs-number">431</span>         <span class="hljs-string">&quot;&quot;&quot;
    432         batch = {&quot;predictions&quot;: predictions, &quot;references&quot;: references}
--&gt; 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()</span>`}}),vi=new $({props:{code:`predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))`,highlighted:`predictions = outputs.logits.cpu().numpy()
labels = batch[<span class="hljs-string">&quot;labels&quot;</span>].cpu().numpy()

compute_metrics((predictions, labels))`}}),_i=new $({props:{code:"TypeError: only size-1 arrays can be converted to Python scalars",highlighted:'TypeError: only size-<span class="hljs-number">1</span> arrays can be converted to Python scalars'}}),gi=new $({props:{code:"predictions.shape, labels.shape",highlighted:"predictions.shape, labels.shape"}}),$i=new $({props:{code:"((8, 3), (8,))",highlighted:'((<span class="hljs-number">8</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">8</span>,))'}}),ki=new $({props:{code:`import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


compute_metrics((predictions, labels))`}}),qi=new $({props:{code:"{'accuracy': 0.625}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.625</span>}'}}),zi=new $({props:{code:`import numpy as np
from datasets import load_dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = load_metric("glue", "mnli")


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_metric
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;premise&quot;</span>], examples[<span class="hljs-string">&quot;hypothesis&quot;</span>], truncation=<span class="hljs-literal">True</span>)


tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=<span class="hljs-number">3</span>)

args = TrainingArguments(
    <span class="hljs-string">f&quot;distilbert-finetuned-mnli&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

metric = load_metric(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation_matched&quot;</span>],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()`}}),Be=new Li({props:{$$slots:{default:[Cv]},$$scope:{ctx:A}}}),wi=new L({}),Ei=new L({}),Ye=new Li({props:{warning:!0,$$slots:{default:[Av]},$$scope:{ctx:A}}}),ji=new L({}),yi=new $({props:{code:`for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()`,highlighted:`<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> trainer.get_train_dataloader():
    <span class="hljs-keyword">break</span>

batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
trainer.create_optimizer()

<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()`}}),aa=new Li({props:{$$slots:{default:[Dv]},$$scope:{ctx:A}}}),Pi=new $({props:{code:`with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))`,highlighted:`<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))`}}),Ci=new $({props:{code:"{'accuracy': 1.0}",highlighted:'{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">1.0</span>}'}}),ta=new Li({props:{warning:!0,$$slots:{default:[Tv]},$$scope:{ctx:A}}}),Ai=new L({}),Di=new L({}),{c(){_=l("meta"),z=c(),u(k.$$.fragment),q=c(),j=l("h1"),w=l("a"),E=l("span"),u(y.$$.fragment),T=c(),P=l("span"),K=s("Fare il debug della training pipeline"),U=c(),u(C.$$.fragment),ha=c(),I=l("p"),Qi=s("Hai scritto un bello script per addestrare o affinare un modello su un determinato compito, seguendo scrupolosamente i consigli del "),Mi=l("a"),dp=s("Capitolo 7"),up=s(". Ma quando lanci il comando "),It=l("code"),mp=s("trainer.train()"),fp=s(", succede qualcosa di orribile: si ottiene un errore \u{1F631}! O peggio, tutto sembra andare bene e il training viene eseguito senza errori, ma il modello che ne risulta fa schifo. In questa sezione mostreremo cosa \xE8 possibile fare per eseguire il debug di questo tipo di problemi."),Io=c(),ne=l("h2"),ke=l("a"),Lt=l("span"),u(ba.$$.fragment),hp=c(),Qt=l("span"),bp=s("Fare il debug della training pipeline"),Lo=c(),u(va.$$.fragment),Qo=c(),S=l("p"),vp=s("Il problema quando si ha un errore da "),Mt=l("code"),_p=s("trainer.train()"),gp=s(" \xE8 che potrebbe provenire da pi\xF9 fonti, poich\xE9 il "),Ft=l("code"),$p=s("Trainer"),kp=s(" di solito mette insieme molte cose. Converte i dataset in "),Gt=l("em"),qp=s("dataloader"),zp=s(", quindi l\u2019errore potrebbe essere dato da qualcosa di sbagliato nel dataset stesso, o da un problema qualche problema nel provare a raggruppare in un batch elementi del dataset. Poi prende un batch di dati e lo invia al modello, quindi il problema potrebbe anche essere nel codice del modello. Successivamente, calcola i gradienti ed esegue la fase di ottimizzazione, quindi il problema potrebbe essere nel tuo "),Ht=l("em"),wp=s("optimizer"),Ep=s(". E anche se tutto va bene per il training, qualcosa potrebbe andare storto durante la valutazione se c\u2019\xE8 un problema con la metrica selezionata."),Mo=c(),qe=l("p"),jp=s("Il modo migliore per eseguire il debug di un errore che si verifica in "),Rt=l("code"),yp=s("trainer.train()"),Pp=s(" \xE8 quello di esaminare manualmente l\u2019intera pipeline per vedere dove le cose sono andate storte. L\u2019errore \xE8 spesso molto facile da risolvere."),Fo=c(),ze=l("p"),Cp=s("Per dimostrarlo, useremo il seguente script che ha lo scopo di affinare un modello DistilBERT sul "),_a=l("a"),Ap=s("dataset MNLI"),Dp=s(":"),Go=c(),u(ga.$$.fragment),Ho=c(),Fi=l("p"),Tp=s("Se provi a eseguirlo, otterrai un errore piuttosto criptico:"),Ro=c(),u($a.$$.fragment),Wo=c(),pe=l("h3"),we=l("a"),Wt=l("span"),u(ka.$$.fragment),Sp=c(),Bt=l("span"),xp=s("Controlla i dati"),Bo=c(),Y=l("p"),Op=s("Non c\u2019\xE8 bisogno di dirlo, ma se i dati sono corrotti, il "),Vt=l("code"),Np=s("Trainer"),Up=s(" non sar\xE0 in grado di formare i batch e tanto meno di addestrare il modello. Quindi, per prima cosa, \xE8 necessario dare un\u2019occhiata a cosa c\u2019\xE8 nel training set("),Kt=l("em"),Ip=s("insieme di addestramento"),Lp=s(")."),Vo=c(),Ee=l("p"),Qp=s("Per evitare di passare infinite ore a cercare di risolvere qualcosa che non \xE8 la fonte del bug, consigliamo di usare "),Yt=l("code"),Mp=s("trainer.train_dataset"),Fp=s(" per controllare l\u2019insieme di dati e nient\u2019altro. Quindi facciamo cos\xEC:"),Ko=c(),u(qa.$$.fragment),Yo=c(),u(za.$$.fragment),Jo=c(),Q=l("p"),Gp=s("Hai notato qualcosa di sbagliato? Questo, insieme al messaggio di errore sulla mancanza di "),Jt=l("code"),Hp=s("input_ids"),Rp=s(", dovrebbe farci capire che qui abbiamo testo, non numeri che invece il modello pu\xF2 interpretare. In questo caso, l\u2019errore originale \xE8 molto fuorviante, perch\xE9 il "),Zt=l("code"),Wp=s("Trainer"),Bp=s(" rimuove automaticamente le colonne che non corrispondono alla firma del modello (cio\xE8 i parametri che il modello si aspetta). Ci\xF2 significa che in questo caso tutto, a parte "),Xt=l("em"),Vp=s("label"),Kp=s(", \xE8 stato scartato. Non c\u2019\xE8 stato quindi nessun problema nel creare i batch di dati e poi inviarli al modello, invece \xE8 il modello che a sua volta si \xE8 lamentato di non aver ricevuto l\u2019input corretto."),Zo=c(),D=l("p"),Yp=s("Perch\xE9 i dati non sono stati processati? Abbiamo usato il metodo "),es=l("code"),Jp=s("Dataset.map()"),Zp=s(" sui set di dati per applicare il tokenizer a ogni campione. Ma se si osserva attentamente il codice, si noter\xE0 che abbiamo commesso un errore nel passare i training set e il validation set ("),as=l("em"),Xp=s("insieme di valutazione"),ec=s(") al "),is=l("code"),ac=s("Trainer"),ic=s(". Qui invece di usare "),ts=l("code"),tc=s("tokenized_datasets"),sc=s(", abbiamo usato "),ss=l("code"),oc=s("raw_datasets"),rc=s(" \u{1F926}. Quindi correggiamo questo errore!"),Xo=c(),u(wa.$$.fragment),er=c(),Gi=l("p"),lc=s("Questo nuovo codice ora dar\xE0 un errore diverso (un miglioramento!):"),ar=c(),u(Ea.$$.fragment),ir=c(),Hi=l("p"),nc=s("Osservando il traceback, si nota che l\u2019errore si verifica nel punto in cui i dati vengono raccolti:"),tr=c(),u(ja.$$.fragment),sr=c(),Ri=l("p"),pc=s("Quindi, bisogna concentrarsi su questo. Prima di farlo, per\xF2, finiamo d\u2019ispezionare i nostri dati, per essere sicuri al 100% che siano corretti."),or=c(),Wi=l("p"),cc=s("Una cosa da fare sempre quando si esegue il debug di una sessione di addestramento \xE8 dare un\u2019occhiata agli input del modello decodificati. Non possiamo dare un senso ai numeri che gli diamo direttamente in pasto, quindi dobbiamo guardare cosa rappresentano quei numeri. Nella computer vision, ad esempio, ci\xF2 significa guardare le immagini decodificate dei pixel passati, nel campo del riconoscimento vocale significa ascoltare i campioni audio decodificati e per il nostro esempio di NLP significa usare il nostro tokenizer per decodificare gli input:"),rr=c(),u(ya.$$.fragment),lr=c(),u(Pa.$$.fragment),nr=c(),Bi=l("p"),dc=s("Questo sembra corretto. Si dovrebbe fare cos\xEC per tutte le chiavi degli input:"),pr=c(),u(Ca.$$.fragment),cr=c(),u(Aa.$$.fragment),dr=c(),x=l("p"),uc=s("Si noti che le chiavi che non corrispondono a input accettati dal modello saranno automaticamente scartate, quindi qui terremo solo "),os=l("code"),mc=s("input_ids"),fc=s(", "),rs=l("code"),hc=s("attention_mask"),bc=s(" e "),ls=l("code"),vc=s("label"),_c=s(" (che sar\xE0 rinominata "),ns=l("code"),gc=s("labels"),$c=s("). Per ricontrollare la firma del modello, si pu\xF2 stampare la classe del modello e poi controllare la sua documentazione:"),ur=c(),u(Da.$$.fragment),mr=c(),u(Ta.$$.fragment),fr=c(),J=l("p"),kc=s("Quindi, nel nostro caso, possiamo controllare i parametri accettati in "),Sa=l("a"),qc=s("questa pagina"),zc=s(". Il "),ps=l("code"),wc=s("Trainer"),Ec=s(" registrer\xE0 anche le colonne che sta scartando."),hr=c(),je=l("p"),jc=s("Abbiamo controllato che gli ID in ingresso siano corretti decodificandoli. Il prossimo passo \xE8 la "),cs=l("code"),yc=s("attention_mask"),Pc=s(":"),br=c(),u(xa.$$.fragment),vr=c(),u(Oa.$$.fragment),_r=c(),ye=l("p"),Cc=s("Poich\xE9 non abbiamo applicato il padding nel nostro preprocessing, questo sembra perfettamente naturale. Per essere sicuri che non ci siano problemi con la attention mask ("),ds=l("em"),Ac=s("maschera di attenzione"),Dc=s("), controlliamo che sia della stessa lunghezza dei nostri ID di input:"),gr=c(),u(Na.$$.fragment),$r=c(),u(Ua.$$.fragment),kr=c(),Vi=l("p"),Tc=s("Bene! Infine, controlliamo la nostra label:"),qr=c(),u(Ia.$$.fragment),zr=c(),u(La.$$.fragment),wr=c(),Z=l("p"),Sc=s("Come gli ID degli input, si tratta di un numero che non ha senso di per s\xE9. Come abbiamo visto prima, la mappa tra gli interi e i nomi delle label \xE8 memorizzata all\u2019interno dell\u2019attributo "),us=l("code"),xc=s("names"),Oc=s(" della corrispondente "),ms=l("em"),Nc=s("feature"),Uc=s(" del dataset:"),Er=c(),u(Qa.$$.fragment),jr=c(),u(Ma.$$.fragment),yr=c(),M=l("p"),Ic=s("Quindi "),fs=l("code"),Lc=s("1"),Qc=s(" significa "),hs=l("code"),Mc=s("neutral"),Fc=s(" ("),bs=l("em"),Gc=s("neutro"),Hc=s("), il che significa che le due frasi viste sopra non sono in contraddizione e che la prima non implica la seconda. Sembra corretto!"),Pr=c(),Pe=l("p"),Rc=s("Non abbiamo token type ID ("),vs=l("em"),Wc=s("ID del tipo di token"),Bc=s(") qui, perch\xE9 DistilBERT non li prevede; se li hai nel tuo modello, devi anche assicurarti che corrispondano correttamente alla posizione della prima e della seconda frase nell\u2019input."),Cr=c(),u(Ce.$$.fragment),Ar=c(),Ki=l("p"),Vc=s("In questo caso, il controllo viene effettuato solo sul training set, ma \xE8 necessario ricontrollare allo stesso modo anche il validation set e il test set."),Dr=c(),Yi=l("p"),Kc=s("Ora che sappiamo che i nostri set di dati sono corretti, \xE8 il momento di verificare la fase successiva della pipeline di addestramento."),Tr=c(),ce=l("h3"),Ae=l("a"),_s=l("span"),u(Fa.$$.fragment),Yc=c(),gs=l("span"),Jc=s("Dai dataset ai dataloader"),Sr=c(),O=l("p"),Zc=s("La prossima cosa che pu\xF2 andare storta nella pipeline di addestramento \xE8 quando il "),$s=l("code"),Xc=s("Trainer"),ed=s(" cerca di formare dei batch dal training o dal validation set. Una volta che si \xE8 sicuri che i set di dati del "),ks=l("code"),ad=s("Trainer"),id=s(" sono corretti, si pu\xF2 provare a formare manualmente un batch eseguendo quanto segue (sostituire "),qs=l("code"),td=s("train"),sd=s(" con "),zs=l("code"),od=s("eval"),rd=s(" per il dataloader di validazione):"),xr=c(),u(Ga.$$.fragment),Or=c(),De=l("p"),ld=s("Questo codice crea il training dataloader ("),ws=l("em"),nd=s("caricatore di dati di addestramento"),pd=s("), quindi lo itera, fermandosi alla prima iterazione. Se il codice viene eseguito senza errori, si ha il primo batch di addestramento che pu\xF2 essere ispezionato; se il codice d\xE0 errore, si sa con certezza che il problema \xE8 nel dataloader, come in questo caso:"),Nr=c(),u(Ha.$$.fragment),Ur=c(),X=l("p"),cd=s("L\u2019ispezione dell\u2019ultimo frame del traceback dovrebbe essere sufficiente a fornire un indizio, ma cerchiamo di scavare un po\u2019 pi\xF9 a fondo. La maggior parte dei problemi durante la creazione dei batch si verifica a causa del raggruppamento degli esempi in un singolo batch, quindi la prima cosa da controllare in caso di dubbio \xE8 quale "),Es=l("code"),dd=s("collate_fn"),ud=s(" il tuo "),js=l("code"),md=s("DataLoader"),fd=s(" sta usando:"),Ir=c(),u(Ra.$$.fragment),Lr=c(),u(Wa.$$.fragment),Qr=c(),F=l("p"),hd=s("\xC8 il "),ys=l("code"),bd=s("default_data_collator"),vd=s(", ma non \xE8 quello che vogliamo in questo caso. Vogliamo che i nostri esempi siano espansi fino ad essere come la frase pi\xF9 lunga del batch, cosa che viene fatta dal collettore "),Ps=l("code"),_d=s("DataCollatorWithPadding"),gd=s(". Questo collatore di dati dovrebbe essere usato di default da "),Cs=l("code"),$d=s("Trainer"),kd=s(", quindi perch\xE9 non viene usato qui?"),Mr=c(),G=l("p"),qd=s("La risposta \xE8 che non abbiamo passato il "),As=l("code"),zd=s("tokenizer"),wd=s(" al "),Ds=l("code"),Ed=s("Trainer"),jd=s(", quindi non ha potuto creare il "),Ts=l("code"),yd=s("DataCollatorWithPadding"),Pd=s(" che volevamo. In pratica, non si dovrebbe mai esitare a passare esplicitamente il collettore di dati che si vuole usare, per essere sicuri di evitare questo tipo di errori. Adattiamo il nostro codice per fare esattamente questo:"),Fr=c(),u(Ba.$$.fragment),Gr=c(),Ji=l("p"),Cd=s("La buona notizia? Non riceviamo pi\xF9 lo stesso errore di prima, il che \xE8 sicuramente un miglioramento. La cattiva notizia? Otteniamo invece un famigerato errore CUDA:"),Hr=c(),u(Va.$$.fragment),Rr=c(),Zi=l("p"),Ad=s("Questo \xE8 un male perch\xE9 gli errori di CUDA sono estremamente difficili da debuggare in generale. Vedremo tra poco come risolvere questo problema, ma prima terminiamo l\u2019analisi della creazione di batch."),Wr=c(),Xi=l("p"),Dd=s("Se siete sicuri che il tuo collettore di dati \xE8 quello giusto, dovresti provare ad applicarlo su un paio di campioni del tuo set di dati:"),Br=c(),u(Ka.$$.fragment),Vr=c(),N=l("p"),Td=s("Questo codice fallir\xE0 perch\xE9 il "),Ss=l("code"),Sd=s("train_dataset"),xd=s(" contiene colonne di tipo stringa, che il "),xs=l("code"),Od=s("Trainer"),Nd=s(" solitamente rimuove. \xC8 possibile rimuoverle manualmente o, se si vuole replicare esattamente ci\xF2 che il "),Os=l("code"),Ud=s("Trainer"),Id=s(" fa dietro le quinte, si pu\xF2 chiamare il metodo privato "),Ns=l("code"),Ld=s("Trainer._remove_unused_columns()"),Qd=s(" che fa questo:"),Kr=c(),u(Ya.$$.fragment),Yr=c(),et=l("p"),Md=s("Se l\u2019errore persiste, si potrebbe eseguire manualmente il debug di ci\xF2 che accade all\u2019interno del collettore di dati."),Jr=c(),at=l("p"),Fd=s("Ora che abbiamo eseguito il debug del processo di creazione del batch, \xE8 il momento di passarne uno attraverso il modello!"),Zr=c(),de=l("h3"),Te=l("a"),Us=l("span"),u(Ja.$$.fragment),Gd=c(),Is=l("span"),Hd=s("Passaggio attraverso il modello"),Xr=c(),it=l("p"),Rd=s("Dovrebbe essere possibile ottenere un batch eseguendo il seguente comando:"),el=c(),u(Za.$$.fragment),al=c(),Se=l("p"),Wd=s("Se si esegue questo codice in un notebook, \xE8 possibile che si verifichi un errore CUDA simile a quello visto in precedenza, nel qual caso \xE8 necessario riavviare il notebook e rieseguire l\u2019ultimo snippet senza la riga "),Ls=l("code"),Bd=s("trainer.train()"),Vd=s(". Questa \xE8 la seconda cosa pi\xF9 fastidiosa degli errori CUDA: rompono irrimediabilmente il kernel. La cosa pi\xF9 fastidiosa \xE8 che sono difficili da debuggare."),il=c(),ee=l("p"),Kd=s("Perch\xE9? Questo ha a che fare con il modo in cui funzionano le GPU. Sono estremamente efficienti nell\u2019eseguire molte operazioni in parallelo, ma l\u2019inconveniente \xE8 che quando una di queste istruzioni produce un errore, non lo si sa immediatamente. \xC8 solo quando il programma chiama una sincronizzazione dei processi multipli sulla GPU che esso si accorge che qualcosa \xE8 andato storto, quindi l\u2019errore viene effettivamente sollevato in un punto che non ha niente a che fare con ci\xF2 che lo ha creato. Per esempio, se guardiamo il nostro traceback precedente, l\u2019errore \xE8 stato sollevato durante il backward pass ("),Qs=l("em"),Yd=s("percorso discendente"),Jd=s("), ma vedremo tra un minuto che in realt\xE0 deriva da qualcosa nel forward pass ("),Ms=l("em"),Zd=s("percorso ascendente"),Xd=s(")."),tl=c(),tt=l("p"),eu=s("Come si fa a fare il debug di questi errori? La risposta \xE8 semplice: non lo facciamo. A meno che l\u2019errore CUDA non sia un errore out-of-memory (il che significa che la memoria della GPU non \xE8 sufficiente), si dovrebbe sempre tornare alla CPU per eseguire il debug."),sl=c(),xe=l("p"),au=s("Per fare questo nel nostro caso, dobbiamo semplicemente rimettere il modello sulla CPU e chiamarlo sul nostro batch \u2014 il batch restituito dal "),Fs=l("code"),iu=s("DataLoader"),tu=s(" non \xE8 ancora stato spostato sulla GPU:"),ol=c(),u(Xa.$$.fragment),rl=c(),u(ei.$$.fragment),ll=c(),ae=l("p"),su=s("Quindi, il quadro si fa pi\xF9 chiaro. Invece di avere un errore CUDA, ora abbiamo un "),Gs=l("code"),ou=s("IndexError"),ru=s(" nel calcolo della loss ("),Hs=l("em"),lu=s("funzione di perdita"),nu=s(") (quindi niente a che fare con il backward pass, come abbiamo detto prima). Pi\xF9 precisamente, possiamo vedere che \xE8 il target 2 a creare l\u2019errore, quindi questo \xE8 un ottimo momento per controllare il numero di label del nostro modello:"),nl=c(),u(ai.$$.fragment),pl=c(),u(ii.$$.fragment),cl=c(),st=l("p"),pu=s("Con due label, solo gli 0 e gli 1 sono ammessi come target, ma secondo il messaggio di errore abbiamo ottenuto un 2. Ottenere un 2 \xE8 in realt\xE0 normale: se ricordiamo i nomi delle etichette che abbiamo estratto in precedenza, ce n\u2019erano tre, quindi abbiamo gli indici 0, 1 e 2 nel nostro dataset. Il problema \xE8 che non l\u2019abbiamo detto al nostro modello, il quale si sarebbe dovuto creare con tre label. Quindi, risolviamo il problema!"),dl=c(),u(ti.$$.fragment),ul=c(),Oe=l("p"),cu=s("Non abbiamo ancora incluso la riga "),Rs=l("code"),du=s("trainer.train()"),uu=s(", per prendere tempo e verificare che tutto sia a posto. Se richiediamo un batch e lo passiamo al nostro modello, ora funziona senza errori!"),ml=c(),u(si.$$.fragment),fl=c(),ot=l("p"),mu=s("Il passo successivo consiste nel tornare a usare la GPU e verificare che tutto funzioni ancora:"),hl=c(),u(oi.$$.fragment),bl=c(),rt=l("p"),fu=s("Se si verifica ancora un errore, assicurarsi di riavviare il notebook ed eseguire solo l\u2019ultima versione dello script."),vl=c(),ue=l("h3"),Ne=l("a"),Ws=l("span"),u(ri.$$.fragment),hu=c(),Bs=l("span"),bu=s("Esecuzione di un passaggio di ottimizzazione"),_l=c(),lt=l("p"),vu=s("Ora che sappiamo che possiamo costruire batch che passano effettivamente attraverso il modello, siamo pronti per la fase successiva della pipeline di addestramento: calcolare i gradienti ed eseguire una fase di ottimizzazione."),gl=c(),Ue=l("p"),_u=s("La prima parte consiste nel richiamare il metodo "),Vs=l("code"),gu=s("backward()"),$u=s(" sulla loss:"),$l=c(),u(li.$$.fragment),kl=c(),nt=l("p"),ku=s("\xC8 abbastanza raro che si verifichi un errore in questa fase, ma se si verifica, assicurati di tornare ad usare la CPU per ottenere un messaggio di errore pi\xF9 utile."),ql=c(),ie=l("p"),qu=s("Per eseguire la fase di ottimizzazione, \xE8 sufficiente creare l\u2019oggetto "),Ks=l("code"),zu=s("optimizer"),wu=s(" e richiamare il suo metodo "),Ys=l("code"),Eu=s("step()"),ju=s(":"),zl=c(),u(ni.$$.fragment),wl=c(),Ie=l("p"),yu=s("Anche in questo caso, se si utilizza l\u2019ottimizzatore predefinito nel "),Js=l("code"),Pu=s("Trainer"),Cu=s(", non si dovrebbe ottenere un errore in questa fase, ma se hai un ottimizzatore personalizzato, potrebbero esserci dei problemi da risolvere. Non dimenticare di tornare alla CPU se ottieni uno strano errore CUDA in questa fase. A proposito di errori CUDA, prima abbiamo menzionato un caso speciale. Vediamo ora questo caso."),El=c(),me=l("h3"),Le=l("a"),Zs=l("span"),u(pi.$$.fragment),Au=c(),Xs=l("span"),Du=s("Come gestire gli errori out-of-memory di CUDA"),jl=c(),Qe=l("p"),Tu=s("Ogni volta che si riceve un messaggio di errore che inizia con "),eo=l("code"),Su=s("RuntimeError: CUDA out of memory"),xu=s(", indica che la memoria della GPU \xE8 esaurita. Questo errore non \xE8 direttamente collegato al codice e pu\xF2 verificarsi anche con uno script che funziona perfettamente. Questo errore significa che si \xE8 tentato di mettere troppe cose nella memoria interna della GPU e che si \xE8 verificato un errore. Come per altri errori di CUDA, \xE8 necessario riavviare il kernel per poter eseguire nuovamente l\u2019allenamento."),yl=c(),pt=l("p"),Ou=s("Per risolvere questo problema, \xE8 sufficiente utilizzare meno spazio sulla GPU, cosa che spesso \xE8 pi\xF9 facile a dirsi che a farsi. Per prima cosa, assicuratevi di non avere due modelli sulla GPU contemporaneamente (a meno che non sia necessario per il vostro problema, ovviamente). Poi, \xE8 probabile che si debba ridurre la dimensione del batch, in quanto influisce direttamente sulle dimensioni di tutti gli output intermedi del modello e dei loro gradienti. Se il problema persiste, si pu\xF2 considerare di utilizzare una versione pi\xF9 piccola del modello."),Pl=c(),u(Me.$$.fragment),Cl=c(),fe=l("h3"),Fe=l("a"),ao=l("span"),u(ci.$$.fragment),Nu=c(),io=l("span"),Uu=s("Valutazione del modello"),Al=c(),Ge=l("p"),Iu=s("Ora che abbiamo risolto tutti i problemi con il nostro codice, tutto \xE8 perfetto e l\u2019addestramento dovrebbe girare senza intoppi, giusto? Non cos\xEC veloce! Se si esegue il comando "),to=l("code"),Lu=s("trainer.train()"),Qu=s(", all\u2019inizio sembrer\xE0 tutto a posto, ma dopo un po\u2019 si otterr\xE0 il seguente risultato:"),Dl=c(),u(di.$$.fragment),Tl=c(),u(ui.$$.fragment),Sl=c(),ct=l("p"),Mu=s("Ti accorgerai che questo errore compare durante la fase di valutazione, quindi \xE8 l\u2019ultima cosa che dobbiamo debuggare."),xl=c(),He=l("p"),Fu=s("\xC8 possibile eseguire il ciclo di valutazione del "),so=l("code"),Gu=s("Trainer"),Hu=s(" indipendentemente dall\u2019addestramento, in questo modo:"),Ol=c(),u(mi.$$.fragment),Nl=c(),u(fi.$$.fragment),Ul=c(),u(Re.$$.fragment),Il=c(),dt=l("p"),Ru=s("Prima di tentare il debug di un problema nel ciclo di valutazione, \xE8 necessario assicurarsi di aver dato un\u2019occhiata ai dati, di essere in grado di generare correttamente un batch e di poter eseguire il modello su di esso. Abbiamo completato tutti questi passaggi, quindi il codice seguente pu\xF2 essere eseguito senza errori:"),Ll=c(),u(hi.$$.fragment),Ql=c(),ut=l("p"),Wu=s("L\u2019errore arriva pi\xF9 tardi, alla fine della fase di valutazione, e se guardiamo il traceback vediamo questo:"),Ml=c(),u(bi.$$.fragment),Fl=c(),te=l("p"),Bu=s("Questo ci dice che l\u2019errore ha origine nel modulo "),oo=l("code"),Vu=s("datasets/metric.py"),Ku=s(", quindi si tratta di un problema con la nostra funzione "),ro=l("code"),Yu=s("compute_metrics()"),Ju=s(". La funzione accetta una tupla con i logit e le label come array NumPy, quindi proviamo a dargliela in pasto:"),Gl=c(),u(vi.$$.fragment),Hl=c(),u(_i.$$.fragment),Rl=c(),H=l("p"),Zu=s("Otteniamo lo stesso errore, quindi il problema risiede sicuramente in quella funzione. Se guardiamo al suo codice, vediamo che sta solo trasferendo le "),lo=l("code"),Xu=s("predictions"),em=s(" e le "),no=l("code"),am=s("labels"),im=s(" a "),po=l("code"),tm=s("metric.compute()"),sm=s(". C\u2019\xE8 quindi un problema con questo metodo? Non proprio. Diamo una rapida occhiata alle dimensioni:"),Wl=c(),u(gi.$$.fragment),Bl=c(),u($i.$$.fragment),Vl=c(),We=l("p"),om=s("Le nostre previsioni sono ancora dei logit, non le vere previsioni, ed \xE8 per questo che la metrica restituisce questo errore (un po\u2019 oscuro). La soluzione \xE8 abbastanza semplice: basta aggiungere un argmax nella funzione "),co=l("code"),rm=s("compute_metrics()"),lm=s(":"),Kl=c(),u(ki.$$.fragment),Yl=c(),u(qi.$$.fragment),Jl=c(),mt=l("p"),nm=s("Ora il nostro errore \xE8 stato risolto! Questo era l\u2019ultimo, quindi il nostro script ora addestrer\xE0 correttamente un modello."),Zl=c(),ft=l("p"),pm=s("Per riferimento, ecco lo script completamente corretto:"),Xl=c(),u(zi.$$.fragment),en=c(),ht=l("p"),cm=s("In questo caso, non ci sono pi\xF9 problemi e il nostro script affiner\xE0 un modello che dovrebbe dare risultati ragionevoli. Ma cosa possiamo fare quando l\u2019addestramento procede senza errori e il modello addestrato non funziona affatto bene? Questa \xE8 la parte pi\xF9 difficile di machine learning e ti mostreremo alcune tecniche che possono aiutarti."),an=c(),u(Be.$$.fragment),tn=c(),he=l("h2"),Ve=l("a"),uo=l("span"),u(wi.$$.fragment),dm=c(),mo=l("span"),um=s("Debug degli errori silenziosi durante l'addestramento"),sn=c(),bt=l("p"),mm=s("Cosa possiamo fare per eseguire il debug di un addestramento che viene completato senza errori, ma che non produce buoni risultati? Qui ti daremo alcuni suggerimenti, ma sappi che questo tipo di debugging \xE8 la parte pi\xF9 difficile di machine learning e non esiste una soluzione magica."),on=c(),be=l("h3"),Ke=l("a"),fo=l("span"),u(Ei.$$.fragment),fm=c(),ho=l("span"),hm=s("Controllare i dati (di nuovo!)"),rn=c(),vt=l("p"),bm=s("Il tuo modello imparer\xE0 qualcosa solo se \xE8 effettivamente possibile imparare qualcosa dai tuoi dati. Se c\u2019\xE8 un bug che corrompe i dati o le label sono assegnate in modo casuale, \xE8 molto probabile che non si riesca ad addestrare il modello sul dataset. Quindi, inizia sempre con un doppio controllo degli input e delle label decodificate e poniti le seguenti domande:"),ln=c(),R=l("ul"),bo=l("li"),vm=s("I dati decodificati sono comprensibili?"),_m=c(),vo=l("li"),gm=s("Sei d\u2019accordo con le label?"),$m=c(),_o=l("li"),km=s("C\u2019\xE8 una label pi\xF9 comune delle altre?"),qm=c(),go=l("li"),zm=s("Quale dovrebbe essere la funzione di perdita/metrica se il modello predicesse una risposta a caso/sempre la stessa risposta?"),nn=c(),u(Ye.$$.fragment),pn=c(),Je=l("p"),wm=s("Dopo aver esaminato i dati, esamina alcune previsioni del modello e decodificale. Se il modello prevede sempre la stessa cosa, potrebbe essere perch\xE9 il tuo set di dati \xE8 influenzato verso una categoria (per i problemi di classificazione); tecniche come fare oversampling ("),$o=l("em"),Em=s("sovra-campionamento"),jm=s(") delle classi rare potrebbero aiutare."),cn=c(),_t=l("p"),ym=s("Se la funzione di perdita/metrica ottenuta con il tuo modello iniziale \xE8 molto diversa da quella che ci si aspetterebbe per le previsioni casuali, ricontrolla il modo in cui viene calcolata la funzione o la metrica, perch\xE9 probabilmente c\u2019\xE8 un bug. Se si utilizzano diverse funzioni che aggiungi alla fine, assicurati che siano della stessa grandezza."),dn=c(),gt=l("p"),Pm=s("Quando sei sicuro/a che i dati sono perfetti, puoi verificare se il modello \xE8 in grado di addestrarsi su di essi con un semplice test."),un=c(),ve=l("h3"),Ze=l("a"),ko=l("span"),u(ji.$$.fragment),Cm=c(),qo=l("span"),Am=s("Fare overfitting del modello su un batch"),mn=c(),Xe=l("p"),Dm=s("L\u2019overfitting \xE8 di solito qualcosa che cerchiamo di evitare durante l\u2019addestramento, poich\xE9 significa che il modello non sta imparando a riconoscere le propriet\xE0 generali che vogliamo, ma sta invece memorizzando i campioni di addestramento. Tuttavia, provare ad addestrare il modello su un batch pi\xF9 e pi\xF9 volte \xE8 un buon test per verificare se il problema cos\xEC come \xE8 stato inquadrato pu\xF2 essere risolto dal modello che si sta cercando di addestrare. Inoltre, ti aiuter\xE0 a capire se il learning rate ("),zo=l("em"),Tm=s("tasso di apprendimento"),Sm=s(") iniziale \xE8 troppo alta."),fn=c(),ea=l("p"),xm=s("Una volta definito il "),wo=l("code"),Om=s("Trainer"),Nm=s(", \xE8 molto semplice: basta prendere un batch dal training set, ed eseguire un piccolo ciclo di addestramento manuale utilizzando solo quel batch per qualcosa come 20 step:"),hn=c(),u(yi.$$.fragment),bn=c(),u(aa.$$.fragment),vn=c(),ia=l("p"),Um=s("Il modello risultante dovrebbe avere risultati quasi perfetti sullo stesso "),Eo=l("code"),Im=s("batch"),Lm=s(". Calcoliamo la metrica sulle previsioni risultanti:"),_n=c(),u(Pi.$$.fragment),gn=c(),u(Ci.$$.fragment),$n=c(),$t=l("p"),Qm=s("100% di accuratezza, questo \xE8 un bell\u2019esempio di overfitting (il che significa che se provi il tuo modello su qualsiasi altra frase, molto probabilmente ti dar\xE0 una risposta sbagliata)!"),kn=c(),kt=l("p"),Mm=s("Se non si riesci a far s\xEC che il modello ottenga risultati perfetti come questo, significa che c\u2019\xE8 qualcosa di sbagliato nel modo in cui si \xE8 impostato il problema o con i dati, e quindi dovresti risolvere questa cosa. Solo quando riesci a superare il test di overfitting puoi essere sicuro/a che il tuo modello possa effettivamente imparare qualcosa."),qn=c(),u(ta.$$.fragment),zn=c(),_e=l("h3"),sa=l("a"),jo=l("span"),u(Ai.$$.fragment),Fm=c(),yo=l("span"),Gm=s("Non calibrare niente prima di avere una prima baseline"),wn=c(),W=l("p"),Hm=s("Hyperparameter tuning ("),Po=l("em"),Rm=s("calibrazione degli iperparametri"),Wm=s(") \xE8 sempre considerato come la parte pi\xF9 difficile di machine learning, ma \xE8 solo l\u2019ultimo passo per aiutarti a migliorare un po\u2019 la metrica. Nella maggior parte dei casi, gli iperparametri predefiniti del "),Co=l("code"),Bm=s("Trainer"),Vm=s(" funzionano bene per dare buoni risultati, quindi non ci si deve lanciare in una ricerca di iperparametri dispendiosa in termini di tempo e di costi, finch\xE9 non si \xE8 ottenuto qualcosa che batta la baseline ("),Ao=l("em"),Km=s("base di partenza"),Ym=s(") che si ha sul dataset."),En=c(),qt=l("p"),Jm=s("Una volta ottenuto un modello sufficientemente buono, si pu\xF2 iniziare a modificarlo un po\u2019. Non provare a eseguire l\u2019addestramento un migliaio di volte con iperparametri diversi, ma confronta un paio di esecuzioni che hanno valori diversi per un iperparametro cos\xEC da avere un\u2019idea di quale abbia il maggiore impatto."),jn=c(),zt=l("p"),Zm=s("Se stai modificando il modello stesso, mantieni le cose semplici e non provare nulla che non possa essere ragionevolmente giustificato. Assicurati sempre di rifare il test di overfitting per verificare che la modifica non abbia avuto conseguenze indesiderate."),yn=c(),ge=l("h3"),oa=l("a"),Do=l("span"),u(Di.$$.fragment),Xm=c(),To=l("span"),ef=s("Chiedere aiuto"),Pn=c(),ra=l("p"),af=s("Speriamo che in questa sezione tu abbia trovato qualche consiglio utile a risolvere il tuo problema, ma se cos\xEC non fosse, ricordati che puoi sempre chiedere aiuto alla community nei "),Ti=l("a"),tf=s("forum"),sf=s("."),Cn=c(),wt=l("p"),of=s("Qui di seguito sono riportate alcune risorse aggiuntive che potrebbero rivelarsi utili:"),An=c(),B=l("ul"),Et=l("li"),Si=l("a"),rf=s("\u201CReproducibility as a vehicle for engineering best practices\u201D"),lf=s(" di Joel Grus"),nf=c(),jt=l("li"),xi=l("a"),pf=s("\u201CChecklist for debugging neural networks\u201D"),cf=s(" di Cecelia Shao"),df=c(),yt=l("li"),Oi=l("a"),uf=s("\u201CHow to unit test machine learning code\u201D"),mf=s(" di Chase Roberts"),ff=c(),Pt=l("li"),Ni=l("a"),hf=s("\u201CA Recipe for Training Neural Networks\u201D"),bf=s(" di Andrej Karpathy"),Dn=c(),Ct=l("p"),vf=s("Naturalmente, non tutti i problemi che incontrerai durante l\u2019addestramento delle reti neurali sono colpa tua! Se si incontra qualcosa nella libreria \u{1F917} Transformers o \u{1F917} Datasets che non sembra corretto, \xE8 possibile che si sia trovato un bug. Dovresti assolutamente segnalarcelo e nella prossima sezione ti spiegheremo esattamente come fare."),this.h()},l(e){const t=kv('[data-svelte="svelte-1phssyn"]',document.head);_=n(t,"META",{name:!0,content:!0}),t.forEach(a),z=d(e),m(k.$$.fragment,e),q=d(e),j=n(e,"H1",{class:!0});var Ui=p(j);w=n(Ui,"A",{id:!0,class:!0,href:!0});var So=p(w);E=n(So,"SPAN",{});var xo=p(E);m(y.$$.fragment,xo),xo.forEach(a),So.forEach(a),T=d(Ui),P=n(Ui,"SPAN",{});var Oo=p(P);K=o(Oo,"Fare il debug della training pipeline"),Oo.forEach(a),Ui.forEach(a),U=d(e),m(C.$$.fragment,e),ha=d(e),I=n(e,"P",{});var $e=p(I);Qi=o($e,"Hai scritto un bello script per addestrare o affinare un modello su un determinato compito, seguendo scrupolosamente i consigli del "),Mi=n($e,"A",{href:!0});var No=p(Mi);dp=o(No,"Capitolo 7"),No.forEach(a),up=o($e,". Ma quando lanci il comando "),It=n($e,"CODE",{});var Uo=p(It);mp=o(Uo,"trainer.train()"),Uo.forEach(a),fp=o($e,", succede qualcosa di orribile: si ottiene un errore \u{1F631}! O peggio, tutto sembra andare bene e il training viene eseguito senza errori, ma il modello che ne risulta fa schifo. In questa sezione mostreremo cosa \xE8 possibile fare per eseguire il debug di questo tipo di problemi."),$e.forEach(a),Io=d(e),ne=n(e,"H2",{class:!0});var Ii=p(ne);ke=n(Ii,"A",{id:!0,class:!0,href:!0});var qf=p(ke);Lt=n(qf,"SPAN",{});var zf=p(Lt);m(ba.$$.fragment,zf),zf.forEach(a),qf.forEach(a),hp=d(Ii),Qt=n(Ii,"SPAN",{});var wf=p(Qt);bp=o(wf,"Fare il debug della training pipeline"),wf.forEach(a),Ii.forEach(a),Lo=d(e),m(va.$$.fragment,e),Qo=d(e),S=n(e,"P",{});var se=p(S);vp=o(se,"Il problema quando si ha un errore da "),Mt=n(se,"CODE",{});var Ef=p(Mt);_p=o(Ef,"trainer.train()"),Ef.forEach(a),gp=o(se," \xE8 che potrebbe provenire da pi\xF9 fonti, poich\xE9 il "),Ft=n(se,"CODE",{});var jf=p(Ft);$p=o(jf,"Trainer"),jf.forEach(a),kp=o(se," di solito mette insieme molte cose. Converte i dataset in "),Gt=n(se,"EM",{});var yf=p(Gt);qp=o(yf,"dataloader"),yf.forEach(a),zp=o(se,", quindi l\u2019errore potrebbe essere dato da qualcosa di sbagliato nel dataset stesso, o da un problema qualche problema nel provare a raggruppare in un batch elementi del dataset. Poi prende un batch di dati e lo invia al modello, quindi il problema potrebbe anche essere nel codice del modello. Successivamente, calcola i gradienti ed esegue la fase di ottimizzazione, quindi il problema potrebbe essere nel tuo "),Ht=n(se,"EM",{});var Pf=p(Ht);wp=o(Pf,"optimizer"),Pf.forEach(a),Ep=o(se,". E anche se tutto va bene per il training, qualcosa potrebbe andare storto durante la valutazione se c\u2019\xE8 un problema con la metrica selezionata."),se.forEach(a),Mo=d(e),qe=n(e,"P",{});var Sn=p(qe);jp=o(Sn,"Il modo migliore per eseguire il debug di un errore che si verifica in "),Rt=n(Sn,"CODE",{});var Cf=p(Rt);yp=o(Cf,"trainer.train()"),Cf.forEach(a),Pp=o(Sn," \xE8 quello di esaminare manualmente l\u2019intera pipeline per vedere dove le cose sono andate storte. L\u2019errore \xE8 spesso molto facile da risolvere."),Sn.forEach(a),Fo=d(e),ze=n(e,"P",{});var xn=p(ze);Cp=o(xn,"Per dimostrarlo, useremo il seguente script che ha lo scopo di affinare un modello DistilBERT sul "),_a=n(xn,"A",{href:!0,rel:!0});var Af=p(_a);Ap=o(Af,"dataset MNLI"),Af.forEach(a),Dp=o(xn,":"),xn.forEach(a),Go=d(e),m(ga.$$.fragment,e),Ho=d(e),Fi=n(e,"P",{});var Df=p(Fi);Tp=o(Df,"Se provi a eseguirlo, otterrai un errore piuttosto criptico:"),Df.forEach(a),Ro=d(e),m($a.$$.fragment,e),Wo=d(e),pe=n(e,"H3",{class:!0});var On=p(pe);we=n(On,"A",{id:!0,class:!0,href:!0});var Tf=p(we);Wt=n(Tf,"SPAN",{});var Sf=p(Wt);m(ka.$$.fragment,Sf),Sf.forEach(a),Tf.forEach(a),Sp=d(On),Bt=n(On,"SPAN",{});var xf=p(Bt);xp=o(xf,"Controlla i dati"),xf.forEach(a),On.forEach(a),Bo=d(e),Y=n(e,"P",{});var At=p(Y);Op=o(At,"Non c\u2019\xE8 bisogno di dirlo, ma se i dati sono corrotti, il "),Vt=n(At,"CODE",{});var Of=p(Vt);Np=o(Of,"Trainer"),Of.forEach(a),Up=o(At," non sar\xE0 in grado di formare i batch e tanto meno di addestrare il modello. Quindi, per prima cosa, \xE8 necessario dare un\u2019occhiata a cosa c\u2019\xE8 nel training set("),Kt=n(At,"EM",{});var Nf=p(Kt);Ip=o(Nf,"insieme di addestramento"),Nf.forEach(a),Lp=o(At,")."),At.forEach(a),Vo=d(e),Ee=n(e,"P",{});var Nn=p(Ee);Qp=o(Nn,"Per evitare di passare infinite ore a cercare di risolvere qualcosa che non \xE8 la fonte del bug, consigliamo di usare "),Yt=n(Nn,"CODE",{});var Uf=p(Yt);Mp=o(Uf,"trainer.train_dataset"),Uf.forEach(a),Fp=o(Nn," per controllare l\u2019insieme di dati e nient\u2019altro. Quindi facciamo cos\xEC:"),Nn.forEach(a),Ko=d(e),m(qa.$$.fragment,e),Yo=d(e),m(za.$$.fragment,e),Jo=d(e),Q=n(e,"P",{});var la=p(Q);Gp=o(la,"Hai notato qualcosa di sbagliato? Questo, insieme al messaggio di errore sulla mancanza di "),Jt=n(la,"CODE",{});var If=p(Jt);Hp=o(If,"input_ids"),If.forEach(a),Rp=o(la,", dovrebbe farci capire che qui abbiamo testo, non numeri che invece il modello pu\xF2 interpretare. In questo caso, l\u2019errore originale \xE8 molto fuorviante, perch\xE9 il "),Zt=n(la,"CODE",{});var Lf=p(Zt);Wp=o(Lf,"Trainer"),Lf.forEach(a),Bp=o(la," rimuove automaticamente le colonne che non corrispondono alla firma del modello (cio\xE8 i parametri che il modello si aspetta). Ci\xF2 significa che in questo caso tutto, a parte "),Xt=n(la,"EM",{});var Qf=p(Xt);Vp=o(Qf,"label"),Qf.forEach(a),Kp=o(la,", \xE8 stato scartato. Non c\u2019\xE8 stato quindi nessun problema nel creare i batch di dati e poi inviarli al modello, invece \xE8 il modello che a sua volta si \xE8 lamentato di non aver ricevuto l\u2019input corretto."),la.forEach(a),Zo=d(e),D=n(e,"P",{});var V=p(D);Yp=o(V,"Perch\xE9 i dati non sono stati processati? Abbiamo usato il metodo "),es=n(V,"CODE",{});var Mf=p(es);Jp=o(Mf,"Dataset.map()"),Mf.forEach(a),Zp=o(V," sui set di dati per applicare il tokenizer a ogni campione. Ma se si osserva attentamente il codice, si noter\xE0 che abbiamo commesso un errore nel passare i training set e il validation set ("),as=n(V,"EM",{});var Ff=p(as);Xp=o(Ff,"insieme di valutazione"),Ff.forEach(a),ec=o(V,") al "),is=n(V,"CODE",{});var Gf=p(is);ac=o(Gf,"Trainer"),Gf.forEach(a),ic=o(V,". Qui invece di usare "),ts=n(V,"CODE",{});var Hf=p(ts);tc=o(Hf,"tokenized_datasets"),Hf.forEach(a),sc=o(V,", abbiamo usato "),ss=n(V,"CODE",{});var Rf=p(ss);oc=o(Rf,"raw_datasets"),Rf.forEach(a),rc=o(V," \u{1F926}. Quindi correggiamo questo errore!"),V.forEach(a),Xo=d(e),m(wa.$$.fragment,e),er=d(e),Gi=n(e,"P",{});var Wf=p(Gi);lc=o(Wf,"Questo nuovo codice ora dar\xE0 un errore diverso (un miglioramento!):"),Wf.forEach(a),ar=d(e),m(Ea.$$.fragment,e),ir=d(e),Hi=n(e,"P",{});var Bf=p(Hi);nc=o(Bf,"Osservando il traceback, si nota che l\u2019errore si verifica nel punto in cui i dati vengono raccolti:"),Bf.forEach(a),tr=d(e),m(ja.$$.fragment,e),sr=d(e),Ri=n(e,"P",{});var Vf=p(Ri);pc=o(Vf,"Quindi, bisogna concentrarsi su questo. Prima di farlo, per\xF2, finiamo d\u2019ispezionare i nostri dati, per essere sicuri al 100% che siano corretti."),Vf.forEach(a),or=d(e),Wi=n(e,"P",{});var Kf=p(Wi);cc=o(Kf,"Una cosa da fare sempre quando si esegue il debug di una sessione di addestramento \xE8 dare un\u2019occhiata agli input del modello decodificati. Non possiamo dare un senso ai numeri che gli diamo direttamente in pasto, quindi dobbiamo guardare cosa rappresentano quei numeri. Nella computer vision, ad esempio, ci\xF2 significa guardare le immagini decodificate dei pixel passati, nel campo del riconoscimento vocale significa ascoltare i campioni audio decodificati e per il nostro esempio di NLP significa usare il nostro tokenizer per decodificare gli input:"),Kf.forEach(a),rr=d(e),m(ya.$$.fragment,e),lr=d(e),m(Pa.$$.fragment,e),nr=d(e),Bi=n(e,"P",{});var Yf=p(Bi);dc=o(Yf,"Questo sembra corretto. Si dovrebbe fare cos\xEC per tutte le chiavi degli input:"),Yf.forEach(a),pr=d(e),m(Ca.$$.fragment,e),cr=d(e),m(Aa.$$.fragment,e),dr=d(e),x=n(e,"P",{});var oe=p(x);uc=o(oe,"Si noti che le chiavi che non corrispondono a input accettati dal modello saranno automaticamente scartate, quindi qui terremo solo "),os=n(oe,"CODE",{});var Jf=p(os);mc=o(Jf,"input_ids"),Jf.forEach(a),fc=o(oe,", "),rs=n(oe,"CODE",{});var Zf=p(rs);hc=o(Zf,"attention_mask"),Zf.forEach(a),bc=o(oe," e "),ls=n(oe,"CODE",{});var Xf=p(ls);vc=o(Xf,"label"),Xf.forEach(a),_c=o(oe," (che sar\xE0 rinominata "),ns=n(oe,"CODE",{});var eh=p(ns);gc=o(eh,"labels"),eh.forEach(a),$c=o(oe,"). Per ricontrollare la firma del modello, si pu\xF2 stampare la classe del modello e poi controllare la sua documentazione:"),oe.forEach(a),ur=d(e),m(Da.$$.fragment,e),mr=d(e),m(Ta.$$.fragment,e),fr=d(e),J=n(e,"P",{});var Dt=p(J);kc=o(Dt,"Quindi, nel nostro caso, possiamo controllare i parametri accettati in "),Sa=n(Dt,"A",{href:!0,rel:!0});var ah=p(Sa);qc=o(ah,"questa pagina"),ah.forEach(a),zc=o(Dt,". Il "),ps=n(Dt,"CODE",{});var ih=p(ps);wc=o(ih,"Trainer"),ih.forEach(a),Ec=o(Dt," registrer\xE0 anche le colonne che sta scartando."),Dt.forEach(a),hr=d(e),je=n(e,"P",{});var Un=p(je);jc=o(Un,"Abbiamo controllato che gli ID in ingresso siano corretti decodificandoli. Il prossimo passo \xE8 la "),cs=n(Un,"CODE",{});var th=p(cs);yc=o(th,"attention_mask"),th.forEach(a),Pc=o(Un,":"),Un.forEach(a),br=d(e),m(xa.$$.fragment,e),vr=d(e),m(Oa.$$.fragment,e),_r=d(e),ye=n(e,"P",{});var In=p(ye);Cc=o(In,"Poich\xE9 non abbiamo applicato il padding nel nostro preprocessing, questo sembra perfettamente naturale. Per essere sicuri che non ci siano problemi con la attention mask ("),ds=n(In,"EM",{});var sh=p(ds);Ac=o(sh,"maschera di attenzione"),sh.forEach(a),Dc=o(In,"), controlliamo che sia della stessa lunghezza dei nostri ID di input:"),In.forEach(a),gr=d(e),m(Na.$$.fragment,e),$r=d(e),m(Ua.$$.fragment,e),kr=d(e),Vi=n(e,"P",{});var oh=p(Vi);Tc=o(oh,"Bene! Infine, controlliamo la nostra label:"),oh.forEach(a),qr=d(e),m(Ia.$$.fragment,e),zr=d(e),m(La.$$.fragment,e),wr=d(e),Z=n(e,"P",{});var Tt=p(Z);Sc=o(Tt,"Come gli ID degli input, si tratta di un numero che non ha senso di per s\xE9. Come abbiamo visto prima, la mappa tra gli interi e i nomi delle label \xE8 memorizzata all\u2019interno dell\u2019attributo "),us=n(Tt,"CODE",{});var rh=p(us);xc=o(rh,"names"),rh.forEach(a),Oc=o(Tt," della corrispondente "),ms=n(Tt,"EM",{});var lh=p(ms);Nc=o(lh,"feature"),lh.forEach(a),Uc=o(Tt," del dataset:"),Tt.forEach(a),Er=d(e),m(Qa.$$.fragment,e),jr=d(e),m(Ma.$$.fragment,e),yr=d(e),M=n(e,"P",{});var na=p(M);Ic=o(na,"Quindi "),fs=n(na,"CODE",{});var nh=p(fs);Lc=o(nh,"1"),nh.forEach(a),Qc=o(na," significa "),hs=n(na,"CODE",{});var ph=p(hs);Mc=o(ph,"neutral"),ph.forEach(a),Fc=o(na," ("),bs=n(na,"EM",{});var ch=p(bs);Gc=o(ch,"neutro"),ch.forEach(a),Hc=o(na,"), il che significa che le due frasi viste sopra non sono in contraddizione e che la prima non implica la seconda. Sembra corretto!"),na.forEach(a),Pr=d(e),Pe=n(e,"P",{});var Ln=p(Pe);Rc=o(Ln,"Non abbiamo token type ID ("),vs=n(Ln,"EM",{});var dh=p(vs);Wc=o(dh,"ID del tipo di token"),dh.forEach(a),Bc=o(Ln,") qui, perch\xE9 DistilBERT non li prevede; se li hai nel tuo modello, devi anche assicurarti che corrispondano correttamente alla posizione della prima e della seconda frase nell\u2019input."),Ln.forEach(a),Cr=d(e),m(Ce.$$.fragment,e),Ar=d(e),Ki=n(e,"P",{});var uh=p(Ki);Vc=o(uh,"In questo caso, il controllo viene effettuato solo sul training set, ma \xE8 necessario ricontrollare allo stesso modo anche il validation set e il test set."),uh.forEach(a),Dr=d(e),Yi=n(e,"P",{});var mh=p(Yi);Kc=o(mh,"Ora che sappiamo che i nostri set di dati sono corretti, \xE8 il momento di verificare la fase successiva della pipeline di addestramento."),mh.forEach(a),Tr=d(e),ce=n(e,"H3",{class:!0});var Qn=p(ce);Ae=n(Qn,"A",{id:!0,class:!0,href:!0});var fh=p(Ae);_s=n(fh,"SPAN",{});var hh=p(_s);m(Fa.$$.fragment,hh),hh.forEach(a),fh.forEach(a),Yc=d(Qn),gs=n(Qn,"SPAN",{});var bh=p(gs);Jc=o(bh,"Dai dataset ai dataloader"),bh.forEach(a),Qn.forEach(a),Sr=d(e),O=n(e,"P",{});var re=p(O);Zc=o(re,"La prossima cosa che pu\xF2 andare storta nella pipeline di addestramento \xE8 quando il "),$s=n(re,"CODE",{});var vh=p($s);Xc=o(vh,"Trainer"),vh.forEach(a),ed=o(re," cerca di formare dei batch dal training o dal validation set. Una volta che si \xE8 sicuri che i set di dati del "),ks=n(re,"CODE",{});var _h=p(ks);ad=o(_h,"Trainer"),_h.forEach(a),id=o(re," sono corretti, si pu\xF2 provare a formare manualmente un batch eseguendo quanto segue (sostituire "),qs=n(re,"CODE",{});var gh=p(qs);td=o(gh,"train"),gh.forEach(a),sd=o(re," con "),zs=n(re,"CODE",{});var $h=p(zs);od=o($h,"eval"),$h.forEach(a),rd=o(re," per il dataloader di validazione):"),re.forEach(a),xr=d(e),m(Ga.$$.fragment,e),Or=d(e),De=n(e,"P",{});var Mn=p(De);ld=o(Mn,"Questo codice crea il training dataloader ("),ws=n(Mn,"EM",{});var kh=p(ws);nd=o(kh,"caricatore di dati di addestramento"),kh.forEach(a),pd=o(Mn,"), quindi lo itera, fermandosi alla prima iterazione. Se il codice viene eseguito senza errori, si ha il primo batch di addestramento che pu\xF2 essere ispezionato; se il codice d\xE0 errore, si sa con certezza che il problema \xE8 nel dataloader, come in questo caso:"),Mn.forEach(a),Nr=d(e),m(Ha.$$.fragment,e),Ur=d(e),X=n(e,"P",{});var St=p(X);cd=o(St,"L\u2019ispezione dell\u2019ultimo frame del traceback dovrebbe essere sufficiente a fornire un indizio, ma cerchiamo di scavare un po\u2019 pi\xF9 a fondo. La maggior parte dei problemi durante la creazione dei batch si verifica a causa del raggruppamento degli esempi in un singolo batch, quindi la prima cosa da controllare in caso di dubbio \xE8 quale "),Es=n(St,"CODE",{});var qh=p(Es);dd=o(qh,"collate_fn"),qh.forEach(a),ud=o(St," il tuo "),js=n(St,"CODE",{});var zh=p(js);md=o(zh,"DataLoader"),zh.forEach(a),fd=o(St," sta usando:"),St.forEach(a),Ir=d(e),m(Ra.$$.fragment,e),Lr=d(e),m(Wa.$$.fragment,e),Qr=d(e),F=n(e,"P",{});var pa=p(F);hd=o(pa,"\xC8 il "),ys=n(pa,"CODE",{});var wh=p(ys);bd=o(wh,"default_data_collator"),wh.forEach(a),vd=o(pa,", ma non \xE8 quello che vogliamo in questo caso. Vogliamo che i nostri esempi siano espansi fino ad essere come la frase pi\xF9 lunga del batch, cosa che viene fatta dal collettore "),Ps=n(pa,"CODE",{});var Eh=p(Ps);_d=o(Eh,"DataCollatorWithPadding"),Eh.forEach(a),gd=o(pa,". Questo collatore di dati dovrebbe essere usato di default da "),Cs=n(pa,"CODE",{});var jh=p(Cs);$d=o(jh,"Trainer"),jh.forEach(a),kd=o(pa,", quindi perch\xE9 non viene usato qui?"),pa.forEach(a),Mr=d(e),G=n(e,"P",{});var ca=p(G);qd=o(ca,"La risposta \xE8 che non abbiamo passato il "),As=n(ca,"CODE",{});var yh=p(As);zd=o(yh,"tokenizer"),yh.forEach(a),wd=o(ca," al "),Ds=n(ca,"CODE",{});var Ph=p(Ds);Ed=o(Ph,"Trainer"),Ph.forEach(a),jd=o(ca,", quindi non ha potuto creare il "),Ts=n(ca,"CODE",{});var Ch=p(Ts);yd=o(Ch,"DataCollatorWithPadding"),Ch.forEach(a),Pd=o(ca," che volevamo. In pratica, non si dovrebbe mai esitare a passare esplicitamente il collettore di dati che si vuole usare, per essere sicuri di evitare questo tipo di errori. Adattiamo il nostro codice per fare esattamente questo:"),ca.forEach(a),Fr=d(e),m(Ba.$$.fragment,e),Gr=d(e),Ji=n(e,"P",{});var Ah=p(Ji);Cd=o(Ah,"La buona notizia? Non riceviamo pi\xF9 lo stesso errore di prima, il che \xE8 sicuramente un miglioramento. La cattiva notizia? Otteniamo invece un famigerato errore CUDA:"),Ah.forEach(a),Hr=d(e),m(Va.$$.fragment,e),Rr=d(e),Zi=n(e,"P",{});var Dh=p(Zi);Ad=o(Dh,"Questo \xE8 un male perch\xE9 gli errori di CUDA sono estremamente difficili da debuggare in generale. Vedremo tra poco come risolvere questo problema, ma prima terminiamo l\u2019analisi della creazione di batch."),Dh.forEach(a),Wr=d(e),Xi=n(e,"P",{});var Th=p(Xi);Dd=o(Th,"Se siete sicuri che il tuo collettore di dati \xE8 quello giusto, dovresti provare ad applicarlo su un paio di campioni del tuo set di dati:"),Th.forEach(a),Br=d(e),m(Ka.$$.fragment,e),Vr=d(e),N=n(e,"P",{});var le=p(N);Td=o(le,"Questo codice fallir\xE0 perch\xE9 il "),Ss=n(le,"CODE",{});var Sh=p(Ss);Sd=o(Sh,"train_dataset"),Sh.forEach(a),xd=o(le," contiene colonne di tipo stringa, che il "),xs=n(le,"CODE",{});var xh=p(xs);Od=o(xh,"Trainer"),xh.forEach(a),Nd=o(le," solitamente rimuove. \xC8 possibile rimuoverle manualmente o, se si vuole replicare esattamente ci\xF2 che il "),Os=n(le,"CODE",{});var Oh=p(Os);Ud=o(Oh,"Trainer"),Oh.forEach(a),Id=o(le," fa dietro le quinte, si pu\xF2 chiamare il metodo privato "),Ns=n(le,"CODE",{});var Nh=p(Ns);Ld=o(Nh,"Trainer._remove_unused_columns()"),Nh.forEach(a),Qd=o(le," che fa questo:"),le.forEach(a),Kr=d(e),m(Ya.$$.fragment,e),Yr=d(e),et=n(e,"P",{});var Uh=p(et);Md=o(Uh,"Se l\u2019errore persiste, si potrebbe eseguire manualmente il debug di ci\xF2 che accade all\u2019interno del collettore di dati."),Uh.forEach(a),Jr=d(e),at=n(e,"P",{});var Ih=p(at);Fd=o(Ih,"Ora che abbiamo eseguito il debug del processo di creazione del batch, \xE8 il momento di passarne uno attraverso il modello!"),Ih.forEach(a),Zr=d(e),de=n(e,"H3",{class:!0});var Fn=p(de);Te=n(Fn,"A",{id:!0,class:!0,href:!0});var Lh=p(Te);Us=n(Lh,"SPAN",{});var Qh=p(Us);m(Ja.$$.fragment,Qh),Qh.forEach(a),Lh.forEach(a),Gd=d(Fn),Is=n(Fn,"SPAN",{});var Mh=p(Is);Hd=o(Mh,"Passaggio attraverso il modello"),Mh.forEach(a),Fn.forEach(a),Xr=d(e),it=n(e,"P",{});var Fh=p(it);Rd=o(Fh,"Dovrebbe essere possibile ottenere un batch eseguendo il seguente comando:"),Fh.forEach(a),el=d(e),m(Za.$$.fragment,e),al=d(e),Se=n(e,"P",{});var Gn=p(Se);Wd=o(Gn,"Se si esegue questo codice in un notebook, \xE8 possibile che si verifichi un errore CUDA simile a quello visto in precedenza, nel qual caso \xE8 necessario riavviare il notebook e rieseguire l\u2019ultimo snippet senza la riga "),Ls=n(Gn,"CODE",{});var Gh=p(Ls);Bd=o(Gh,"trainer.train()"),Gh.forEach(a),Vd=o(Gn,". Questa \xE8 la seconda cosa pi\xF9 fastidiosa degli errori CUDA: rompono irrimediabilmente il kernel. La cosa pi\xF9 fastidiosa \xE8 che sono difficili da debuggare."),Gn.forEach(a),il=d(e),ee=n(e,"P",{});var xt=p(ee);Kd=o(xt,"Perch\xE9? Questo ha a che fare con il modo in cui funzionano le GPU. Sono estremamente efficienti nell\u2019eseguire molte operazioni in parallelo, ma l\u2019inconveniente \xE8 che quando una di queste istruzioni produce un errore, non lo si sa immediatamente. \xC8 solo quando il programma chiama una sincronizzazione dei processi multipli sulla GPU che esso si accorge che qualcosa \xE8 andato storto, quindi l\u2019errore viene effettivamente sollevato in un punto che non ha niente a che fare con ci\xF2 che lo ha creato. Per esempio, se guardiamo il nostro traceback precedente, l\u2019errore \xE8 stato sollevato durante il backward pass ("),Qs=n(xt,"EM",{});var Hh=p(Qs);Yd=o(Hh,"percorso discendente"),Hh.forEach(a),Jd=o(xt,"), ma vedremo tra un minuto che in realt\xE0 deriva da qualcosa nel forward pass ("),Ms=n(xt,"EM",{});var Rh=p(Ms);Zd=o(Rh,"percorso ascendente"),Rh.forEach(a),Xd=o(xt,")."),xt.forEach(a),tl=d(e),tt=n(e,"P",{});var Wh=p(tt);eu=o(Wh,"Come si fa a fare il debug di questi errori? La risposta \xE8 semplice: non lo facciamo. A meno che l\u2019errore CUDA non sia un errore out-of-memory (il che significa che la memoria della GPU non \xE8 sufficiente), si dovrebbe sempre tornare alla CPU per eseguire il debug."),Wh.forEach(a),sl=d(e),xe=n(e,"P",{});var Hn=p(xe);au=o(Hn,"Per fare questo nel nostro caso, dobbiamo semplicemente rimettere il modello sulla CPU e chiamarlo sul nostro batch \u2014 il batch restituito dal "),Fs=n(Hn,"CODE",{});var Bh=p(Fs);iu=o(Bh,"DataLoader"),Bh.forEach(a),tu=o(Hn," non \xE8 ancora stato spostato sulla GPU:"),Hn.forEach(a),ol=d(e),m(Xa.$$.fragment,e),rl=d(e),m(ei.$$.fragment,e),ll=d(e),ae=n(e,"P",{});var Ot=p(ae);su=o(Ot,"Quindi, il quadro si fa pi\xF9 chiaro. Invece di avere un errore CUDA, ora abbiamo un "),Gs=n(Ot,"CODE",{});var Vh=p(Gs);ou=o(Vh,"IndexError"),Vh.forEach(a),ru=o(Ot," nel calcolo della loss ("),Hs=n(Ot,"EM",{});var Kh=p(Hs);lu=o(Kh,"funzione di perdita"),Kh.forEach(a),nu=o(Ot,") (quindi niente a che fare con il backward pass, come abbiamo detto prima). Pi\xF9 precisamente, possiamo vedere che \xE8 il target 2 a creare l\u2019errore, quindi questo \xE8 un ottimo momento per controllare il numero di label del nostro modello:"),Ot.forEach(a),nl=d(e),m(ai.$$.fragment,e),pl=d(e),m(ii.$$.fragment,e),cl=d(e),st=n(e,"P",{});var Yh=p(st);pu=o(Yh,"Con due label, solo gli 0 e gli 1 sono ammessi come target, ma secondo il messaggio di errore abbiamo ottenuto un 2. Ottenere un 2 \xE8 in realt\xE0 normale: se ricordiamo i nomi delle etichette che abbiamo estratto in precedenza, ce n\u2019erano tre, quindi abbiamo gli indici 0, 1 e 2 nel nostro dataset. Il problema \xE8 che non l\u2019abbiamo detto al nostro modello, il quale si sarebbe dovuto creare con tre label. Quindi, risolviamo il problema!"),Yh.forEach(a),dl=d(e),m(ti.$$.fragment,e),ul=d(e),Oe=n(e,"P",{});var Rn=p(Oe);cu=o(Rn,"Non abbiamo ancora incluso la riga "),Rs=n(Rn,"CODE",{});var Jh=p(Rs);du=o(Jh,"trainer.train()"),Jh.forEach(a),uu=o(Rn,", per prendere tempo e verificare che tutto sia a posto. Se richiediamo un batch e lo passiamo al nostro modello, ora funziona senza errori!"),Rn.forEach(a),ml=d(e),m(si.$$.fragment,e),fl=d(e),ot=n(e,"P",{});var Zh=p(ot);mu=o(Zh,"Il passo successivo consiste nel tornare a usare la GPU e verificare che tutto funzioni ancora:"),Zh.forEach(a),hl=d(e),m(oi.$$.fragment,e),bl=d(e),rt=n(e,"P",{});var Xh=p(rt);fu=o(Xh,"Se si verifica ancora un errore, assicurarsi di riavviare il notebook ed eseguire solo l\u2019ultima versione dello script."),Xh.forEach(a),vl=d(e),ue=n(e,"H3",{class:!0});var Wn=p(ue);Ne=n(Wn,"A",{id:!0,class:!0,href:!0});var eb=p(Ne);Ws=n(eb,"SPAN",{});var ab=p(Ws);m(ri.$$.fragment,ab),ab.forEach(a),eb.forEach(a),hu=d(Wn),Bs=n(Wn,"SPAN",{});var ib=p(Bs);bu=o(ib,"Esecuzione di un passaggio di ottimizzazione"),ib.forEach(a),Wn.forEach(a),_l=d(e),lt=n(e,"P",{});var tb=p(lt);vu=o(tb,"Ora che sappiamo che possiamo costruire batch che passano effettivamente attraverso il modello, siamo pronti per la fase successiva della pipeline di addestramento: calcolare i gradienti ed eseguire una fase di ottimizzazione."),tb.forEach(a),gl=d(e),Ue=n(e,"P",{});var Bn=p(Ue);_u=o(Bn,"La prima parte consiste nel richiamare il metodo "),Vs=n(Bn,"CODE",{});var sb=p(Vs);gu=o(sb,"backward()"),sb.forEach(a),$u=o(Bn," sulla loss:"),Bn.forEach(a),$l=d(e),m(li.$$.fragment,e),kl=d(e),nt=n(e,"P",{});var ob=p(nt);ku=o(ob,"\xC8 abbastanza raro che si verifichi un errore in questa fase, ma se si verifica, assicurati di tornare ad usare la CPU per ottenere un messaggio di errore pi\xF9 utile."),ob.forEach(a),ql=d(e),ie=n(e,"P",{});var Nt=p(ie);qu=o(Nt,"Per eseguire la fase di ottimizzazione, \xE8 sufficiente creare l\u2019oggetto "),Ks=n(Nt,"CODE",{});var rb=p(Ks);zu=o(rb,"optimizer"),rb.forEach(a),wu=o(Nt," e richiamare il suo metodo "),Ys=n(Nt,"CODE",{});var lb=p(Ys);Eu=o(lb,"step()"),lb.forEach(a),ju=o(Nt,":"),Nt.forEach(a),zl=d(e),m(ni.$$.fragment,e),wl=d(e),Ie=n(e,"P",{});var Vn=p(Ie);yu=o(Vn,"Anche in questo caso, se si utilizza l\u2019ottimizzatore predefinito nel "),Js=n(Vn,"CODE",{});var nb=p(Js);Pu=o(nb,"Trainer"),nb.forEach(a),Cu=o(Vn,", non si dovrebbe ottenere un errore in questa fase, ma se hai un ottimizzatore personalizzato, potrebbero esserci dei problemi da risolvere. Non dimenticare di tornare alla CPU se ottieni uno strano errore CUDA in questa fase. A proposito di errori CUDA, prima abbiamo menzionato un caso speciale. Vediamo ora questo caso."),Vn.forEach(a),El=d(e),me=n(e,"H3",{class:!0});var Kn=p(me);Le=n(Kn,"A",{id:!0,class:!0,href:!0});var pb=p(Le);Zs=n(pb,"SPAN",{});var cb=p(Zs);m(pi.$$.fragment,cb),cb.forEach(a),pb.forEach(a),Au=d(Kn),Xs=n(Kn,"SPAN",{});var db=p(Xs);Du=o(db,"Come gestire gli errori out-of-memory di CUDA"),db.forEach(a),Kn.forEach(a),jl=d(e),Qe=n(e,"P",{});var Yn=p(Qe);Tu=o(Yn,"Ogni volta che si riceve un messaggio di errore che inizia con "),eo=n(Yn,"CODE",{});var ub=p(eo);Su=o(ub,"RuntimeError: CUDA out of memory"),ub.forEach(a),xu=o(Yn,", indica che la memoria della GPU \xE8 esaurita. Questo errore non \xE8 direttamente collegato al codice e pu\xF2 verificarsi anche con uno script che funziona perfettamente. Questo errore significa che si \xE8 tentato di mettere troppe cose nella memoria interna della GPU e che si \xE8 verificato un errore. Come per altri errori di CUDA, \xE8 necessario riavviare il kernel per poter eseguire nuovamente l\u2019allenamento."),Yn.forEach(a),yl=d(e),pt=n(e,"P",{});var mb=p(pt);Ou=o(mb,"Per risolvere questo problema, \xE8 sufficiente utilizzare meno spazio sulla GPU, cosa che spesso \xE8 pi\xF9 facile a dirsi che a farsi. Per prima cosa, assicuratevi di non avere due modelli sulla GPU contemporaneamente (a meno che non sia necessario per il vostro problema, ovviamente). Poi, \xE8 probabile che si debba ridurre la dimensione del batch, in quanto influisce direttamente sulle dimensioni di tutti gli output intermedi del modello e dei loro gradienti. Se il problema persiste, si pu\xF2 considerare di utilizzare una versione pi\xF9 piccola del modello."),mb.forEach(a),Pl=d(e),m(Me.$$.fragment,e),Cl=d(e),fe=n(e,"H3",{class:!0});var Jn=p(fe);Fe=n(Jn,"A",{id:!0,class:!0,href:!0});var fb=p(Fe);ao=n(fb,"SPAN",{});var hb=p(ao);m(ci.$$.fragment,hb),hb.forEach(a),fb.forEach(a),Nu=d(Jn),io=n(Jn,"SPAN",{});var bb=p(io);Uu=o(bb,"Valutazione del modello"),bb.forEach(a),Jn.forEach(a),Al=d(e),Ge=n(e,"P",{});var Zn=p(Ge);Iu=o(Zn,"Ora che abbiamo risolto tutti i problemi con il nostro codice, tutto \xE8 perfetto e l\u2019addestramento dovrebbe girare senza intoppi, giusto? Non cos\xEC veloce! Se si esegue il comando "),to=n(Zn,"CODE",{});var vb=p(to);Lu=o(vb,"trainer.train()"),vb.forEach(a),Qu=o(Zn,", all\u2019inizio sembrer\xE0 tutto a posto, ma dopo un po\u2019 si otterr\xE0 il seguente risultato:"),Zn.forEach(a),Dl=d(e),m(di.$$.fragment,e),Tl=d(e),m(ui.$$.fragment,e),Sl=d(e),ct=n(e,"P",{});var _b=p(ct);Mu=o(_b,"Ti accorgerai che questo errore compare durante la fase di valutazione, quindi \xE8 l\u2019ultima cosa che dobbiamo debuggare."),_b.forEach(a),xl=d(e),He=n(e,"P",{});var Xn=p(He);Fu=o(Xn,"\xC8 possibile eseguire il ciclo di valutazione del "),so=n(Xn,"CODE",{});var gb=p(so);Gu=o(gb,"Trainer"),gb.forEach(a),Hu=o(Xn," indipendentemente dall\u2019addestramento, in questo modo:"),Xn.forEach(a),Ol=d(e),m(mi.$$.fragment,e),Nl=d(e),m(fi.$$.fragment,e),Ul=d(e),m(Re.$$.fragment,e),Il=d(e),dt=n(e,"P",{});var $b=p(dt);Ru=o($b,"Prima di tentare il debug di un problema nel ciclo di valutazione, \xE8 necessario assicurarsi di aver dato un\u2019occhiata ai dati, di essere in grado di generare correttamente un batch e di poter eseguire il modello su di esso. Abbiamo completato tutti questi passaggi, quindi il codice seguente pu\xF2 essere eseguito senza errori:"),$b.forEach(a),Ll=d(e),m(hi.$$.fragment,e),Ql=d(e),ut=n(e,"P",{});var kb=p(ut);Wu=o(kb,"L\u2019errore arriva pi\xF9 tardi, alla fine della fase di valutazione, e se guardiamo il traceback vediamo questo:"),kb.forEach(a),Ml=d(e),m(bi.$$.fragment,e),Fl=d(e),te=n(e,"P",{});var Ut=p(te);Bu=o(Ut,"Questo ci dice che l\u2019errore ha origine nel modulo "),oo=n(Ut,"CODE",{});var qb=p(oo);Vu=o(qb,"datasets/metric.py"),qb.forEach(a),Ku=o(Ut,", quindi si tratta di un problema con la nostra funzione "),ro=n(Ut,"CODE",{});var zb=p(ro);Yu=o(zb,"compute_metrics()"),zb.forEach(a),Ju=o(Ut,". La funzione accetta una tupla con i logit e le label come array NumPy, quindi proviamo a dargliela in pasto:"),Ut.forEach(a),Gl=d(e),m(vi.$$.fragment,e),Hl=d(e),m(_i.$$.fragment,e),Rl=d(e),H=n(e,"P",{});var da=p(H);Zu=o(da,"Otteniamo lo stesso errore, quindi il problema risiede sicuramente in quella funzione. Se guardiamo al suo codice, vediamo che sta solo trasferendo le "),lo=n(da,"CODE",{});var wb=p(lo);Xu=o(wb,"predictions"),wb.forEach(a),em=o(da," e le "),no=n(da,"CODE",{});var Eb=p(no);am=o(Eb,"labels"),Eb.forEach(a),im=o(da," a "),po=n(da,"CODE",{});var jb=p(po);tm=o(jb,"metric.compute()"),jb.forEach(a),sm=o(da,". C\u2019\xE8 quindi un problema con questo metodo? Non proprio. Diamo una rapida occhiata alle dimensioni:"),da.forEach(a),Wl=d(e),m(gi.$$.fragment,e),Bl=d(e),m($i.$$.fragment,e),Vl=d(e),We=n(e,"P",{});var ep=p(We);om=o(ep,"Le nostre previsioni sono ancora dei logit, non le vere previsioni, ed \xE8 per questo che la metrica restituisce questo errore (un po\u2019 oscuro). La soluzione \xE8 abbastanza semplice: basta aggiungere un argmax nella funzione "),co=n(ep,"CODE",{});var yb=p(co);rm=o(yb,"compute_metrics()"),yb.forEach(a),lm=o(ep,":"),ep.forEach(a),Kl=d(e),m(ki.$$.fragment,e),Yl=d(e),m(qi.$$.fragment,e),Jl=d(e),mt=n(e,"P",{});var Pb=p(mt);nm=o(Pb,"Ora il nostro errore \xE8 stato risolto! Questo era l\u2019ultimo, quindi il nostro script ora addestrer\xE0 correttamente un modello."),Pb.forEach(a),Zl=d(e),ft=n(e,"P",{});var Cb=p(ft);pm=o(Cb,"Per riferimento, ecco lo script completamente corretto:"),Cb.forEach(a),Xl=d(e),m(zi.$$.fragment,e),en=d(e),ht=n(e,"P",{});var Ab=p(ht);cm=o(Ab,"In questo caso, non ci sono pi\xF9 problemi e il nostro script affiner\xE0 un modello che dovrebbe dare risultati ragionevoli. Ma cosa possiamo fare quando l\u2019addestramento procede senza errori e il modello addestrato non funziona affatto bene? Questa \xE8 la parte pi\xF9 difficile di machine learning e ti mostreremo alcune tecniche che possono aiutarti."),Ab.forEach(a),an=d(e),m(Be.$$.fragment,e),tn=d(e),he=n(e,"H2",{class:!0});var ap=p(he);Ve=n(ap,"A",{id:!0,class:!0,href:!0});var Db=p(Ve);uo=n(Db,"SPAN",{});var Tb=p(uo);m(wi.$$.fragment,Tb),Tb.forEach(a),Db.forEach(a),dm=d(ap),mo=n(ap,"SPAN",{});var Sb=p(mo);um=o(Sb,"Debug degli errori silenziosi durante l'addestramento"),Sb.forEach(a),ap.forEach(a),sn=d(e),bt=n(e,"P",{});var xb=p(bt);mm=o(xb,"Cosa possiamo fare per eseguire il debug di un addestramento che viene completato senza errori, ma che non produce buoni risultati? Qui ti daremo alcuni suggerimenti, ma sappi che questo tipo di debugging \xE8 la parte pi\xF9 difficile di machine learning e non esiste una soluzione magica."),xb.forEach(a),on=d(e),be=n(e,"H3",{class:!0});var ip=p(be);Ke=n(ip,"A",{id:!0,class:!0,href:!0});var Ob=p(Ke);fo=n(Ob,"SPAN",{});var Nb=p(fo);m(Ei.$$.fragment,Nb),Nb.forEach(a),Ob.forEach(a),fm=d(ip),ho=n(ip,"SPAN",{});var Ub=p(ho);hm=o(Ub,"Controllare i dati (di nuovo!)"),Ub.forEach(a),ip.forEach(a),rn=d(e),vt=n(e,"P",{});var Ib=p(vt);bm=o(Ib,"Il tuo modello imparer\xE0 qualcosa solo se \xE8 effettivamente possibile imparare qualcosa dai tuoi dati. Se c\u2019\xE8 un bug che corrompe i dati o le label sono assegnate in modo casuale, \xE8 molto probabile che non si riesca ad addestrare il modello sul dataset. Quindi, inizia sempre con un doppio controllo degli input e delle label decodificate e poniti le seguenti domande:"),Ib.forEach(a),ln=d(e),R=n(e,"UL",{});var ua=p(R);bo=n(ua,"LI",{});var Lb=p(bo);vm=o(Lb,"I dati decodificati sono comprensibili?"),Lb.forEach(a),_m=d(ua),vo=n(ua,"LI",{});var Qb=p(vo);gm=o(Qb,"Sei d\u2019accordo con le label?"),Qb.forEach(a),$m=d(ua),_o=n(ua,"LI",{});var Mb=p(_o);km=o(Mb,"C\u2019\xE8 una label pi\xF9 comune delle altre?"),Mb.forEach(a),qm=d(ua),go=n(ua,"LI",{});var Fb=p(go);zm=o(Fb,"Quale dovrebbe essere la funzione di perdita/metrica se il modello predicesse una risposta a caso/sempre la stessa risposta?"),Fb.forEach(a),ua.forEach(a),nn=d(e),m(Ye.$$.fragment,e),pn=d(e),Je=n(e,"P",{});var tp=p(Je);wm=o(tp,"Dopo aver esaminato i dati, esamina alcune previsioni del modello e decodificale. Se il modello prevede sempre la stessa cosa, potrebbe essere perch\xE9 il tuo set di dati \xE8 influenzato verso una categoria (per i problemi di classificazione); tecniche come fare oversampling ("),$o=n(tp,"EM",{});var Gb=p($o);Em=o(Gb,"sovra-campionamento"),Gb.forEach(a),jm=o(tp,") delle classi rare potrebbero aiutare."),tp.forEach(a),cn=d(e),_t=n(e,"P",{});var Hb=p(_t);ym=o(Hb,"Se la funzione di perdita/metrica ottenuta con il tuo modello iniziale \xE8 molto diversa da quella che ci si aspetterebbe per le previsioni casuali, ricontrolla il modo in cui viene calcolata la funzione o la metrica, perch\xE9 probabilmente c\u2019\xE8 un bug. Se si utilizzano diverse funzioni che aggiungi alla fine, assicurati che siano della stessa grandezza."),Hb.forEach(a),dn=d(e),gt=n(e,"P",{});var Rb=p(gt);Pm=o(Rb,"Quando sei sicuro/a che i dati sono perfetti, puoi verificare se il modello \xE8 in grado di addestrarsi su di essi con un semplice test."),Rb.forEach(a),un=d(e),ve=n(e,"H3",{class:!0});var sp=p(ve);Ze=n(sp,"A",{id:!0,class:!0,href:!0});var Wb=p(Ze);ko=n(Wb,"SPAN",{});var Bb=p(ko);m(ji.$$.fragment,Bb),Bb.forEach(a),Wb.forEach(a),Cm=d(sp),qo=n(sp,"SPAN",{});var Vb=p(qo);Am=o(Vb,"Fare overfitting del modello su un batch"),Vb.forEach(a),sp.forEach(a),mn=d(e),Xe=n(e,"P",{});var op=p(Xe);Dm=o(op,"L\u2019overfitting \xE8 di solito qualcosa che cerchiamo di evitare durante l\u2019addestramento, poich\xE9 significa che il modello non sta imparando a riconoscere le propriet\xE0 generali che vogliamo, ma sta invece memorizzando i campioni di addestramento. Tuttavia, provare ad addestrare il modello su un batch pi\xF9 e pi\xF9 volte \xE8 un buon test per verificare se il problema cos\xEC come \xE8 stato inquadrato pu\xF2 essere risolto dal modello che si sta cercando di addestrare. Inoltre, ti aiuter\xE0 a capire se il learning rate ("),zo=n(op,"EM",{});var Kb=p(zo);Tm=o(Kb,"tasso di apprendimento"),Kb.forEach(a),Sm=o(op,") iniziale \xE8 troppo alta."),op.forEach(a),fn=d(e),ea=n(e,"P",{});var rp=p(ea);xm=o(rp,"Una volta definito il "),wo=n(rp,"CODE",{});var Yb=p(wo);Om=o(Yb,"Trainer"),Yb.forEach(a),Nm=o(rp,", \xE8 molto semplice: basta prendere un batch dal training set, ed eseguire un piccolo ciclo di addestramento manuale utilizzando solo quel batch per qualcosa come 20 step:"),rp.forEach(a),hn=d(e),m(yi.$$.fragment,e),bn=d(e),m(aa.$$.fragment,e),vn=d(e),ia=n(e,"P",{});var lp=p(ia);Um=o(lp,"Il modello risultante dovrebbe avere risultati quasi perfetti sullo stesso "),Eo=n(lp,"CODE",{});var Jb=p(Eo);Im=o(Jb,"batch"),Jb.forEach(a),Lm=o(lp,". Calcoliamo la metrica sulle previsioni risultanti:"),lp.forEach(a),_n=d(e),m(Pi.$$.fragment,e),gn=d(e),m(Ci.$$.fragment,e),$n=d(e),$t=n(e,"P",{});var Zb=p($t);Qm=o(Zb,"100% di accuratezza, questo \xE8 un bell\u2019esempio di overfitting (il che significa che se provi il tuo modello su qualsiasi altra frase, molto probabilmente ti dar\xE0 una risposta sbagliata)!"),Zb.forEach(a),kn=d(e),kt=n(e,"P",{});var Xb=p(kt);Mm=o(Xb,"Se non si riesci a far s\xEC che il modello ottenga risultati perfetti come questo, significa che c\u2019\xE8 qualcosa di sbagliato nel modo in cui si \xE8 impostato il problema o con i dati, e quindi dovresti risolvere questa cosa. Solo quando riesci a superare il test di overfitting puoi essere sicuro/a che il tuo modello possa effettivamente imparare qualcosa."),Xb.forEach(a),qn=d(e),m(ta.$$.fragment,e),zn=d(e),_e=n(e,"H3",{class:!0});var np=p(_e);sa=n(np,"A",{id:!0,class:!0,href:!0});var ev=p(sa);jo=n(ev,"SPAN",{});var av=p(jo);m(Ai.$$.fragment,av),av.forEach(a),ev.forEach(a),Fm=d(np),yo=n(np,"SPAN",{});var iv=p(yo);Gm=o(iv,"Non calibrare niente prima di avere una prima baseline"),iv.forEach(a),np.forEach(a),wn=d(e),W=n(e,"P",{});var ma=p(W);Hm=o(ma,"Hyperparameter tuning ("),Po=n(ma,"EM",{});var tv=p(Po);Rm=o(tv,"calibrazione degli iperparametri"),tv.forEach(a),Wm=o(ma,") \xE8 sempre considerato come la parte pi\xF9 difficile di machine learning, ma \xE8 solo l\u2019ultimo passo per aiutarti a migliorare un po\u2019 la metrica. Nella maggior parte dei casi, gli iperparametri predefiniti del "),Co=n(ma,"CODE",{});var sv=p(Co);Bm=o(sv,"Trainer"),sv.forEach(a),Vm=o(ma," funzionano bene per dare buoni risultati, quindi non ci si deve lanciare in una ricerca di iperparametri dispendiosa in termini di tempo e di costi, finch\xE9 non si \xE8 ottenuto qualcosa che batta la baseline ("),Ao=n(ma,"EM",{});var ov=p(Ao);Km=o(ov,"base di partenza"),ov.forEach(a),Ym=o(ma,") che si ha sul dataset."),ma.forEach(a),En=d(e),qt=n(e,"P",{});var rv=p(qt);Jm=o(rv,"Una volta ottenuto un modello sufficientemente buono, si pu\xF2 iniziare a modificarlo un po\u2019. Non provare a eseguire l\u2019addestramento un migliaio di volte con iperparametri diversi, ma confronta un paio di esecuzioni che hanno valori diversi per un iperparametro cos\xEC da avere un\u2019idea di quale abbia il maggiore impatto."),rv.forEach(a),jn=d(e),zt=n(e,"P",{});var lv=p(zt);Zm=o(lv,"Se stai modificando il modello stesso, mantieni le cose semplici e non provare nulla che non possa essere ragionevolmente giustificato. Assicurati sempre di rifare il test di overfitting per verificare che la modifica non abbia avuto conseguenze indesiderate."),lv.forEach(a),yn=d(e),ge=n(e,"H3",{class:!0});var pp=p(ge);oa=n(pp,"A",{id:!0,class:!0,href:!0});var nv=p(oa);Do=n(nv,"SPAN",{});var pv=p(Do);m(Di.$$.fragment,pv),pv.forEach(a),nv.forEach(a),Xm=d(pp),To=n(pp,"SPAN",{});var cv=p(To);ef=o(cv,"Chiedere aiuto"),cv.forEach(a),pp.forEach(a),Pn=d(e),ra=n(e,"P",{});var cp=p(ra);af=o(cp,"Speriamo che in questa sezione tu abbia trovato qualche consiglio utile a risolvere il tuo problema, ma se cos\xEC non fosse, ricordati che puoi sempre chiedere aiuto alla community nei "),Ti=n(cp,"A",{href:!0,rel:!0});var dv=p(Ti);tf=o(dv,"forum"),dv.forEach(a),sf=o(cp,"."),cp.forEach(a),Cn=d(e),wt=n(e,"P",{});var uv=p(wt);of=o(uv,"Qui di seguito sono riportate alcune risorse aggiuntive che potrebbero rivelarsi utili:"),uv.forEach(a),An=d(e),B=n(e,"UL",{});var fa=p(B);Et=n(fa,"LI",{});var _f=p(Et);Si=n(_f,"A",{href:!0,rel:!0});var mv=p(Si);rf=o(mv,"\u201CReproducibility as a vehicle for engineering best practices\u201D"),mv.forEach(a),lf=o(_f," di Joel Grus"),_f.forEach(a),nf=d(fa),jt=n(fa,"LI",{});var gf=p(jt);xi=n(gf,"A",{href:!0,rel:!0});var fv=p(xi);pf=o(fv,"\u201CChecklist for debugging neural networks\u201D"),fv.forEach(a),cf=o(gf," di Cecelia Shao"),gf.forEach(a),df=d(fa),yt=n(fa,"LI",{});var $f=p(yt);Oi=n($f,"A",{href:!0,rel:!0});var hv=p(Oi);uf=o(hv,"\u201CHow to unit test machine learning code\u201D"),hv.forEach(a),mf=o($f," di Chase Roberts"),$f.forEach(a),ff=d(fa),Pt=n(fa,"LI",{});var kf=p(Pt);Ni=n(kf,"A",{href:!0,rel:!0});var bv=p(Ni);hf=o(bv,"\u201CA Recipe for Training Neural Networks\u201D"),bv.forEach(a),bf=o(kf," di Andrej Karpathy"),kf.forEach(a),fa.forEach(a),Dn=d(e),Ct=n(e,"P",{});var vv=p(Ct);vf=o(vv,"Naturalmente, non tutti i problemi che incontrerai durante l\u2019addestramento delle reti neurali sono colpa tua! Se si incontra qualcosa nella libreria \u{1F917} Transformers o \u{1F917} Datasets che non sembra corretto, \xE8 possibile che si sia trovato un bug. Dovresti assolutamente segnalarcelo e nella prossima sezione ti spiegheremo esattamente come fare."),vv.forEach(a),this.h()},h(){g(_,"name","hf:doc:metadata"),g(_,"content",JSON.stringify(xv)),g(w,"id","fare-il-debug-della-training-pipeline"),g(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(w,"href","#fare-il-debug-della-training-pipeline"),g(j,"class","relative group"),g(Mi,"href","/course/chapter7"),g(ke,"id","fare-il-debug-della-training-pipeline"),g(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(ke,"href","#fare-il-debug-della-training-pipeline"),g(ne,"class","relative group"),g(_a,"href","https://huggingface.co/datasets/glue"),g(_a,"rel","nofollow"),g(we,"id","controlla-i-dati"),g(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(we,"href","#controlla-i-dati"),g(pe,"class","relative group"),g(Sa,"href","https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification"),g(Sa,"rel","nofollow"),g(Ae,"id","dai-dataset-ai-dataloader"),g(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(Ae,"href","#dai-dataset-ai-dataloader"),g(ce,"class","relative group"),g(Te,"id","passaggio-attraverso-il-modello"),g(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(Te,"href","#passaggio-attraverso-il-modello"),g(de,"class","relative group"),g(Ne,"id","esecuzione-di-un-passaggio-di-ottimizzazione"),g(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(Ne,"href","#esecuzione-di-un-passaggio-di-ottimizzazione"),g(ue,"class","relative group"),g(Le,"id","come-gestire-gli-errori-outofmemory-di-cuda"),g(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(Le,"href","#come-gestire-gli-errori-outofmemory-di-cuda"),g(me,"class","relative group"),g(Fe,"id","valutazione-del-modello"),g(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(Fe,"href","#valutazione-del-modello"),g(fe,"class","relative group"),g(Ve,"id","debug-degli-errori-silenziosi-durante-laddestramento"),g(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(Ve,"href","#debug-degli-errori-silenziosi-durante-laddestramento"),g(he,"class","relative group"),g(Ke,"id","controllare-i-dati-di-nuovo"),g(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(Ke,"href","#controllare-i-dati-di-nuovo"),g(be,"class","relative group"),g(Ze,"id","fare-overfitting-del-modello-su-un-batch"),g(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(Ze,"href","#fare-overfitting-del-modello-su-un-batch"),g(ve,"class","relative group"),g(sa,"id","non-calibrare-niente-prima-di-avere-una-prima-baseline"),g(sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(sa,"href","#non-calibrare-niente-prima-di-avere-una-prima-baseline"),g(_e,"class","relative group"),g(oa,"id","chiedere-aiuto"),g(oa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(oa,"href","#chiedere-aiuto"),g(ge,"class","relative group"),g(Ti,"href","https://discuss.huggingface.co/"),g(Ti,"rel","nofollow"),g(Si,"href","https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p"),g(Si,"rel","nofollow"),g(xi,"href","https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21"),g(xi,"rel","nofollow"),g(Oi,"href","https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765"),g(Oi,"rel","nofollow"),g(Ni,"href","http://karpathy.github.io/2019/04/25/recipe/"),g(Ni,"rel","nofollow")},m(e,t){i(document.head,_),r(e,z,t),f(k,e,t),r(e,q,t),r(e,j,t),i(j,w),i(w,E),f(y,E,null),i(j,T),i(j,P),i(P,K),r(e,U,t),f(C,e,t),r(e,ha,t),r(e,I,t),i(I,Qi),i(I,Mi),i(Mi,dp),i(I,up),i(I,It),i(It,mp),i(I,fp),r(e,Io,t),r(e,ne,t),i(ne,ke),i(ke,Lt),f(ba,Lt,null),i(ne,hp),i(ne,Qt),i(Qt,bp),r(e,Lo,t),f(va,e,t),r(e,Qo,t),r(e,S,t),i(S,vp),i(S,Mt),i(Mt,_p),i(S,gp),i(S,Ft),i(Ft,$p),i(S,kp),i(S,Gt),i(Gt,qp),i(S,zp),i(S,Ht),i(Ht,wp),i(S,Ep),r(e,Mo,t),r(e,qe,t),i(qe,jp),i(qe,Rt),i(Rt,yp),i(qe,Pp),r(e,Fo,t),r(e,ze,t),i(ze,Cp),i(ze,_a),i(_a,Ap),i(ze,Dp),r(e,Go,t),f(ga,e,t),r(e,Ho,t),r(e,Fi,t),i(Fi,Tp),r(e,Ro,t),f($a,e,t),r(e,Wo,t),r(e,pe,t),i(pe,we),i(we,Wt),f(ka,Wt,null),i(pe,Sp),i(pe,Bt),i(Bt,xp),r(e,Bo,t),r(e,Y,t),i(Y,Op),i(Y,Vt),i(Vt,Np),i(Y,Up),i(Y,Kt),i(Kt,Ip),i(Y,Lp),r(e,Vo,t),r(e,Ee,t),i(Ee,Qp),i(Ee,Yt),i(Yt,Mp),i(Ee,Fp),r(e,Ko,t),f(qa,e,t),r(e,Yo,t),f(za,e,t),r(e,Jo,t),r(e,Q,t),i(Q,Gp),i(Q,Jt),i(Jt,Hp),i(Q,Rp),i(Q,Zt),i(Zt,Wp),i(Q,Bp),i(Q,Xt),i(Xt,Vp),i(Q,Kp),r(e,Zo,t),r(e,D,t),i(D,Yp),i(D,es),i(es,Jp),i(D,Zp),i(D,as),i(as,Xp),i(D,ec),i(D,is),i(is,ac),i(D,ic),i(D,ts),i(ts,tc),i(D,sc),i(D,ss),i(ss,oc),i(D,rc),r(e,Xo,t),f(wa,e,t),r(e,er,t),r(e,Gi,t),i(Gi,lc),r(e,ar,t),f(Ea,e,t),r(e,ir,t),r(e,Hi,t),i(Hi,nc),r(e,tr,t),f(ja,e,t),r(e,sr,t),r(e,Ri,t),i(Ri,pc),r(e,or,t),r(e,Wi,t),i(Wi,cc),r(e,rr,t),f(ya,e,t),r(e,lr,t),f(Pa,e,t),r(e,nr,t),r(e,Bi,t),i(Bi,dc),r(e,pr,t),f(Ca,e,t),r(e,cr,t),f(Aa,e,t),r(e,dr,t),r(e,x,t),i(x,uc),i(x,os),i(os,mc),i(x,fc),i(x,rs),i(rs,hc),i(x,bc),i(x,ls),i(ls,vc),i(x,_c),i(x,ns),i(ns,gc),i(x,$c),r(e,ur,t),f(Da,e,t),r(e,mr,t),f(Ta,e,t),r(e,fr,t),r(e,J,t),i(J,kc),i(J,Sa),i(Sa,qc),i(J,zc),i(J,ps),i(ps,wc),i(J,Ec),r(e,hr,t),r(e,je,t),i(je,jc),i(je,cs),i(cs,yc),i(je,Pc),r(e,br,t),f(xa,e,t),r(e,vr,t),f(Oa,e,t),r(e,_r,t),r(e,ye,t),i(ye,Cc),i(ye,ds),i(ds,Ac),i(ye,Dc),r(e,gr,t),f(Na,e,t),r(e,$r,t),f(Ua,e,t),r(e,kr,t),r(e,Vi,t),i(Vi,Tc),r(e,qr,t),f(Ia,e,t),r(e,zr,t),f(La,e,t),r(e,wr,t),r(e,Z,t),i(Z,Sc),i(Z,us),i(us,xc),i(Z,Oc),i(Z,ms),i(ms,Nc),i(Z,Uc),r(e,Er,t),f(Qa,e,t),r(e,jr,t),f(Ma,e,t),r(e,yr,t),r(e,M,t),i(M,Ic),i(M,fs),i(fs,Lc),i(M,Qc),i(M,hs),i(hs,Mc),i(M,Fc),i(M,bs),i(bs,Gc),i(M,Hc),r(e,Pr,t),r(e,Pe,t),i(Pe,Rc),i(Pe,vs),i(vs,Wc),i(Pe,Bc),r(e,Cr,t),f(Ce,e,t),r(e,Ar,t),r(e,Ki,t),i(Ki,Vc),r(e,Dr,t),r(e,Yi,t),i(Yi,Kc),r(e,Tr,t),r(e,ce,t),i(ce,Ae),i(Ae,_s),f(Fa,_s,null),i(ce,Yc),i(ce,gs),i(gs,Jc),r(e,Sr,t),r(e,O,t),i(O,Zc),i(O,$s),i($s,Xc),i(O,ed),i(O,ks),i(ks,ad),i(O,id),i(O,qs),i(qs,td),i(O,sd),i(O,zs),i(zs,od),i(O,rd),r(e,xr,t),f(Ga,e,t),r(e,Or,t),r(e,De,t),i(De,ld),i(De,ws),i(ws,nd),i(De,pd),r(e,Nr,t),f(Ha,e,t),r(e,Ur,t),r(e,X,t),i(X,cd),i(X,Es),i(Es,dd),i(X,ud),i(X,js),i(js,md),i(X,fd),r(e,Ir,t),f(Ra,e,t),r(e,Lr,t),f(Wa,e,t),r(e,Qr,t),r(e,F,t),i(F,hd),i(F,ys),i(ys,bd),i(F,vd),i(F,Ps),i(Ps,_d),i(F,gd),i(F,Cs),i(Cs,$d),i(F,kd),r(e,Mr,t),r(e,G,t),i(G,qd),i(G,As),i(As,zd),i(G,wd),i(G,Ds),i(Ds,Ed),i(G,jd),i(G,Ts),i(Ts,yd),i(G,Pd),r(e,Fr,t),f(Ba,e,t),r(e,Gr,t),r(e,Ji,t),i(Ji,Cd),r(e,Hr,t),f(Va,e,t),r(e,Rr,t),r(e,Zi,t),i(Zi,Ad),r(e,Wr,t),r(e,Xi,t),i(Xi,Dd),r(e,Br,t),f(Ka,e,t),r(e,Vr,t),r(e,N,t),i(N,Td),i(N,Ss),i(Ss,Sd),i(N,xd),i(N,xs),i(xs,Od),i(N,Nd),i(N,Os),i(Os,Ud),i(N,Id),i(N,Ns),i(Ns,Ld),i(N,Qd),r(e,Kr,t),f(Ya,e,t),r(e,Yr,t),r(e,et,t),i(et,Md),r(e,Jr,t),r(e,at,t),i(at,Fd),r(e,Zr,t),r(e,de,t),i(de,Te),i(Te,Us),f(Ja,Us,null),i(de,Gd),i(de,Is),i(Is,Hd),r(e,Xr,t),r(e,it,t),i(it,Rd),r(e,el,t),f(Za,e,t),r(e,al,t),r(e,Se,t),i(Se,Wd),i(Se,Ls),i(Ls,Bd),i(Se,Vd),r(e,il,t),r(e,ee,t),i(ee,Kd),i(ee,Qs),i(Qs,Yd),i(ee,Jd),i(ee,Ms),i(Ms,Zd),i(ee,Xd),r(e,tl,t),r(e,tt,t),i(tt,eu),r(e,sl,t),r(e,xe,t),i(xe,au),i(xe,Fs),i(Fs,iu),i(xe,tu),r(e,ol,t),f(Xa,e,t),r(e,rl,t),f(ei,e,t),r(e,ll,t),r(e,ae,t),i(ae,su),i(ae,Gs),i(Gs,ou),i(ae,ru),i(ae,Hs),i(Hs,lu),i(ae,nu),r(e,nl,t),f(ai,e,t),r(e,pl,t),f(ii,e,t),r(e,cl,t),r(e,st,t),i(st,pu),r(e,dl,t),f(ti,e,t),r(e,ul,t),r(e,Oe,t),i(Oe,cu),i(Oe,Rs),i(Rs,du),i(Oe,uu),r(e,ml,t),f(si,e,t),r(e,fl,t),r(e,ot,t),i(ot,mu),r(e,hl,t),f(oi,e,t),r(e,bl,t),r(e,rt,t),i(rt,fu),r(e,vl,t),r(e,ue,t),i(ue,Ne),i(Ne,Ws),f(ri,Ws,null),i(ue,hu),i(ue,Bs),i(Bs,bu),r(e,_l,t),r(e,lt,t),i(lt,vu),r(e,gl,t),r(e,Ue,t),i(Ue,_u),i(Ue,Vs),i(Vs,gu),i(Ue,$u),r(e,$l,t),f(li,e,t),r(e,kl,t),r(e,nt,t),i(nt,ku),r(e,ql,t),r(e,ie,t),i(ie,qu),i(ie,Ks),i(Ks,zu),i(ie,wu),i(ie,Ys),i(Ys,Eu),i(ie,ju),r(e,zl,t),f(ni,e,t),r(e,wl,t),r(e,Ie,t),i(Ie,yu),i(Ie,Js),i(Js,Pu),i(Ie,Cu),r(e,El,t),r(e,me,t),i(me,Le),i(Le,Zs),f(pi,Zs,null),i(me,Au),i(me,Xs),i(Xs,Du),r(e,jl,t),r(e,Qe,t),i(Qe,Tu),i(Qe,eo),i(eo,Su),i(Qe,xu),r(e,yl,t),r(e,pt,t),i(pt,Ou),r(e,Pl,t),f(Me,e,t),r(e,Cl,t),r(e,fe,t),i(fe,Fe),i(Fe,ao),f(ci,ao,null),i(fe,Nu),i(fe,io),i(io,Uu),r(e,Al,t),r(e,Ge,t),i(Ge,Iu),i(Ge,to),i(to,Lu),i(Ge,Qu),r(e,Dl,t),f(di,e,t),r(e,Tl,t),f(ui,e,t),r(e,Sl,t),r(e,ct,t),i(ct,Mu),r(e,xl,t),r(e,He,t),i(He,Fu),i(He,so),i(so,Gu),i(He,Hu),r(e,Ol,t),f(mi,e,t),r(e,Nl,t),f(fi,e,t),r(e,Ul,t),f(Re,e,t),r(e,Il,t),r(e,dt,t),i(dt,Ru),r(e,Ll,t),f(hi,e,t),r(e,Ql,t),r(e,ut,t),i(ut,Wu),r(e,Ml,t),f(bi,e,t),r(e,Fl,t),r(e,te,t),i(te,Bu),i(te,oo),i(oo,Vu),i(te,Ku),i(te,ro),i(ro,Yu),i(te,Ju),r(e,Gl,t),f(vi,e,t),r(e,Hl,t),f(_i,e,t),r(e,Rl,t),r(e,H,t),i(H,Zu),i(H,lo),i(lo,Xu),i(H,em),i(H,no),i(no,am),i(H,im),i(H,po),i(po,tm),i(H,sm),r(e,Wl,t),f(gi,e,t),r(e,Bl,t),f($i,e,t),r(e,Vl,t),r(e,We,t),i(We,om),i(We,co),i(co,rm),i(We,lm),r(e,Kl,t),f(ki,e,t),r(e,Yl,t),f(qi,e,t),r(e,Jl,t),r(e,mt,t),i(mt,nm),r(e,Zl,t),r(e,ft,t),i(ft,pm),r(e,Xl,t),f(zi,e,t),r(e,en,t),r(e,ht,t),i(ht,cm),r(e,an,t),f(Be,e,t),r(e,tn,t),r(e,he,t),i(he,Ve),i(Ve,uo),f(wi,uo,null),i(he,dm),i(he,mo),i(mo,um),r(e,sn,t),r(e,bt,t),i(bt,mm),r(e,on,t),r(e,be,t),i(be,Ke),i(Ke,fo),f(Ei,fo,null),i(be,fm),i(be,ho),i(ho,hm),r(e,rn,t),r(e,vt,t),i(vt,bm),r(e,ln,t),r(e,R,t),i(R,bo),i(bo,vm),i(R,_m),i(R,vo),i(vo,gm),i(R,$m),i(R,_o),i(_o,km),i(R,qm),i(R,go),i(go,zm),r(e,nn,t),f(Ye,e,t),r(e,pn,t),r(e,Je,t),i(Je,wm),i(Je,$o),i($o,Em),i(Je,jm),r(e,cn,t),r(e,_t,t),i(_t,ym),r(e,dn,t),r(e,gt,t),i(gt,Pm),r(e,un,t),r(e,ve,t),i(ve,Ze),i(Ze,ko),f(ji,ko,null),i(ve,Cm),i(ve,qo),i(qo,Am),r(e,mn,t),r(e,Xe,t),i(Xe,Dm),i(Xe,zo),i(zo,Tm),i(Xe,Sm),r(e,fn,t),r(e,ea,t),i(ea,xm),i(ea,wo),i(wo,Om),i(ea,Nm),r(e,hn,t),f(yi,e,t),r(e,bn,t),f(aa,e,t),r(e,vn,t),r(e,ia,t),i(ia,Um),i(ia,Eo),i(Eo,Im),i(ia,Lm),r(e,_n,t),f(Pi,e,t),r(e,gn,t),f(Ci,e,t),r(e,$n,t),r(e,$t,t),i($t,Qm),r(e,kn,t),r(e,kt,t),i(kt,Mm),r(e,qn,t),f(ta,e,t),r(e,zn,t),r(e,_e,t),i(_e,sa),i(sa,jo),f(Ai,jo,null),i(_e,Fm),i(_e,yo),i(yo,Gm),r(e,wn,t),r(e,W,t),i(W,Hm),i(W,Po),i(Po,Rm),i(W,Wm),i(W,Co),i(Co,Bm),i(W,Vm),i(W,Ao),i(Ao,Km),i(W,Ym),r(e,En,t),r(e,qt,t),i(qt,Jm),r(e,jn,t),r(e,zt,t),i(zt,Zm),r(e,yn,t),r(e,ge,t),i(ge,oa),i(oa,Do),f(Di,Do,null),i(ge,Xm),i(ge,To),i(To,ef),r(e,Pn,t),r(e,ra,t),i(ra,af),i(ra,Ti),i(Ti,tf),i(ra,sf),r(e,Cn,t),r(e,wt,t),i(wt,of),r(e,An,t),r(e,B,t),i(B,Et),i(Et,Si),i(Si,rf),i(Et,lf),i(B,nf),i(B,jt),i(jt,xi),i(xi,pf),i(jt,cf),i(B,df),i(B,yt),i(yt,Oi),i(Oi,uf),i(yt,mf),i(B,ff),i(B,Pt),i(Pt,Ni),i(Ni,hf),i(Pt,bf),r(e,Dn,t),r(e,Ct,t),i(Ct,vf),Tn=!0},p(e,[t]){const Ui={};t&1&&(Ui.fw=e[0]),k.$set(Ui);const So={};t&2&&(So.$$scope={dirty:t,ctx:e}),Ce.$set(So);const xo={};t&2&&(xo.$$scope={dirty:t,ctx:e}),Me.$set(xo);const Oo={};t&2&&(Oo.$$scope={dirty:t,ctx:e}),Re.$set(Oo);const $e={};t&2&&($e.$$scope={dirty:t,ctx:e}),Be.$set($e);const No={};t&2&&(No.$$scope={dirty:t,ctx:e}),Ye.$set(No);const Uo={};t&2&&(Uo.$$scope={dirty:t,ctx:e}),aa.$set(Uo);const Ii={};t&2&&(Ii.$$scope={dirty:t,ctx:e}),ta.$set(Ii)},i(e){Tn||(h(k.$$.fragment,e),h(y.$$.fragment,e),h(C.$$.fragment,e),h(ba.$$.fragment,e),h(va.$$.fragment,e),h(ga.$$.fragment,e),h($a.$$.fragment,e),h(ka.$$.fragment,e),h(qa.$$.fragment,e),h(za.$$.fragment,e),h(wa.$$.fragment,e),h(Ea.$$.fragment,e),h(ja.$$.fragment,e),h(ya.$$.fragment,e),h(Pa.$$.fragment,e),h(Ca.$$.fragment,e),h(Aa.$$.fragment,e),h(Da.$$.fragment,e),h(Ta.$$.fragment,e),h(xa.$$.fragment,e),h(Oa.$$.fragment,e),h(Na.$$.fragment,e),h(Ua.$$.fragment,e),h(Ia.$$.fragment,e),h(La.$$.fragment,e),h(Qa.$$.fragment,e),h(Ma.$$.fragment,e),h(Ce.$$.fragment,e),h(Fa.$$.fragment,e),h(Ga.$$.fragment,e),h(Ha.$$.fragment,e),h(Ra.$$.fragment,e),h(Wa.$$.fragment,e),h(Ba.$$.fragment,e),h(Va.$$.fragment,e),h(Ka.$$.fragment,e),h(Ya.$$.fragment,e),h(Ja.$$.fragment,e),h(Za.$$.fragment,e),h(Xa.$$.fragment,e),h(ei.$$.fragment,e),h(ai.$$.fragment,e),h(ii.$$.fragment,e),h(ti.$$.fragment,e),h(si.$$.fragment,e),h(oi.$$.fragment,e),h(ri.$$.fragment,e),h(li.$$.fragment,e),h(ni.$$.fragment,e),h(pi.$$.fragment,e),h(Me.$$.fragment,e),h(ci.$$.fragment,e),h(di.$$.fragment,e),h(ui.$$.fragment,e),h(mi.$$.fragment,e),h(fi.$$.fragment,e),h(Re.$$.fragment,e),h(hi.$$.fragment,e),h(bi.$$.fragment,e),h(vi.$$.fragment,e),h(_i.$$.fragment,e),h(gi.$$.fragment,e),h($i.$$.fragment,e),h(ki.$$.fragment,e),h(qi.$$.fragment,e),h(zi.$$.fragment,e),h(Be.$$.fragment,e),h(wi.$$.fragment,e),h(Ei.$$.fragment,e),h(Ye.$$.fragment,e),h(ji.$$.fragment,e),h(yi.$$.fragment,e),h(aa.$$.fragment,e),h(Pi.$$.fragment,e),h(Ci.$$.fragment,e),h(ta.$$.fragment,e),h(Ai.$$.fragment,e),h(Di.$$.fragment,e),Tn=!0)},o(e){b(k.$$.fragment,e),b(y.$$.fragment,e),b(C.$$.fragment,e),b(ba.$$.fragment,e),b(va.$$.fragment,e),b(ga.$$.fragment,e),b($a.$$.fragment,e),b(ka.$$.fragment,e),b(qa.$$.fragment,e),b(za.$$.fragment,e),b(wa.$$.fragment,e),b(Ea.$$.fragment,e),b(ja.$$.fragment,e),b(ya.$$.fragment,e),b(Pa.$$.fragment,e),b(Ca.$$.fragment,e),b(Aa.$$.fragment,e),b(Da.$$.fragment,e),b(Ta.$$.fragment,e),b(xa.$$.fragment,e),b(Oa.$$.fragment,e),b(Na.$$.fragment,e),b(Ua.$$.fragment,e),b(Ia.$$.fragment,e),b(La.$$.fragment,e),b(Qa.$$.fragment,e),b(Ma.$$.fragment,e),b(Ce.$$.fragment,e),b(Fa.$$.fragment,e),b(Ga.$$.fragment,e),b(Ha.$$.fragment,e),b(Ra.$$.fragment,e),b(Wa.$$.fragment,e),b(Ba.$$.fragment,e),b(Va.$$.fragment,e),b(Ka.$$.fragment,e),b(Ya.$$.fragment,e),b(Ja.$$.fragment,e),b(Za.$$.fragment,e),b(Xa.$$.fragment,e),b(ei.$$.fragment,e),b(ai.$$.fragment,e),b(ii.$$.fragment,e),b(ti.$$.fragment,e),b(si.$$.fragment,e),b(oi.$$.fragment,e),b(ri.$$.fragment,e),b(li.$$.fragment,e),b(ni.$$.fragment,e),b(pi.$$.fragment,e),b(Me.$$.fragment,e),b(ci.$$.fragment,e),b(di.$$.fragment,e),b(ui.$$.fragment,e),b(mi.$$.fragment,e),b(fi.$$.fragment,e),b(Re.$$.fragment,e),b(hi.$$.fragment,e),b(bi.$$.fragment,e),b(vi.$$.fragment,e),b(_i.$$.fragment,e),b(gi.$$.fragment,e),b($i.$$.fragment,e),b(ki.$$.fragment,e),b(qi.$$.fragment,e),b(zi.$$.fragment,e),b(Be.$$.fragment,e),b(wi.$$.fragment,e),b(Ei.$$.fragment,e),b(Ye.$$.fragment,e),b(ji.$$.fragment,e),b(yi.$$.fragment,e),b(aa.$$.fragment,e),b(Pi.$$.fragment,e),b(Ci.$$.fragment,e),b(ta.$$.fragment,e),b(Ai.$$.fragment,e),b(Di.$$.fragment,e),Tn=!1},d(e){a(_),e&&a(z),v(k,e),e&&a(q),e&&a(j),v(y),e&&a(U),v(C,e),e&&a(ha),e&&a(I),e&&a(Io),e&&a(ne),v(ba),e&&a(Lo),v(va,e),e&&a(Qo),e&&a(S),e&&a(Mo),e&&a(qe),e&&a(Fo),e&&a(ze),e&&a(Go),v(ga,e),e&&a(Ho),e&&a(Fi),e&&a(Ro),v($a,e),e&&a(Wo),e&&a(pe),v(ka),e&&a(Bo),e&&a(Y),e&&a(Vo),e&&a(Ee),e&&a(Ko),v(qa,e),e&&a(Yo),v(za,e),e&&a(Jo),e&&a(Q),e&&a(Zo),e&&a(D),e&&a(Xo),v(wa,e),e&&a(er),e&&a(Gi),e&&a(ar),v(Ea,e),e&&a(ir),e&&a(Hi),e&&a(tr),v(ja,e),e&&a(sr),e&&a(Ri),e&&a(or),e&&a(Wi),e&&a(rr),v(ya,e),e&&a(lr),v(Pa,e),e&&a(nr),e&&a(Bi),e&&a(pr),v(Ca,e),e&&a(cr),v(Aa,e),e&&a(dr),e&&a(x),e&&a(ur),v(Da,e),e&&a(mr),v(Ta,e),e&&a(fr),e&&a(J),e&&a(hr),e&&a(je),e&&a(br),v(xa,e),e&&a(vr),v(Oa,e),e&&a(_r),e&&a(ye),e&&a(gr),v(Na,e),e&&a($r),v(Ua,e),e&&a(kr),e&&a(Vi),e&&a(qr),v(Ia,e),e&&a(zr),v(La,e),e&&a(wr),e&&a(Z),e&&a(Er),v(Qa,e),e&&a(jr),v(Ma,e),e&&a(yr),e&&a(M),e&&a(Pr),e&&a(Pe),e&&a(Cr),v(Ce,e),e&&a(Ar),e&&a(Ki),e&&a(Dr),e&&a(Yi),e&&a(Tr),e&&a(ce),v(Fa),e&&a(Sr),e&&a(O),e&&a(xr),v(Ga,e),e&&a(Or),e&&a(De),e&&a(Nr),v(Ha,e),e&&a(Ur),e&&a(X),e&&a(Ir),v(Ra,e),e&&a(Lr),v(Wa,e),e&&a(Qr),e&&a(F),e&&a(Mr),e&&a(G),e&&a(Fr),v(Ba,e),e&&a(Gr),e&&a(Ji),e&&a(Hr),v(Va,e),e&&a(Rr),e&&a(Zi),e&&a(Wr),e&&a(Xi),e&&a(Br),v(Ka,e),e&&a(Vr),e&&a(N),e&&a(Kr),v(Ya,e),e&&a(Yr),e&&a(et),e&&a(Jr),e&&a(at),e&&a(Zr),e&&a(de),v(Ja),e&&a(Xr),e&&a(it),e&&a(el),v(Za,e),e&&a(al),e&&a(Se),e&&a(il),e&&a(ee),e&&a(tl),e&&a(tt),e&&a(sl),e&&a(xe),e&&a(ol),v(Xa,e),e&&a(rl),v(ei,e),e&&a(ll),e&&a(ae),e&&a(nl),v(ai,e),e&&a(pl),v(ii,e),e&&a(cl),e&&a(st),e&&a(dl),v(ti,e),e&&a(ul),e&&a(Oe),e&&a(ml),v(si,e),e&&a(fl),e&&a(ot),e&&a(hl),v(oi,e),e&&a(bl),e&&a(rt),e&&a(vl),e&&a(ue),v(ri),e&&a(_l),e&&a(lt),e&&a(gl),e&&a(Ue),e&&a($l),v(li,e),e&&a(kl),e&&a(nt),e&&a(ql),e&&a(ie),e&&a(zl),v(ni,e),e&&a(wl),e&&a(Ie),e&&a(El),e&&a(me),v(pi),e&&a(jl),e&&a(Qe),e&&a(yl),e&&a(pt),e&&a(Pl),v(Me,e),e&&a(Cl),e&&a(fe),v(ci),e&&a(Al),e&&a(Ge),e&&a(Dl),v(di,e),e&&a(Tl),v(ui,e),e&&a(Sl),e&&a(ct),e&&a(xl),e&&a(He),e&&a(Ol),v(mi,e),e&&a(Nl),v(fi,e),e&&a(Ul),v(Re,e),e&&a(Il),e&&a(dt),e&&a(Ll),v(hi,e),e&&a(Ql),e&&a(ut),e&&a(Ml),v(bi,e),e&&a(Fl),e&&a(te),e&&a(Gl),v(vi,e),e&&a(Hl),v(_i,e),e&&a(Rl),e&&a(H),e&&a(Wl),v(gi,e),e&&a(Bl),v($i,e),e&&a(Vl),e&&a(We),e&&a(Kl),v(ki,e),e&&a(Yl),v(qi,e),e&&a(Jl),e&&a(mt),e&&a(Zl),e&&a(ft),e&&a(Xl),v(zi,e),e&&a(en),e&&a(ht),e&&a(an),v(Be,e),e&&a(tn),e&&a(he),v(wi),e&&a(sn),e&&a(bt),e&&a(on),e&&a(be),v(Ei),e&&a(rn),e&&a(vt),e&&a(ln),e&&a(R),e&&a(nn),v(Ye,e),e&&a(pn),e&&a(Je),e&&a(cn),e&&a(_t),e&&a(dn),e&&a(gt),e&&a(un),e&&a(ve),v(ji),e&&a(mn),e&&a(Xe),e&&a(fn),e&&a(ea),e&&a(hn),v(yi,e),e&&a(bn),v(aa,e),e&&a(vn),e&&a(ia),e&&a(_n),v(Pi,e),e&&a(gn),v(Ci,e),e&&a($n),e&&a($t),e&&a(kn),e&&a(kt),e&&a(qn),v(ta,e),e&&a(zn),e&&a(_e),v(Ai),e&&a(wn),e&&a(W),e&&a(En),e&&a(qt),e&&a(jn),e&&a(zt),e&&a(yn),e&&a(ge),v(Di),e&&a(Pn),e&&a(ra),e&&a(Cn),e&&a(wt),e&&a(An),e&&a(B),e&&a(Dn),e&&a(Ct)}}}const xv={local:"fare-il-debug-della-training-pipeline",sections:[{local:"fare-il-debug-della-training-pipeline",sections:[{local:"controlla-i-dati",title:"Controlla i dati"},{local:"dai-dataset-ai-dataloader",title:"Dai dataset ai dataloader"},{local:"passaggio-attraverso-il-modello",title:"Passaggio attraverso il modello"},{local:"esecuzione-di-un-passaggio-di-ottimizzazione",title:"Esecuzione di un passaggio di ottimizzazione"},{local:"come-gestire-gli-errori-outofmemory-di-cuda",title:"Come gestire gli errori out-of-memory di CUDA"},{local:"valutazione-del-modello",title:"Valutazione del modello"}],title:"Fare il debug della training pipeline"},{local:"debug-degli-errori-silenziosi-durante-laddestramento",sections:[{local:"controllare-i-dati-di-nuovo",title:"Controllare i dati (di nuovo!)"},{local:"fare-overfitting-del-modello-su-un-batch",title:"Fare overfitting del modello su un batch"},{local:"non-calibrare-niente-prima-di-avere-una-prima-baseline",title:"Non calibrare niente prima di avere una prima baseline"},{local:"chiedere-aiuto",title:"Chiedere aiuto"}],title:"Debug degli errori silenziosi durante l'addestramento"}],title:"Fare il debug della training pipeline"};function Ov(A,_,z){let k="pt";return qv(()=>{const q=new URLSearchParams(window.location.search);z(0,k=q.get("fw")||"pt")}),[k]}class Gv extends _v{constructor(_){super();gv(this,_,Ov,Sv,$v,{})}}export{Gv as default,xv as metadata};
