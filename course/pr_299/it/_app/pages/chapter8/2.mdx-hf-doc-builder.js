import{S as Wn,i as Bn,s as Jn,e as n,k as p,w as f,t as s,M as Yn,c as l,d as o,m as d,a as u,x as _,h as a,b as c,N as Yi,G as t,g as r,y as h,q as g,o as b,B as v,v as Xn}from"../../chunks/vendor-hf-doc-builder.js";import{T as $r}from"../../chunks/Tip-hf-doc-builder.js";import{Y as kr}from"../../chunks/Youtube-hf-doc-builder.js";import{I as qr}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as $}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Kn}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Zn(K){let m,y;return{c(){m=n("p"),y=s("\u{1F6A8} Hai notato quel riquadro blu intorno a \u201C6 frames\u201D nel traceback di Google Colab? \xC8 una funzionalit\xE0 speciale di Colab, che comprime il traceback in \u201Cframe\u201D. Se non riesci a trovare l\u2019origine di un errore, assicurati di espandere l\u2019intero traceback facendo clic su quelle due piccole frecce.")},l(w){m=l(w,"P",{});var k=u(m);y=a(k,"\u{1F6A8} Hai notato quel riquadro blu intorno a \u201C6 frames\u201D nel traceback di Google Colab? \xC8 una funzionalit\xE0 speciale di Colab, che comprime il traceback in \u201Cframe\u201D. Se non riesci a trovare l\u2019origine di un errore, assicurati di espandere l\u2019intero traceback facendo clic su quelle due piccole frecce."),k.forEach(o)},m(w,k){r(w,m,k),t(m,y)},d(w){w&&o(m)}}}function el(K){let m,y,w,k,D,q,P,O,Q,Z,M;return{c(){m=n("p"),y=s("\u{1F4A1} Se vedi un messaggio di errore che \xE8 difficile da capire, copia e incolla il messaggio nella barra di ricerca di Google o di "),w=n("a"),k=s("Stack Overflow"),D=s(" (s\xEC, davvero!). C\u2019\xE8 una buona probabilit\xE0 che non sei la prima persona a riscontrare l\u2019errore, e questo \xE8 un buon modo per trovare le soluzioni pubblicate da altri utenti della community. Ad esempio, cercando "),q=n("code"),P=s("OSError: Can't load config for"),O=s(" su Stack Overflow si ottengono diversi "),Q=n("a"),Z=s("risultati"),M=s(" che possono essere usati come punto di partenza per risolvere il problema."),this.h()},l(N){m=l(N,"P",{});var E=u(m);y=a(E,"\u{1F4A1} Se vedi un messaggio di errore che \xE8 difficile da capire, copia e incolla il messaggio nella barra di ricerca di Google o di "),w=l(E,"A",{href:!0,rel:!0});var ro=u(w);k=a(ro,"Stack Overflow"),ro.forEach(o),D=a(E," (s\xEC, davvero!). C\u2019\xE8 una buona probabilit\xE0 che non sei la prima persona a riscontrare l\u2019errore, e questo \xE8 un buon modo per trovare le soluzioni pubblicate da altri utenti della community. Ad esempio, cercando "),q=l(E,"CODE",{});var J=u(q);P=a(J,"OSError: Can't load config for"),J.forEach(o),O=a(E," su Stack Overflow si ottengono diversi "),Q=l(E,"A",{href:!0,rel:!0});var no=u(Q);Z=a(no,"risultati"),no.forEach(o),M=a(E," che possono essere usati come punto di partenza per risolvere il problema."),E.forEach(o),this.h()},h(){c(w,"href","https://stackoverflow.com/"),c(w,"rel","nofollow"),c(Q,"href","https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+"),c(Q,"rel","nofollow")},m(N,E){r(N,m,E),t(m,y),t(m,w),t(w,k),t(m,D),t(m,q),t(q,P),t(m,O),t(m,Q),t(Q,Z),t(m,M)},d(N){N&&o(m)}}}function ol(K){let m,y,w,k,D;return{c(){m=n("p"),y=s("\u{1F6A8} L\u2019approccio che stiamo adottando non \xE8 infallibile, poich\xE9 il/la nostro/a collega potrebbe aver modificato la configurazione di "),w=n("code"),k=s("distilbert-base-uncased"),D=s(" prima di affinare il modello. Nella vita reale, dovremmo verificare prima con loro, ma per lo scopo di questa sezione assumeremo che abbiano usato la configurazione predefinita.")},l(q){m=l(q,"P",{});var P=u(m);y=a(P,"\u{1F6A8} L\u2019approccio che stiamo adottando non \xE8 infallibile, poich\xE9 il/la nostro/a collega potrebbe aver modificato la configurazione di "),w=l(P,"CODE",{});var O=u(w);k=a(O,"distilbert-base-uncased"),O.forEach(o),D=a(P," prima di affinare il modello. Nella vita reale, dovremmo verificare prima con loro, ma per lo scopo di questa sezione assumeremo che abbiano usato la configurazione predefinita."),P.forEach(o)},m(q,P){r(q,m,P),t(m,y),t(m,w),t(w,k),t(m,D)},d(q){q&&o(m)}}}function tl(K){let m,y,w,k,D,q,P,O,Q,Z,M,N,E,ro,J,no,Xi,kt,fe,qt,L,Ki,_e,Zi,es,he,os,ts,Et,ge,yt,lo,is,zt,be,jt,ee,ss,Ao,as,rs,xt,ve,Pt,oe,ns,Do,ls,us,Ct,Y,te,Oo,we,ps,So,ds,At,uo,cs,Dt,po,$e,ms,co,fs,_s,Ot,ie,hs,To,gs,bs,St,ke,Tt,qe,It,G,vs,Io,ws,$s,Ho,ks,qs,Ht,Ee,ye,Er,Qt,C,Es,Qo,ys,zs,Mo,js,xs,mo,Ps,Cs,No,As,Ds,Mt,se,Nt,F,Os,Lo,Ss,Ts,Go,Is,Hs,Lt,ze,Gt,ae,Ft,fo,Qs,Rt,je,xe,yr,Ut,_o,Ms,Vt,Pe,Ce,zr,Wt,ho,Ns,Bt,Ae,Jt,De,Yt,R,Ls,Fo,Gs,Fs,Ro,Rs,Us,Xt,Oe,Kt,Se,Zt,z,Vs,Uo,Ws,Bs,Vo,Js,Ys,Te,Wo,Xs,Ks,go,Zs,ea,Bo,oa,ta,ei,Ie,oi,re,ti,ne,ia,Jo,sa,aa,ii,He,si,le,ra,Yo,na,la,ai,Qe,ri,Me,ni,bo,ua,li,S,Ne,pa,Xo,da,ca,ma,Ko,fa,_a,Zo,ha,ga,et,ba,ui,vo,va,pi,X,ue,ot,Le,wa,tt,$a,di,U,ka,it,qa,Ea,st,ya,za,ci,Ge,mi,wo,ja,fi,Fe,_i,pe,xa,$o,Pa,Ca,hi,Re,gi,Ue,bi,ko,Aa,vi,Ve,wi,qo,Da,$i,We,ki,j,Oa,at,Sa,Ta,rt,Ia,Ha,nt,Qa,Ma,lt,Na,La,ut,Ga,Fa,qi,Be,Ei,Je,yi,V,Ra,pt,Ua,Va,dt,Wa,Ba,zi,Ye,ji,Xe,xi,x,Ja,ct,Ya,Xa,Eo,Ka,Za,mt,er,or,ft,tr,ir,_t,sr,ar,Pi,Ke,Ci,A,rr,ht,nr,lr,gt,ur,pr,bt,dr,cr,Ze,mr,fr,Ai,eo,oo,jr,Di,de,_r,vt,hr,gr,Oi,to,Si,io,Ti,ce,br,so,vr,wr,Ii;return q=new qr({}),M=new Kn({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter8/section2.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter8/section2.ipynb"}]}}),fe=new kr({props:{id:"DQ-CpJn6Rc4"}}),ge=new $({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),be=new $({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}}),ve=new $({props:{code:`from distutils.dir_util import copy_tree
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name


def copy_repository_template():
    # Clone the repo and extract the local path
    template_repo_id = "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"
    commit_hash = "be3eaffc28669d7932492681cd5f3e8905e358b4"
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    # Create an empty repo on the Hub
    model_name = template_repo_id.split("/")[1]
    create_repo(model_name, exist_ok=True)
    # Clone the empty repo
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    # Copy files
    copy_tree(template_repo_dir, new_repo_dir)
    # Push to Hub
    repo.push_to_hub()`,highlighted:`<span class="hljs-keyword">from</span> distutils.dir_util <span class="hljs-keyword">import</span> copy_tree
<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository, snapshot_download, create_repo, get_full_repo_name


<span class="hljs-keyword">def</span> <span class="hljs-title function_">copy_repository_template</span>():
    <span class="hljs-comment"># Clone the repo and extract the local path</span>
    template_repo_id = <span class="hljs-string">&quot;lewtun/distilbert-base-uncased-finetuned-squad-d5716d28&quot;</span>
    commit_hash = <span class="hljs-string">&quot;be3eaffc28669d7932492681cd5f3e8905e358b4&quot;</span>
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    <span class="hljs-comment"># Create an empty repo on the Hub</span>
    model_name = template_repo_id.split(<span class="hljs-string">&quot;/&quot;</span>)[<span class="hljs-number">1</span>]
    create_repo(model_name, exist_ok=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Clone the empty repo</span>
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    <span class="hljs-comment"># Copy files</span>
    copy_tree(template_repo_dir, new_repo_dir)
    <span class="hljs-comment"># Push to Hub</span>
    repo.push_to_hub()`}}),we=new qr({}),ke=new $({props:{code:`from transformers import pipeline

model_checkpoint = get_full_repo_name("distillbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

model_checkpoint = get_full_repo_name(<span class="hljs-string">&quot;distillbert-base-uncased-finetuned-squad-d5716d28&quot;</span>)
reader = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model_checkpoint)`}}),qe=new $({props:{code:`"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
OSError: Can&#x27;t load config for &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27;. Make sure that:

- &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27; is a correct model identifier listed on &#x27;https://huggingface.co/models&#x27;

- or &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27; is the correct path to a directory containing a config.json file
&quot;&quot;&quot;</span>`}}),se=new $r({props:{$$slots:{default:[Zn]},$$scope:{ctx:K}}}),ze=new $({props:{code:`"""
Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
Make sure that:

- &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27; is a correct model identifier listed on &#x27;https://huggingface.co/models&#x27;

- or &#x27;lewtun/distillbert-base-uncased-finetuned-squad-d5716d28&#x27; is the correct path to a directory containing a config.json file
&quot;&quot;&quot;</span>`}}),ae=new $r({props:{$$slots:{default:[el]},$$scope:{ctx:K}}}),Ae=new $({props:{code:`model_checkpoint = get_full_repo_name("distilbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)`,highlighted:`model_checkpoint = get_full_repo_name(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-squad-d5716d28&quot;</span>)
reader = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model_checkpoint)`}}),De=new $({props:{code:`"""
OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
OSError: Can&#x27;t load config for &#x27;lewtun/distilbert-base-uncased-finetuned-squad-d5716d28&#x27;. Make sure that:

- &#x27;lewtun/distilbert-base-uncased-finetuned-squad-d5716d28&#x27; is a correct model identifier listed on &#x27;https://huggingface.co/models&#x27;

- or &#x27;lewtun/distilbert-base-uncased-finetuned-squad-d5716d28&#x27; is the correct path to a directory containing a config.json file
&quot;&quot;&quot;</span>`}}),Oe=new $({props:{code:`from huggingface_hub import list_repo_files

list_repo_files(repo_id=model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> list_repo_files

list_repo_files(repo_id=model_checkpoint)`}}),Se=new $({props:{code:"['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt']",highlighted:'[<span class="hljs-string">&#x27;.gitattributes&#x27;</span>, <span class="hljs-string">&#x27;README.md&#x27;</span>, <span class="hljs-string">&#x27;pytorch_model.bin&#x27;</span>, <span class="hljs-string">&#x27;special_tokens_map.json&#x27;</span>, <span class="hljs-string">&#x27;tokenizer_config.json&#x27;</span>, <span class="hljs-string">&#x27;training_args.bin&#x27;</span>, <span class="hljs-string">&#x27;vocab.txt&#x27;</span>]'}}),Ie=new $({props:{code:`from transformers import AutoConfig

pretrained_checkpoint = "distilbert-base-uncased"
config = AutoConfig.from_pretrained(pretrained_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

pretrained_checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
config = AutoConfig.from_pretrained(pretrained_checkpoint)`}}),re=new $r({props:{warning:!0,$$slots:{default:[ol]},$$scope:{ctx:K}}}),He=new $({props:{code:'config.push_to_hub(model_checkpoint, commit_message="Add config.json")',highlighted:'config.push_to_hub(model_checkpoint, commit_message=<span class="hljs-string">&quot;Add config.json&quot;</span>)'}}),Qe=new $({props:{code:`reader = pipeline("question-answering", model=model_checkpoint, revision="main")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

\u{1F917} Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
"""

question = "What is extractive question answering?"
reader(question=question, context=context)`,highlighted:`reader = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model_checkpoint, revision=<span class="hljs-string">&quot;main&quot;</span>)

context = <span class="hljs-string">r&quot;&quot;&quot;
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

\u{1F917} Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
&quot;&quot;&quot;</span>

question = <span class="hljs-string">&quot;What is extractive question answering?&quot;</span>
reader(question=question, context=context)`}}),Me=new $({props:{code:`{'score': 0.38669535517692566,
 'start': 34,
 'end': 95,
 'answer': 'the task of extracting an answer from a text given a question'}`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.38669535517692566</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">34</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">95</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;the task of extracting an answer from a text given a question&#x27;</span>}`}}),Le=new qr({}),Ge=new $({props:{code:`tokenizer = reader.tokenizer
model = reader.model`,highlighted:`tokenizer = reader.tokenizer
model = reader.model`}}),Fe=new $({props:{code:'question = "Which frameworks can I use?"',highlighted:'question = <span class="hljs-string">&quot;Which frameworks can I use?&quot;</span>'}}),Re=new $({props:{code:`import torch

inputs = tokenizer(question, context, add_special_tokens=True)
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")`,highlighted:`<span class="hljs-keyword">import</span> torch

inputs = tokenizer(question, context, add_special_tokens=<span class="hljs-literal">True</span>)
input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
<span class="hljs-comment"># Get the most likely beginning of answer with the argmax of the score</span>
answer_start = torch.argmax(answer_start_scores)
<span class="hljs-comment"># Get the most likely end of answer with the argmax of the score</span>
answer_end = torch.argmax(answer_end_scores) + <span class="hljs-number">1</span>
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Question: <span class="hljs-subst">{question}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Answer: <span class="hljs-subst">{answer}</span>&quot;</span>)`}}),Ue=new $({props:{code:`"""
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs["input_ids"]
----> 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724
--> 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in &lt;module&gt;
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs[&quot;input_ids&quot;]
----&gt; 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724
--&gt; 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError(&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;)
    472         elif input_ids is not None:
--&gt; 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: &#x27;list&#x27; object has no attribute &#x27;size&#x27;
&quot;&quot;&quot;</span>`}}),Ve=new kr({props:{id:"rSPyvPw0p9k"}}),We=new kr({props:{id:"5PkZ4rbHL6c"}}),Be=new $({props:{code:'inputs["input_ids"][:5]',highlighted:'inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][:<span class="hljs-number">5</span>]'}}),Je=new $({props:{code:"[101, 2029, 7705, 2015, 2064]",highlighted:'[<span class="hljs-number">101</span>, <span class="hljs-number">2029</span>, <span class="hljs-number">7705</span>, <span class="hljs-number">2015</span>, <span class="hljs-number">2064</span>]'}}),Ye=new $({props:{code:'type(inputs["input_ids"])',highlighted:'<span class="hljs-built_in">type</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),Xe=new $({props:{code:"list",highlighted:'<span class="hljs-built_in">list</span>'}}),Ke=new $({props:{code:`~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'`,highlighted:`~<span class="hljs-regexp">/miniconda3/</span>envs<span class="hljs-regexp">/huggingface/</span>lib<span class="hljs-regexp">/python3.8/</span>site-packages<span class="hljs-regexp">/transformers/m</span>odels<span class="hljs-regexp">/distilbert/m</span>odeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    <span class="hljs-number">471</span>             raise ValueError(<span class="hljs-string">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span>)
    <span class="hljs-number">472</span>         elif input_ids is not None:
--&gt; <span class="hljs-number">473</span>             input_shape = input_ids.<span class="hljs-keyword">size</span>()
    <span class="hljs-number">474</span>         elif inputs_embeds is not None:
    <span class="hljs-number">475</span>             input_shape = inputs_embeds.<span class="hljs-keyword">size</span>()[:-<span class="hljs-number">1</span>]

AttributeError: <span class="hljs-string">&#x27;list&#x27;</span> object has no attribute <span class="hljs-string">&#x27;size&#x27;</span>`}}),to=new $({props:{code:`inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# Get the most likely beginning of answer with the argmax of the score
answer_start = torch.argmax(answer_start_scores)
# Get the most likely end of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")`,highlighted:`inputs = tokenizer(question, context, add_special_tokens=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
<span class="hljs-comment"># Get the most likely beginning of answer with the argmax of the score</span>
answer_start = torch.argmax(answer_start_scores)
<span class="hljs-comment"># Get the most likely end of answer with the argmax of the score</span>
answer_end = torch.argmax(answer_end_scores) + <span class="hljs-number">1</span>
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Question: <span class="hljs-subst">{question}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Answer: <span class="hljs-subst">{answer}</span>&quot;</span>)`}}),io=new $({props:{code:`"""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
&quot;&quot;&quot;</span>`}}),{c(){m=n("meta"),y=p(),w=n("h1"),k=n("a"),D=n("span"),f(q.$$.fragment),P=p(),O=n("span"),Q=s("Cosa fare quando si riceve un errore"),Z=p(),f(M.$$.fragment),N=p(),E=n("p"),ro=s("In questa sezione esamineremo alcuni errori comuni che possono verificarsi quando si cerca di generare previsioni dal modello Transformer appena affinato. Questo ti preparer\xE0 alla "),J=n("a"),no=s("sezione 4"),Xi=s(", in cui esploreremo come eseguire il debug della fase di training."),kt=p(),f(fe.$$.fragment),qt=p(),L=n("p"),Ki=s("Per questa sezione, abbiamo preparato un "),_e=n("a"),Zi=s("template repository del modello"),es=s(" e, se vuoi eseguire il codice di questo capitolo, dovrai prima copiare il modello nel tuo account su "),he=n("a"),os=s("Hugging Face Hub"),ts=s(". Per farlo, occorre innanzitutto effettuare il login eseguendo una delle seguenti operazioni in un Jupyter notebook:"),Et=p(),f(ge.$$.fragment),yt=p(),lo=n("p"),is=s("o il seguente nel tuo terminale preferito:"),zt=p(),f(be.$$.fragment),jt=p(),ee=n("p"),ss=s("Questo chieder\xE0 di inserire il nome utente e la password e salver\xE0 un token in "),Ao=n("em"),as=s("~/.cache/huggingface/"),rs=s(". Una volta effettuato l\u2019accesso, \xE8 possibile copiare il template repository con la seguente funzione:"),xt=p(),f(ve.$$.fragment),Pt=p(),oe=n("p"),ns=s("A questo punto, quando si esegue "),Do=n("code"),ls=s("copy_repository_template()"),us=s(", verr\xE0 creata una copia del template repository nel proprio account."),Ct=p(),Y=n("h2"),te=n("a"),Oo=n("span"),f(we.$$.fragment),ps=p(),So=n("span"),ds=s("Fare il debug della pipeline di \u{1F917} Transformers"),At=p(),uo=n("p"),cs=s("Per iniziare il nostro viaggio nel fantastico mondo del debug dei modelli Transformer, considera lo scenario seguente: stai lavorando con un/a collega a un progetto di risposta alle domande per aiutare i clienti di un sito web di e-commerce a trovare risposte sui prodotti di consumo. Il/La collega ti invia un messaggio del tipo:"),Dt=p(),po=n("blockquote"),$e=n("p"),ms=s("Ciao! Ho appena fatto un esperimento utilizzando le tecniche del "),co=n("a"),fs=s("Capitolo 7"),_s=s(" del corso di Hugging Face e ho ottenuto ottimi risultati su SQuAD! Penso che possiamo usare questo modello come punto di partenza per il nostro progetto. L\u2019ID del modello sull\u2019Hub \xE8 \u201Clewtun/distillbert-base-uncased-finetuned-squad-d5716d28\u201D. Provalo pure :)"),Ot=p(),ie=n("p"),hs=s("e la prima cosa che pensi \xE8 di caricare il modello usando la "),To=n("code"),gs=s("pipeline"),bs=s(" di \u{1F917} Transformers:"),St=p(),f(ke.$$.fragment),Tt=p(),f(qe.$$.fragment),It=p(),G=n("p"),vs=s("Oh no, sembra che qualcosa sia andato storto! Se sei alle prime armi con la programmazione, questo tipo di errori pu\xF2 sembrare un po\u2019 criptico all\u2019inizio (cos\u2019\xE8 un "),Io=n("code"),ws=s("OSError"),$s=s("?!). L\u2019errore visualizzato qui \xE8 solo l\u2019ultima parte di un messaggio di errore molto pi\xF9 ampio, chiamato "),Ho=n("em"),ks=s("Python traceback"),qs=s(" (anche detto stack trace). Per esempio, se si esegue questo codice su Google Colab, si dovrebbe vedere qualcosa di simile alla seguente schermata:"),Ht=p(),Ee=n("div"),ye=n("img"),Qt=p(),C=n("p"),Es=s("Questi messaggi contengono molte informazioni, quindi analizziamo insieme le parti principali. La prima cosa da notare \xE8 che i traceback devono essere letti "),Qo=n("em"),ys=s("dal basso verso l\u2019alto"),zs=s(". Questo pu\xF2 sembrare strano se si \xE8 abituati a leggere dall\u2019alto verso il basso, ma riflette il fatto che il traceback mostra la sequenza di chiamate delle funzioni che la "),Mo=n("code"),js=s("pipeline"),xs=s(" effettua quando scarica il modello e il tokenizer. (Dai un\u2019occhiata al "),mo=n("a"),Ps=s("Capitolo 2"),Cs=s(" per maggiori dettagli su come funziona la "),No=n("code"),As=s("pipeline"),Ds=s(".)"),Mt=p(),f(se.$$.fragment),Nt=p(),F=n("p"),Os=s("Ci\xF2 significa che l\u2019ultima riga del traceback indica l\u2019ultimo messaggio di errore e fornisce il nome dell\u2019eccezione sollevata. In questo caso, il tipo di eccezione \xE8 "),Lo=n("code"),Ss=s("OSError"),Ts=s(", che indica un errore legato al sistema. Leggendo il messaggio di errore, si pu\xF2 notare che sembra esserci un problema con il file "),Go=n("em"),Is=s("config.json"),Hs=s(" del modello e vengono forniti due suggerimenti per risolverlo:"),Lt=p(),f(ze.$$.fragment),Gt=p(),f(ae.$$.fragment),Ft=p(),fo=n("p"),Qs=s("Il primo suggerimento ci chiede di verificare se l\u2019ID del modello \xE8 effettivamente corretto, quindi la prima cosa da fare \xE8 copiare l\u2019identificativo e incollarlo nella barra di ricerca dell\u2019Hub:"),Rt=p(),je=n("div"),xe=n("img"),Ut=p(),_o=n("p"),Ms=s("Mmm, sembra proprio che il modello del/la nostro/a collega non sia sull\u2019Hub\u2026 aha, ma c\u2019\xE8 un errore di battitura nel nome del modello! DistilBERT ha solo una \u201Cl\u201D nel suo nome, quindi correggiamolo e cerchiamo invece \u201Clewtun/distilbert-base-uncased-finetuned-squad-d5716d28\u201D:"),Vt=p(),Pe=n("div"),Ce=n("img"),Wt=p(),ho=n("p"),Ns=s("Ok, questo c\u2019\xE8. Ora proviamo a scaricare di nuovo il modello con l\u2019ID corretto:"),Bt=p(),f(Ae.$$.fragment),Jt=p(),f(De.$$.fragment),Yt=p(),R=n("p"),Ls=s("Argh, \xE8 fallito ancora - benvenuti nella routine quotidiana di un machine learning engineer! Poich\xE9 abbiamo aggiustato l\u2019ID del modello, il problema deve essere nel repository stesso. Un modo rapido per accedere al contenuto di un repository sul \u{1F917} Hub \xE8 la funzione "),Fo=n("code"),Gs=s("list_repo_files()"),Fs=s(" della libreria "),Ro=n("code"),Rs=s("huggingface_hub"),Us=s(":"),Xt=p(),f(Oe.$$.fragment),Kt=p(),f(Se.$$.fragment),Zt=p(),z=n("p"),Vs=s("Interessante: non sembra esserci un file "),Uo=n("em"),Ws=s("config.json"),Bs=s(" nel repository! Non c\u2019\xE8 da stupirsi che la nostra "),Vo=n("code"),Js=s("pipeline"),Ys=s(" non riesca a caricare il modello; il/la nostro/a collega deve aver dimenticato di inserire questo file nell\u2019Hub dopo averlo affinato. In questo caso, il problema sembra abbastanza semplice da risolvere: potremmo chiedere loro di aggiungere il file, oppure, dato che possiamo vedere dall\u2019ID del modello che il modello pre-addestrato usato \xE8 "),Te=n("a"),Wo=n("code"),Xs=s("distilbert-base-uncased"),Ks=s(", possiamo scaricare la configurazione di questo modello e inserirla nel nostro repository per vedere se questo risolve il problema. Proviamo. Utilizzando le tecniche apprese nel "),go=n("a"),Zs=s("Capitolo 2"),ea=s(", possiamo scaricare la configurazione del modello con la classe "),Bo=n("code"),oa=s("AutoConfig"),ta=s(":"),ei=p(),f(Ie.$$.fragment),oi=p(),f(re.$$.fragment),ti=p(),ne=n("p"),ia=s("Possiamo quindi inviarlo al nostro repository del modello con la funzione "),Jo=n("code"),sa=s("push_to_hub()"),aa=s(" della configurazione:"),ii=p(),f(He.$$.fragment),si=p(),le=n("p"),ra=s("Ora possiamo verificare se ha funzionato, caricando il modello dall\u2019ultimo commit del ramo "),Yo=n("code"),na=s("main"),la=s(":"),ai=p(),f(Qe.$$.fragment),ri=p(),f(Me.$$.fragment),ni=p(),bo=n("p"),ua=s("Woohoo, ha funzionato! Riassumiamo quello che hai appena imparato:"),li=p(),S=n("ul"),Ne=n("li"),pa=s("I messaggi di errore in Python sono noti come "),Xo=n("em"),da=s("traceback"),ca=s(" e vengono letti dal basso verso l\u2019alto. L\u2019ultima riga del messaggio di errore di solito contiene le informazioni necessarie per individuare l\u2019origine del problema."),ma=p(),Ko=n("li"),fa=s("Se l\u2019ultima riga non contiene informazioni sufficienti, risali il traceback e vedi se riesci a identificare in quale punto del codice sorgente si verifica l\u2019errore."),_a=p(),Zo=n("li"),ha=s("Se nessuno dei messaggi di errore pu\xF2 aiutare a individuare il problema, provare a cercare online una soluzione a un problema simile."),ga=p(),et=n("li"),ba=s(`Il \`huggingface_hub\u2019
// \u{1F917} Hub?
fornisce una serie di strumenti che si possono usare per interagire e fare il debug dei repository su Hub.`),ui=p(),vo=n("p"),va=s("Ora che sappiamo come eseguire il debug di una pipeline, diamo un\u2019occhiata a un esempio pi\xF9 complicato nel forward pass del modello stesso."),pi=p(),X=n("h2"),ue=n("a"),ot=n("span"),f(Le.$$.fragment),wa=p(),tt=n("span"),$a=s("Debug del forward pass del modello"),di=p(),U=n("p"),ka=s("Sebbene la "),it=n("code"),qa=s("pipeline"),Ea=s(" sia ottima per la maggior parte delle applicazioni in cui \xE8 necessario generare previsioni rapidamente, a volte \xE8 necessario accedere ai logit del modello (ad esempio, se si desidera applicare un post-processing personalizzato). Per vedere cosa potrebbe andare storto in questo caso, iniziamo prendendo il modello e il tokenizer dalla nostra "),st=n("code"),ya=s("pipeline"),za=s(":"),ci=p(),f(Ge.$$.fragment),mi=p(),wo=n("p"),ja=s("Poi abbiamo bisogno di una domanda, quindi vediamo se i nostri framework preferiti sono supportati:"),fi=p(),f(Fe.$$.fragment),_i=p(),pe=n("p"),xa=s("Come abbiamo visto nel "),$o=n("a"),Pa=s("Capitolo 7"),Ca=s(", i passaggi tipici da svolgere sono la tokenizzazione degli input, l\u2019estrazione dei logit dei token iniziali e finali e la decodifica dell\u2019intervallo di risposta:"),hi=p(),f(Re.$$.fragment),gi=p(),f(Ue.$$.fragment),bi=p(),ko=n("p"),Aa=s("Oh cielo, sembra che ci sia un bug nel nostro codice! Ma non avere paura di un po\u2019 di debug. Puoi usare il debugger di Python in un notebook:"),vi=p(),f(Ve.$$.fragment),wi=p(),qo=n("p"),Da=s("o dal terminale:"),$i=p(),f(We.$$.fragment),ki=p(),j=n("p"),Oa=s("Qui, leggendo il messaggio di errore vediamo che l\u2019oggetto "),at=n("code"),Sa=s("'list'"),Ta=s(" non ha attributo "),rt=n("code"),Ia=s("'size'"),Ha=s(", e possiamo vedere una "),nt=n("code"),Qa=s("-->"),Ma=s(" freccia che punta alla riga in cui il problema \xE8 stato sollevato in "),lt=n("code"),Na=s("model(**inputs)"),La=s(". Possiamo eseguire il debug interattivo utilizzando il debugger di Python, ma per ora ci limiteremo a stampare una parte di "),ut=n("code"),Ga=s("input"),Fa=s(" per vedere cosa abbiamo:"),qi=p(),f(Be.$$.fragment),Ei=p(),f(Je.$$.fragment),yi=p(),V=n("p"),Ra=s("Questo sembra certamente una normale "),pt=n("code"),Ua=s("list"),Va=s(" di Python, ma ricontrolliamo il "),dt=n("code"),Wa=s("type"),Ba=s(":"),zi=p(),f(Ye.$$.fragment),ji=p(),f(Xe.$$.fragment),xi=p(),x=n("p"),Ja=s("S\xEC, questa \xE8 sicuramente una "),ct=n("code"),Ya=s("list"),Xa=s(" di Python. Cos\u2019\xE8 andato storto? Ricordiamo dal "),Eo=n("a"),Ka=s("Capitolo 2"),Za=s(" che le classi "),mt=n("code"),er=s("AutoModelForXxx"),or=s(" in \u{1F917} Transformers operano su "),ft=n("em"),tr=s("tensori"),ir=s(" (sia in PyTorch che in TensorFlow), e un\u2019operazione comune \xE8 quella di estrarre le dimensioni di un tensore usando "),_t=n("code"),sr=s("Tensor.size()"),ar=s(" in PyTorch, per esempio. Diamo un\u2019altra occhiata al traceback, per vedere quale riga ha causato l\u2019eccezione:"),Pi=p(),f(Ke.$$.fragment),Ci=p(),A=n("p"),rr=s("Sembra che il nostro codice abbia provato a chiamare "),ht=n("code"),nr=s("input_ids.size()"),lr=s(", ma questo chiaramente non funziona per una "),gt=n("code"),ur=s("list"),pr=s("di Python, che \xE8 solo un "),bt=n("em"),dr=s("container"),cr=s(". Come possiamo risolvere questo problema? Cercando il messaggio di errore su Stack Overflow si ottengono alcuni "),Ze=n("a"),mr=s("risultati"),fr=s(" pertinenti. Cliccando sul primo, viene visualizzata una domanda simile alla nostra, con la risposta mostrata nello screenshot seguente:"),Ai=p(),eo=n("div"),oo=n("img"),Di=p(),de=n("p"),_r=s("La risposta raccomanda di aggiungere "),vt=n("code"),hr=s("return_tensors='pt'"),gr=s(" al tokenizer, quindi proviamo se funziona:"),Oi=p(),f(to.$$.fragment),Si=p(),f(io.$$.fragment),Ti=p(),ce=n("p"),br=s("Bene, ha funzionato! Questo \xE8 un ottimo esempio di quanto possa essere utile Stack Overflow: identificando un problema simile, abbiamo potuto beneficiare dell\u2019esperienza di altri membri della community. Tuttavia, una ricerca come questa non sempre produce una risposta pertinente, quindi cosa si pu\xF2 fare in questi casi? Fortunatamente, sul "),so=n("a"),vr=s("forum di Hugging Face"),wr=s(" c\u2019\xE8 un\u2019accogliente community di sviluppatori che pu\xF2 aiutarti! Nella prossima sezione, vedremo come creare bene delle domande sul forum che abbiano buona probabilit\xE0 di ricevere una risposta."),this.h()},l(e){const i=Yn('[data-svelte="svelte-1phssyn"]',document.head);m=l(i,"META",{name:!0,content:!0}),i.forEach(o),y=d(e),w=l(e,"H1",{class:!0});var ao=u(w);k=l(ao,"A",{id:!0,class:!0,href:!0});var wt=u(k);D=l(wt,"SPAN",{});var $t=u(D);_(q.$$.fragment,$t),$t.forEach(o),wt.forEach(o),P=d(ao),O=l(ao,"SPAN",{});var xr=u(O);Q=a(xr,"Cosa fare quando si riceve un errore"),xr.forEach(o),ao.forEach(o),Z=d(e),_(M.$$.fragment,e),N=d(e),E=l(e,"P",{});var Hi=u(E);ro=a(Hi,"In questa sezione esamineremo alcuni errori comuni che possono verificarsi quando si cerca di generare previsioni dal modello Transformer appena affinato. Questo ti preparer\xE0 alla "),J=l(Hi,"A",{href:!0});var Pr=u(J);no=a(Pr,"sezione 4"),Pr.forEach(o),Xi=a(Hi,", in cui esploreremo come eseguire il debug della fase di training."),Hi.forEach(o),kt=d(e),_(fe.$$.fragment,e),qt=d(e),L=l(e,"P",{});var yo=u(L);Ki=a(yo,"Per questa sezione, abbiamo preparato un "),_e=l(yo,"A",{href:!0,rel:!0});var Cr=u(_e);Zi=a(Cr,"template repository del modello"),Cr.forEach(o),es=a(yo," e, se vuoi eseguire il codice di questo capitolo, dovrai prima copiare il modello nel tuo account su "),he=l(yo,"A",{href:!0,rel:!0});var Ar=u(he);os=a(Ar,"Hugging Face Hub"),Ar.forEach(o),ts=a(yo,". Per farlo, occorre innanzitutto effettuare il login eseguendo una delle seguenti operazioni in un Jupyter notebook:"),yo.forEach(o),Et=d(e),_(ge.$$.fragment,e),yt=d(e),lo=l(e,"P",{});var Dr=u(lo);is=a(Dr,"o il seguente nel tuo terminale preferito:"),Dr.forEach(o),zt=d(e),_(be.$$.fragment,e),jt=d(e),ee=l(e,"P",{});var Qi=u(ee);ss=a(Qi,"Questo chieder\xE0 di inserire il nome utente e la password e salver\xE0 un token in "),Ao=l(Qi,"EM",{});var Or=u(Ao);as=a(Or,"~/.cache/huggingface/"),Or.forEach(o),rs=a(Qi,". Una volta effettuato l\u2019accesso, \xE8 possibile copiare il template repository con la seguente funzione:"),Qi.forEach(o),xt=d(e),_(ve.$$.fragment,e),Pt=d(e),oe=l(e,"P",{});var Mi=u(oe);ns=a(Mi,"A questo punto, quando si esegue "),Do=l(Mi,"CODE",{});var Sr=u(Do);ls=a(Sr,"copy_repository_template()"),Sr.forEach(o),us=a(Mi,", verr\xE0 creata una copia del template repository nel proprio account."),Mi.forEach(o),Ct=d(e),Y=l(e,"H2",{class:!0});var Ni=u(Y);te=l(Ni,"A",{id:!0,class:!0,href:!0});var Tr=u(te);Oo=l(Tr,"SPAN",{});var Ir=u(Oo);_(we.$$.fragment,Ir),Ir.forEach(o),Tr.forEach(o),ps=d(Ni),So=l(Ni,"SPAN",{});var Hr=u(So);ds=a(Hr,"Fare il debug della pipeline di \u{1F917} Transformers"),Hr.forEach(o),Ni.forEach(o),At=d(e),uo=l(e,"P",{});var Qr=u(uo);cs=a(Qr,"Per iniziare il nostro viaggio nel fantastico mondo del debug dei modelli Transformer, considera lo scenario seguente: stai lavorando con un/a collega a un progetto di risposta alle domande per aiutare i clienti di un sito web di e-commerce a trovare risposte sui prodotti di consumo. Il/La collega ti invia un messaggio del tipo:"),Qr.forEach(o),Dt=d(e),po=l(e,"BLOCKQUOTE",{});var Mr=u(po);$e=l(Mr,"P",{});var Li=u($e);ms=a(Li,"Ciao! Ho appena fatto un esperimento utilizzando le tecniche del "),co=l(Li,"A",{href:!0});var Nr=u(co);fs=a(Nr,"Capitolo 7"),Nr.forEach(o),_s=a(Li," del corso di Hugging Face e ho ottenuto ottimi risultati su SQuAD! Penso che possiamo usare questo modello come punto di partenza per il nostro progetto. L\u2019ID del modello sull\u2019Hub \xE8 \u201Clewtun/distillbert-base-uncased-finetuned-squad-d5716d28\u201D. Provalo pure :)"),Li.forEach(o),Mr.forEach(o),Ot=d(e),ie=l(e,"P",{});var Gi=u(ie);hs=a(Gi,"e la prima cosa che pensi \xE8 di caricare il modello usando la "),To=l(Gi,"CODE",{});var Lr=u(To);gs=a(Lr,"pipeline"),Lr.forEach(o),bs=a(Gi," di \u{1F917} Transformers:"),Gi.forEach(o),St=d(e),_(ke.$$.fragment,e),Tt=d(e),_(qe.$$.fragment,e),It=d(e),G=l(e,"P",{});var zo=u(G);vs=a(zo,"Oh no, sembra che qualcosa sia andato storto! Se sei alle prime armi con la programmazione, questo tipo di errori pu\xF2 sembrare un po\u2019 criptico all\u2019inizio (cos\u2019\xE8 un "),Io=l(zo,"CODE",{});var Gr=u(Io);ws=a(Gr,"OSError"),Gr.forEach(o),$s=a(zo,"?!). L\u2019errore visualizzato qui \xE8 solo l\u2019ultima parte di un messaggio di errore molto pi\xF9 ampio, chiamato "),Ho=l(zo,"EM",{});var Fr=u(Ho);ks=a(Fr,"Python traceback"),Fr.forEach(o),qs=a(zo," (anche detto stack trace). Per esempio, se si esegue questo codice su Google Colab, si dovrebbe vedere qualcosa di simile alla seguente schermata:"),zo.forEach(o),Ht=d(e),Ee=l(e,"DIV",{class:!0});var Rr=u(Ee);ye=l(Rr,"IMG",{src:!0,alt:!0,width:!0}),Rr.forEach(o),Qt=d(e),C=l(e,"P",{});var W=u(C);Es=a(W,"Questi messaggi contengono molte informazioni, quindi analizziamo insieme le parti principali. La prima cosa da notare \xE8 che i traceback devono essere letti "),Qo=l(W,"EM",{});var Ur=u(Qo);ys=a(Ur,"dal basso verso l\u2019alto"),Ur.forEach(o),zs=a(W,". Questo pu\xF2 sembrare strano se si \xE8 abituati a leggere dall\u2019alto verso il basso, ma riflette il fatto che il traceback mostra la sequenza di chiamate delle funzioni che la "),Mo=l(W,"CODE",{});var Vr=u(Mo);js=a(Vr,"pipeline"),Vr.forEach(o),xs=a(W," effettua quando scarica il modello e il tokenizer. (Dai un\u2019occhiata al "),mo=l(W,"A",{href:!0});var Wr=u(mo);Ps=a(Wr,"Capitolo 2"),Wr.forEach(o),Cs=a(W," per maggiori dettagli su come funziona la "),No=l(W,"CODE",{});var Br=u(No);As=a(Br,"pipeline"),Br.forEach(o),Ds=a(W,".)"),W.forEach(o),Mt=d(e),_(se.$$.fragment,e),Nt=d(e),F=l(e,"P",{});var jo=u(F);Os=a(jo,"Ci\xF2 significa che l\u2019ultima riga del traceback indica l\u2019ultimo messaggio di errore e fornisce il nome dell\u2019eccezione sollevata. In questo caso, il tipo di eccezione \xE8 "),Lo=l(jo,"CODE",{});var Jr=u(Lo);Ss=a(Jr,"OSError"),Jr.forEach(o),Ts=a(jo,", che indica un errore legato al sistema. Leggendo il messaggio di errore, si pu\xF2 notare che sembra esserci un problema con il file "),Go=l(jo,"EM",{});var Yr=u(Go);Is=a(Yr,"config.json"),Yr.forEach(o),Hs=a(jo," del modello e vengono forniti due suggerimenti per risolverlo:"),jo.forEach(o),Lt=d(e),_(ze.$$.fragment,e),Gt=d(e),_(ae.$$.fragment,e),Ft=d(e),fo=l(e,"P",{});var Xr=u(fo);Qs=a(Xr,"Il primo suggerimento ci chiede di verificare se l\u2019ID del modello \xE8 effettivamente corretto, quindi la prima cosa da fare \xE8 copiare l\u2019identificativo e incollarlo nella barra di ricerca dell\u2019Hub:"),Xr.forEach(o),Rt=d(e),je=l(e,"DIV",{class:!0});var Kr=u(je);xe=l(Kr,"IMG",{src:!0,alt:!0,width:!0}),Kr.forEach(o),Ut=d(e),_o=l(e,"P",{});var Zr=u(_o);Ms=a(Zr,"Mmm, sembra proprio che il modello del/la nostro/a collega non sia sull\u2019Hub\u2026 aha, ma c\u2019\xE8 un errore di battitura nel nome del modello! DistilBERT ha solo una \u201Cl\u201D nel suo nome, quindi correggiamolo e cerchiamo invece \u201Clewtun/distilbert-base-uncased-finetuned-squad-d5716d28\u201D:"),Zr.forEach(o),Vt=d(e),Pe=l(e,"DIV",{class:!0});var en=u(Pe);Ce=l(en,"IMG",{src:!0,alt:!0,width:!0}),en.forEach(o),Wt=d(e),ho=l(e,"P",{});var on=u(ho);Ns=a(on,"Ok, questo c\u2019\xE8. Ora proviamo a scaricare di nuovo il modello con l\u2019ID corretto:"),on.forEach(o),Bt=d(e),_(Ae.$$.fragment,e),Jt=d(e),_(De.$$.fragment,e),Yt=d(e),R=l(e,"P",{});var xo=u(R);Ls=a(xo,"Argh, \xE8 fallito ancora - benvenuti nella routine quotidiana di un machine learning engineer! Poich\xE9 abbiamo aggiustato l\u2019ID del modello, il problema deve essere nel repository stesso. Un modo rapido per accedere al contenuto di un repository sul \u{1F917} Hub \xE8 la funzione "),Fo=l(xo,"CODE",{});var tn=u(Fo);Gs=a(tn,"list_repo_files()"),tn.forEach(o),Fs=a(xo," della libreria "),Ro=l(xo,"CODE",{});var sn=u(Ro);Rs=a(sn,"huggingface_hub"),sn.forEach(o),Us=a(xo,":"),xo.forEach(o),Xt=d(e),_(Oe.$$.fragment,e),Kt=d(e),_(Se.$$.fragment,e),Zt=d(e),z=l(e,"P",{});var T=u(z);Vs=a(T,"Interessante: non sembra esserci un file "),Uo=l(T,"EM",{});var an=u(Uo);Ws=a(an,"config.json"),an.forEach(o),Bs=a(T," nel repository! Non c\u2019\xE8 da stupirsi che la nostra "),Vo=l(T,"CODE",{});var rn=u(Vo);Js=a(rn,"pipeline"),rn.forEach(o),Ys=a(T," non riesca a caricare il modello; il/la nostro/a collega deve aver dimenticato di inserire questo file nell\u2019Hub dopo averlo affinato. In questo caso, il problema sembra abbastanza semplice da risolvere: potremmo chiedere loro di aggiungere il file, oppure, dato che possiamo vedere dall\u2019ID del modello che il modello pre-addestrato usato \xE8 "),Te=l(T,"A",{href:!0,rel:!0});var nn=u(Te);Wo=l(nn,"CODE",{});var ln=u(Wo);Xs=a(ln,"distilbert-base-uncased"),ln.forEach(o),nn.forEach(o),Ks=a(T,", possiamo scaricare la configurazione di questo modello e inserirla nel nostro repository per vedere se questo risolve il problema. Proviamo. Utilizzando le tecniche apprese nel "),go=l(T,"A",{href:!0});var un=u(go);Zs=a(un,"Capitolo 2"),un.forEach(o),ea=a(T,", possiamo scaricare la configurazione del modello con la classe "),Bo=l(T,"CODE",{});var pn=u(Bo);oa=a(pn,"AutoConfig"),pn.forEach(o),ta=a(T,":"),T.forEach(o),ei=d(e),_(Ie.$$.fragment,e),oi=d(e),_(re.$$.fragment,e),ti=d(e),ne=l(e,"P",{});var Fi=u(ne);ia=a(Fi,"Possiamo quindi inviarlo al nostro repository del modello con la funzione "),Jo=l(Fi,"CODE",{});var dn=u(Jo);sa=a(dn,"push_to_hub()"),dn.forEach(o),aa=a(Fi," della configurazione:"),Fi.forEach(o),ii=d(e),_(He.$$.fragment,e),si=d(e),le=l(e,"P",{});var Ri=u(le);ra=a(Ri,"Ora possiamo verificare se ha funzionato, caricando il modello dall\u2019ultimo commit del ramo "),Yo=l(Ri,"CODE",{});var cn=u(Yo);na=a(cn,"main"),cn.forEach(o),la=a(Ri,":"),Ri.forEach(o),ai=d(e),_(Qe.$$.fragment,e),ri=d(e),_(Me.$$.fragment,e),ni=d(e),bo=l(e,"P",{});var mn=u(bo);ua=a(mn,"Woohoo, ha funzionato! Riassumiamo quello che hai appena imparato:"),mn.forEach(o),li=d(e),S=l(e,"UL",{});var me=u(S);Ne=l(me,"LI",{});var Ui=u(Ne);pa=a(Ui,"I messaggi di errore in Python sono noti come "),Xo=l(Ui,"EM",{});var fn=u(Xo);da=a(fn,"traceback"),fn.forEach(o),ca=a(Ui," e vengono letti dal basso verso l\u2019alto. L\u2019ultima riga del messaggio di errore di solito contiene le informazioni necessarie per individuare l\u2019origine del problema."),Ui.forEach(o),ma=d(me),Ko=l(me,"LI",{});var _n=u(Ko);fa=a(_n,"Se l\u2019ultima riga non contiene informazioni sufficienti, risali il traceback e vedi se riesci a identificare in quale punto del codice sorgente si verifica l\u2019errore."),_n.forEach(o),_a=d(me),Zo=l(me,"LI",{});var hn=u(Zo);ha=a(hn,"Se nessuno dei messaggi di errore pu\xF2 aiutare a individuare il problema, provare a cercare online una soluzione a un problema simile."),hn.forEach(o),ga=d(me),et=l(me,"LI",{});var gn=u(et);ba=a(gn,`Il \`huggingface_hub\u2019
// \u{1F917} Hub?
fornisce una serie di strumenti che si possono usare per interagire e fare il debug dei repository su Hub.`),gn.forEach(o),me.forEach(o),ui=d(e),vo=l(e,"P",{});var bn=u(vo);va=a(bn,"Ora che sappiamo come eseguire il debug di una pipeline, diamo un\u2019occhiata a un esempio pi\xF9 complicato nel forward pass del modello stesso."),bn.forEach(o),pi=d(e),X=l(e,"H2",{class:!0});var Vi=u(X);ue=l(Vi,"A",{id:!0,class:!0,href:!0});var vn=u(ue);ot=l(vn,"SPAN",{});var wn=u(ot);_(Le.$$.fragment,wn),wn.forEach(o),vn.forEach(o),wa=d(Vi),tt=l(Vi,"SPAN",{});var $n=u(tt);$a=a($n,"Debug del forward pass del modello"),$n.forEach(o),Vi.forEach(o),di=d(e),U=l(e,"P",{});var Po=u(U);ka=a(Po,"Sebbene la "),it=l(Po,"CODE",{});var kn=u(it);qa=a(kn,"pipeline"),kn.forEach(o),Ea=a(Po," sia ottima per la maggior parte delle applicazioni in cui \xE8 necessario generare previsioni rapidamente, a volte \xE8 necessario accedere ai logit del modello (ad esempio, se si desidera applicare un post-processing personalizzato). Per vedere cosa potrebbe andare storto in questo caso, iniziamo prendendo il modello e il tokenizer dalla nostra "),st=l(Po,"CODE",{});var qn=u(st);ya=a(qn,"pipeline"),qn.forEach(o),za=a(Po,":"),Po.forEach(o),ci=d(e),_(Ge.$$.fragment,e),mi=d(e),wo=l(e,"P",{});var En=u(wo);ja=a(En,"Poi abbiamo bisogno di una domanda, quindi vediamo se i nostri framework preferiti sono supportati:"),En.forEach(o),fi=d(e),_(Fe.$$.fragment,e),_i=d(e),pe=l(e,"P",{});var Wi=u(pe);xa=a(Wi,"Come abbiamo visto nel "),$o=l(Wi,"A",{href:!0});var yn=u($o);Pa=a(yn,"Capitolo 7"),yn.forEach(o),Ca=a(Wi,", i passaggi tipici da svolgere sono la tokenizzazione degli input, l\u2019estrazione dei logit dei token iniziali e finali e la decodifica dell\u2019intervallo di risposta:"),Wi.forEach(o),hi=d(e),_(Re.$$.fragment,e),gi=d(e),_(Ue.$$.fragment,e),bi=d(e),ko=l(e,"P",{});var zn=u(ko);Aa=a(zn,"Oh cielo, sembra che ci sia un bug nel nostro codice! Ma non avere paura di un po\u2019 di debug. Puoi usare il debugger di Python in un notebook:"),zn.forEach(o),vi=d(e),_(Ve.$$.fragment,e),wi=d(e),qo=l(e,"P",{});var jn=u(qo);Da=a(jn,"o dal terminale:"),jn.forEach(o),$i=d(e),_(We.$$.fragment,e),ki=d(e),j=l(e,"P",{});var I=u(j);Oa=a(I,"Qui, leggendo il messaggio di errore vediamo che l\u2019oggetto "),at=l(I,"CODE",{});var xn=u(at);Sa=a(xn,"'list'"),xn.forEach(o),Ta=a(I," non ha attributo "),rt=l(I,"CODE",{});var Pn=u(rt);Ia=a(Pn,"'size'"),Pn.forEach(o),Ha=a(I,", e possiamo vedere una "),nt=l(I,"CODE",{});var Cn=u(nt);Qa=a(Cn,"-->"),Cn.forEach(o),Ma=a(I," freccia che punta alla riga in cui il problema \xE8 stato sollevato in "),lt=l(I,"CODE",{});var An=u(lt);Na=a(An,"model(**inputs)"),An.forEach(o),La=a(I,". Possiamo eseguire il debug interattivo utilizzando il debugger di Python, ma per ora ci limiteremo a stampare una parte di "),ut=l(I,"CODE",{});var Dn=u(ut);Ga=a(Dn,"input"),Dn.forEach(o),Fa=a(I," per vedere cosa abbiamo:"),I.forEach(o),qi=d(e),_(Be.$$.fragment,e),Ei=d(e),_(Je.$$.fragment,e),yi=d(e),V=l(e,"P",{});var Co=u(V);Ra=a(Co,"Questo sembra certamente una normale "),pt=l(Co,"CODE",{});var On=u(pt);Ua=a(On,"list"),On.forEach(o),Va=a(Co," di Python, ma ricontrolliamo il "),dt=l(Co,"CODE",{});var Sn=u(dt);Wa=a(Sn,"type"),Sn.forEach(o),Ba=a(Co,":"),Co.forEach(o),zi=d(e),_(Ye.$$.fragment,e),ji=d(e),_(Xe.$$.fragment,e),xi=d(e),x=l(e,"P",{});var H=u(x);Ja=a(H,"S\xEC, questa \xE8 sicuramente una "),ct=l(H,"CODE",{});var Tn=u(ct);Ya=a(Tn,"list"),Tn.forEach(o),Xa=a(H," di Python. Cos\u2019\xE8 andato storto? Ricordiamo dal "),Eo=l(H,"A",{href:!0});var In=u(Eo);Ka=a(In,"Capitolo 2"),In.forEach(o),Za=a(H," che le classi "),mt=l(H,"CODE",{});var Hn=u(mt);er=a(Hn,"AutoModelForXxx"),Hn.forEach(o),or=a(H," in \u{1F917} Transformers operano su "),ft=l(H,"EM",{});var Qn=u(ft);tr=a(Qn,"tensori"),Qn.forEach(o),ir=a(H," (sia in PyTorch che in TensorFlow), e un\u2019operazione comune \xE8 quella di estrarre le dimensioni di un tensore usando "),_t=l(H,"CODE",{});var Mn=u(_t);sr=a(Mn,"Tensor.size()"),Mn.forEach(o),ar=a(H," in PyTorch, per esempio. Diamo un\u2019altra occhiata al traceback, per vedere quale riga ha causato l\u2019eccezione:"),H.forEach(o),Pi=d(e),_(Ke.$$.fragment,e),Ci=d(e),A=l(e,"P",{});var B=u(A);rr=a(B,"Sembra che il nostro codice abbia provato a chiamare "),ht=l(B,"CODE",{});var Nn=u(ht);nr=a(Nn,"input_ids.size()"),Nn.forEach(o),lr=a(B,", ma questo chiaramente non funziona per una "),gt=l(B,"CODE",{});var Ln=u(gt);ur=a(Ln,"list"),Ln.forEach(o),pr=a(B,"di Python, che \xE8 solo un "),bt=l(B,"EM",{});var Gn=u(bt);dr=a(Gn,"container"),Gn.forEach(o),cr=a(B,". Come possiamo risolvere questo problema? Cercando il messaggio di errore su Stack Overflow si ottengono alcuni "),Ze=l(B,"A",{href:!0,rel:!0});var Fn=u(Ze);mr=a(Fn,"risultati"),Fn.forEach(o),fr=a(B," pertinenti. Cliccando sul primo, viene visualizzata una domanda simile alla nostra, con la risposta mostrata nello screenshot seguente:"),B.forEach(o),Ai=d(e),eo=l(e,"DIV",{class:!0});var Rn=u(eo);oo=l(Rn,"IMG",{src:!0,alt:!0,width:!0}),Rn.forEach(o),Di=d(e),de=l(e,"P",{});var Bi=u(de);_r=a(Bi,"La risposta raccomanda di aggiungere "),vt=l(Bi,"CODE",{});var Un=u(vt);hr=a(Un,"return_tensors='pt'"),Un.forEach(o),gr=a(Bi," al tokenizer, quindi proviamo se funziona:"),Bi.forEach(o),Oi=d(e),_(to.$$.fragment,e),Si=d(e),_(io.$$.fragment,e),Ti=d(e),ce=l(e,"P",{});var Ji=u(ce);br=a(Ji,"Bene, ha funzionato! Questo \xE8 un ottimo esempio di quanto possa essere utile Stack Overflow: identificando un problema simile, abbiamo potuto beneficiare dell\u2019esperienza di altri membri della community. Tuttavia, una ricerca come questa non sempre produce una risposta pertinente, quindi cosa si pu\xF2 fare in questi casi? Fortunatamente, sul "),so=l(Ji,"A",{href:!0,rel:!0});var Vn=u(so);vr=a(Vn,"forum di Hugging Face"),Vn.forEach(o),wr=a(Ji," c\u2019\xE8 un\u2019accogliente community di sviluppatori che pu\xF2 aiutarti! Nella prossima sezione, vedremo come creare bene delle domande sul forum che abbiano buona probabilit\xE0 di ricevere una risposta."),Ji.forEach(o),this.h()},h(){c(m,"name","hf:doc:metadata"),c(m,"content",JSON.stringify(il)),c(k,"id","cosa-fare-quando-si-riceve-un-errore"),c(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k,"href","#cosa-fare-quando-si-riceve-un-errore"),c(w,"class","relative group"),c(J,"href","/course/chapter8/section4"),c(_e,"href","https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"),c(_e,"rel","nofollow"),c(he,"href","https://huggingface.co"),c(he,"rel","nofollow"),c(te,"id","fare-il-debug-della-pipeline-di-transformers"),c(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(te,"href","#fare-il-debug-della-pipeline-di-transformers"),c(Y,"class","relative group"),c(co,"href","/course/chapter7/7"),Yi(ye.src,Er="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png")||c(ye,"src",Er),c(ye,"alt","A Python traceback."),c(ye,"width","100%"),c(Ee,"class","flex justify-center"),c(mo,"href","/course/chapter2"),Yi(xe.src,yr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png")||c(xe,"src",yr),c(xe,"alt","The wrong model name."),c(xe,"width","100%"),c(je,"class","flex justify-center"),Yi(Ce.src,zr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png")||c(Ce,"src",zr),c(Ce,"alt","The right model name."),c(Ce,"width","100%"),c(Pe,"class","flex justify-center"),c(Te,"href","https://huggingface.co/distilbert-base-uncased"),c(Te,"rel","nofollow"),c(go,"href","/course/chapter2"),c(ue,"id","debug-del-forward-pass-del-modello"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#debug-del-forward-pass-del-modello"),c(X,"class","relative group"),c($o,"href","/course/chapter7"),c(Eo,"href","/course/chapter2"),c(Ze,"href","https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f"),c(Ze,"rel","nofollow"),Yi(oo.src,jr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png")||c(oo,"src",jr),c(oo,"alt","An answer from Stack Overflow."),c(oo,"width","100%"),c(eo,"class","flex justify-center"),c(so,"href","https://discuss.huggingface.co/"),c(so,"rel","nofollow")},m(e,i){t(document.head,m),r(e,y,i),r(e,w,i),t(w,k),t(k,D),h(q,D,null),t(w,P),t(w,O),t(O,Q),r(e,Z,i),h(M,e,i),r(e,N,i),r(e,E,i),t(E,ro),t(E,J),t(J,no),t(E,Xi),r(e,kt,i),h(fe,e,i),r(e,qt,i),r(e,L,i),t(L,Ki),t(L,_e),t(_e,Zi),t(L,es),t(L,he),t(he,os),t(L,ts),r(e,Et,i),h(ge,e,i),r(e,yt,i),r(e,lo,i),t(lo,is),r(e,zt,i),h(be,e,i),r(e,jt,i),r(e,ee,i),t(ee,ss),t(ee,Ao),t(Ao,as),t(ee,rs),r(e,xt,i),h(ve,e,i),r(e,Pt,i),r(e,oe,i),t(oe,ns),t(oe,Do),t(Do,ls),t(oe,us),r(e,Ct,i),r(e,Y,i),t(Y,te),t(te,Oo),h(we,Oo,null),t(Y,ps),t(Y,So),t(So,ds),r(e,At,i),r(e,uo,i),t(uo,cs),r(e,Dt,i),r(e,po,i),t(po,$e),t($e,ms),t($e,co),t(co,fs),t($e,_s),r(e,Ot,i),r(e,ie,i),t(ie,hs),t(ie,To),t(To,gs),t(ie,bs),r(e,St,i),h(ke,e,i),r(e,Tt,i),h(qe,e,i),r(e,It,i),r(e,G,i),t(G,vs),t(G,Io),t(Io,ws),t(G,$s),t(G,Ho),t(Ho,ks),t(G,qs),r(e,Ht,i),r(e,Ee,i),t(Ee,ye),r(e,Qt,i),r(e,C,i),t(C,Es),t(C,Qo),t(Qo,ys),t(C,zs),t(C,Mo),t(Mo,js),t(C,xs),t(C,mo),t(mo,Ps),t(C,Cs),t(C,No),t(No,As),t(C,Ds),r(e,Mt,i),h(se,e,i),r(e,Nt,i),r(e,F,i),t(F,Os),t(F,Lo),t(Lo,Ss),t(F,Ts),t(F,Go),t(Go,Is),t(F,Hs),r(e,Lt,i),h(ze,e,i),r(e,Gt,i),h(ae,e,i),r(e,Ft,i),r(e,fo,i),t(fo,Qs),r(e,Rt,i),r(e,je,i),t(je,xe),r(e,Ut,i),r(e,_o,i),t(_o,Ms),r(e,Vt,i),r(e,Pe,i),t(Pe,Ce),r(e,Wt,i),r(e,ho,i),t(ho,Ns),r(e,Bt,i),h(Ae,e,i),r(e,Jt,i),h(De,e,i),r(e,Yt,i),r(e,R,i),t(R,Ls),t(R,Fo),t(Fo,Gs),t(R,Fs),t(R,Ro),t(Ro,Rs),t(R,Us),r(e,Xt,i),h(Oe,e,i),r(e,Kt,i),h(Se,e,i),r(e,Zt,i),r(e,z,i),t(z,Vs),t(z,Uo),t(Uo,Ws),t(z,Bs),t(z,Vo),t(Vo,Js),t(z,Ys),t(z,Te),t(Te,Wo),t(Wo,Xs),t(z,Ks),t(z,go),t(go,Zs),t(z,ea),t(z,Bo),t(Bo,oa),t(z,ta),r(e,ei,i),h(Ie,e,i),r(e,oi,i),h(re,e,i),r(e,ti,i),r(e,ne,i),t(ne,ia),t(ne,Jo),t(Jo,sa),t(ne,aa),r(e,ii,i),h(He,e,i),r(e,si,i),r(e,le,i),t(le,ra),t(le,Yo),t(Yo,na),t(le,la),r(e,ai,i),h(Qe,e,i),r(e,ri,i),h(Me,e,i),r(e,ni,i),r(e,bo,i),t(bo,ua),r(e,li,i),r(e,S,i),t(S,Ne),t(Ne,pa),t(Ne,Xo),t(Xo,da),t(Ne,ca),t(S,ma),t(S,Ko),t(Ko,fa),t(S,_a),t(S,Zo),t(Zo,ha),t(S,ga),t(S,et),t(et,ba),r(e,ui,i),r(e,vo,i),t(vo,va),r(e,pi,i),r(e,X,i),t(X,ue),t(ue,ot),h(Le,ot,null),t(X,wa),t(X,tt),t(tt,$a),r(e,di,i),r(e,U,i),t(U,ka),t(U,it),t(it,qa),t(U,Ea),t(U,st),t(st,ya),t(U,za),r(e,ci,i),h(Ge,e,i),r(e,mi,i),r(e,wo,i),t(wo,ja),r(e,fi,i),h(Fe,e,i),r(e,_i,i),r(e,pe,i),t(pe,xa),t(pe,$o),t($o,Pa),t(pe,Ca),r(e,hi,i),h(Re,e,i),r(e,gi,i),h(Ue,e,i),r(e,bi,i),r(e,ko,i),t(ko,Aa),r(e,vi,i),h(Ve,e,i),r(e,wi,i),r(e,qo,i),t(qo,Da),r(e,$i,i),h(We,e,i),r(e,ki,i),r(e,j,i),t(j,Oa),t(j,at),t(at,Sa),t(j,Ta),t(j,rt),t(rt,Ia),t(j,Ha),t(j,nt),t(nt,Qa),t(j,Ma),t(j,lt),t(lt,Na),t(j,La),t(j,ut),t(ut,Ga),t(j,Fa),r(e,qi,i),h(Be,e,i),r(e,Ei,i),h(Je,e,i),r(e,yi,i),r(e,V,i),t(V,Ra),t(V,pt),t(pt,Ua),t(V,Va),t(V,dt),t(dt,Wa),t(V,Ba),r(e,zi,i),h(Ye,e,i),r(e,ji,i),h(Xe,e,i),r(e,xi,i),r(e,x,i),t(x,Ja),t(x,ct),t(ct,Ya),t(x,Xa),t(x,Eo),t(Eo,Ka),t(x,Za),t(x,mt),t(mt,er),t(x,or),t(x,ft),t(ft,tr),t(x,ir),t(x,_t),t(_t,sr),t(x,ar),r(e,Pi,i),h(Ke,e,i),r(e,Ci,i),r(e,A,i),t(A,rr),t(A,ht),t(ht,nr),t(A,lr),t(A,gt),t(gt,ur),t(A,pr),t(A,bt),t(bt,dr),t(A,cr),t(A,Ze),t(Ze,mr),t(A,fr),r(e,Ai,i),r(e,eo,i),t(eo,oo),r(e,Di,i),r(e,de,i),t(de,_r),t(de,vt),t(vt,hr),t(de,gr),r(e,Oi,i),h(to,e,i),r(e,Si,i),h(io,e,i),r(e,Ti,i),r(e,ce,i),t(ce,br),t(ce,so),t(so,vr),t(ce,wr),Ii=!0},p(e,[i]){const ao={};i&2&&(ao.$$scope={dirty:i,ctx:e}),se.$set(ao);const wt={};i&2&&(wt.$$scope={dirty:i,ctx:e}),ae.$set(wt);const $t={};i&2&&($t.$$scope={dirty:i,ctx:e}),re.$set($t)},i(e){Ii||(g(q.$$.fragment,e),g(M.$$.fragment,e),g(fe.$$.fragment,e),g(ge.$$.fragment,e),g(be.$$.fragment,e),g(ve.$$.fragment,e),g(we.$$.fragment,e),g(ke.$$.fragment,e),g(qe.$$.fragment,e),g(se.$$.fragment,e),g(ze.$$.fragment,e),g(ae.$$.fragment,e),g(Ae.$$.fragment,e),g(De.$$.fragment,e),g(Oe.$$.fragment,e),g(Se.$$.fragment,e),g(Ie.$$.fragment,e),g(re.$$.fragment,e),g(He.$$.fragment,e),g(Qe.$$.fragment,e),g(Me.$$.fragment,e),g(Le.$$.fragment,e),g(Ge.$$.fragment,e),g(Fe.$$.fragment,e),g(Re.$$.fragment,e),g(Ue.$$.fragment,e),g(Ve.$$.fragment,e),g(We.$$.fragment,e),g(Be.$$.fragment,e),g(Je.$$.fragment,e),g(Ye.$$.fragment,e),g(Xe.$$.fragment,e),g(Ke.$$.fragment,e),g(to.$$.fragment,e),g(io.$$.fragment,e),Ii=!0)},o(e){b(q.$$.fragment,e),b(M.$$.fragment,e),b(fe.$$.fragment,e),b(ge.$$.fragment,e),b(be.$$.fragment,e),b(ve.$$.fragment,e),b(we.$$.fragment,e),b(ke.$$.fragment,e),b(qe.$$.fragment,e),b(se.$$.fragment,e),b(ze.$$.fragment,e),b(ae.$$.fragment,e),b(Ae.$$.fragment,e),b(De.$$.fragment,e),b(Oe.$$.fragment,e),b(Se.$$.fragment,e),b(Ie.$$.fragment,e),b(re.$$.fragment,e),b(He.$$.fragment,e),b(Qe.$$.fragment,e),b(Me.$$.fragment,e),b(Le.$$.fragment,e),b(Ge.$$.fragment,e),b(Fe.$$.fragment,e),b(Re.$$.fragment,e),b(Ue.$$.fragment,e),b(Ve.$$.fragment,e),b(We.$$.fragment,e),b(Be.$$.fragment,e),b(Je.$$.fragment,e),b(Ye.$$.fragment,e),b(Xe.$$.fragment,e),b(Ke.$$.fragment,e),b(to.$$.fragment,e),b(io.$$.fragment,e),Ii=!1},d(e){o(m),e&&o(y),e&&o(w),v(q),e&&o(Z),v(M,e),e&&o(N),e&&o(E),e&&o(kt),v(fe,e),e&&o(qt),e&&o(L),e&&o(Et),v(ge,e),e&&o(yt),e&&o(lo),e&&o(zt),v(be,e),e&&o(jt),e&&o(ee),e&&o(xt),v(ve,e),e&&o(Pt),e&&o(oe),e&&o(Ct),e&&o(Y),v(we),e&&o(At),e&&o(uo),e&&o(Dt),e&&o(po),e&&o(Ot),e&&o(ie),e&&o(St),v(ke,e),e&&o(Tt),v(qe,e),e&&o(It),e&&o(G),e&&o(Ht),e&&o(Ee),e&&o(Qt),e&&o(C),e&&o(Mt),v(se,e),e&&o(Nt),e&&o(F),e&&o(Lt),v(ze,e),e&&o(Gt),v(ae,e),e&&o(Ft),e&&o(fo),e&&o(Rt),e&&o(je),e&&o(Ut),e&&o(_o),e&&o(Vt),e&&o(Pe),e&&o(Wt),e&&o(ho),e&&o(Bt),v(Ae,e),e&&o(Jt),v(De,e),e&&o(Yt),e&&o(R),e&&o(Xt),v(Oe,e),e&&o(Kt),v(Se,e),e&&o(Zt),e&&o(z),e&&o(ei),v(Ie,e),e&&o(oi),v(re,e),e&&o(ti),e&&o(ne),e&&o(ii),v(He,e),e&&o(si),e&&o(le),e&&o(ai),v(Qe,e),e&&o(ri),v(Me,e),e&&o(ni),e&&o(bo),e&&o(li),e&&o(S),e&&o(ui),e&&o(vo),e&&o(pi),e&&o(X),v(Le),e&&o(di),e&&o(U),e&&o(ci),v(Ge,e),e&&o(mi),e&&o(wo),e&&o(fi),v(Fe,e),e&&o(_i),e&&o(pe),e&&o(hi),v(Re,e),e&&o(gi),v(Ue,e),e&&o(bi),e&&o(ko),e&&o(vi),v(Ve,e),e&&o(wi),e&&o(qo),e&&o($i),v(We,e),e&&o(ki),e&&o(j),e&&o(qi),v(Be,e),e&&o(Ei),v(Je,e),e&&o(yi),e&&o(V),e&&o(zi),v(Ye,e),e&&o(ji),v(Xe,e),e&&o(xi),e&&o(x),e&&o(Pi),v(Ke,e),e&&o(Ci),e&&o(A),e&&o(Ai),e&&o(eo),e&&o(Di),e&&o(de),e&&o(Oi),v(to,e),e&&o(Si),v(io,e),e&&o(Ti),e&&o(ce)}}}const il={local:"cosa-fare-quando-si-riceve-un-errore",sections:[{local:"fare-il-debug-della-pipeline-di-transformers",title:"Fare il debug della pipeline di \u{1F917} Transformers"},{local:"debug-del-forward-pass-del-modello",title:"Debug del forward pass del modello"}],title:"Cosa fare quando si riceve un errore"};function sl(K){return Xn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class dl extends Wn{constructor(m){super();Bn(this,m,sl,tl,Jn,{})}}export{dl as default,il as metadata};
