import{S as Ir,i as Ur,s as Gr,e as i,t as e,k as m,w as $,c as r,a as o,h as c,d as s,m as g,x,g as p,G as n,y as j,q as v,o as b,B as E,l as Rr,M as Vr,b as q,p as Tn,v as Kr,n as Dn}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ql}from"../../chunks/Tip-hf-doc-builder.js";import{Y as xe}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Fc}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as P}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Wr}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Xr}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Jr(w){let a,u;return a=new Wr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_tf.ipynb"}]}}),{c(){$(a.$$.fragment)},l(h){x(a.$$.fragment,h)},m(h,_){j(a,h,_),u=!0},i(h){u||(v(a.$$.fragment,h),u=!0)},o(h){b(a.$$.fragment,h),u=!1},d(h){E(a,h)}}}function Yr(w){let a,u;return a=new Wr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter3/section2_pt.ipynb"}]}}),{c(){$(a.$$.fragment)},l(h){x(a.$$.fragment,h)},m(h,_){j(a,h,_),u=!0},i(h){u||(v(a.$$.fragment,h),u=!0)},o(h){b(a.$$.fragment,h),u=!1},d(h){E(a,h)}}}function Zr(w){let a,u,h,_,y,d,f,C;return f=new P({props:{code:`import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# T\u01B0\u01A1ng t\u1EF1 nh\u01B0 v\xED d\u1EE5 tr\u01B0\u1EDBc
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# \u0110\xE2y l\xE0 ph\u1EA7n m\u1EDBi
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-comment"># T\u01B0\u01A1ng t\u1EF1 nh\u01B0 v\xED d\u1EE5 tr\u01B0\u1EDBc</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,
]
batch = <span class="hljs-built_in">dict</span>(tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>))

<span class="hljs-comment"># \u0110\xE2y l\xE0 ph\u1EA7n m\u1EDBi</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&quot;adam&quot;</span>, loss=<span class="hljs-string">&quot;sparse_categorical_crossentropy&quot;</span>)
labels = tf.convert_to_tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
model.train_on_batch(batch, labels)`}}),{c(){a=i("p"),u=e("Ti\u1EBFp t\u1EE5c v\u1EDBi v\xED d\u1EE5 t\u1EEB "),h=i("a"),_=e("ch\u01B0\u01A1ng tr\u01B0\u1EDBc"),y=e(", \u0111\xE2y l\xE0 c\xE1ch ch\xFAng ta s\u1EBD hu\u1EA5n luy\u1EC7n m\u1ED9t b\u1ED9 ph\xE2n lo\u1EA1i chu\u1ED7i tr\xEAn m\u1ED9t l\xF4 trong TensorFlow:"),d=m(),$(f.$$.fragment),this.h()},l(k){a=r(k,"P",{});var T=o(a);u=c(T,"Ti\u1EBFp t\u1EE5c v\u1EDBi v\xED d\u1EE5 t\u1EEB "),h=r(T,"A",{href:!0});var M=o(h);_=c(M,"ch\u01B0\u01A1ng tr\u01B0\u1EDBc"),M.forEach(s),y=c(T,", \u0111\xE2y l\xE0 c\xE1ch ch\xFAng ta s\u1EBD hu\u1EA5n luy\u1EC7n m\u1ED9t b\u1ED9 ph\xE2n lo\u1EA1i chu\u1ED7i tr\xEAn m\u1ED9t l\xF4 trong TensorFlow:"),T.forEach(s),d=g(k),x(f.$$.fragment,k),this.h()},h(){q(h,"href","/course/chapter2")},m(k,T){p(k,a,T),n(a,u),n(a,h),n(h,_),n(a,y),p(k,d,T),j(f,k,T),C=!0},i(k){C||(v(f.$$.fragment,k),C=!0)},o(k){b(f.$$.fragment,k),C=!1},d(k){k&&s(a),k&&s(d),E(f,k)}}}function Qr(w){let a,u,h,_,y,d,f,C;return f=new P({props:{code:`import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# T\u01B0\u01A1ng t\u1EF1 nh\u01B0 v\xED d\u1EE5 tr\u01B0\u1EDBc
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# \u0110\xE2y l\xE0 ph\u1EA7n m\u1EDBi
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-comment"># T\u01B0\u01A1ng t\u1EF1 nh\u01B0 v\xED d\u1EE5 tr\u01B0\u1EDBc</span>
checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-string">&quot;This course is amazing!&quot;</span>,
]
batch = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># \u0110\xE2y l\xE0 ph\u1EA7n m\u1EDBi</span>
batch[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()`}}),{c(){a=i("p"),u=e("Ti\u1EBFp t\u1EE5c v\u1EDBi v\xED d\u1EE5 t\u1EEB "),h=i("a"),_=e("ch\u01B0\u01A1ng tr\u01B0\u1EDBc"),y=e(", \u0111\xE2y l\xE0 c\xE1ch ch\xFAng ta s\u1EBD hu\u1EA5n luy\u1EC7n m\u1ED9t b\u1ED9 ph\xE2n lo\u1EA1i chu\u1ED7i tr\xEAn m\u1ED9t l\xF4 trong PyTorch:"),d=m(),$(f.$$.fragment),this.h()},l(k){a=r(k,"P",{});var T=o(a);u=c(T,"Ti\u1EBFp t\u1EE5c v\u1EDBi v\xED d\u1EE5 t\u1EEB "),h=r(T,"A",{href:!0});var M=o(h);_=c(M,"ch\u01B0\u01A1ng tr\u01B0\u1EDBc"),M.forEach(s),y=c(T,", \u0111\xE2y l\xE0 c\xE1ch ch\xFAng ta s\u1EBD hu\u1EA5n luy\u1EC7n m\u1ED9t b\u1ED9 ph\xE2n lo\u1EA1i chu\u1ED7i tr\xEAn m\u1ED9t l\xF4 trong PyTorch:"),T.forEach(s),d=g(k),x(f.$$.fragment,k),this.h()},h(){q(h,"href","/course/chapter2")},m(k,T){p(k,a,T),n(a,u),n(a,h),n(h,_),n(a,y),p(k,d,T),j(f,k,T),C=!0},i(k){C||(v(f.$$.fragment,k),C=!0)},o(k){b(f.$$.fragment,k),C=!1},d(k){k&&s(a),k&&s(d),E(f,k)}}}function to(w){let a,u;return a=new xe({props:{id:"W_gMJF0xomE"}}),{c(){$(a.$$.fragment)},l(h){x(a.$$.fragment,h)},m(h,_){j(a,h,_),u=!0},i(h){u||(v(a.$$.fragment,h),u=!0)},o(h){b(a.$$.fragment,h),u=!1},d(h){E(a,h)}}}function no(w){let a,u;return a=new xe({props:{id:"_BZearw7f0w"}}),{c(){$(a.$$.fragment)},l(h){x(a.$$.fragment,h)},m(h,_){j(a,h,_),u=!0},i(h){u||(v(a.$$.fragment,h),u=!0)},o(h){b(a.$$.fragment,h),u=!1},d(h){E(a,h)}}}function so(w){let a,u,h,_,y;return{c(){a=i("p"),u=e("\u270F\uFE0F "),h=i("strong"),_=e("Th\u1EED nghi\u1EC7m th\xF4i!"),y=e(" Nh\xECn v\xE0o ph\u1EA7n t\u1EED th\u1EE9 15 c\u1EE7a t\u1EADp hu\u1EA5n luy\u1EC7n v\xE0 ph\u1EA7n t\u1EED 87 c\u1EE7a t\u1EADp ki\u1EC3m \u0111\u1ECBnh. Nh\xE3n c\u1EE7a ch\xFAng l\xE0 g\xEC?")},l(d){a=r(d,"P",{});var f=o(a);u=c(f,"\u270F\uFE0F "),h=r(f,"STRONG",{});var C=o(h);_=c(C,"Th\u1EED nghi\u1EC7m th\xF4i!"),C.forEach(s),y=c(f," Nh\xECn v\xE0o ph\u1EA7n t\u1EED th\u1EE9 15 c\u1EE7a t\u1EADp hu\u1EA5n luy\u1EC7n v\xE0 ph\u1EA7n t\u1EED 87 c\u1EE7a t\u1EADp ki\u1EC3m \u0111\u1ECBnh. Nh\xE3n c\u1EE7a ch\xFAng l\xE0 g\xEC?"),f.forEach(s)},m(d,f){p(d,a,f),n(a,u),n(a,h),n(h,_),n(a,y)},d(d){d&&s(a)}}}function eo(w){let a,u;return a=new xe({props:{id:"P-rZWqcB6CE"}}),{c(){$(a.$$.fragment)},l(h){x(a.$$.fragment,h)},m(h,_){j(a,h,_),u=!0},i(h){u||(v(a.$$.fragment,h),u=!0)},o(h){b(a.$$.fragment,h),u=!1},d(h){E(a,h)}}}function co(w){let a,u;return a=new xe({props:{id:"0u3ioSwev3s"}}),{c(){$(a.$$.fragment)},l(h){x(a.$$.fragment,h)},m(h,_){j(a,h,_),u=!0},i(h){u||(v(a.$$.fragment,h),u=!0)},o(h){b(a.$$.fragment,h),u=!1},d(h){E(a,h)}}}function ao(w){let a,u,h,_,y;return{c(){a=i("p"),u=e("\u270F\uFE0F "),h=i("strong"),_=e("Th\u1EED nghi\u1EC7m th\xF4i!"),y=e(" L\u1EA5y ph\u1EA7n t\u1EED 15 c\u1EE7a t\u1EADp hu\u1EA5n luy\u1EC7n v\xE0 tokenize hai c\xE2u ri\xEAng bi\u1EC7t v\xE0 nh\u01B0 m\u1ED9t c\u1EB7p. S\u1EF1 kh\xE1c bi\u1EC7t gi\u1EEFa hai k\u1EBFt qu\u1EA3 l\xE0 g\xEC?")},l(d){a=r(d,"P",{});var f=o(a);u=c(f,"\u270F\uFE0F "),h=r(f,"STRONG",{});var C=o(h);_=c(C,"Th\u1EED nghi\u1EC7m th\xF4i!"),C.forEach(s),y=c(f," L\u1EA5y ph\u1EA7n t\u1EED 15 c\u1EE7a t\u1EADp hu\u1EA5n luy\u1EC7n v\xE0 tokenize hai c\xE2u ri\xEAng bi\u1EC7t v\xE0 nh\u01B0 m\u1ED9t c\u1EB7p. S\u1EF1 kh\xE1c bi\u1EC7t gi\u1EEFa hai k\u1EBFt qu\u1EA3 l\xE0 g\xEC?"),f.forEach(s)},m(d,f){p(d,a,f),n(a,u),n(a,h),n(h,_),n(a,y)},d(d){d&&s(a)}}}function ho(w){let a,u,h,_,y,d,f,C,k,T,M;return{c(){a=i("p"),u=e("H\xE0m ch\u1ECBu tr\xE1ch nhi\u1EC7m t\u1EADp h\u1EE3p c\xE1c m\u1EABu l\u1EA1i v\u1EDBi nhau trong m\u1ED9t l\xF4 \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),h=i("em"),_=e("collate function"),y=e(" hay "),d=i("em"),f=e("h\xE0m \u0111\u1ED1i chi\u1EBFu"),C=e(". \u0110\xF3 l\xE0 m\u1ED9t tham s\u1ED1 b\u1EA1n c\xF3 th\u1EC3 \u0111\u01B0a v\xE0o khi x\xE2y d\u1EF1ng m\u1ED9t "),k=i("code"),T=e("DataLoader"),M=e(", m\u1EB7c \u0111\u1ECBnh \u0111\xE2y l\xE0 m\u1ED9t h\xE0m s\u1EBD ch\u1EC9 chuy\u1EC3n \u0111\u1ED5i c\xE1c m\u1EABu c\u1EE7a b\u1EA1n th\xE0nh c\xE1c tensors PyTorch v\xE0 n\u1ED1i ch\xFAng (\u0111\u1EC7 quy n\u1EBFu c\xE1c ph\u1EA7n t\u1EED c\u1EE7a b\u1EA1n l\xE0 list, tuple ho\u1EB7c dict). \u0110i\u1EC1u n\xE0y s\u1EBD kh\xF4ng th\u1EC3 x\u1EA3y ra trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng ta v\xEC t\u1EA5t c\u1EA3 c\xE1c \u0111\u1EA7u v\xE0o ta c\xF3 s\u1EBD kh\xF4ng c\xF3 c\xF9ng k\xEDch th\u01B0\u1EDBc. Ch\xFAng ta \u0111\xE3 c\u1ED1 t\xECnh ho\xE3n vi\u1EC7c b\u1ED5 sung \u0111\u1EC7m, \u0111\u1EC3 ch\u1EC9 \xE1p d\u1EE5ng n\xF3 khi c\u1EA7n thi\u1EBFt tr\xEAn m\u1ED7i l\xF4 v\xE0 tr\xE1nh \u0111\u1EC3 c\xE1c \u0111\u1EA7u v\xE0o qu\xE1 d\xE0i v\u1EDBi nhi\u1EC1u \u0111\u1EC7m. \u0110i\u1EC1u n\xE0y s\u1EBD \u0111\u1EA9y nhanh qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n l\xEAn m\u1ED9t ch\xFAt, nh\u01B0ng l\u01B0u \xFD r\u1EB1ng n\u1EBFu b\u1EA1n \u0111ang hu\u1EA5n luy\u1EC7n tr\xEAn TPU th\xEC n\xF3 c\xF3 th\u1EC3 g\xE2y ra v\u1EA5n \u0111\u1EC1 - TPU th\xEDch c\xE1c h\xECnh d\u1EA1ng c\u1ED1 \u0111\u1ECBnh, ngay c\u1EA3 khi \u0111i\u1EC1u \u0111\xF3 y\xEAu c\u1EA7u th\xEAm \u0111\u1EC7m.")},l(N){a=r(N,"P",{});var z=o(a);u=c(z,"H\xE0m ch\u1ECBu tr\xE1ch nhi\u1EC7m t\u1EADp h\u1EE3p c\xE1c m\u1EABu l\u1EA1i v\u1EDBi nhau trong m\u1ED9t l\xF4 \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),h=r(z,"EM",{});var L=o(h);_=c(L,"collate function"),L.forEach(s),y=c(z," hay "),d=r(z,"EM",{});var B=o(d);f=c(B,"h\xE0m \u0111\u1ED1i chi\u1EBFu"),B.forEach(s),C=c(z,". \u0110\xF3 l\xE0 m\u1ED9t tham s\u1ED1 b\u1EA1n c\xF3 th\u1EC3 \u0111\u01B0a v\xE0o khi x\xE2y d\u1EF1ng m\u1ED9t "),k=r(z,"CODE",{});var S=o(k);T=c(S,"DataLoader"),S.forEach(s),M=c(z,", m\u1EB7c \u0111\u1ECBnh \u0111\xE2y l\xE0 m\u1ED9t h\xE0m s\u1EBD ch\u1EC9 chuy\u1EC3n \u0111\u1ED5i c\xE1c m\u1EABu c\u1EE7a b\u1EA1n th\xE0nh c\xE1c tensors PyTorch v\xE0 n\u1ED1i ch\xFAng (\u0111\u1EC7 quy n\u1EBFu c\xE1c ph\u1EA7n t\u1EED c\u1EE7a b\u1EA1n l\xE0 list, tuple ho\u1EB7c dict). \u0110i\u1EC1u n\xE0y s\u1EBD kh\xF4ng th\u1EC3 x\u1EA3y ra trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng ta v\xEC t\u1EA5t c\u1EA3 c\xE1c \u0111\u1EA7u v\xE0o ta c\xF3 s\u1EBD kh\xF4ng c\xF3 c\xF9ng k\xEDch th\u01B0\u1EDBc. Ch\xFAng ta \u0111\xE3 c\u1ED1 t\xECnh ho\xE3n vi\u1EC7c b\u1ED5 sung \u0111\u1EC7m, \u0111\u1EC3 ch\u1EC9 \xE1p d\u1EE5ng n\xF3 khi c\u1EA7n thi\u1EBFt tr\xEAn m\u1ED7i l\xF4 v\xE0 tr\xE1nh \u0111\u1EC3 c\xE1c \u0111\u1EA7u v\xE0o qu\xE1 d\xE0i v\u1EDBi nhi\u1EC1u \u0111\u1EC7m. \u0110i\u1EC1u n\xE0y s\u1EBD \u0111\u1EA9y nhanh qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n l\xEAn m\u1ED9t ch\xFAt, nh\u01B0ng l\u01B0u \xFD r\u1EB1ng n\u1EBFu b\u1EA1n \u0111ang hu\u1EA5n luy\u1EC7n tr\xEAn TPU th\xEC n\xF3 c\xF3 th\u1EC3 g\xE2y ra v\u1EA5n \u0111\u1EC1 - TPU th\xEDch c\xE1c h\xECnh d\u1EA1ng c\u1ED1 \u0111\u1ECBnh, ngay c\u1EA3 khi \u0111i\u1EC1u \u0111\xF3 y\xEAu c\u1EA7u th\xEAm \u0111\u1EC7m."),z.forEach(s)},m(N,z){p(N,a,z),n(a,u),n(a,h),n(h,_),n(a,y),n(a,d),n(d,f),n(a,C),n(a,k),n(k,T),n(a,M)},d(N){N&&s(a)}}}function lo(w){let a,u,h,_,y,d,f,C,k,T,M;return{c(){a=i("p"),u=e("H\xE0m ch\u1ECBu tr\xE1ch nhi\u1EC7m t\u1EADp h\u1EE3p c\xE1c m\u1EABu l\u1EA1i v\u1EDBi nhau trong m\u1ED9t l\xF4 \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),h=i("em"),_=e("collate function"),y=e(" hay "),d=i("em"),f=e("h\xE0m \u0111\u1ED1i chi\u1EBFu"),C=e(". \u0110\xF3 l\xE0 m\u1ED9t tham s\u1ED1 b\u1EA1n c\xF3 th\u1EC3 \u0111\u01B0a v\xE0o khi x\xE2y d\u1EF1ng m\u1ED9t "),k=i("code"),T=e("DataLoader"),M=e(", m\u1EB7c \u0111\u1ECBnh \u0111\xE2y l\xE0 m\u1ED9t h\xE0m s\u1EBD ch\u1EC9 chuy\u1EC3n \u0111\u1ED5i c\xE1c m\u1EABu c\u1EE7a b\u1EA1n th\xE0nh c\xE1c tensors PyTorch v\xE0 n\u1ED1i ch\xFAng (\u0111\u1EC7 quy n\u1EBFu c\xE1c ph\u1EA7n t\u1EED c\u1EE7a b\u1EA1n l\xE0 list, tuple ho\u1EB7c dict). \u0110i\u1EC1u n\xE0y s\u1EBD kh\xF4ng th\u1EC3 x\u1EA3y ra trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng ta v\xEC t\u1EA5t c\u1EA3 c\xE1c \u0111\u1EA7u v\xE0o ta c\xF3 s\u1EBD kh\xF4ng c\xF3 c\xF9ng k\xEDch th\u01B0\u1EDBc. Ch\xFAng ta \u0111\xE3 c\u1ED1 t\xECnh ho\xE3n vi\u1EC7c b\u1ED5 sung \u0111\u1EC7m, \u0111\u1EC3 ch\u1EC9 \xE1p d\u1EE5ng n\xF3 khi c\u1EA7n thi\u1EBFt tr\xEAn m\u1ED7i l\xF4 v\xE0 tr\xE1nh \u0111\u1EC3 c\xE1c \u0111\u1EA7u v\xE0o qu\xE1 d\xE0i v\u1EDBi nhi\u1EC1u \u0111\u1EC7m. \u0110i\u1EC1u n\xE0y s\u1EBD \u0111\u1EA9y nhanh qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n l\xEAn m\u1ED9t ch\xFAt, nh\u01B0ng l\u01B0u \xFD r\u1EB1ng n\u1EBFu b\u1EA1n \u0111ang hu\u1EA5n luy\u1EC7n tr\xEAn TPU th\xEC n\xF3 c\xF3 th\u1EC3 g\xE2y ra v\u1EA5n \u0111\u1EC1 - TPU th\xEDch c\xE1c h\xECnh d\u1EA1ng c\u1ED1 \u0111\u1ECBnh, ngay c\u1EA3 khi \u0111i\u1EC1u \u0111\xF3 y\xEAu c\u1EA7u th\xEAm \u0111\u1EC7m.")},l(N){a=r(N,"P",{});var z=o(a);u=c(z,"H\xE0m ch\u1ECBu tr\xE1ch nhi\u1EC7m t\u1EADp h\u1EE3p c\xE1c m\u1EABu l\u1EA1i v\u1EDBi nhau trong m\u1ED9t l\xF4 \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),h=r(z,"EM",{});var L=o(h);_=c(L,"collate function"),L.forEach(s),y=c(z," hay "),d=r(z,"EM",{});var B=o(d);f=c(B,"h\xE0m \u0111\u1ED1i chi\u1EBFu"),B.forEach(s),C=c(z,". \u0110\xF3 l\xE0 m\u1ED9t tham s\u1ED1 b\u1EA1n c\xF3 th\u1EC3 \u0111\u01B0a v\xE0o khi x\xE2y d\u1EF1ng m\u1ED9t "),k=r(z,"CODE",{});var S=o(k);T=c(S,"DataLoader"),S.forEach(s),M=c(z,", m\u1EB7c \u0111\u1ECBnh \u0111\xE2y l\xE0 m\u1ED9t h\xE0m s\u1EBD ch\u1EC9 chuy\u1EC3n \u0111\u1ED5i c\xE1c m\u1EABu c\u1EE7a b\u1EA1n th\xE0nh c\xE1c tensors PyTorch v\xE0 n\u1ED1i ch\xFAng (\u0111\u1EC7 quy n\u1EBFu c\xE1c ph\u1EA7n t\u1EED c\u1EE7a b\u1EA1n l\xE0 list, tuple ho\u1EB7c dict). \u0110i\u1EC1u n\xE0y s\u1EBD kh\xF4ng th\u1EC3 x\u1EA3y ra trong tr\u01B0\u1EDDng h\u1EE3p c\u1EE7a ch\xFAng ta v\xEC t\u1EA5t c\u1EA3 c\xE1c \u0111\u1EA7u v\xE0o ta c\xF3 s\u1EBD kh\xF4ng c\xF3 c\xF9ng k\xEDch th\u01B0\u1EDBc. Ch\xFAng ta \u0111\xE3 c\u1ED1 t\xECnh ho\xE3n vi\u1EC7c b\u1ED5 sung \u0111\u1EC7m, \u0111\u1EC3 ch\u1EC9 \xE1p d\u1EE5ng n\xF3 khi c\u1EA7n thi\u1EBFt tr\xEAn m\u1ED7i l\xF4 v\xE0 tr\xE1nh \u0111\u1EC3 c\xE1c \u0111\u1EA7u v\xE0o qu\xE1 d\xE0i v\u1EDBi nhi\u1EC1u \u0111\u1EC7m. \u0110i\u1EC1u n\xE0y s\u1EBD \u0111\u1EA9y nhanh qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n l\xEAn m\u1ED9t ch\xFAt, nh\u01B0ng l\u01B0u \xFD r\u1EB1ng n\u1EBFu b\u1EA1n \u0111ang hu\u1EA5n luy\u1EC7n tr\xEAn TPU th\xEC n\xF3 c\xF3 th\u1EC3 g\xE2y ra v\u1EA5n \u0111\u1EC1 - TPU th\xEDch c\xE1c h\xECnh d\u1EA1ng c\u1ED1 \u0111\u1ECBnh, ngay c\u1EA3 khi \u0111i\u1EC1u \u0111\xF3 y\xEAu c\u1EA7u th\xEAm \u0111\u1EC7m."),z.forEach(s)},m(N,z){p(N,a,z),n(a,u),n(a,h),n(h,_),n(a,y),n(a,d),n(d,f),n(a,C),n(a,k),n(k,T),n(a,M)},d(N){N&&s(a)}}}function io(w){let a,u;return a=new P({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){$(a.$$.fragment)},l(h){x(a.$$.fragment,h)},m(h,_){j(a,h,_),u=!0},i(h){u||(v(a.$$.fragment,h),u=!0)},o(h){b(a.$$.fragment,h),u=!1},d(h){E(a,h)}}}function ro(w){let a,u;return a=new P({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){$(a.$$.fragment)},l(h){x(a.$$.fragment,h)},m(h,_){j(a,h,_),u=!0},i(h){u||(v(a.$$.fragment,h),u=!0)},o(h){b(a.$$.fragment,h),u=!1},d(h){E(a,h)}}}function oo(w){let a,u,h,_,y;return a=new P({props:{code:`{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: torch.Size([<span class="hljs-number">8</span>])}`}}),{c(){$(a.$$.fragment),u=m(),h=i("p"),_=e("Tr\xF4ng kh\xE1 \u1ED5n! Gi\u1EDD ta \u0111\xE3 chuy\u1EC3n t\u1EEB v\u0103n b\u1EA3n th\xF4 sang c\xE1c l\xF4 m\xE0 m\xF4 h\xECnh c\xF3 th\u1EC3 x\u1EED l\xFD, v\xE0 ta \u0111\xE3 s\u1EB5n s\xE0ng tinh ch\u1EC9nh n\xF3!")},l(d){x(a.$$.fragment,d),u=g(d),h=r(d,"P",{});var f=o(h);_=c(f,"Tr\xF4ng kh\xE1 \u1ED5n! Gi\u1EDD ta \u0111\xE3 chuy\u1EC3n t\u1EEB v\u0103n b\u1EA3n th\xF4 sang c\xE1c l\xF4 m\xE0 m\xF4 h\xECnh c\xF3 th\u1EC3 x\u1EED l\xFD, v\xE0 ta \u0111\xE3 s\u1EB5n s\xE0ng tinh ch\u1EC9nh n\xF3!"),f.forEach(s)},m(d,f){j(a,d,f),p(d,u,f),p(d,h,f),n(h,_),y=!0},i(d){y||(v(a.$$.fragment,d),y=!0)},o(d){b(a.$$.fragment,d),y=!1},d(d){E(a,d),d&&s(u),d&&s(h)}}}function po(w){let a,u;return a=new P({props:{code:`{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: TensorShape([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),
 <span class="hljs-string">&#x27;labels&#x27;</span>: TensorShape([<span class="hljs-number">8</span>])}`}}),{c(){$(a.$$.fragment)},l(h){x(a.$$.fragment,h)},m(h,_){j(a,h,_),u=!0},i(h){u||(v(a.$$.fragment,h),u=!0)},o(h){b(a.$$.fragment,h),u=!1},d(h){E(a,h)}}}function uo(w){let a,u,h,_,y;return{c(){a=i("p"),u=e("\u270F\uFE0F "),h=i("strong"),_=e("Th\u1EED nghi\u1EC7m th\xF4i!"),y=e(" Sao ch\xE9p ti\u1EC1n x\u1EED l\xFD tr\xEAn t\u1EADp d\u1EEF li\u1EC7u GLUE SST-2. N\xF3 h\u01A1i kh\xE1c m\u1ED9t ch\xFAt v\xEC n\xF3 bao g\u1ED3m c\xE1c c\xE2u \u0111\u01A1n thay v\xEC c\xE1c c\u1EB7p, nh\u01B0ng ph\u1EA7n c\xF2n l\u1EA1i c\u1EE7a nh\u1EEFng g\xEC ta \u0111\xE3 l\xE0m s\u1EBD t\u01B0\u01A1ng t\u1EF1 nhau. V\u1EDBi m\u1ED9t th\u1EED th\xE1ch kh\xF3 h\u01A1n, h\xE3y c\u1ED1 g\u1EAFng vi\u1EBFt m\u1ED9t h\xE0m ti\u1EC1n x\u1EED l\xFD ho\u1EA1t \u0111\u1ED9ng tr\xEAn b\u1EA5t k\u1EF3 t\xE1c v\u1EE5 GLUE n\xE0o.")},l(d){a=r(d,"P",{});var f=o(a);u=c(f,"\u270F\uFE0F "),h=r(f,"STRONG",{});var C=o(h);_=c(C,"Th\u1EED nghi\u1EC7m th\xF4i!"),C.forEach(s),y=c(f," Sao ch\xE9p ti\u1EC1n x\u1EED l\xFD tr\xEAn t\u1EADp d\u1EEF li\u1EC7u GLUE SST-2. N\xF3 h\u01A1i kh\xE1c m\u1ED9t ch\xFAt v\xEC n\xF3 bao g\u1ED3m c\xE1c c\xE2u \u0111\u01A1n thay v\xEC c\xE1c c\u1EB7p, nh\u01B0ng ph\u1EA7n c\xF2n l\u1EA1i c\u1EE7a nh\u1EEFng g\xEC ta \u0111\xE3 l\xE0m s\u1EBD t\u01B0\u01A1ng t\u1EF1 nhau. V\u1EDBi m\u1ED9t th\u1EED th\xE1ch kh\xF3 h\u01A1n, h\xE3y c\u1ED1 g\u1EAFng vi\u1EBFt m\u1ED9t h\xE0m ti\u1EC1n x\u1EED l\xFD ho\u1EA1t \u0111\u1ED9ng tr\xEAn b\u1EA5t k\u1EF3 t\xE1c v\u1EE5 GLUE n\xE0o."),f.forEach(s)},m(d,f){p(d,a,f),n(a,u),n(a,h),n(h,_),n(a,y)},d(d){d&&s(a)}}}function Br(w){let a,u,h,_,y,d,f,C,k,T,M,N,z,L,B,S,W,J,rt,wt;return S=new P({props:{code:`tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)`,highlighted:`tf_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)

tf_validation_dataset = tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>],
    label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)`}}),{c(){a=i("p"),u=e("B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 b\u1ED9 d\u1EEF li\u1EC7u v\xE0 b\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u, ta c\u1EA7n ph\u1EA3i k\u1EBFt h\u1EE3p ch\xFAng l\u1EA1i v\u1EDBi nhau. Ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i c\xE1c l\xF4 v\xE0 \u0111\u1ED1i chi\u1EBFu theo c\xE1ch th\u1EE7 c\xF4ng, nh\u01B0ng c\xE1ch n\xE0y r\u1EA5t t\u1ED1n c\xF4ng s\u1EE9c v\xE0 c\xF3 l\u1EBD c\u0169ng kh\xF4ng hi\u1EC7u qu\u1EA3 l\u1EAFm. Thay v\xE0o \u0111\xF3, c\xF3 m\u1ED9t ph\u01B0\u01A1ng ph\xE1p \u0111\u01A1n gi\u1EA3n cung c\u1EA5p gi\u1EA3i ph\xE1p hi\u1EC7u qu\u1EA3 cho v\u1EA5n \u0111\u1EC1 n\xE0y: "),h=i("code"),_=e("to_tf_dataset()"),y=e(". Nso \u0111\u01B0\u1EE3c bao m\u1ED9t "),d=i("code"),f=e("tf.data.Dataset"),C=e(" xung quanh t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a b\u1EA1n, v\u1EDBi m\u1ED9t ch\u1EE9c n\u0103ng \u0111\u1ED1i chi\u1EBFu t\xF9y ch\u1ECDn. "),k=i("code"),T=e("tf.data.Dataset"),M=e(" l\xE0 m\u1ED9t \u0111\u1ECBnh d\u1EA1ng TensorFlow g\u1ED1c m\xE0 Keras c\xF3 th\u1EC3 s\u1EED d\u1EE5ng cho "),N=i("code"),z=e("model.fit()"),L=e(", v\xEC v\u1EADy ph\u01B0\u01A1ng ph\xE1p n\xE0y ngay l\u1EADp t\u1EE9c chuy\u1EC3n \u0111\u1ED5i m\u1ED9t \u{1F917} Dataset sang m\u1ED9t \u0111\u1ECBnh d\u1EA1ng s\u1EB5n s\xE0ng \u0111\u1EC3 hu\u1EA5n luy\u1EC7n. H\xE3y xem n\xF3 ho\u1EA1t \u0111\u1ED9ng v\u1EDBi t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng t\xF4i!"),B=m(),$(S.$$.fragment),W=m(),J=i("p"),rt=e("V\xE0 n\xF3 \u0111\xF3! Ch\xFAng ta c\xF3 th\u1EC3 chuy\u1EC3n nh\u1EEFng b\u1ED9 d\u1EEF li\u1EC7u \u0111\xF3 sang b\xE0i gi\u1EA3ng ti\u1EBFp theo, n\u01A1i vi\u1EC7c hu\u1EA5n luy\u1EC7n s\u1EBD tr\u1EDF n\xEAn \u0111\u01A1n gi\u1EA3n m\u1ED9t c\xE1ch d\u1EC5 ch\u1ECBu sau t\u1EA5t c\u1EA3 nh\u1EEFng c\xF4ng vi\u1EC7c kh\xF3 kh\u0103n c\u1EE7a vi\u1EC7c x\u1EED l\xFD tr\u01B0\u1EDBc d\u1EEF li\u1EC7u.")},l(D){a=r(D,"P",{});var O=o(a);u=c(O,"B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 c\xF3 b\u1ED9 d\u1EEF li\u1EC7u v\xE0 b\u1ED9 \u0111\u1ED1i chi\u1EBFu d\u1EEF li\u1EC7u, ta c\u1EA7n ph\u1EA3i k\u1EBFt h\u1EE3p ch\xFAng l\u1EA1i v\u1EDBi nhau. Ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i c\xE1c l\xF4 v\xE0 \u0111\u1ED1i chi\u1EBFu theo c\xE1ch th\u1EE7 c\xF4ng, nh\u01B0ng c\xE1ch n\xE0y r\u1EA5t t\u1ED1n c\xF4ng s\u1EE9c v\xE0 c\xF3 l\u1EBD c\u0169ng kh\xF4ng hi\u1EC7u qu\u1EA3 l\u1EAFm. Thay v\xE0o \u0111\xF3, c\xF3 m\u1ED9t ph\u01B0\u01A1ng ph\xE1p \u0111\u01A1n gi\u1EA3n cung c\u1EA5p gi\u1EA3i ph\xE1p hi\u1EC7u qu\u1EA3 cho v\u1EA5n \u0111\u1EC1 n\xE0y: "),h=r(O,"CODE",{});var Pn=o(h);_=c(Pn,"to_tf_dataset()"),Pn.forEach(s),y=c(O,". Nso \u0111\u01B0\u1EE3c bao m\u1ED9t "),d=r(O,"CODE",{});var ot=o(d);f=c(ot,"tf.data.Dataset"),ot.forEach(s),C=c(O," xung quanh t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a b\u1EA1n, v\u1EDBi m\u1ED9t ch\u1EE9c n\u0103ng \u0111\u1ED1i chi\u1EBFu t\xF9y ch\u1ECDn. "),k=r(O,"CODE",{});var Sn=o(k);T=c(Sn,"tf.data.Dataset"),Sn.forEach(s),M=c(O," l\xE0 m\u1ED9t \u0111\u1ECBnh d\u1EA1ng TensorFlow g\u1ED1c m\xE0 Keras c\xF3 th\u1EC3 s\u1EED d\u1EE5ng cho "),N=r(O,"CODE",{});var On=o(N);z=c(On,"model.fit()"),On.forEach(s),L=c(O,", v\xEC v\u1EADy ph\u01B0\u01A1ng ph\xE1p n\xE0y ngay l\u1EADp t\u1EE9c chuy\u1EC3n \u0111\u1ED5i m\u1ED9t \u{1F917} Dataset sang m\u1ED9t \u0111\u1ECBnh d\u1EA1ng s\u1EB5n s\xE0ng \u0111\u1EC3 hu\u1EA5n luy\u1EC7n. H\xE3y xem n\xF3 ho\u1EA1t \u0111\u1ED9ng v\u1EDBi t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng t\xF4i!"),O.forEach(s),B=g(D),x(S.$$.fragment,D),W=g(D),J=r(D,"P",{});var Gt=o(J);rt=c(Gt,"V\xE0 n\xF3 \u0111\xF3! Ch\xFAng ta c\xF3 th\u1EC3 chuy\u1EC3n nh\u1EEFng b\u1ED9 d\u1EEF li\u1EC7u \u0111\xF3 sang b\xE0i gi\u1EA3ng ti\u1EBFp theo, n\u01A1i vi\u1EC7c hu\u1EA5n luy\u1EC7n s\u1EBD tr\u1EDF n\xEAn \u0111\u01A1n gi\u1EA3n m\u1ED9t c\xE1ch d\u1EC5 ch\u1ECBu sau t\u1EA5t c\u1EA3 nh\u1EEFng c\xF4ng vi\u1EC7c kh\xF3 kh\u0103n c\u1EE7a vi\u1EC7c x\u1EED l\xFD tr\u01B0\u1EDBc d\u1EEF li\u1EC7u."),Gt.forEach(s)},m(D,O){p(D,a,O),n(a,u),n(a,h),n(h,_),n(a,y),n(a,d),n(d,f),n(a,C),n(a,k),n(k,T),n(a,M),n(a,N),n(N,z),n(a,L),p(D,B,O),j(S,D,O),p(D,W,O),p(D,J,O),n(J,rt),wt=!0},i(D){wt||(v(S.$$.fragment,D),wt=!0)},o(D){b(S.$$.fragment,D),wt=!1},d(D){D&&s(a),D&&s(B),E(S,D),D&&s(W),D&&s(J)}}}function mo(w){let a,u,h,_,y,d,f,C,k,T,M,N,z,L,B,S,W,J,rt,wt,D,O,Pn,ot,Sn,On,Gt,$t,qt,rs,Vt,Rc,os,Bc,je,tt,nt,An,Y,Wc,Kt,Ic,Uc,Xt,Gc,Vc,Jt,Kc,Xc,Ee,Nn,Jc,we,Yt,qe,Zt,Ce,I,Yc,ps,Zc,Qc,us,ta,na,ms,sa,ea,gs,ca,aa,ds,ha,la,ze,pt,ia,fs,ra,oa,_s,pa,ua,Te,Ct,ma,vs,ga,da,De,Qt,Pe,tn,Se,ut,fa,bs,_a,va,ks,ba,ka,Oe,nn,Ae,sn,Ne,H,ya,ys,$a,xa,$s,ja,Ea,xs,wa,qa,js,Ca,za,Es,Ta,Da,ws,Pa,Sa,qs,Oa,Aa,Me,zt,Le,xt,Tt,Cs,en,Na,zs,Ma,He,st,et,Mn,Dt,La,Ln,Ha,Fa,Fe,cn,Re,Hn,Ra,Be,an,We,hn,Ie,V,Ba,Ts,Wa,Ia,Ds,Ua,Ga,Fn,Va,Ka,Ps,Xa,Ja,Ue,Pt,Ge,St,Ya,Ss,Za,Qa,Ve,ln,Ke,Rn,th,Xe,rn,Je,mt,nh,Os,sh,eh,As,ch,ah,Ye,on,Ze,K,hh,Ns,lh,ih,Ms,rh,oh,Ls,ph,uh,Hs,mh,gh,Qe,Ot,dh,Fs,fh,_h,tc,gt,vh,Bn,bh,kh,Rs,yh,$h,nc,Wn,xh,sc,At,jh,Bs,Eh,wh,ec,dt,qh,In,Ch,zh,Un,Th,Dh,cc,pn,ac,X,Ph,Ws,Sh,Oh,Is,Ah,Nh,Us,Mh,Lh,un,Hh,Fh,hc,ft,Rh,mn,Gs,Bh,Wh,Vs,Ih,Uh,lc,gn,ic,A,Gh,Ks,Vh,Kh,Xs,Xh,Jh,Js,Yh,Zh,Ys,Qh,tl,Zs,nl,sl,Qs,el,cl,te,al,hl,ne,ll,il,dn,rl,ol,rc,Nt,pl,se,ul,ml,oc,_t,gl,ee,dl,fl,ce,_l,vl,pc,fn,uc,Gn,bl,mc,_n,gc,vt,kl,ae,yl,$l,he,xl,jl,dc,G,le,El,wl,ie,ql,Cl,re,zl,Tl,oe,Dl,Pl,pe,Sl,Ol,fc,Mt,Al,ue,Nl,Ml,_c,jt,Lt,me,vn,Ll,ge,Hl,vc,bn,bc,Vn,Ht,Fl,de,Rl,Bl,kc,ct,at,Kn,Z,Wl,fe,Il,Ul,_e,Gl,Vl,ve,Kl,Xl,yc,kn,$c,yn,xc,Ft,Jl,be,Yl,Zl,jc,$n,Ec,ht,lt,Xn,Rt,wc,Jn,qc;h=new Xr({props:{fw:w[0]}}),C=new Fc({});const ti=[Yr,Jr],xn=[];function ni(t,l){return t[0]==="pt"?0:1}z=ni(w),L=xn[z]=ti[z](w);const si=[Qr,Zr],jn=[];function ei(t,l){return t[0]==="pt"?0:1}S=ei(w),W=jn[S]=si[S](w),Vt=new Fc({});const ci=[no,to],En=[];function ai(t,l){return t[0]==="pt"?0:1}tt=ai(w),nt=En[tt]=ci[tt](w),Yt=new P({props:{code:`from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

raw_datasets = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>)
raw_datasets`}}),Zt=new P({props:{code:`DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),Qt=new P({props:{code:`raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]`,highlighted:`raw_train_dataset = raw_datasets[<span class="hljs-string">&quot;train&quot;</span>]
raw_train_dataset[<span class="hljs-number">0</span>]`}}),tn=new P({props:{code:`{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}`,highlighted:`{<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>}`}}),nn=new P({props:{code:"raw_train_dataset.features",highlighted:"raw_train_dataset.features"}}),sn=new P({props:{code:`{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}`,highlighted:`{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),zt=new Ql({props:{$$slots:{default:[so]},$$scope:{ctx:w}}}),en=new Fc({});const hi=[co,eo],wn=[];function li(t,l){return t[0]==="pt"?0:1}st=li(w),et=wn[st]=hi[st](w),cn=new P({props:{code:`from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>])
tokenized_sentences_2 = tokenizer(raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>])`}}),an=new P({props:{code:`inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs`,highlighted:`inputs = tokenizer(<span class="hljs-string">&quot;This is the first sentence.&quot;</span>, <span class="hljs-string">&quot;This is the second one.&quot;</span>)
inputs`}}),hn=new P({props:{code:`{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}`,highlighted:`{ 
  <span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2034</span>, <span class="hljs-number">6251</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>, <span class="hljs-number">2023</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2117</span>, <span class="hljs-number">2028</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
  <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
  <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),Pt=new Ql({props:{$$slots:{default:[ao]},$$scope:{ctx:w}}}),ln=new P({props:{code:'tokenizer.convert_ids_to_tokens(inputs["input_ids"])',highlighted:'tokenizer.convert_ids_to_tokens(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])'}}),rn=new P({props:{code:"['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']",highlighted:'[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]'}}),on=new P({props:{code:`['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;first&#x27;</span>, <span class="hljs-string">&#x27;sentence&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>, <span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;second&#x27;</span>, <span class="hljs-string">&#x27;one&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]
[      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,          <span class="hljs-number">0</span>,   <span class="hljs-number">0</span>,       <span class="hljs-number">0</span>,      <span class="hljs-number">1</span>,    <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,        <span class="hljs-number">1</span>,     <span class="hljs-number">1</span>,   <span class="hljs-number">1</span>,       <span class="hljs-number">1</span>]`}}),pn=new P({props:{code:`tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)`,highlighted:`tokenized_dataset = tokenizer(
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence1&quot;</span>],
    raw_datasets[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-string">&quot;sentence2&quot;</span>],
    padding=<span class="hljs-literal">True</span>,
    truncation=<span class="hljs-literal">True</span>,
)`}}),gn=new P({props:{code:`def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">&quot;sentence1&quot;</span>], example[<span class="hljs-string">&quot;sentence2&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),fn=new P({props:{code:`tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets`,highlighted:`tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)
tokenized_datasets`}}),_n=new P({props:{code:`DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">3668</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">408</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>],
        num_rows: <span class="hljs-number">1725</span>
    })
})`}}),vn=new Fc({}),bn=new xe({props:{id:"7q5NyFT8REg"}});function ii(t,l){return t[0]==="pt"?lo:ho}let Cc=ii(w),Et=Cc(w);const ri=[ro,io],qn=[];function oi(t,l){return t[0]==="pt"?0:1}ct=oi(w),at=qn[ct]=ri[ct](w),kn=new P({props:{code:`samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]`,highlighted:`samples = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][:<span class="hljs-number">8</span>]
samples = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> samples.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idx&quot;</span>, <span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentence2&quot;</span>]}
[<span class="hljs-built_in">len</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> samples[<span class="hljs-string">&quot;input_ids&quot;</span>]]`}}),yn=new P({props:{code:"[50, 59, 47, 67, 59, 50, 62, 32]",highlighted:'[<span class="hljs-number">50</span>, <span class="hljs-number">59</span>, <span class="hljs-number">47</span>, <span class="hljs-number">67</span>, <span class="hljs-number">59</span>, <span class="hljs-number">50</span>, <span class="hljs-number">62</span>, <span class="hljs-number">32</span>]'}}),$n=new P({props:{code:`batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}`,highlighted:`batch = data_collator(samples)
{k: v.shape <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}`}});const pi=[po,oo],Cn=[];function ui(t,l){return t[0]==="tf"?0:1}ht=ui(w),lt=Cn[ht]=pi[ht](w),Rt=new Ql({props:{$$slots:{default:[uo]},$$scope:{ctx:w}}});let R=w[0]==="tf"&&Br();return{c(){a=i("meta"),u=m(),$(h.$$.fragment),_=m(),y=i("h1"),d=i("a"),f=i("span"),$(C.$$.fragment),k=m(),T=i("span"),M=e("X\u1EED l\xFD d\u1EEF li\u1EC7u"),N=m(),L.c(),B=m(),W.c(),J=m(),rt=i("p"),wt=e("T\u1EA5t nhi\xEAn, ch\u1EC9 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh tr\xEAn hai c\xE2u s\u1EBD kh\xF4ng mang l\u1EA1i k\u1EBFt qu\u1EA3 t\u1ED1t. \u0110\u1EC3 c\xF3 \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3 t\u1ED1t h\u01A1n, b\u1EA1n s\u1EBD c\u1EA7n chu\u1EA9n b\u1ECB m\u1ED9t b\u1ED9 d\u1EEF li\u1EC7u l\u1EDBn h\u01A1n."),D=m(),O=i("p"),Pn=e("Trong ph\u1EA7n n\xE0y, ch\xFAng t\xF4i s\u1EBD s\u1EED d\u1EE5ng t\u1EADp d\u1EEF li\u1EC7u MRPC (Microsoft Research Paraphrase Corpus) l\xE0m v\xED d\u1EE5, \u0111\u01B0\u1EE3c gi\u1EDBi thi\u1EC7u trong "),ot=i("a"),Sn=e("b\xE0i b\xE1o"),On=e(" c\u1EE7a William B. Dolan v\xE0 Chris Brockett. T\u1EADp d\u1EEF li\u1EC7u bao g\u1ED3m 5,801 c\u1EB7p c\xE2u, v\u1EDBi nh\xE3n cho bi\u1EBFt ch\xFAng c\xF3 ph\u1EA3i l\xE0 c\xE2u di\u1EC5n gi\u1EA3i hay kh\xF4ng (t\u1EE9c l\xE0 n\u1EBFu c\u1EA3 hai c\xE2u \u0111\u1EC1u c\xF3 ngh\u0129a gi\u1ED1ng nhau). Ch\xFAng t\xF4i \u0111\xE3 ch\u1ECDn n\xF3 cho ch\u01B0\u01A1ng n\xE0y v\xEC n\xF3 l\xE0 m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u nh\u1ECF, v\xEC v\u1EADy th\u1EADt d\u1EC5 d\xE0ng \u0111\u1EC3 th\u1EED nghi\u1EC7m v\u1EDBi vi\u1EC7c hu\u1EA5n luy\u1EC7n v\u1EC1 n\xF3."),Gt=m(),$t=i("h3"),qt=i("a"),rs=i("span"),$(Vt.$$.fragment),Rc=m(),os=i("span"),Bc=e("T\u1EA3i b\u1ED9 d\u1EEF li\u1EC7u t\u1EEB Hub"),je=m(),nt.c(),An=m(),Y=i("p"),Wc=e("Hub kh\xF4ng ch\u1EC9 ch\u1EE9a c\xE1c m\xF4 h\xECnh; n\xF3 c\u0169ng c\xF3 nhi\u1EC1u b\u1ED9 d\u1EEF li\u1EC7u nhi\u1EC1u ng\xF4n ng\u1EEF kh\xE1c nhau. B\u1EA1n c\xF3 th\u1EC3 xem qua t\u1EADp d\u1EEF li\u1EC7u "),Kt=i("a"),Ic=e("t\u1EA1i \u0111\xE2y"),Uc=e(" v\xE0 ch\xFAng t\xF4i khuy\xEAn b\u1EA1n n\xEAn th\u1EED t\u1EA3i v\xE0 x\u1EED l\xFD b\u1ED9 d\u1EEF li\u1EC7u m\u1EDBi khi b\u1EA1n \u0111\xE3 xem qua ph\u1EA7n n\xE0y (xem t\xE0i li\u1EC7u chung "),Xt=i("a"),Gc=e("t\u1EA1i \u0111\xE2y"),Vc=e("). Nh\u01B0ng hi\u1EC7n t\u1EA1i, h\xE3y t\u1EADp trung v\xE0o b\u1ED9 d\u1EEF li\u1EC7u MRPC! \u0110\xE2y l\xE0 m\u1ED9t trong 10 b\u1ED9 d\u1EEF li\u1EC7u t\u1EA1o n\xEAn "),Jt=i("a"),Kc=e("b\u1ED9 chu\u1EA9n GLUE"),Xc=e(", l\xE0 m\u1ED9t \u0111i\u1EC3m chu\u1EA9n h\u1ECDc thu\u1EADt \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 \u0111o hi\u1EC7u su\u1EA5t c\u1EE7a c\xE1c m\xF4 h\xECnh ML tr\xEAn 10 t\xE1c v\u1EE5 ph\xE2n lo\u1EA1i v\u0103n b\u1EA3n kh\xE1c nhau."),Ee=m(),Nn=i("p"),Jc=e("Th\u01B0 vi\u1EC7n \u{1F917} Datasets cung c\u1EA5p m\u1ED9t l\u1EC7nh r\u1EA5t \u0111\u01A1n gi\u1EA3n \u0111\u1EC3 t\u1EA3i xu\u1ED1ng v\xE0 l\u01B0u v\xE0o b\u1ED9 nh\u1EDB cache m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u tr\xEAn Hub. Ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i xu\u1ED1ng b\u1ED9 d\u1EEF li\u1EC7u MRPC nh\u01B0 sau:"),we=m(),$(Yt.$$.fragment),qe=m(),$(Zt.$$.fragment),Ce=m(),I=i("p"),Yc=e("Nh\u01B0 b\u1EA1n c\xF3 th\u1EC3 th\u1EA5y, ch\xFAng ta nh\u1EADn \u0111\u01B0\u1EE3c m\u1ED9t \u0111\u1ED1i t\u01B0\u1EE3ng "),ps=i("code"),Zc=e("DatasetDict"),Qc=e(" ch\u1EE9a t\u1EADp hu\u1EA5n luy\u1EC7n, t\u1EADp ki\u1EC3m \u0111\u1ECBnh v\xE0 t\u1EADp ki\u1EC3m th\u1EED. M\u1ED7i t\u1EADp ch\u1EE9a m\u1ED9t s\u1ED1 c\u1ED9t ("),us=i("code"),ta=e("sentence1"),na=e(", "),ms=i("code"),sa=e("sentence2"),ea=e(", "),gs=i("code"),ca=e("label"),aa=e(", v\xE0 "),ds=i("code"),ha=e("idx"),la=e(") v\xE0 m\u1ED9t s\u1ED1 h\xE0ng thay \u0111\u1ED5i, l\xE0 s\u1ED1 ph\u1EA7n t\u1EED trong m\u1ED7i t\u1EADp (v\xEC v\u1EADy, c\xF3 3,668 c\u1EB7p c\xE2u trong t\u1EADp hu\u1EA5n luy\u1EC7n, 408 trong t\u1EADp ki\u1EC3m ch\u1EE9ng v\xE0 1,725 trong t\u1EADp ki\u1EC3m \u0111\u1ECBnh)."),ze=m(),pt=i("p"),ia=e("L\u1EC7nh n\xE0y t\u1EA3i xu\u1ED1ng v\xE0 l\u01B0u v\xE0o b\u1ED9 nh\u1EDB cache c\xE1c t\u1EADp d\u1EEF li\u1EC7u, m\u1EB7c \u0111\u1ECBnh l\u01B0u trong "),fs=i("em"),ra=e("~/.cache/huggingface/datasets"),oa=e(". Nh\u1EDB l\u1EA1i t\u1EEB Ch\u01B0\u01A1ng 2 r\u1EB1ng b\u1EA1n c\xF3 th\u1EC3 t\xF9y ch\u1EC9nh th\u01B0 m\u1EE5c b\u1ED9 nh\u1EDB cache c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch \u0111\u1EB7t bi\u1EBFn m\xF4i tr\u01B0\u1EDDng "),_s=i("code"),pa=e("HF_HOME"),ua=e("."),Te=m(),Ct=i("p"),ma=e("Ch\xFAng ta c\xF3 th\u1EC3 truy c\u1EADp t\u1EEBng c\u1EB7p c\xE2u trong \u0111\u1ED1i t\u01B0\u1EE3ng "),vs=i("code"),ga=e("raw_datasets"),da=e(" c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch l\u1EADp ch\u1EC9 m\u1EE5c, gi\u1ED1ng nh\u01B0 v\u1EDBi t\u1EEB \u0111i\u1EC3n:"),De=m(),$(Qt.$$.fragment),Pe=m(),$(tn.$$.fragment),Se=m(),ut=i("p"),fa=e("Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y c\xE1c nh\xE3n v\u1ED1n l\xE0 s\u1ED1 nguy\xEAn, v\xEC v\u1EADy ch\xFAng ta kh\xF4ng ph\u1EA3i th\u1EF1c hi\u1EC7n b\u1EA5t k\u1EF3 b\u01B0\u1EDBc x\u1EED l\xFD tr\u01B0\u1EDBc n\xE0o \u1EDF \u0111\xF3. \u0110\u1EC3 bi\u1EBFt s\u1ED1 nguy\xEAn n\xE0o t\u01B0\u01A1ng \u1EE9ng v\u1EDBi nh\xE3n n\xE0o, ch\xFAng ta c\xF3 th\u1EC3 ki\u1EC3m tra "),bs=i("code"),_a=e("features"),va=e(" c\u1EE7a "),ks=i("code"),ba=e("raw_train_dataset"),ka=e(". \u0110i\u1EC1u n\xE0y s\u1EBD cho ch\xFAng t\xF4i bi\u1EBFt lo\u1EA1i c\u1EE7a m\u1ED7i c\u1ED9t:"),Oe=m(),$(nn.$$.fragment),Ae=m(),$(sn.$$.fragment),Ne=m(),H=i("p"),ya=e("Ph\xEDa sau, "),ys=i("code"),$a=e("label"),xa=e(" thu\u1ED9c lo\u1EA1i "),$s=i("code"),ja=e("ClassLabel"),Ea=e(" v\xE0 \xE1nh x\u1EA1 c\xE1c s\u1ED1 nguy\xEAn th\xE0nh t\xEAn nh\xE3n \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF trong th\u01B0 m\u1EE5c "),xs=i("em"),wa=e("names"),qa=e(". "),js=i("code"),Ca=e("0"),za=e(" t\u01B0\u01A1ng \u1EE9ng v\u1EDBi "),Es=i("code"),Ta=e("kh\xF4ng t\u01B0\u01A1ng \u0111\u01B0\u01A1ng"),Da=e(", v\xE0 "),ws=i("code"),Pa=e("1"),Sa=e(" t\u01B0\u01A1ng \u1EE9ng v\u1EDBi "),qs=i("code"),Oa=e("t\u01B0\u01A1ng \u0111\u01B0\u01A1ng"),Aa=e("."),Me=m(),$(zt.$$.fragment),Le=m(),xt=i("h3"),Tt=i("a"),Cs=i("span"),$(en.$$.fragment),Na=m(),zs=i("span"),Ma=e("Ti\u1EC1n x\u1EED l\xFD m\u1ED9t b\u1ED9 d\u1EEF li\u1EC7u"),He=m(),et.c(),Mn=m(),Dt=i("p"),La=e("\u0110\u1EC3 ti\u1EC1n x\u1EED l\xFD b\u1ED9 d\u1EEF li\u1EC7u, ch\xFAng ta c\u1EA7n chuy\u1EC3n v\u0103n b\u1EA3n th\xE0nh c\xE1c s\u1ED1 m\xE0 m\xF4 h\xECnh c\xF3 th\u1EC3 hi\u1EC3u \u0111\u01B0\u1EE3c. Nh\u01B0 b\u1EA1n \u0111\xE3 th\u1EA5y trong "),Ln=i("a"),Ha=e("ch\u01B0\u01A1ng tr\u01B0\u1EDBc"),Fa=e(", \u0111i\u1EC1u n\xE0y \u0111\u01B0\u1EE3c th\u1EF1c hi\u1EC7n v\u1EDBi m\u1ED9t tokenizer. Ch\xFAng ta c\xF3 th\u1EC3 cung c\u1EA5p cho tokenizer m\u1ED9t c\xE2u ho\u1EB7c m\u1ED9t danh s\xE1ch c\xE1c c\xE2u, v\xEC v\u1EADy ch\xFAng ta c\xF3 th\u1EC3 tokenizer tr\u1EF1c ti\u1EBFp t\u1EA5t c\u1EA3 c\xE1c c\xE2u \u0111\u1EA7u ti\xEAn v\xE0 t\u1EA5t c\u1EA3 c\xE1c c\xE2u th\u1EE9 hai c\u1EE7a m\u1ED7i c\u1EB7p nh\u01B0 sau:"),Fe=m(),$(cn.$$.fragment),Re=m(),Hn=i("p"),Ra=e("Tuy nhi\xEAn, ch\xFAng ta kh\xF4ng th\u1EC3 ch\u1EC9 chuy\u1EC3n hai chu\u1ED7i v\xE0o m\xF4 h\xECnh v\xE0 nh\u1EADn \u0111\u01B0\u1EE3c d\u1EF1 \u0111o\xE1n li\u1EC7u hai c\xE2u c\xF3 ph\u1EA3i l\xE0 di\u1EC5n gi\u1EA3i hay kh\xF4ng. Ch\xFAng ta c\u1EA7n x\u1EED l\xFD hai chu\u1ED7i nh\u01B0 m\u1ED9t c\u1EB7p v\xE0 \xE1p d\u1EE5ng ti\u1EC1n x\u1EED l\xFD th\xEDch h\u1EE3p. May m\u1EAFn thay, tokenizer c\u0169ng c\xF3 th\u1EC3 nh\u1EADn m\u1ED9t c\u1EB7p chu\u1ED7i v\xE0 chu\u1EA9n b\u1ECB n\xF3 theo c\xE1ch m\xE0 m\xF4 h\xECnh BERT c\u1EE7a ta mong \u0111\u1EE3i:"),Be=m(),$(an.$$.fragment),We=m(),$(hn.$$.fragment),Ie=m(),V=i("p"),Ba=e("Ch\xFAng ta \u0111\xE3 th\u1EA3o lu\u1EADn v\u1EC1 "),Ts=i("code"),Wa=e("input_ids"),Ia=e(" v\xE0 "),Ds=i("code"),Ua=e("attention_mask"),Ga=e(" trong "),Fn=i("a"),Va=e("Ch\u01B0\u01A1ng 2"),Ka=e(", nh\u01B0ng ch\xFAng ta t\u1EA1m d\u1EEBng \u0111\u1EC3 n\xF3i v\u1EC1 "),Ps=i("code"),Xa=e("token_type_ids"),Ja=e(". Trong v\xED d\u1EE5 n\xE0y, \u0111\xE2y l\xE0 ph\u1EA7n cho m\xF4 h\xECnh bi\u1EBFt ph\u1EA7n n\xE0o c\u1EE7a \u0111\u1EA7u v\xE0o l\xE0 c\xE2u \u0111\u1EA7u ti\xEAn v\xE0 ph\u1EA7n n\xE0o l\xE0 c\xE2u th\u1EE9 hai."),Ue=m(),$(Pt.$$.fragment),Ge=m(),St=i("p"),Ya=e("N\u1EBFu ch\xFAng ta gi\u1EA3i m\xE3 c\xE1c ID b\xEAn trong "),Ss=i("code"),Za=e("input_ids"),Qa=e(" tr\u1EDF l\u1EA1i c\xE1c t\u1EEB:"),Ve=m(),$(ln.$$.fragment),Ke=m(),Rn=i("p"),th=e("ta s\u1EBD nh\u1EADn \u0111\u01B0\u1EE3c:"),Xe=m(),$(rn.$$.fragment),Je=m(),mt=i("p"),nh=e("C\xF3 th\u1EC3 th\u1EA5y m\xF4 h\xECnh k\xEC v\u1ECDng c\xE1c \u0111\u1EA7u v\xE0o c\xF3 d\u1EA1ng "),Os=i("code"),sh=e("[CLS] c\xE2u1 [SEP] c\xE2u2 [SEP]"),eh=e(" khi c\xF3 hai c\xE2u. C\u0103n ch\u1EC9nh \u0111i\u1EC1u n\xE0y v\u1EDBi "),As=i("code"),ch=e("token_type_ids"),ah=e(" cho ta k\u1EBFt qu\u1EA3:"),Ye=m(),$(on.$$.fragment),Ze=m(),K=i("p"),hh=e("Nh\u01B0 b\u1EA1n c\xF3 th\u1EC3 th\u1EA5y, c\xE1c ph\u1EA7n c\u1EE7a \u0111\u1EA7u v\xE0o t\u01B0\u01A1ng \u1EE9ng v\u1EDBi "),Ns=i("code"),lh=e("[CLS] c\xE2u1 [SEP]"),ih=e(" \u0111\u1EC1u c\xF3 lo\u1EA1i token ID l\xE0 "),Ms=i("code"),rh=e("0"),oh=e(", trong khi c\xE1c ph\u1EA7n kh\xE1c, t\u01B0\u01A1ng \u1EE9ng v\u1EDBi "),Ls=i("code"),ph=e("c\xE2u2 [SEP]"),uh=e(", t\u1EA5t c\u1EA3 \u0111\u1EC1u c\xF3 lo\u1EA1i token ID l\xE0 "),Hs=i("code"),mh=e("1"),gh=e("."),Qe=m(),Ot=i("p"),dh=e("L\u01B0u \xFD r\u1EB1ng n\u1EBFu b\u1EA1n ch\u1ECDn m\u1ED9t checkpoint kh\xE1c, b\u1EA1n s\u1EBD kh\xF4ng nh\u1EA5t thi\u1EBFt ph\u1EA3i c\xF3 "),Fs=i("code"),fh=e("token_type_ids"),_h=e(" trong \u0111\u1EA7u v\xE0o \u0111\u01B0\u1EE3c tokenize c\u1EE7a m\xECnh (v\xED d\u1EE5: ch\xFAng s\u1EBD kh\xF4ng \u0111\u01B0\u1EE3c tr\u1EA3 l\u1EA1i n\u1EBFu b\u1EA1n s\u1EED d\u1EE5ng m\xF4 h\xECnh DistilBERT). Ch\xFAng ch\u1EC9 \u0111\u01B0\u1EE3c tr\u1EA3 l\u1EA1i khi m\xF4 h\xECnh bi\u1EBFt ph\u1EA3i l\xE0m g\xEC v\u1EDBi ch\xFAng, b\u1EDFi v\xEC n\xF3 \u0111\xE3 nh\xECn th\u1EA5y ch\xFAng trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc."),tc=m(),gt=i("p"),vh=e("\u1EDE \u0111\xE2y, BERT \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc v\u1EDBi c\xE1c token ID v\xE0 tr\xEAn \u0111\u1EA7u m\u1EE5c ti\xEAu m\xF4 h\xECnh ng\xF4n ng\u1EEF \u0111\u01B0\u1EE3c che m\xE0 ch\xFAng ta \u0111\xE3 \u0111\u1EC1 c\u1EADp trong "),Bn=i("a"),bh=e("Ch\u01B0\u01A1ng 1"),kh=e(", n\xF3 c\xF3 m\u1ED9t m\u1EE5c ti\xEAu b\u1ED5 sung \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),Rs=i("em"),yh=e("d\u1EF1 \u0111o\xE1n c\xE2u ti\u1EBFp theo"),$h=e(". M\u1EE5c ti\xEAu c\u1EE7a t\xE1c v\u1EE5 n\xE0y l\xE0 m\xF4 h\xECnh h\xF3a m\u1ED1i quan h\u1EC7 gi\u1EEFa c\xE1c c\u1EB7p c\xE2u."),nc=m(),Wn=i("p"),xh=e("V\u1EDBi d\u1EF1 \u0111o\xE1n c\xE2u ti\u1EBFp theo, m\xF4 h\xECnh \u0111\u01B0\u1EE3c cung c\u1EA5p c\xE1c c\u1EB7p c\xE2u (v\u1EDBi c\xE1c token \u0111\u01B0\u1EE3c che ng\u1EABu nhi\xEAn) v\xE0 \u0111\u01B0\u1EE3c y\xEAu c\u1EA7u d\u1EF1 \u0111o\xE1n li\u1EC7u c\xE2u th\u1EE9 hai c\xF3 theo sau c\xE2u \u0111\u1EA7u ti\xEAn hay kh\xF4ng. \u0110\u1EC3 l\xE0m cho t\xE1c v\u1EE5 tr\u1EDF n\xEAn kh\xF4ng t\u1EA7m th\u01B0\u1EDDng, m\u1ED9t n\u1EEDa l\xE0 c\xE1c c\xE2u ti\u1EBFp n\u1ED1i nhau trong t\xE0i li\u1EC7u g\u1ED1c m\xE0 ch\xFAng \u0111\u01B0\u1EE3c tr\xEDch xu\u1EA5t, v\xE0 n\u1EEDa c\xF2n l\u1EA1i l\xE0 hai c\xE2u \u0111\u1EBFn t\u1EEB hai t\xE0i li\u1EC7u kh\xE1c nhau."),sc=m(),At=i("p"),jh=e("N\xF3i chung, b\u1EA1n kh\xF4ng c\u1EA7n ph\u1EA3i lo l\u1EAFng v\u1EC1 vi\u1EC7c c\xF3 hay kh\xF4ng c\xF3 "),Bs=i("code"),Eh=e("token_type_ids"),wh=e(" trong \u0111\u1EA7u v\xE0o \u0111\u01B0\u1EE3c tokenize c\u1EE7a m\xECnh: mi\u1EC5n l\xE0 b\u1EA1n s\u1EED d\u1EE5ng c\xF9ng m\u1ED9t checkpoint cho tr\xECnh tokenize v\xE0 m\xF4 h\xECnh, m\u1ECDi th\u1EE9 s\u1EBD \u1ED5n v\xEC tr\xECnh tokenize nh\u1EADn bi\u1EBFt c\u1EA7n cung c\u1EA5p nh\u1EEFng g\xEC v\u1EDBi m\xF4 h\xECnh c\u1EE7a n\xF3."),ec=m(),dt=i("p"),qh=e("B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 th\u1EA5y c\xE1ch tr\xECnh tokenize c\u1EE7a ch\xFAng ta c\xF3 th\u1EC3 x\u1EED l\xFD m\u1ED9t c\u1EB7p c\xE2u, ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng n\xF3 \u0111\u1EC3 m\xE3 h\xF3a to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh: gi\u1ED1ng nh\u01B0 trong "),In=i("a"),Ch=e("ch\u01B0\u01A1ng tr\u01B0\u1EDBc"),zh=e(", ch\xFAng ta c\xF3 th\u1EC3 cung c\u1EA5p cho tr\xECnh tokenize danh s\xE1ch c\xE1c c\u1EB7p b\u1EB1ng c\xE1ch \u0111\u01B0a cho n\xF3 danh s\xE1ch c\xE1c c\xE2u \u0111\u1EA7u ti\xEAn, sau \u0111\xF3 l\xE0 danh s\xE1ch c\xE1c c\xE2u th\u1EE9 hai. \u0110i\u1EC1u n\xE0y c\u0169ng t\u01B0\u01A1ng th\xEDch v\u1EDBi c\xE1c t\xF9y ch\u1ECDn \u0111\u1EC7m v\xE0 c\u1EAFt b\u1EDBt m\xE0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),Un=i("a"),Th=e("Ch\u01B0\u01A1ng 2"),Dh=e(". V\xEC v\u1EADy, m\u1ED9t c\xE1ch \u0111\u1EC3 ti\u1EC1n x\u1EED l\xFD tr\u01B0\u1EDBc t\u1EADp d\u1EEF li\u1EC7u hu\u1EA5n luy\u1EC7n l\xE0:"),cc=m(),$(pn.$$.fragment),ac=m(),X=i("p"),Ph=e("\u0110i\u1EC1u n\xE0y ho\u1EA1t \u0111\u1ED9ng t\u1ED1t, nh\u01B0ng n\xF3 c\xF3 nh\u01B0\u1EE3c \u0111i\u1EC3m l\xE0 tr\u1EA3 v\u1EC1 t\u1EEB \u0111i\u1EC3n (v\u1EDBi c\xE1c kh\xF3a c\u1EE7a ch\xFAng t\xF4i, "),Ws=i("code"),Sh=e("input_ids"),Oh=e(", "),Is=i("code"),Ah=e("attention_mask"),Nh=e(" v\xE0 "),Us=i("code"),Mh=e("token_type_ids"),Lh=e(", v\xE0 c\xE1c gi\xE1 tr\u1ECB l\xE0 danh s\xE1ch c\xE1c danh s\xE1ch). N\xF3 c\u0169ng s\u1EBD ch\u1EC9 ho\u1EA1t \u0111\u1ED9ng n\u1EBFu b\u1EA1n c\xF3 \u0111\u1EE7 RAM \u0111\u1EC3 l\u01B0u tr\u1EEF to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh trong qu\xE1 tr\xECnh tokenize (trong khi c\xE1c t\u1EADp d\u1EEF li\u1EC7u t\u1EEB th\u01B0 vi\u1EC7n \u{1F917} Datasets l\xE0 c\xE1c t\u1EC7p "),un=i("a"),Hh=e("Apache Arrow"),Fh=e(" \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF tr\xEAn \u0111\u0129a, v\xEC v\u1EADy b\u1EA1n ch\u1EC9 gi\u1EEF c\xE1c m\u1EABu b\u1EA1n y\xEAu c\u1EA7u \u0111\xE3 t\u1EA3i trong b\u1ED9 nh\u1EDB)."),hc=m(),ft=i("p"),Rh=e("\u0110\u1EC3 gi\u1EEF d\u1EEF li\u1EC7u d\u01B0\u1EDBi d\u1EA1ng t\u1EADp d\u1EEF li\u1EC7u, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng ph\u01B0\u01A1ng th\u1EE9c "),mn=i("a"),Gs=i("code"),Bh=e("Dataset.map()"),Wh=e(". \u0110i\u1EC1u n\xE0y c\u0169ng cho ph\xE9p ch\xFAng ta linh ho\u1EA1t h\u01A1n, n\u1EBFu ch\xFAng ta c\u1EA7n th\u1EF1c hi\u1EC7n nhi\u1EC1u ti\u1EC1n x\u1EED l\xFD h\u01A1n l\xE0 ch\u1EC9 tokenize. Ph\u01B0\u01A1ng th\u1EE9c "),Vs=i("code"),Ih=e("map()"),Uh=e(" ho\u1EA1t \u0111\u1ED9ng b\u1EB1ng c\xE1ch \xE1p d\u1EE5ng m\u1ED9t h\xE0m tr\xEAn m\u1ED7i ph\u1EA7n t\u1EED c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u, v\xEC v\u1EADy h\xE3y x\xE1c \u0111\u1ECBnh m\u1ED9t h\xE0m tokenize c\xE1c \u0111\u1EA7u v\xE0o c\u1EE7a ch\xFAng ta:"),lc=m(),$(gn.$$.fragment),ic=m(),A=i("p"),Gh=e("H\xE0m n\xE0y l\u1EA5y m\u1ED9t t\u1EEB \u0111i\u1EC3n (gi\u1ED1ng nh\u01B0 c\xE1c m\u1EE5c trong t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng ta) v\xE0 tr\u1EA3 v\u1EC1 m\u1ED9t t\u1EEB \u0111i\u1EC3n m\u1EDBi v\u1EDBi c\xE1c kh\xF3a "),Ks=i("code"),Vh=e("input_ids"),Kh=e(", "),Xs=i("code"),Xh=e("attention_mask"),Jh=e(" v\xE0 "),Js=i("code"),Yh=e("token_type_ids"),Zh=e(". L\u01B0u \xFD r\u1EB1ng n\xF3 c\u0169ng ho\u1EA1t \u0111\u1ED9ng n\u1EBFu t\u1EEB \u0111i\u1EC3n "),Ys=i("code"),Qh=e("example"),tl=e(" ch\u1EE9a m\u1ED9t s\u1ED1 m\u1EABu (m\u1ED7i kh\xF3a l\xE0 m\u1ED9t danh s\xE1ch c\xE1c c\xE2u) v\xEC "),Zs=i("code"),nl=e("tokenizer"),sl=e(" ho\u1EA1t \u0111\u1ED9ng tr\xEAn danh s\xE1ch c\xE1c c\u1EB7p c\xE2u, nh\u01B0 \u0111\xE3 th\u1EA5y tr\u01B0\u1EDBc \u0111\xE2y. \u0110i\u1EC1u n\xE0y s\u1EBD cho ph\xE9p ch\xFAng ta s\u1EED d\u1EE5ng t\xF9y ch\u1ECDn "),Qs=i("code"),el=e("batch = True"),cl=e(" trong l\u1EC7nh g\u1ECDi "),te=i("code"),al=e("map()"),hl=e(", t\u1EEB \u0111\xF3 s\u1EBD t\u0103ng t\u1ED1c \u0111\xE1ng k\u1EC3 qu\xE1 tr\xECnh tokenize. "),ne=i("code"),ll=e("Tokenizer"),il=e(" \u0111\u01B0\u1EE3c h\u1ED7 tr\u1EE3 b\u1EDFi m\u1ED9t tokenizer \u0111\u01B0\u1EE3c vi\u1EBFt b\u1EB1ng Rust t\u1EEB th\u01B0 vi\u1EC7n "),dn=i("a"),rl=e("\u{1F917} Tokenizer"),ol=e(". Tokenizer n\xE0y c\xF3 th\u1EC3 r\u1EA5t nhanh, nh\u01B0ng ch\u1EC9 khi ch\xFAng ta cung c\u1EA5p nhi\u1EC1u \u0111\u1EA7u v\xE0o c\xF9ng m\u1ED9t l\xFAc."),rc=m(),Nt=i("p"),pl=e("L\u01B0u \xFD r\u1EB1ng ch\xFAng ta \u0111\xE3 \u0111\u1EC3 t\u1EA1m b\u1ECF qua tham s\u1ED1 "),se=i("code"),ul=e("padding"),ml=e(" trong h\xE0m tokenize c\u1EE7a ta. \u0110i\u1EC1u n\xE0y l\xE0 do vi\u1EC7c \u0111\u1EC7m t\u1EA5t c\u1EA3 c\xE1c m\u1EABu \u0111\u1EBFn chi\u1EC1u d\xE0i t\u1ED1i \u0111a kh\xF4ng hi\u1EC7u qu\u1EA3: t\u1ED1t h\u01A1n n\xEAn \u0111\u1EC7m c\xE1c m\u1EABu khi ch\xFAng ta \u0111ang t\u1EA1o m\u1ED9t l\xF4, v\xEC khi \u0111\xF3 ch\xFAng ta ch\u1EC9 c\u1EA7n \u0111\u1EC7m \u0111\u1EBFn chi\u1EC1u d\xE0i t\u1ED1i \u0111a trong l\xF4 \u0111\xF3 ch\u1EE9 kh\xF4ng ph\u1EA3i chi\u1EC1u d\xE0i t\u1ED1i \u0111a trong to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u. \u0110i\u1EC1u n\xE0y c\xF3 th\u1EC3 ti\u1EBFt ki\u1EC7m r\u1EA5t nhi\u1EC1u th\u1EDDi gian v\xE0 c\xF4ng su\u1EA5t x\u1EED l\xFD khi c\xE1c \u0111\u1EA7u v\xE0o c\xF3 \u0111\u1ED9 d\xE0i r\u1EA5t thay \u0111\u1ED5i!"),oc=m(),_t=i("p"),gl=e("\u0110\xE2y l\xE0 c\xE1ch ch\xFAng ta \xE1p d\u1EE5ng ch\u1EE9c n\u0103ng m\xE3 h\xF3a tr\xEAn t\u1EA5t c\u1EA3 c\xE1c t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ta c\xF9ng m\u1ED9t l\xFAc. Ch\xFAng ta \u0111ang s\u1EED d\u1EE5ng "),ee=i("code"),dl=e("batch = True"),fl=e(" trong l\u1EC7nh g\u1ECDi t\u1EDBi "),ce=i("code"),_l=e("map"),vl=e(", v\xEC v\u1EADy, h\xE0m \u0111\u01B0\u1EE3c \xE1p d\u1EE5ng cho nhi\u1EC1u ph\u1EA7n t\u1EED c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u c\xF9ng m\u1ED9t l\xFAc, ch\u1EE9 kh\xF4ng ph\u1EA3i tr\xEAn t\u1EEBng ph\u1EA7n t\u1EED ri\xEAng bi\u1EC7t. \u0110i\u1EC1u n\xE0y cho ph\xE9p vi\u1EC7c ti\u1EC1n x\u1EED l\xFD nhanh h\u01A1n."),pc=m(),$(fn.$$.fragment),uc=m(),Gn=i("p"),bl=e("C\xE1ch th\u01B0 vi\u1EC7n \u{1F917} Datasets \xE1p d\u1EE5ng b\u01B0\u1EDBc x\u1EED l\xFD n\xE0y l\xE0 th\xEAm c\xE1c tr\u01B0\u1EDDng m\u1EDBi v\xE0o b\u1ED9 d\u1EEF li\u1EC7u, m\u1ED7i kh\xF3a trong t\u1EEB \u0111i\u1EC3n \u0111\u01B0\u1EE3c tr\u1EA3 v\u1EC1 b\u1EDFi h\xE0m ti\u1EC1n x\u1EED l\xFD m\u1ED9t tr\u01B0\u1EDDng:"),mc=m(),$(_n.$$.fragment),gc=m(),vt=i("p"),kl=e("B\u1EA1n th\u1EADm ch\xED c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u0111a x\u1EED l\xFD khi \xE1p d\u1EE5ng ch\u1EE9c n\u0103ng ti\u1EC1n x\u1EED l\xFD c\u1EE7a m\xECnh v\u1EDBi "),ae=i("code"),yl=e("map()"),$l=e(" b\u1EB1ng c\xE1ch truy\u1EC1n tham s\u1ED1 "),he=i("code"),xl=e("num_proc"),jl=e(". Ch\xFAng ta kh\xF4ng l\xE0m \u0111i\u1EC1u n\xE0y \u1EDF \u0111\xE2y v\xEC th\u01B0 vi\u1EC7n \u{1F917} Tokenizers \u0111\xE3 s\u1EED d\u1EE5ng nhi\u1EC1u chu\u1ED7i \u0111\u1EC3 tokenize \xE1c m\u1EABu c\u1EE7a nhanh h\u01A1n, nh\u01B0ng n\u1EBFu b\u1EA1n kh\xF4ng s\u1EED d\u1EE5ng tr\xECnh tokenize nhanh \u0111\u01B0\u1EE3c th\u01B0 vi\u1EC7n n\xE0y h\u1ED7 tr\u1EE3, b\u01B0\u1EDBc tr\xEAn c\xF3 th\u1EC3 t\u0103ng t\u1ED1c qu\xE1 tr\xECnh x\u1EED l\xFD tr\u01B0\u1EDBc c\u1EE7a b\u1EA1n."),dc=m(),G=i("p"),le=i("code"),El=e("Tokenize_function"),wl=e(" c\u1EE7a ch\xFAng ta tr\u1EA3 v\u1EC1 m\u1ED9t t\u1EEB \u0111i\u1EC3n v\u1EDBi c\xE1c kh\xF3a "),ie=i("code"),ql=e("input_ids"),Cl=e(", "),re=i("code"),zl=e("attention_mask"),Tl=e(" v\xE0 "),oe=i("code"),Dl=e("token_type_ids"),Pl=e(", v\xEC v\u1EADy ba tr\u01B0\u1EDDng \u0111\xF3 \u0111\u01B0\u1EE3c th\xEAm v\xE0o t\u1EA5t c\u1EA3 c\xE1c ph\u1EA7n b\u1ED9 d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng ta. L\u01B0u \xFD r\u1EB1ng ta c\u0169ng c\xF3 th\u1EC3 \u0111\xE3 thay \u0111\u1ED5i c\xE1c tr\u01B0\u1EDDng hi\u1EC7n c\xF3 n\u1EBFu h\xE0m ti\u1EC1n x\u1EED l\xFD tr\u1EA3 v\u1EC1 m\u1ED9t gi\xE1 tr\u1ECB m\u1EDBi cho m\u1ED9t kh\xF3a hi\u1EC7n c\xF3 trong t\u1EADp d\u1EEF li\u1EC7u m\xE0 ta \u0111\xE3 \xE1p d\u1EE5ng "),pe=i("code"),Sl=e("map()"),Ol=e("."),fc=m(),Mt=i("p"),Al=e("\u0110i\u1EC1u cu\u1ED1i c\xF9ng ch\xFAng ta s\u1EBD c\u1EA7n l\xE0m l\xE0 \u0111\u1EC7m t\u1EA5t c\u1EA3 c\xE1c v\xED d\u1EE5 \u0111\u1EC3 c\xF3 \u0111\u1ED9 d\xE0i c\u1EE7a ph\u1EA7n t\u1EED d\xE0i nh\u1EA5t khi ch\xFAng t\xF4i g\u1ED9p c\xE1c ph\u1EA7n t\u1EED l\u1EA1i v\u1EDBi nhau - m\u1ED9t k\u1EF9 thu\u1EADt m\xE0 ch\xFAng t\xF4i g\u1ECDi l\xE0 "),ue=i("em"),Nl=e("\u0111\u1EC7m \u0111\u1ED9ng"),Ml=e("."),_c=m(),jt=i("h3"),Lt=i("a"),me=i("span"),$(vn.$$.fragment),Ll=m(),ge=i("span"),Hl=e("Ph\u1EA7n \u0111\u1EC7m \u0111\u1ED9ng"),vc=m(),$(bn.$$.fragment),bc=m(),Et.c(),Vn=m(),Ht=i("p"),Fl=e("\u0110\u1EC3 th\u1EF1c hi\u1EC7n \u0111i\u1EC1u n\xE0y trong th\u1EF1c t\u1EBF, ch\xFAng ta ph\u1EA3i \u0111\u1ECBnh ngh\u0129a m\u1ED9t h\xE0m \u0111\u1ED1i chi\u1EBFu s\u1EBD \xE1p d\u1EE5ng \u0111\xFAng s\u1ED1 l\u01B0\u1EE3ng \u0111\u1EC7m cho c\xE1c m\u1EE5c c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u m\xE0 ch\xFAng ta mu\u1ED1n g\u1ED9p h\xE0ng lo\u1EA1t l\u1EA1i v\u1EDBi nhau. May m\u1EAFn thay, th\u01B0 vi\u1EC7n \u{1F917} Transformers cung c\u1EA5p cho ch\xFAng ta m\u1ED9t ch\u1EE9c n\u0103ng nh\u01B0 v\u1EADy th\xF4ng qua "),de=i("code"),Rl=e("DataCollatorWithPadding"),Bl=e(". C\u1EA7n c\xF3 tr\xECnh tokenize khi b\u1EA1n kh\u1EDFi t\u1EA1o n\xF3 (\u0111\u1EC3 bi\u1EBFt c\u1EA7n s\u1EED d\u1EE5ng token \u0111\u1EC7m n\xE0o v\xE0 li\u1EC7u m\xF4 h\xECnh mong \u0111\u1EE3i \u0111\u1EC7m \u1EDF b\xEAn tr\xE1i hay b\xEAn ph\u1EA3i c\u1EE7a c\xE1c \u0111\u1EA7u v\xE0o) v\xE0 s\u1EBD th\u1EF1c hi\u1EC7n m\u1ECDi th\u1EE9 b\u1EA1n c\u1EA7n:"),kc=m(),at.c(),Kn=m(),Z=i("p"),Wl=e("\u0110\u1EC3 ki\u1EC3m tra m\xF3n m\u1EDBi n\xE0y, ch\xFAng ta h\xE3y l\u1EA5y m\u1ED9t v\xE0i m\u1EABu t\u1EEB t\u1EADp hu\u1EA5n luy\u1EC7n m\xE0 ch\xFAng ta mu\u1ED1n gh\xE9p l\u1EA1i v\u1EDBi nhau. \u1EDE \u0111\xE2y, ch\xFAng ta x\xF3a c\xE1c c\u1ED9t "),fe=i("code"),Il=e("idx"),Ul=e(", "),_e=i("code"),Gl=e("sentence1"),Vl=e(", v\xE0 "),ve=i("code"),Kl=e("sentence2"),Xl=e(" v\xEC ch\xFAng kh\xF4ng c\u1EA7n thi\u1EBFt v\xE0 ch\u1EE9a c\xE1c chu\u1ED7i (v\xE0 ch\xFAng ta kh\xF4ng th\u1EC3 t\u1EA1o tensor b\u1EB1ng chu\u1ED7i) v\xE0 xem \u0111\u1ED9 d\xE0i c\u1EE7a m\u1ED7i m\u1EE5c trong l\xF4:"),yc=m(),$(kn.$$.fragment),$c=m(),$(yn.$$.fragment),xc=m(),Ft=i("p"),Jl=e("Kh\xF4ng c\xF3 g\xEC ng\u1EA1c nhi\xEAn, ta nh\u1EADn \u0111\u01B0\u1EE3c c\xE1c m\u1EABu c\xF3 \u0111\u1ED9 d\xE0i kh\xE1c nhau, t\u1EEB 32 \u0111\u1EBFn 67. \u0110\u1EC7m \u0111\u1ED9ng c\xF3 ngh\u0129a l\xE0 t\u1EA5t c\u1EA3 c\xE1c m\u1EABu trong l\xF4 n\xE0y ph\u1EA3i \u0111\u01B0\u1EE3c \u0111\u1EC7m \u0111\u1EBFn chi\u1EC1u d\xE0i 67, chi\u1EC1u d\xE0i t\u1ED1i \u0111a b\xEAn trong l\xF4. N\u1EBFu kh\xF4ng c\xF3 \u0111\u1EC7m \u0111\u1ED9ng, t\u1EA5t c\u1EA3 c\xE1c m\u1EABu s\u1EBD ph\u1EA3i \u0111\u01B0\u1EE3c \u0111\u1EC7m \u0111\u1EBFn \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a trong to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u ho\u1EB7c \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a m\xE0 m\xF4 h\xECnh c\xF3 th\u1EC3 ch\u1EA5p nh\u1EADn. H\xE3y ki\u1EC3m tra k\u1EF9 xem "),be=i("code"),Yl=e("data_collator"),Zl=e(" c\u1EE7a ch\xFAng ta c\xF3 t\u1EF1 \u0111\u1ED9ng \u0111\u1EC7m l\xF4 \u0111\xFAng c\xE1ch hay kh\xF4ng:"),jc=m(),$($n.$$.fragment),Ec=m(),lt.c(),Xn=m(),$(Rt.$$.fragment),wc=m(),R&&R.c(),Jn=Rr(),this.h()},l(t){const l=Vr('[data-svelte="svelte-1phssyn"]',document.head);a=r(l,"META",{name:!0,content:!0}),l.forEach(s),u=g(t),x(h.$$.fragment,t),_=g(t),y=r(t,"H1",{class:!0});var zn=o(y);d=r(zn,"A",{id:!0,class:!0,href:!0});var Yn=o(d);f=r(Yn,"SPAN",{});var Zn=o(f);x(C.$$.fragment,Zn),Zn.forEach(s),Yn.forEach(s),k=g(zn),T=r(zn,"SPAN",{});var Qn=o(T);M=c(Qn,"X\u1EED l\xFD d\u1EEF li\u1EC7u"),Qn.forEach(s),zn.forEach(s),N=g(t),L.l(t),B=g(t),W.l(t),J=g(t),rt=r(t,"P",{});var ke=o(rt);wt=c(ke,"T\u1EA5t nhi\xEAn, ch\u1EC9 hu\u1EA5n luy\u1EC7n m\xF4 h\xECnh tr\xEAn hai c\xE2u s\u1EBD kh\xF4ng mang l\u1EA1i k\u1EBFt qu\u1EA3 t\u1ED1t. \u0110\u1EC3 c\xF3 \u0111\u01B0\u1EE3c k\u1EBFt qu\u1EA3 t\u1ED1t h\u01A1n, b\u1EA1n s\u1EBD c\u1EA7n chu\u1EA9n b\u1ECB m\u1ED9t b\u1ED9 d\u1EEF li\u1EC7u l\u1EDBn h\u01A1n."),ke.forEach(s),D=g(t),O=r(t,"P",{});var Bt=o(O);Pn=c(Bt,"Trong ph\u1EA7n n\xE0y, ch\xFAng t\xF4i s\u1EBD s\u1EED d\u1EE5ng t\u1EADp d\u1EEF li\u1EC7u MRPC (Microsoft Research Paraphrase Corpus) l\xE0m v\xED d\u1EE5, \u0111\u01B0\u1EE3c gi\u1EDBi thi\u1EC7u trong "),ot=r(Bt,"A",{href:!0,rel:!0});var ye=o(ot);Sn=c(ye,"b\xE0i b\xE1o"),ye.forEach(s),On=c(Bt," c\u1EE7a William B. Dolan v\xE0 Chris Brockett. T\u1EADp d\u1EEF li\u1EC7u bao g\u1ED3m 5,801 c\u1EB7p c\xE2u, v\u1EDBi nh\xE3n cho bi\u1EBFt ch\xFAng c\xF3 ph\u1EA3i l\xE0 c\xE2u di\u1EC5n gi\u1EA3i hay kh\xF4ng (t\u1EE9c l\xE0 n\u1EBFu c\u1EA3 hai c\xE2u \u0111\u1EC1u c\xF3 ngh\u0129a gi\u1ED1ng nhau). Ch\xFAng t\xF4i \u0111\xE3 ch\u1ECDn n\xF3 cho ch\u01B0\u01A1ng n\xE0y v\xEC n\xF3 l\xE0 m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u nh\u1ECF, v\xEC v\u1EADy th\u1EADt d\u1EC5 d\xE0ng \u0111\u1EC3 th\u1EED nghi\u1EC7m v\u1EDBi vi\u1EC7c hu\u1EA5n luy\u1EC7n v\u1EC1 n\xF3."),Bt.forEach(s),Gt=g(t),$t=r(t,"H3",{class:!0});var Wt=o($t);qt=r(Wt,"A",{id:!0,class:!0,href:!0});var ts=o(qt);rs=r(ts,"SPAN",{});var $e=o(rs);x(Vt.$$.fragment,$e),$e.forEach(s),ts.forEach(s),Rc=g(Wt),os=r(Wt,"SPAN",{});var mi=o(os);Bc=c(mi,"T\u1EA3i b\u1ED9 d\u1EEF li\u1EC7u t\u1EEB Hub"),mi.forEach(s),Wt.forEach(s),je=g(t),nt.l(t),An=g(t),Y=r(t,"P",{});var It=o(Y);Wc=c(It,"Hub kh\xF4ng ch\u1EC9 ch\u1EE9a c\xE1c m\xF4 h\xECnh; n\xF3 c\u0169ng c\xF3 nhi\u1EC1u b\u1ED9 d\u1EEF li\u1EC7u nhi\u1EC1u ng\xF4n ng\u1EEF kh\xE1c nhau. B\u1EA1n c\xF3 th\u1EC3 xem qua t\u1EADp d\u1EEF li\u1EC7u "),Kt=r(It,"A",{href:!0,rel:!0});var gi=o(Kt);Ic=c(gi,"t\u1EA1i \u0111\xE2y"),gi.forEach(s),Uc=c(It," v\xE0 ch\xFAng t\xF4i khuy\xEAn b\u1EA1n n\xEAn th\u1EED t\u1EA3i v\xE0 x\u1EED l\xFD b\u1ED9 d\u1EEF li\u1EC7u m\u1EDBi khi b\u1EA1n \u0111\xE3 xem qua ph\u1EA7n n\xE0y (xem t\xE0i li\u1EC7u chung "),Xt=r(It,"A",{href:!0,rel:!0});var di=o(Xt);Gc=c(di,"t\u1EA1i \u0111\xE2y"),di.forEach(s),Vc=c(It,"). Nh\u01B0ng hi\u1EC7n t\u1EA1i, h\xE3y t\u1EADp trung v\xE0o b\u1ED9 d\u1EEF li\u1EC7u MRPC! \u0110\xE2y l\xE0 m\u1ED9t trong 10 b\u1ED9 d\u1EEF li\u1EC7u t\u1EA1o n\xEAn "),Jt=r(It,"A",{href:!0,rel:!0});var fi=o(Jt);Kc=c(fi,"b\u1ED9 chu\u1EA9n GLUE"),fi.forEach(s),Xc=c(It,", l\xE0 m\u1ED9t \u0111i\u1EC3m chu\u1EA9n h\u1ECDc thu\u1EADt \u0111\u01B0\u1EE3c s\u1EED d\u1EE5ng \u0111\u1EC3 \u0111o hi\u1EC7u su\u1EA5t c\u1EE7a c\xE1c m\xF4 h\xECnh ML tr\xEAn 10 t\xE1c v\u1EE5 ph\xE2n lo\u1EA1i v\u0103n b\u1EA3n kh\xE1c nhau."),It.forEach(s),Ee=g(t),Nn=r(t,"P",{});var _i=o(Nn);Jc=c(_i,"Th\u01B0 vi\u1EC7n \u{1F917} Datasets cung c\u1EA5p m\u1ED9t l\u1EC7nh r\u1EA5t \u0111\u01A1n gi\u1EA3n \u0111\u1EC3 t\u1EA3i xu\u1ED1ng v\xE0 l\u01B0u v\xE0o b\u1ED9 nh\u1EDB cache m\u1ED9t t\u1EADp d\u1EEF li\u1EC7u tr\xEAn Hub. Ch\xFAng ta c\xF3 th\u1EC3 t\u1EA3i xu\u1ED1ng b\u1ED9 d\u1EEF li\u1EC7u MRPC nh\u01B0 sau:"),_i.forEach(s),we=g(t),x(Yt.$$.fragment,t),qe=g(t),x(Zt.$$.fragment,t),Ce=g(t),I=r(t,"P",{});var Q=o(I);Yc=c(Q,"Nh\u01B0 b\u1EA1n c\xF3 th\u1EC3 th\u1EA5y, ch\xFAng ta nh\u1EADn \u0111\u01B0\u1EE3c m\u1ED9t \u0111\u1ED1i t\u01B0\u1EE3ng "),ps=r(Q,"CODE",{});var vi=o(ps);Zc=c(vi,"DatasetDict"),vi.forEach(s),Qc=c(Q," ch\u1EE9a t\u1EADp hu\u1EA5n luy\u1EC7n, t\u1EADp ki\u1EC3m \u0111\u1ECBnh v\xE0 t\u1EADp ki\u1EC3m th\u1EED. M\u1ED7i t\u1EADp ch\u1EE9a m\u1ED9t s\u1ED1 c\u1ED9t ("),us=r(Q,"CODE",{});var bi=o(us);ta=c(bi,"sentence1"),bi.forEach(s),na=c(Q,", "),ms=r(Q,"CODE",{});var ki=o(ms);sa=c(ki,"sentence2"),ki.forEach(s),ea=c(Q,", "),gs=r(Q,"CODE",{});var yi=o(gs);ca=c(yi,"label"),yi.forEach(s),aa=c(Q,", v\xE0 "),ds=r(Q,"CODE",{});var $i=o(ds);ha=c($i,"idx"),$i.forEach(s),la=c(Q,") v\xE0 m\u1ED9t s\u1ED1 h\xE0ng thay \u0111\u1ED5i, l\xE0 s\u1ED1 ph\u1EA7n t\u1EED trong m\u1ED7i t\u1EADp (v\xEC v\u1EADy, c\xF3 3,668 c\u1EB7p c\xE2u trong t\u1EADp hu\u1EA5n luy\u1EC7n, 408 trong t\u1EADp ki\u1EC3m ch\u1EE9ng v\xE0 1,725 trong t\u1EADp ki\u1EC3m \u0111\u1ECBnh)."),Q.forEach(s),ze=g(t),pt=r(t,"P",{});var ns=o(pt);ia=c(ns,"L\u1EC7nh n\xE0y t\u1EA3i xu\u1ED1ng v\xE0 l\u01B0u v\xE0o b\u1ED9 nh\u1EDB cache c\xE1c t\u1EADp d\u1EEF li\u1EC7u, m\u1EB7c \u0111\u1ECBnh l\u01B0u trong "),fs=r(ns,"EM",{});var xi=o(fs);ra=c(xi,"~/.cache/huggingface/datasets"),xi.forEach(s),oa=c(ns,". Nh\u1EDB l\u1EA1i t\u1EEB Ch\u01B0\u01A1ng 2 r\u1EB1ng b\u1EA1n c\xF3 th\u1EC3 t\xF9y ch\u1EC9nh th\u01B0 m\u1EE5c b\u1ED9 nh\u1EDB cache c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch \u0111\u1EB7t bi\u1EBFn m\xF4i tr\u01B0\u1EDDng "),_s=r(ns,"CODE",{});var ji=o(_s);pa=c(ji,"HF_HOME"),ji.forEach(s),ua=c(ns,"."),ns.forEach(s),Te=g(t),Ct=r(t,"P",{});var zc=o(Ct);ma=c(zc,"Ch\xFAng ta c\xF3 th\u1EC3 truy c\u1EADp t\u1EEBng c\u1EB7p c\xE2u trong \u0111\u1ED1i t\u01B0\u1EE3ng "),vs=r(zc,"CODE",{});var Ei=o(vs);ga=c(Ei,"raw_datasets"),Ei.forEach(s),da=c(zc," c\u1EE7a m\xECnh b\u1EB1ng c\xE1ch l\u1EADp ch\u1EC9 m\u1EE5c, gi\u1ED1ng nh\u01B0 v\u1EDBi t\u1EEB \u0111i\u1EC3n:"),zc.forEach(s),De=g(t),x(Qt.$$.fragment,t),Pe=g(t),x(tn.$$.fragment,t),Se=g(t),ut=r(t,"P",{});var ss=o(ut);fa=c(ss,"Ch\xFAng ta c\xF3 th\u1EC3 th\u1EA5y c\xE1c nh\xE3n v\u1ED1n l\xE0 s\u1ED1 nguy\xEAn, v\xEC v\u1EADy ch\xFAng ta kh\xF4ng ph\u1EA3i th\u1EF1c hi\u1EC7n b\u1EA5t k\u1EF3 b\u01B0\u1EDBc x\u1EED l\xFD tr\u01B0\u1EDBc n\xE0o \u1EDF \u0111\xF3. \u0110\u1EC3 bi\u1EBFt s\u1ED1 nguy\xEAn n\xE0o t\u01B0\u01A1ng \u1EE9ng v\u1EDBi nh\xE3n n\xE0o, ch\xFAng ta c\xF3 th\u1EC3 ki\u1EC3m tra "),bs=r(ss,"CODE",{});var wi=o(bs);_a=c(wi,"features"),wi.forEach(s),va=c(ss," c\u1EE7a "),ks=r(ss,"CODE",{});var qi=o(ks);ba=c(qi,"raw_train_dataset"),qi.forEach(s),ka=c(ss,". \u0110i\u1EC1u n\xE0y s\u1EBD cho ch\xFAng t\xF4i bi\u1EBFt lo\u1EA1i c\u1EE7a m\u1ED7i c\u1ED9t:"),ss.forEach(s),Oe=g(t),x(nn.$$.fragment,t),Ae=g(t),x(sn.$$.fragment,t),Ne=g(t),H=r(t,"P",{});var U=o(H);ya=c(U,"Ph\xEDa sau, "),ys=r(U,"CODE",{});var Ci=o(ys);$a=c(Ci,"label"),Ci.forEach(s),xa=c(U," thu\u1ED9c lo\u1EA1i "),$s=r(U,"CODE",{});var zi=o($s);ja=c(zi,"ClassLabel"),zi.forEach(s),Ea=c(U," v\xE0 \xE1nh x\u1EA1 c\xE1c s\u1ED1 nguy\xEAn th\xE0nh t\xEAn nh\xE3n \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF trong th\u01B0 m\u1EE5c "),xs=r(U,"EM",{});var Ti=o(xs);wa=c(Ti,"names"),Ti.forEach(s),qa=c(U,". "),js=r(U,"CODE",{});var Di=o(js);Ca=c(Di,"0"),Di.forEach(s),za=c(U," t\u01B0\u01A1ng \u1EE9ng v\u1EDBi "),Es=r(U,"CODE",{});var Pi=o(Es);Ta=c(Pi,"kh\xF4ng t\u01B0\u01A1ng \u0111\u01B0\u01A1ng"),Pi.forEach(s),Da=c(U,", v\xE0 "),ws=r(U,"CODE",{});var Si=o(ws);Pa=c(Si,"1"),Si.forEach(s),Sa=c(U," t\u01B0\u01A1ng \u1EE9ng v\u1EDBi "),qs=r(U,"CODE",{});var Oi=o(qs);Oa=c(Oi,"t\u01B0\u01A1ng \u0111\u01B0\u01A1ng"),Oi.forEach(s),Aa=c(U,"."),U.forEach(s),Me=g(t),x(zt.$$.fragment,t),Le=g(t),xt=r(t,"H3",{class:!0});var Tc=o(xt);Tt=r(Tc,"A",{id:!0,class:!0,href:!0});var Ai=o(Tt);Cs=r(Ai,"SPAN",{});var Ni=o(Cs);x(en.$$.fragment,Ni),Ni.forEach(s),Ai.forEach(s),Na=g(Tc),zs=r(Tc,"SPAN",{});var Mi=o(zs);Ma=c(Mi,"Ti\u1EC1n x\u1EED l\xFD m\u1ED9t b\u1ED9 d\u1EEF li\u1EC7u"),Mi.forEach(s),Tc.forEach(s),He=g(t),et.l(t),Mn=g(t),Dt=r(t,"P",{});var Dc=o(Dt);La=c(Dc,"\u0110\u1EC3 ti\u1EC1n x\u1EED l\xFD b\u1ED9 d\u1EEF li\u1EC7u, ch\xFAng ta c\u1EA7n chuy\u1EC3n v\u0103n b\u1EA3n th\xE0nh c\xE1c s\u1ED1 m\xE0 m\xF4 h\xECnh c\xF3 th\u1EC3 hi\u1EC3u \u0111\u01B0\u1EE3c. Nh\u01B0 b\u1EA1n \u0111\xE3 th\u1EA5y trong "),Ln=r(Dc,"A",{href:!0});var Li=o(Ln);Ha=c(Li,"ch\u01B0\u01A1ng tr\u01B0\u1EDBc"),Li.forEach(s),Fa=c(Dc,", \u0111i\u1EC1u n\xE0y \u0111\u01B0\u1EE3c th\u1EF1c hi\u1EC7n v\u1EDBi m\u1ED9t tokenizer. Ch\xFAng ta c\xF3 th\u1EC3 cung c\u1EA5p cho tokenizer m\u1ED9t c\xE2u ho\u1EB7c m\u1ED9t danh s\xE1ch c\xE1c c\xE2u, v\xEC v\u1EADy ch\xFAng ta c\xF3 th\u1EC3 tokenizer tr\u1EF1c ti\u1EBFp t\u1EA5t c\u1EA3 c\xE1c c\xE2u \u0111\u1EA7u ti\xEAn v\xE0 t\u1EA5t c\u1EA3 c\xE1c c\xE2u th\u1EE9 hai c\u1EE7a m\u1ED7i c\u1EB7p nh\u01B0 sau:"),Dc.forEach(s),Fe=g(t),x(cn.$$.fragment,t),Re=g(t),Hn=r(t,"P",{});var Hi=o(Hn);Ra=c(Hi,"Tuy nhi\xEAn, ch\xFAng ta kh\xF4ng th\u1EC3 ch\u1EC9 chuy\u1EC3n hai chu\u1ED7i v\xE0o m\xF4 h\xECnh v\xE0 nh\u1EADn \u0111\u01B0\u1EE3c d\u1EF1 \u0111o\xE1n li\u1EC7u hai c\xE2u c\xF3 ph\u1EA3i l\xE0 di\u1EC5n gi\u1EA3i hay kh\xF4ng. Ch\xFAng ta c\u1EA7n x\u1EED l\xFD hai chu\u1ED7i nh\u01B0 m\u1ED9t c\u1EB7p v\xE0 \xE1p d\u1EE5ng ti\u1EC1n x\u1EED l\xFD th\xEDch h\u1EE3p. May m\u1EAFn thay, tokenizer c\u0169ng c\xF3 th\u1EC3 nh\u1EADn m\u1ED9t c\u1EB7p chu\u1ED7i v\xE0 chu\u1EA9n b\u1ECB n\xF3 theo c\xE1ch m\xE0 m\xF4 h\xECnh BERT c\u1EE7a ta mong \u0111\u1EE3i:"),Hi.forEach(s),Be=g(t),x(an.$$.fragment,t),We=g(t),x(hn.$$.fragment,t),Ie=g(t),V=r(t,"P",{});var bt=o(V);Ba=c(bt,"Ch\xFAng ta \u0111\xE3 th\u1EA3o lu\u1EADn v\u1EC1 "),Ts=r(bt,"CODE",{});var Fi=o(Ts);Wa=c(Fi,"input_ids"),Fi.forEach(s),Ia=c(bt," v\xE0 "),Ds=r(bt,"CODE",{});var Ri=o(Ds);Ua=c(Ri,"attention_mask"),Ri.forEach(s),Ga=c(bt," trong "),Fn=r(bt,"A",{href:!0});var Bi=o(Fn);Va=c(Bi,"Ch\u01B0\u01A1ng 2"),Bi.forEach(s),Ka=c(bt,", nh\u01B0ng ch\xFAng ta t\u1EA1m d\u1EEBng \u0111\u1EC3 n\xF3i v\u1EC1 "),Ps=r(bt,"CODE",{});var Wi=o(Ps);Xa=c(Wi,"token_type_ids"),Wi.forEach(s),Ja=c(bt,". Trong v\xED d\u1EE5 n\xE0y, \u0111\xE2y l\xE0 ph\u1EA7n cho m\xF4 h\xECnh bi\u1EBFt ph\u1EA7n n\xE0o c\u1EE7a \u0111\u1EA7u v\xE0o l\xE0 c\xE2u \u0111\u1EA7u ti\xEAn v\xE0 ph\u1EA7n n\xE0o l\xE0 c\xE2u th\u1EE9 hai."),bt.forEach(s),Ue=g(t),x(Pt.$$.fragment,t),Ge=g(t),St=r(t,"P",{});var Pc=o(St);Ya=c(Pc,"N\u1EBFu ch\xFAng ta gi\u1EA3i m\xE3 c\xE1c ID b\xEAn trong "),Ss=r(Pc,"CODE",{});var Ii=o(Ss);Za=c(Ii,"input_ids"),Ii.forEach(s),Qa=c(Pc," tr\u1EDF l\u1EA1i c\xE1c t\u1EEB:"),Pc.forEach(s),Ve=g(t),x(ln.$$.fragment,t),Ke=g(t),Rn=r(t,"P",{});var Ui=o(Rn);th=c(Ui,"ta s\u1EBD nh\u1EADn \u0111\u01B0\u1EE3c:"),Ui.forEach(s),Xe=g(t),x(rn.$$.fragment,t),Je=g(t),mt=r(t,"P",{});var es=o(mt);nh=c(es,"C\xF3 th\u1EC3 th\u1EA5y m\xF4 h\xECnh k\xEC v\u1ECDng c\xE1c \u0111\u1EA7u v\xE0o c\xF3 d\u1EA1ng "),Os=r(es,"CODE",{});var Gi=o(Os);sh=c(Gi,"[CLS] c\xE2u1 [SEP] c\xE2u2 [SEP]"),Gi.forEach(s),eh=c(es," khi c\xF3 hai c\xE2u. C\u0103n ch\u1EC9nh \u0111i\u1EC1u n\xE0y v\u1EDBi "),As=r(es,"CODE",{});var Vi=o(As);ch=c(Vi,"token_type_ids"),Vi.forEach(s),ah=c(es," cho ta k\u1EBFt qu\u1EA3:"),es.forEach(s),Ye=g(t),x(on.$$.fragment,t),Ze=g(t),K=r(t,"P",{});var kt=o(K);hh=c(kt,"Nh\u01B0 b\u1EA1n c\xF3 th\u1EC3 th\u1EA5y, c\xE1c ph\u1EA7n c\u1EE7a \u0111\u1EA7u v\xE0o t\u01B0\u01A1ng \u1EE9ng v\u1EDBi "),Ns=r(kt,"CODE",{});var Ki=o(Ns);lh=c(Ki,"[CLS] c\xE2u1 [SEP]"),Ki.forEach(s),ih=c(kt," \u0111\u1EC1u c\xF3 lo\u1EA1i token ID l\xE0 "),Ms=r(kt,"CODE",{});var Xi=o(Ms);rh=c(Xi,"0"),Xi.forEach(s),oh=c(kt,", trong khi c\xE1c ph\u1EA7n kh\xE1c, t\u01B0\u01A1ng \u1EE9ng v\u1EDBi "),Ls=r(kt,"CODE",{});var Ji=o(Ls);ph=c(Ji,"c\xE2u2 [SEP]"),Ji.forEach(s),uh=c(kt,", t\u1EA5t c\u1EA3 \u0111\u1EC1u c\xF3 lo\u1EA1i token ID l\xE0 "),Hs=r(kt,"CODE",{});var Yi=o(Hs);mh=c(Yi,"1"),Yi.forEach(s),gh=c(kt,"."),kt.forEach(s),Qe=g(t),Ot=r(t,"P",{});var Sc=o(Ot);dh=c(Sc,"L\u01B0u \xFD r\u1EB1ng n\u1EBFu b\u1EA1n ch\u1ECDn m\u1ED9t checkpoint kh\xE1c, b\u1EA1n s\u1EBD kh\xF4ng nh\u1EA5t thi\u1EBFt ph\u1EA3i c\xF3 "),Fs=r(Sc,"CODE",{});var Zi=o(Fs);fh=c(Zi,"token_type_ids"),Zi.forEach(s),_h=c(Sc," trong \u0111\u1EA7u v\xE0o \u0111\u01B0\u1EE3c tokenize c\u1EE7a m\xECnh (v\xED d\u1EE5: ch\xFAng s\u1EBD kh\xF4ng \u0111\u01B0\u1EE3c tr\u1EA3 l\u1EA1i n\u1EBFu b\u1EA1n s\u1EED d\u1EE5ng m\xF4 h\xECnh DistilBERT). Ch\xFAng ch\u1EC9 \u0111\u01B0\u1EE3c tr\u1EA3 l\u1EA1i khi m\xF4 h\xECnh bi\u1EBFt ph\u1EA3i l\xE0m g\xEC v\u1EDBi ch\xFAng, b\u1EDFi v\xEC n\xF3 \u0111\xE3 nh\xECn th\u1EA5y ch\xFAng trong qu\xE1 tr\xECnh hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc."),Sc.forEach(s),tc=g(t),gt=r(t,"P",{});var cs=o(gt);vh=c(cs,"\u1EDE \u0111\xE2y, BERT \u0111\u01B0\u1EE3c hu\u1EA5n luy\u1EC7n tr\u01B0\u1EDBc v\u1EDBi c\xE1c token ID v\xE0 tr\xEAn \u0111\u1EA7u m\u1EE5c ti\xEAu m\xF4 h\xECnh ng\xF4n ng\u1EEF \u0111\u01B0\u1EE3c che m\xE0 ch\xFAng ta \u0111\xE3 \u0111\u1EC1 c\u1EADp trong "),Bn=r(cs,"A",{href:!0});var Qi=o(Bn);bh=c(Qi,"Ch\u01B0\u01A1ng 1"),Qi.forEach(s),kh=c(cs,", n\xF3 c\xF3 m\u1ED9t m\u1EE5c ti\xEAu b\u1ED5 sung \u0111\u01B0\u1EE3c g\u1ECDi l\xE0 "),Rs=r(cs,"EM",{});var tr=o(Rs);yh=c(tr,"d\u1EF1 \u0111o\xE1n c\xE2u ti\u1EBFp theo"),tr.forEach(s),$h=c(cs,". M\u1EE5c ti\xEAu c\u1EE7a t\xE1c v\u1EE5 n\xE0y l\xE0 m\xF4 h\xECnh h\xF3a m\u1ED1i quan h\u1EC7 gi\u1EEFa c\xE1c c\u1EB7p c\xE2u."),cs.forEach(s),nc=g(t),Wn=r(t,"P",{});var nr=o(Wn);xh=c(nr,"V\u1EDBi d\u1EF1 \u0111o\xE1n c\xE2u ti\u1EBFp theo, m\xF4 h\xECnh \u0111\u01B0\u1EE3c cung c\u1EA5p c\xE1c c\u1EB7p c\xE2u (v\u1EDBi c\xE1c token \u0111\u01B0\u1EE3c che ng\u1EABu nhi\xEAn) v\xE0 \u0111\u01B0\u1EE3c y\xEAu c\u1EA7u d\u1EF1 \u0111o\xE1n li\u1EC7u c\xE2u th\u1EE9 hai c\xF3 theo sau c\xE2u \u0111\u1EA7u ti\xEAn hay kh\xF4ng. \u0110\u1EC3 l\xE0m cho t\xE1c v\u1EE5 tr\u1EDF n\xEAn kh\xF4ng t\u1EA7m th\u01B0\u1EDDng, m\u1ED9t n\u1EEDa l\xE0 c\xE1c c\xE2u ti\u1EBFp n\u1ED1i nhau trong t\xE0i li\u1EC7u g\u1ED1c m\xE0 ch\xFAng \u0111\u01B0\u1EE3c tr\xEDch xu\u1EA5t, v\xE0 n\u1EEDa c\xF2n l\u1EA1i l\xE0 hai c\xE2u \u0111\u1EBFn t\u1EEB hai t\xE0i li\u1EC7u kh\xE1c nhau."),nr.forEach(s),sc=g(t),At=r(t,"P",{});var Oc=o(At);jh=c(Oc,"N\xF3i chung, b\u1EA1n kh\xF4ng c\u1EA7n ph\u1EA3i lo l\u1EAFng v\u1EC1 vi\u1EC7c c\xF3 hay kh\xF4ng c\xF3 "),Bs=r(Oc,"CODE",{});var sr=o(Bs);Eh=c(sr,"token_type_ids"),sr.forEach(s),wh=c(Oc," trong \u0111\u1EA7u v\xE0o \u0111\u01B0\u1EE3c tokenize c\u1EE7a m\xECnh: mi\u1EC5n l\xE0 b\u1EA1n s\u1EED d\u1EE5ng c\xF9ng m\u1ED9t checkpoint cho tr\xECnh tokenize v\xE0 m\xF4 h\xECnh, m\u1ECDi th\u1EE9 s\u1EBD \u1ED5n v\xEC tr\xECnh tokenize nh\u1EADn bi\u1EBFt c\u1EA7n cung c\u1EA5p nh\u1EEFng g\xEC v\u1EDBi m\xF4 h\xECnh c\u1EE7a n\xF3."),Oc.forEach(s),ec=g(t),dt=r(t,"P",{});var as=o(dt);qh=c(as,"B\xE2y gi\u1EDD ch\xFAng ta \u0111\xE3 th\u1EA5y c\xE1ch tr\xECnh tokenize c\u1EE7a ch\xFAng ta c\xF3 th\u1EC3 x\u1EED l\xFD m\u1ED9t c\u1EB7p c\xE2u, ch\xFAng ta c\xF3 th\u1EC3 s\u1EED d\u1EE5ng n\xF3 \u0111\u1EC3 m\xE3 h\xF3a to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh: gi\u1ED1ng nh\u01B0 trong "),In=r(as,"A",{href:!0});var er=o(In);Ch=c(er,"ch\u01B0\u01A1ng tr\u01B0\u1EDBc"),er.forEach(s),zh=c(as,", ch\xFAng ta c\xF3 th\u1EC3 cung c\u1EA5p cho tr\xECnh tokenize danh s\xE1ch c\xE1c c\u1EB7p b\u1EB1ng c\xE1ch \u0111\u01B0a cho n\xF3 danh s\xE1ch c\xE1c c\xE2u \u0111\u1EA7u ti\xEAn, sau \u0111\xF3 l\xE0 danh s\xE1ch c\xE1c c\xE2u th\u1EE9 hai. \u0110i\u1EC1u n\xE0y c\u0169ng t\u01B0\u01A1ng th\xEDch v\u1EDBi c\xE1c t\xF9y ch\u1ECDn \u0111\u1EC7m v\xE0 c\u1EAFt b\u1EDBt m\xE0 ch\xFAng ta \u0111\xE3 th\u1EA5y trong "),Un=r(as,"A",{href:!0});var cr=o(Un);Th=c(cr,"Ch\u01B0\u01A1ng 2"),cr.forEach(s),Dh=c(as,". V\xEC v\u1EADy, m\u1ED9t c\xE1ch \u0111\u1EC3 ti\u1EC1n x\u1EED l\xFD tr\u01B0\u1EDBc t\u1EADp d\u1EEF li\u1EC7u hu\u1EA5n luy\u1EC7n l\xE0:"),as.forEach(s),cc=g(t),x(pn.$$.fragment,t),ac=g(t),X=r(t,"P",{});var yt=o(X);Ph=c(yt,"\u0110i\u1EC1u n\xE0y ho\u1EA1t \u0111\u1ED9ng t\u1ED1t, nh\u01B0ng n\xF3 c\xF3 nh\u01B0\u1EE3c \u0111i\u1EC3m l\xE0 tr\u1EA3 v\u1EC1 t\u1EEB \u0111i\u1EC3n (v\u1EDBi c\xE1c kh\xF3a c\u1EE7a ch\xFAng t\xF4i, "),Ws=r(yt,"CODE",{});var ar=o(Ws);Sh=c(ar,"input_ids"),ar.forEach(s),Oh=c(yt,", "),Is=r(yt,"CODE",{});var hr=o(Is);Ah=c(hr,"attention_mask"),hr.forEach(s),Nh=c(yt," v\xE0 "),Us=r(yt,"CODE",{});var lr=o(Us);Mh=c(lr,"token_type_ids"),lr.forEach(s),Lh=c(yt,", v\xE0 c\xE1c gi\xE1 tr\u1ECB l\xE0 danh s\xE1ch c\xE1c danh s\xE1ch). N\xF3 c\u0169ng s\u1EBD ch\u1EC9 ho\u1EA1t \u0111\u1ED9ng n\u1EBFu b\u1EA1n c\xF3 \u0111\u1EE7 RAM \u0111\u1EC3 l\u01B0u tr\u1EEF to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a m\xECnh trong qu\xE1 tr\xECnh tokenize (trong khi c\xE1c t\u1EADp d\u1EEF li\u1EC7u t\u1EEB th\u01B0 vi\u1EC7n \u{1F917} Datasets l\xE0 c\xE1c t\u1EC7p "),un=r(yt,"A",{href:!0,rel:!0});var ir=o(un);Hh=c(ir,"Apache Arrow"),ir.forEach(s),Fh=c(yt," \u0111\u01B0\u1EE3c l\u01B0u tr\u1EEF tr\xEAn \u0111\u0129a, v\xEC v\u1EADy b\u1EA1n ch\u1EC9 gi\u1EEF c\xE1c m\u1EABu b\u1EA1n y\xEAu c\u1EA7u \u0111\xE3 t\u1EA3i trong b\u1ED9 nh\u1EDB)."),yt.forEach(s),hc=g(t),ft=r(t,"P",{});var hs=o(ft);Rh=c(hs,"\u0110\u1EC3 gi\u1EEF d\u1EEF li\u1EC7u d\u01B0\u1EDBi d\u1EA1ng t\u1EADp d\u1EEF li\u1EC7u, ch\xFAng ta s\u1EBD s\u1EED d\u1EE5ng ph\u01B0\u01A1ng th\u1EE9c "),mn=r(hs,"A",{href:!0,rel:!0});var rr=o(mn);Gs=r(rr,"CODE",{});var or=o(Gs);Bh=c(or,"Dataset.map()"),or.forEach(s),rr.forEach(s),Wh=c(hs,". \u0110i\u1EC1u n\xE0y c\u0169ng cho ph\xE9p ch\xFAng ta linh ho\u1EA1t h\u01A1n, n\u1EBFu ch\xFAng ta c\u1EA7n th\u1EF1c hi\u1EC7n nhi\u1EC1u ti\u1EC1n x\u1EED l\xFD h\u01A1n l\xE0 ch\u1EC9 tokenize. Ph\u01B0\u01A1ng th\u1EE9c "),Vs=r(hs,"CODE",{});var pr=o(Vs);Ih=c(pr,"map()"),pr.forEach(s),Uh=c(hs," ho\u1EA1t \u0111\u1ED9ng b\u1EB1ng c\xE1ch \xE1p d\u1EE5ng m\u1ED9t h\xE0m tr\xEAn m\u1ED7i ph\u1EA7n t\u1EED c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u, v\xEC v\u1EADy h\xE3y x\xE1c \u0111\u1ECBnh m\u1ED9t h\xE0m tokenize c\xE1c \u0111\u1EA7u v\xE0o c\u1EE7a ch\xFAng ta:"),hs.forEach(s),lc=g(t),x(gn.$$.fragment,t),ic=g(t),A=r(t,"P",{});var F=o(A);Gh=c(F,"H\xE0m n\xE0y l\u1EA5y m\u1ED9t t\u1EEB \u0111i\u1EC3n (gi\u1ED1ng nh\u01B0 c\xE1c m\u1EE5c trong t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng ta) v\xE0 tr\u1EA3 v\u1EC1 m\u1ED9t t\u1EEB \u0111i\u1EC3n m\u1EDBi v\u1EDBi c\xE1c kh\xF3a "),Ks=r(F,"CODE",{});var ur=o(Ks);Vh=c(ur,"input_ids"),ur.forEach(s),Kh=c(F,", "),Xs=r(F,"CODE",{});var mr=o(Xs);Xh=c(mr,"attention_mask"),mr.forEach(s),Jh=c(F," v\xE0 "),Js=r(F,"CODE",{});var gr=o(Js);Yh=c(gr,"token_type_ids"),gr.forEach(s),Zh=c(F,". L\u01B0u \xFD r\u1EB1ng n\xF3 c\u0169ng ho\u1EA1t \u0111\u1ED9ng n\u1EBFu t\u1EEB \u0111i\u1EC3n "),Ys=r(F,"CODE",{});var dr=o(Ys);Qh=c(dr,"example"),dr.forEach(s),tl=c(F," ch\u1EE9a m\u1ED9t s\u1ED1 m\u1EABu (m\u1ED7i kh\xF3a l\xE0 m\u1ED9t danh s\xE1ch c\xE1c c\xE2u) v\xEC "),Zs=r(F,"CODE",{});var fr=o(Zs);nl=c(fr,"tokenizer"),fr.forEach(s),sl=c(F," ho\u1EA1t \u0111\u1ED9ng tr\xEAn danh s\xE1ch c\xE1c c\u1EB7p c\xE2u, nh\u01B0 \u0111\xE3 th\u1EA5y tr\u01B0\u1EDBc \u0111\xE2y. \u0110i\u1EC1u n\xE0y s\u1EBD cho ph\xE9p ch\xFAng ta s\u1EED d\u1EE5ng t\xF9y ch\u1ECDn "),Qs=r(F,"CODE",{});var _r=o(Qs);el=c(_r,"batch = True"),_r.forEach(s),cl=c(F," trong l\u1EC7nh g\u1ECDi "),te=r(F,"CODE",{});var vr=o(te);al=c(vr,"map()"),vr.forEach(s),hl=c(F,", t\u1EEB \u0111\xF3 s\u1EBD t\u0103ng t\u1ED1c \u0111\xE1ng k\u1EC3 qu\xE1 tr\xECnh tokenize. "),ne=r(F,"CODE",{});var br=o(ne);ll=c(br,"Tokenizer"),br.forEach(s),il=c(F," \u0111\u01B0\u1EE3c h\u1ED7 tr\u1EE3 b\u1EDFi m\u1ED9t tokenizer \u0111\u01B0\u1EE3c vi\u1EBFt b\u1EB1ng Rust t\u1EEB th\u01B0 vi\u1EC7n "),dn=r(F,"A",{href:!0,rel:!0});var kr=o(dn);rl=c(kr,"\u{1F917} Tokenizer"),kr.forEach(s),ol=c(F,". Tokenizer n\xE0y c\xF3 th\u1EC3 r\u1EA5t nhanh, nh\u01B0ng ch\u1EC9 khi ch\xFAng ta cung c\u1EA5p nhi\u1EC1u \u0111\u1EA7u v\xE0o c\xF9ng m\u1ED9t l\xFAc."),F.forEach(s),rc=g(t),Nt=r(t,"P",{});var Ac=o(Nt);pl=c(Ac,"L\u01B0u \xFD r\u1EB1ng ch\xFAng ta \u0111\xE3 \u0111\u1EC3 t\u1EA1m b\u1ECF qua tham s\u1ED1 "),se=r(Ac,"CODE",{});var yr=o(se);ul=c(yr,"padding"),yr.forEach(s),ml=c(Ac," trong h\xE0m tokenize c\u1EE7a ta. \u0110i\u1EC1u n\xE0y l\xE0 do vi\u1EC7c \u0111\u1EC7m t\u1EA5t c\u1EA3 c\xE1c m\u1EABu \u0111\u1EBFn chi\u1EC1u d\xE0i t\u1ED1i \u0111a kh\xF4ng hi\u1EC7u qu\u1EA3: t\u1ED1t h\u01A1n n\xEAn \u0111\u1EC7m c\xE1c m\u1EABu khi ch\xFAng ta \u0111ang t\u1EA1o m\u1ED9t l\xF4, v\xEC khi \u0111\xF3 ch\xFAng ta ch\u1EC9 c\u1EA7n \u0111\u1EC7m \u0111\u1EBFn chi\u1EC1u d\xE0i t\u1ED1i \u0111a trong l\xF4 \u0111\xF3 ch\u1EE9 kh\xF4ng ph\u1EA3i chi\u1EC1u d\xE0i t\u1ED1i \u0111a trong to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u. \u0110i\u1EC1u n\xE0y c\xF3 th\u1EC3 ti\u1EBFt ki\u1EC7m r\u1EA5t nhi\u1EC1u th\u1EDDi gian v\xE0 c\xF4ng su\u1EA5t x\u1EED l\xFD khi c\xE1c \u0111\u1EA7u v\xE0o c\xF3 \u0111\u1ED9 d\xE0i r\u1EA5t thay \u0111\u1ED5i!"),Ac.forEach(s),oc=g(t),_t=r(t,"P",{});var ls=o(_t);gl=c(ls,"\u0110\xE2y l\xE0 c\xE1ch ch\xFAng ta \xE1p d\u1EE5ng ch\u1EE9c n\u0103ng m\xE3 h\xF3a tr\xEAn t\u1EA5t c\u1EA3 c\xE1c t\u1EADp d\u1EEF li\u1EC7u c\u1EE7a ta c\xF9ng m\u1ED9t l\xFAc. Ch\xFAng ta \u0111ang s\u1EED d\u1EE5ng "),ee=r(ls,"CODE",{});var $r=o(ee);dl=c($r,"batch = True"),$r.forEach(s),fl=c(ls," trong l\u1EC7nh g\u1ECDi t\u1EDBi "),ce=r(ls,"CODE",{});var xr=o(ce);_l=c(xr,"map"),xr.forEach(s),vl=c(ls,", v\xEC v\u1EADy, h\xE0m \u0111\u01B0\u1EE3c \xE1p d\u1EE5ng cho nhi\u1EC1u ph\u1EA7n t\u1EED c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u c\xF9ng m\u1ED9t l\xFAc, ch\u1EE9 kh\xF4ng ph\u1EA3i tr\xEAn t\u1EEBng ph\u1EA7n t\u1EED ri\xEAng bi\u1EC7t. \u0110i\u1EC1u n\xE0y cho ph\xE9p vi\u1EC7c ti\u1EC1n x\u1EED l\xFD nhanh h\u01A1n."),ls.forEach(s),pc=g(t),x(fn.$$.fragment,t),uc=g(t),Gn=r(t,"P",{});var jr=o(Gn);bl=c(jr,"C\xE1ch th\u01B0 vi\u1EC7n \u{1F917} Datasets \xE1p d\u1EE5ng b\u01B0\u1EDBc x\u1EED l\xFD n\xE0y l\xE0 th\xEAm c\xE1c tr\u01B0\u1EDDng m\u1EDBi v\xE0o b\u1ED9 d\u1EEF li\u1EC7u, m\u1ED7i kh\xF3a trong t\u1EEB \u0111i\u1EC3n \u0111\u01B0\u1EE3c tr\u1EA3 v\u1EC1 b\u1EDFi h\xE0m ti\u1EC1n x\u1EED l\xFD m\u1ED9t tr\u01B0\u1EDDng:"),jr.forEach(s),mc=g(t),x(_n.$$.fragment,t),gc=g(t),vt=r(t,"P",{});var is=o(vt);kl=c(is,"B\u1EA1n th\u1EADm ch\xED c\xF3 th\u1EC3 s\u1EED d\u1EE5ng \u0111a x\u1EED l\xFD khi \xE1p d\u1EE5ng ch\u1EE9c n\u0103ng ti\u1EC1n x\u1EED l\xFD c\u1EE7a m\xECnh v\u1EDBi "),ae=r(is,"CODE",{});var Er=o(ae);yl=c(Er,"map()"),Er.forEach(s),$l=c(is," b\u1EB1ng c\xE1ch truy\u1EC1n tham s\u1ED1 "),he=r(is,"CODE",{});var wr=o(he);xl=c(wr,"num_proc"),wr.forEach(s),jl=c(is,". Ch\xFAng ta kh\xF4ng l\xE0m \u0111i\u1EC1u n\xE0y \u1EDF \u0111\xE2y v\xEC th\u01B0 vi\u1EC7n \u{1F917} Tokenizers \u0111\xE3 s\u1EED d\u1EE5ng nhi\u1EC1u chu\u1ED7i \u0111\u1EC3 tokenize \xE1c m\u1EABu c\u1EE7a nhanh h\u01A1n, nh\u01B0ng n\u1EBFu b\u1EA1n kh\xF4ng s\u1EED d\u1EE5ng tr\xECnh tokenize nhanh \u0111\u01B0\u1EE3c th\u01B0 vi\u1EC7n n\xE0y h\u1ED7 tr\u1EE3, b\u01B0\u1EDBc tr\xEAn c\xF3 th\u1EC3 t\u0103ng t\u1ED1c qu\xE1 tr\xECnh x\u1EED l\xFD tr\u01B0\u1EDBc c\u1EE7a b\u1EA1n."),is.forEach(s),dc=g(t),G=r(t,"P",{});var it=o(G);le=r(it,"CODE",{});var qr=o(le);El=c(qr,"Tokenize_function"),qr.forEach(s),wl=c(it," c\u1EE7a ch\xFAng ta tr\u1EA3 v\u1EC1 m\u1ED9t t\u1EEB \u0111i\u1EC3n v\u1EDBi c\xE1c kh\xF3a "),ie=r(it,"CODE",{});var Cr=o(ie);ql=c(Cr,"input_ids"),Cr.forEach(s),Cl=c(it,", "),re=r(it,"CODE",{});var zr=o(re);zl=c(zr,"attention_mask"),zr.forEach(s),Tl=c(it," v\xE0 "),oe=r(it,"CODE",{});var Tr=o(oe);Dl=c(Tr,"token_type_ids"),Tr.forEach(s),Pl=c(it,", v\xEC v\u1EADy ba tr\u01B0\u1EDDng \u0111\xF3 \u0111\u01B0\u1EE3c th\xEAm v\xE0o t\u1EA5t c\u1EA3 c\xE1c ph\u1EA7n b\u1ED9 d\u1EEF li\u1EC7u c\u1EE7a ch\xFAng ta. L\u01B0u \xFD r\u1EB1ng ta c\u0169ng c\xF3 th\u1EC3 \u0111\xE3 thay \u0111\u1ED5i c\xE1c tr\u01B0\u1EDDng hi\u1EC7n c\xF3 n\u1EBFu h\xE0m ti\u1EC1n x\u1EED l\xFD tr\u1EA3 v\u1EC1 m\u1ED9t gi\xE1 tr\u1ECB m\u1EDBi cho m\u1ED9t kh\xF3a hi\u1EC7n c\xF3 trong t\u1EADp d\u1EEF li\u1EC7u m\xE0 ta \u0111\xE3 \xE1p d\u1EE5ng "),pe=r(it,"CODE",{});var Dr=o(pe);Sl=c(Dr,"map()"),Dr.forEach(s),Ol=c(it,"."),it.forEach(s),fc=g(t),Mt=r(t,"P",{});var Nc=o(Mt);Al=c(Nc,"\u0110i\u1EC1u cu\u1ED1i c\xF9ng ch\xFAng ta s\u1EBD c\u1EA7n l\xE0m l\xE0 \u0111\u1EC7m t\u1EA5t c\u1EA3 c\xE1c v\xED d\u1EE5 \u0111\u1EC3 c\xF3 \u0111\u1ED9 d\xE0i c\u1EE7a ph\u1EA7n t\u1EED d\xE0i nh\u1EA5t khi ch\xFAng t\xF4i g\u1ED9p c\xE1c ph\u1EA7n t\u1EED l\u1EA1i v\u1EDBi nhau - m\u1ED9t k\u1EF9 thu\u1EADt m\xE0 ch\xFAng t\xF4i g\u1ECDi l\xE0 "),ue=r(Nc,"EM",{});var Pr=o(ue);Nl=c(Pr,"\u0111\u1EC7m \u0111\u1ED9ng"),Pr.forEach(s),Ml=c(Nc,"."),Nc.forEach(s),_c=g(t),jt=r(t,"H3",{class:!0});var Mc=o(jt);Lt=r(Mc,"A",{id:!0,class:!0,href:!0});var Sr=o(Lt);me=r(Sr,"SPAN",{});var Or=o(me);x(vn.$$.fragment,Or),Or.forEach(s),Sr.forEach(s),Ll=g(Mc),ge=r(Mc,"SPAN",{});var Ar=o(ge);Hl=c(Ar,"Ph\u1EA7n \u0111\u1EC7m \u0111\u1ED9ng"),Ar.forEach(s),Mc.forEach(s),vc=g(t),x(bn.$$.fragment,t),bc=g(t),Et.l(t),Vn=g(t),Ht=r(t,"P",{});var Lc=o(Ht);Fl=c(Lc,"\u0110\u1EC3 th\u1EF1c hi\u1EC7n \u0111i\u1EC1u n\xE0y trong th\u1EF1c t\u1EBF, ch\xFAng ta ph\u1EA3i \u0111\u1ECBnh ngh\u0129a m\u1ED9t h\xE0m \u0111\u1ED1i chi\u1EBFu s\u1EBD \xE1p d\u1EE5ng \u0111\xFAng s\u1ED1 l\u01B0\u1EE3ng \u0111\u1EC7m cho c\xE1c m\u1EE5c c\u1EE7a t\u1EADp d\u1EEF li\u1EC7u m\xE0 ch\xFAng ta mu\u1ED1n g\u1ED9p h\xE0ng lo\u1EA1t l\u1EA1i v\u1EDBi nhau. May m\u1EAFn thay, th\u01B0 vi\u1EC7n \u{1F917} Transformers cung c\u1EA5p cho ch\xFAng ta m\u1ED9t ch\u1EE9c n\u0103ng nh\u01B0 v\u1EADy th\xF4ng qua "),de=r(Lc,"CODE",{});var Nr=o(de);Rl=c(Nr,"DataCollatorWithPadding"),Nr.forEach(s),Bl=c(Lc,". C\u1EA7n c\xF3 tr\xECnh tokenize khi b\u1EA1n kh\u1EDFi t\u1EA1o n\xF3 (\u0111\u1EC3 bi\u1EBFt c\u1EA7n s\u1EED d\u1EE5ng token \u0111\u1EC7m n\xE0o v\xE0 li\u1EC7u m\xF4 h\xECnh mong \u0111\u1EE3i \u0111\u1EC7m \u1EDF b\xEAn tr\xE1i hay b\xEAn ph\u1EA3i c\u1EE7a c\xE1c \u0111\u1EA7u v\xE0o) v\xE0 s\u1EBD th\u1EF1c hi\u1EC7n m\u1ECDi th\u1EE9 b\u1EA1n c\u1EA7n:"),Lc.forEach(s),kc=g(t),at.l(t),Kn=g(t),Z=r(t,"P",{});var Ut=o(Z);Wl=c(Ut,"\u0110\u1EC3 ki\u1EC3m tra m\xF3n m\u1EDBi n\xE0y, ch\xFAng ta h\xE3y l\u1EA5y m\u1ED9t v\xE0i m\u1EABu t\u1EEB t\u1EADp hu\u1EA5n luy\u1EC7n m\xE0 ch\xFAng ta mu\u1ED1n gh\xE9p l\u1EA1i v\u1EDBi nhau. \u1EDE \u0111\xE2y, ch\xFAng ta x\xF3a c\xE1c c\u1ED9t "),fe=r(Ut,"CODE",{});var Mr=o(fe);Il=c(Mr,"idx"),Mr.forEach(s),Ul=c(Ut,", "),_e=r(Ut,"CODE",{});var Lr=o(_e);Gl=c(Lr,"sentence1"),Lr.forEach(s),Vl=c(Ut,", v\xE0 "),ve=r(Ut,"CODE",{});var Hr=o(ve);Kl=c(Hr,"sentence2"),Hr.forEach(s),Xl=c(Ut," v\xEC ch\xFAng kh\xF4ng c\u1EA7n thi\u1EBFt v\xE0 ch\u1EE9a c\xE1c chu\u1ED7i (v\xE0 ch\xFAng ta kh\xF4ng th\u1EC3 t\u1EA1o tensor b\u1EB1ng chu\u1ED7i) v\xE0 xem \u0111\u1ED9 d\xE0i c\u1EE7a m\u1ED7i m\u1EE5c trong l\xF4:"),Ut.forEach(s),yc=g(t),x(kn.$$.fragment,t),$c=g(t),x(yn.$$.fragment,t),xc=g(t),Ft=r(t,"P",{});var Hc=o(Ft);Jl=c(Hc,"Kh\xF4ng c\xF3 g\xEC ng\u1EA1c nhi\xEAn, ta nh\u1EADn \u0111\u01B0\u1EE3c c\xE1c m\u1EABu c\xF3 \u0111\u1ED9 d\xE0i kh\xE1c nhau, t\u1EEB 32 \u0111\u1EBFn 67. \u0110\u1EC7m \u0111\u1ED9ng c\xF3 ngh\u0129a l\xE0 t\u1EA5t c\u1EA3 c\xE1c m\u1EABu trong l\xF4 n\xE0y ph\u1EA3i \u0111\u01B0\u1EE3c \u0111\u1EC7m \u0111\u1EBFn chi\u1EC1u d\xE0i 67, chi\u1EC1u d\xE0i t\u1ED1i \u0111a b\xEAn trong l\xF4. N\u1EBFu kh\xF4ng c\xF3 \u0111\u1EC7m \u0111\u1ED9ng, t\u1EA5t c\u1EA3 c\xE1c m\u1EABu s\u1EBD ph\u1EA3i \u0111\u01B0\u1EE3c \u0111\u1EC7m \u0111\u1EBFn \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a trong to\xE0n b\u1ED9 t\u1EADp d\u1EEF li\u1EC7u ho\u1EB7c \u0111\u1ED9 d\xE0i t\u1ED1i \u0111a m\xE0 m\xF4 h\xECnh c\xF3 th\u1EC3 ch\u1EA5p nh\u1EADn. H\xE3y ki\u1EC3m tra k\u1EF9 xem "),be=r(Hc,"CODE",{});var Fr=o(be);Yl=c(Fr,"data_collator"),Fr.forEach(s),Zl=c(Hc," c\u1EE7a ch\xFAng ta c\xF3 t\u1EF1 \u0111\u1ED9ng \u0111\u1EC7m l\xF4 \u0111\xFAng c\xE1ch hay kh\xF4ng:"),Hc.forEach(s),jc=g(t),x($n.$$.fragment,t),Ec=g(t),lt.l(t),Xn=g(t),x(Rt.$$.fragment,t),wc=g(t),R&&R.l(t),Jn=Rr(),this.h()},h(){q(a,"name","hf:doc:metadata"),q(a,"content",JSON.stringify(go)),q(d,"id","x-l-d-liu"),q(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(d,"href","#x-l-d-liu"),q(y,"class","relative group"),q(ot,"href","https://www.aclweb.org/anthology/I05-5002.pdf"),q(ot,"rel","nofollow"),q(qt,"id","ti-b-d-liu-t-hub"),q(qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(qt,"href","#ti-b-d-liu-t-hub"),q($t,"class","relative group"),q(Kt,"href","https://huggingface.co/datasets"),q(Kt,"rel","nofollow"),q(Xt,"href","https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub"),q(Xt,"rel","nofollow"),q(Jt,"href","https://gluebenchmark.com/"),q(Jt,"rel","nofollow"),q(Tt,"id","tin-x-l-mt-b-d-liu"),q(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Tt,"href","#tin-x-l-mt-b-d-liu"),q(xt,"class","relative group"),q(Ln,"href","/course/chapter2"),q(Fn,"href","/course/chapter2"),q(Bn,"href","/course/chapter1"),q(In,"href","/course/chapter2"),q(Un,"href","/course/chapter2"),q(un,"href","https://arrow.apache.org/"),q(un,"rel","nofollow"),q(mn,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),q(mn,"rel","nofollow"),q(dn,"href","https://github.com/huggingface/tokenizers"),q(dn,"rel","nofollow"),q(Lt,"id","phn-m-ng"),q(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),q(Lt,"href","#phn-m-ng"),q(jt,"class","relative group")},m(t,l){n(document.head,a),p(t,u,l),j(h,t,l),p(t,_,l),p(t,y,l),n(y,d),n(d,f),j(C,f,null),n(y,k),n(y,T),n(T,M),p(t,N,l),xn[z].m(t,l),p(t,B,l),jn[S].m(t,l),p(t,J,l),p(t,rt,l),n(rt,wt),p(t,D,l),p(t,O,l),n(O,Pn),n(O,ot),n(ot,Sn),n(O,On),p(t,Gt,l),p(t,$t,l),n($t,qt),n(qt,rs),j(Vt,rs,null),n($t,Rc),n($t,os),n(os,Bc),p(t,je,l),En[tt].m(t,l),p(t,An,l),p(t,Y,l),n(Y,Wc),n(Y,Kt),n(Kt,Ic),n(Y,Uc),n(Y,Xt),n(Xt,Gc),n(Y,Vc),n(Y,Jt),n(Jt,Kc),n(Y,Xc),p(t,Ee,l),p(t,Nn,l),n(Nn,Jc),p(t,we,l),j(Yt,t,l),p(t,qe,l),j(Zt,t,l),p(t,Ce,l),p(t,I,l),n(I,Yc),n(I,ps),n(ps,Zc),n(I,Qc),n(I,us),n(us,ta),n(I,na),n(I,ms),n(ms,sa),n(I,ea),n(I,gs),n(gs,ca),n(I,aa),n(I,ds),n(ds,ha),n(I,la),p(t,ze,l),p(t,pt,l),n(pt,ia),n(pt,fs),n(fs,ra),n(pt,oa),n(pt,_s),n(_s,pa),n(pt,ua),p(t,Te,l),p(t,Ct,l),n(Ct,ma),n(Ct,vs),n(vs,ga),n(Ct,da),p(t,De,l),j(Qt,t,l),p(t,Pe,l),j(tn,t,l),p(t,Se,l),p(t,ut,l),n(ut,fa),n(ut,bs),n(bs,_a),n(ut,va),n(ut,ks),n(ks,ba),n(ut,ka),p(t,Oe,l),j(nn,t,l),p(t,Ae,l),j(sn,t,l),p(t,Ne,l),p(t,H,l),n(H,ya),n(H,ys),n(ys,$a),n(H,xa),n(H,$s),n($s,ja),n(H,Ea),n(H,xs),n(xs,wa),n(H,qa),n(H,js),n(js,Ca),n(H,za),n(H,Es),n(Es,Ta),n(H,Da),n(H,ws),n(ws,Pa),n(H,Sa),n(H,qs),n(qs,Oa),n(H,Aa),p(t,Me,l),j(zt,t,l),p(t,Le,l),p(t,xt,l),n(xt,Tt),n(Tt,Cs),j(en,Cs,null),n(xt,Na),n(xt,zs),n(zs,Ma),p(t,He,l),wn[st].m(t,l),p(t,Mn,l),p(t,Dt,l),n(Dt,La),n(Dt,Ln),n(Ln,Ha),n(Dt,Fa),p(t,Fe,l),j(cn,t,l),p(t,Re,l),p(t,Hn,l),n(Hn,Ra),p(t,Be,l),j(an,t,l),p(t,We,l),j(hn,t,l),p(t,Ie,l),p(t,V,l),n(V,Ba),n(V,Ts),n(Ts,Wa),n(V,Ia),n(V,Ds),n(Ds,Ua),n(V,Ga),n(V,Fn),n(Fn,Va),n(V,Ka),n(V,Ps),n(Ps,Xa),n(V,Ja),p(t,Ue,l),j(Pt,t,l),p(t,Ge,l),p(t,St,l),n(St,Ya),n(St,Ss),n(Ss,Za),n(St,Qa),p(t,Ve,l),j(ln,t,l),p(t,Ke,l),p(t,Rn,l),n(Rn,th),p(t,Xe,l),j(rn,t,l),p(t,Je,l),p(t,mt,l),n(mt,nh),n(mt,Os),n(Os,sh),n(mt,eh),n(mt,As),n(As,ch),n(mt,ah),p(t,Ye,l),j(on,t,l),p(t,Ze,l),p(t,K,l),n(K,hh),n(K,Ns),n(Ns,lh),n(K,ih),n(K,Ms),n(Ms,rh),n(K,oh),n(K,Ls),n(Ls,ph),n(K,uh),n(K,Hs),n(Hs,mh),n(K,gh),p(t,Qe,l),p(t,Ot,l),n(Ot,dh),n(Ot,Fs),n(Fs,fh),n(Ot,_h),p(t,tc,l),p(t,gt,l),n(gt,vh),n(gt,Bn),n(Bn,bh),n(gt,kh),n(gt,Rs),n(Rs,yh),n(gt,$h),p(t,nc,l),p(t,Wn,l),n(Wn,xh),p(t,sc,l),p(t,At,l),n(At,jh),n(At,Bs),n(Bs,Eh),n(At,wh),p(t,ec,l),p(t,dt,l),n(dt,qh),n(dt,In),n(In,Ch),n(dt,zh),n(dt,Un),n(Un,Th),n(dt,Dh),p(t,cc,l),j(pn,t,l),p(t,ac,l),p(t,X,l),n(X,Ph),n(X,Ws),n(Ws,Sh),n(X,Oh),n(X,Is),n(Is,Ah),n(X,Nh),n(X,Us),n(Us,Mh),n(X,Lh),n(X,un),n(un,Hh),n(X,Fh),p(t,hc,l),p(t,ft,l),n(ft,Rh),n(ft,mn),n(mn,Gs),n(Gs,Bh),n(ft,Wh),n(ft,Vs),n(Vs,Ih),n(ft,Uh),p(t,lc,l),j(gn,t,l),p(t,ic,l),p(t,A,l),n(A,Gh),n(A,Ks),n(Ks,Vh),n(A,Kh),n(A,Xs),n(Xs,Xh),n(A,Jh),n(A,Js),n(Js,Yh),n(A,Zh),n(A,Ys),n(Ys,Qh),n(A,tl),n(A,Zs),n(Zs,nl),n(A,sl),n(A,Qs),n(Qs,el),n(A,cl),n(A,te),n(te,al),n(A,hl),n(A,ne),n(ne,ll),n(A,il),n(A,dn),n(dn,rl),n(A,ol),p(t,rc,l),p(t,Nt,l),n(Nt,pl),n(Nt,se),n(se,ul),n(Nt,ml),p(t,oc,l),p(t,_t,l),n(_t,gl),n(_t,ee),n(ee,dl),n(_t,fl),n(_t,ce),n(ce,_l),n(_t,vl),p(t,pc,l),j(fn,t,l),p(t,uc,l),p(t,Gn,l),n(Gn,bl),p(t,mc,l),j(_n,t,l),p(t,gc,l),p(t,vt,l),n(vt,kl),n(vt,ae),n(ae,yl),n(vt,$l),n(vt,he),n(he,xl),n(vt,jl),p(t,dc,l),p(t,G,l),n(G,le),n(le,El),n(G,wl),n(G,ie),n(ie,ql),n(G,Cl),n(G,re),n(re,zl),n(G,Tl),n(G,oe),n(oe,Dl),n(G,Pl),n(G,pe),n(pe,Sl),n(G,Ol),p(t,fc,l),p(t,Mt,l),n(Mt,Al),n(Mt,ue),n(ue,Nl),n(Mt,Ml),p(t,_c,l),p(t,jt,l),n(jt,Lt),n(Lt,me),j(vn,me,null),n(jt,Ll),n(jt,ge),n(ge,Hl),p(t,vc,l),j(bn,t,l),p(t,bc,l),Et.m(t,l),p(t,Vn,l),p(t,Ht,l),n(Ht,Fl),n(Ht,de),n(de,Rl),n(Ht,Bl),p(t,kc,l),qn[ct].m(t,l),p(t,Kn,l),p(t,Z,l),n(Z,Wl),n(Z,fe),n(fe,Il),n(Z,Ul),n(Z,_e),n(_e,Gl),n(Z,Vl),n(Z,ve),n(ve,Kl),n(Z,Xl),p(t,yc,l),j(kn,t,l),p(t,$c,l),j(yn,t,l),p(t,xc,l),p(t,Ft,l),n(Ft,Jl),n(Ft,be),n(be,Yl),n(Ft,Zl),p(t,jc,l),j($n,t,l),p(t,Ec,l),Cn[ht].m(t,l),p(t,Xn,l),j(Rt,t,l),p(t,wc,l),R&&R.m(t,l),p(t,Jn,l),qc=!0},p(t,[l]){const zn={};l&1&&(zn.fw=t[0]),h.$set(zn);let Yn=z;z=ni(t),z!==Yn&&(Dn(),b(xn[Yn],1,1,()=>{xn[Yn]=null}),Tn(),L=xn[z],L||(L=xn[z]=ti[z](t),L.c()),v(L,1),L.m(B.parentNode,B));let Zn=S;S=ei(t),S!==Zn&&(Dn(),b(jn[Zn],1,1,()=>{jn[Zn]=null}),Tn(),W=jn[S],W||(W=jn[S]=si[S](t),W.c()),v(W,1),W.m(J.parentNode,J));let Qn=tt;tt=ai(t),tt!==Qn&&(Dn(),b(En[Qn],1,1,()=>{En[Qn]=null}),Tn(),nt=En[tt],nt||(nt=En[tt]=ci[tt](t),nt.c()),v(nt,1),nt.m(An.parentNode,An));const ke={};l&2&&(ke.$$scope={dirty:l,ctx:t}),zt.$set(ke);let Bt=st;st=li(t),st!==Bt&&(Dn(),b(wn[Bt],1,1,()=>{wn[Bt]=null}),Tn(),et=wn[st],et||(et=wn[st]=hi[st](t),et.c()),v(et,1),et.m(Mn.parentNode,Mn));const ye={};l&2&&(ye.$$scope={dirty:l,ctx:t}),Pt.$set(ye),Cc!==(Cc=ii(t))&&(Et.d(1),Et=Cc(t),Et&&(Et.c(),Et.m(Vn.parentNode,Vn)));let Wt=ct;ct=oi(t),ct!==Wt&&(Dn(),b(qn[Wt],1,1,()=>{qn[Wt]=null}),Tn(),at=qn[ct],at||(at=qn[ct]=ri[ct](t),at.c()),v(at,1),at.m(Kn.parentNode,Kn));let ts=ht;ht=ui(t),ht!==ts&&(Dn(),b(Cn[ts],1,1,()=>{Cn[ts]=null}),Tn(),lt=Cn[ht],lt||(lt=Cn[ht]=pi[ht](t),lt.c()),v(lt,1),lt.m(Xn.parentNode,Xn));const $e={};l&2&&($e.$$scope={dirty:l,ctx:t}),Rt.$set($e),t[0]==="tf"?R?l&1&&v(R,1):(R=Br(),R.c(),v(R,1),R.m(Jn.parentNode,Jn)):R&&(Dn(),b(R,1,1,()=>{R=null}),Tn())},i(t){qc||(v(h.$$.fragment,t),v(C.$$.fragment,t),v(L),v(W),v(Vt.$$.fragment,t),v(nt),v(Yt.$$.fragment,t),v(Zt.$$.fragment,t),v(Qt.$$.fragment,t),v(tn.$$.fragment,t),v(nn.$$.fragment,t),v(sn.$$.fragment,t),v(zt.$$.fragment,t),v(en.$$.fragment,t),v(et),v(cn.$$.fragment,t),v(an.$$.fragment,t),v(hn.$$.fragment,t),v(Pt.$$.fragment,t),v(ln.$$.fragment,t),v(rn.$$.fragment,t),v(on.$$.fragment,t),v(pn.$$.fragment,t),v(gn.$$.fragment,t),v(fn.$$.fragment,t),v(_n.$$.fragment,t),v(vn.$$.fragment,t),v(bn.$$.fragment,t),v(at),v(kn.$$.fragment,t),v(yn.$$.fragment,t),v($n.$$.fragment,t),v(lt),v(Rt.$$.fragment,t),v(R),qc=!0)},o(t){b(h.$$.fragment,t),b(C.$$.fragment,t),b(L),b(W),b(Vt.$$.fragment,t),b(nt),b(Yt.$$.fragment,t),b(Zt.$$.fragment,t),b(Qt.$$.fragment,t),b(tn.$$.fragment,t),b(nn.$$.fragment,t),b(sn.$$.fragment,t),b(zt.$$.fragment,t),b(en.$$.fragment,t),b(et),b(cn.$$.fragment,t),b(an.$$.fragment,t),b(hn.$$.fragment,t),b(Pt.$$.fragment,t),b(ln.$$.fragment,t),b(rn.$$.fragment,t),b(on.$$.fragment,t),b(pn.$$.fragment,t),b(gn.$$.fragment,t),b(fn.$$.fragment,t),b(_n.$$.fragment,t),b(vn.$$.fragment,t),b(bn.$$.fragment,t),b(at),b(kn.$$.fragment,t),b(yn.$$.fragment,t),b($n.$$.fragment,t),b(lt),b(Rt.$$.fragment,t),b(R),qc=!1},d(t){s(a),t&&s(u),E(h,t),t&&s(_),t&&s(y),E(C),t&&s(N),xn[z].d(t),t&&s(B),jn[S].d(t),t&&s(J),t&&s(rt),t&&s(D),t&&s(O),t&&s(Gt),t&&s($t),E(Vt),t&&s(je),En[tt].d(t),t&&s(An),t&&s(Y),t&&s(Ee),t&&s(Nn),t&&s(we),E(Yt,t),t&&s(qe),E(Zt,t),t&&s(Ce),t&&s(I),t&&s(ze),t&&s(pt),t&&s(Te),t&&s(Ct),t&&s(De),E(Qt,t),t&&s(Pe),E(tn,t),t&&s(Se),t&&s(ut),t&&s(Oe),E(nn,t),t&&s(Ae),E(sn,t),t&&s(Ne),t&&s(H),t&&s(Me),E(zt,t),t&&s(Le),t&&s(xt),E(en),t&&s(He),wn[st].d(t),t&&s(Mn),t&&s(Dt),t&&s(Fe),E(cn,t),t&&s(Re),t&&s(Hn),t&&s(Be),E(an,t),t&&s(We),E(hn,t),t&&s(Ie),t&&s(V),t&&s(Ue),E(Pt,t),t&&s(Ge),t&&s(St),t&&s(Ve),E(ln,t),t&&s(Ke),t&&s(Rn),t&&s(Xe),E(rn,t),t&&s(Je),t&&s(mt),t&&s(Ye),E(on,t),t&&s(Ze),t&&s(K),t&&s(Qe),t&&s(Ot),t&&s(tc),t&&s(gt),t&&s(nc),t&&s(Wn),t&&s(sc),t&&s(At),t&&s(ec),t&&s(dt),t&&s(cc),E(pn,t),t&&s(ac),t&&s(X),t&&s(hc),t&&s(ft),t&&s(lc),E(gn,t),t&&s(ic),t&&s(A),t&&s(rc),t&&s(Nt),t&&s(oc),t&&s(_t),t&&s(pc),E(fn,t),t&&s(uc),t&&s(Gn),t&&s(mc),E(_n,t),t&&s(gc),t&&s(vt),t&&s(dc),t&&s(G),t&&s(fc),t&&s(Mt),t&&s(_c),t&&s(jt),E(vn),t&&s(vc),E(bn,t),t&&s(bc),Et.d(t),t&&s(Vn),t&&s(Ht),t&&s(kc),qn[ct].d(t),t&&s(Kn),t&&s(Z),t&&s(yc),E(kn,t),t&&s($c),E(yn,t),t&&s(xc),t&&s(Ft),t&&s(jc),E($n,t),t&&s(Ec),Cn[ht].d(t),t&&s(Xn),E(Rt,t),t&&s(wc),R&&R.d(t),t&&s(Jn)}}}const go={local:"x-l-d-liu",sections:[{local:"ti-b-d-liu-t-hub",title:"T\u1EA3i b\u1ED9 d\u1EEF li\u1EC7u t\u1EEB Hub"},{local:"tin-x-l-mt-b-d-liu",title:"Ti\u1EC1n x\u1EED l\xFD m\u1ED9t b\u1ED9 d\u1EEF li\u1EC7u"},{local:"phn-m-ng",title:"Ph\u1EA7n \u0111\u1EC7m \u0111\u1ED9ng"}],title:"X\u1EED l\xFD d\u1EEF li\u1EC7u"};function fo(w,a,u){let h="pt";return Kr(()=>{const _=new URLSearchParams(window.location.search);u(0,h=_.get("fw")||"pt")}),[h]}class jo extends Ir{constructor(a){super();Ur(this,a,fo,mo,Gr,{})}}export{jo as default,go as metadata};
