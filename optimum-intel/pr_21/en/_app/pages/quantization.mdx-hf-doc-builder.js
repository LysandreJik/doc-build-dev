import{S as _t,i as gt,s as yt,e as n,k as b,w as Z,t as u,M as zt,c as o,d as a,m as I,a as r,x as tt,h as p,b as i,G as e,g as c,y as at,L as $t,q as et,o as nt,B as ot,v as wt}from"../chunks/vendor-hf-doc-builder.js";import{D as bt}from"../chunks/Docstring-hf-doc-builder.js";import{I as vt}from"../chunks/IconCopyLink-hf-doc-builder.js";function It(rt){let m,N,h,d,Q,g,U,x,B,C,l,G,A,J,R,y,V,j,S,f,v,P,z,F,k,K,O,_,W,E,X,Y,D,$,w,L;return g=new vt({}),z=new vt({}),w=new bt({props:{name:"class optimum.IncQuantizer",anchor:"optimum.IncQuantizer",parameters:[{name:"config",val:": typing.Union[str, optimum.intel.neural_compressor.configuration.IncQuantizationConfig]"},{name:"eval_func",val:": typing.Optional[typing.Callable]"},{name:"train_func",val:": typing.Optional[typing.Callable] = None"},{name:"calib_dataloader",val:": typing.Optional[torch.utils.data.dataloader.DataLoader] = None"}],source:"https://github.com/huggingface/optimum.intel/blob/vr_21/src/optimum/intel/neural_compressor/quantization.py#L65"}}),{c(){m=n("meta"),N=b(),h=n("h1"),d=n("a"),Q=n("span"),Z(g.$$.fragment),U=b(),x=n("span"),B=u("Quantization"),C=b(),l=n("p"),G=u("\u{1F917} Optimum provides an "),A=n("code"),J=u("neural_compressor"),R=u(" package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),y=n("a"),V=u("Intel Neural Compressor"),j=u(" quantization API."),S=b(),f=n("h2"),v=n("a"),P=n("span"),Z(z.$$.fragment),F=b(),k=n("span"),K=u("IncQuantizer"),O=b(),_=n("p"),W=u("The "),E=n("a"),X=u("IncQuantizer"),Y=u(" class allows to apply different quantization approaches such as static, dynamic and aware training quantization using pytorch eager or fx graph mode."),D=b(),$=n("div"),Z(w.$$.fragment),this.h()},l(t){const s=zt('[data-svelte="svelte-1phssyn"]',document.head);m=o(s,"META",{name:!0,content:!0}),s.forEach(a),N=I(t),h=o(t,"H1",{class:!0});var M=r(h);d=o(M,"A",{id:!0,class:!0,href:!0});var it=r(d);Q=o(it,"SPAN",{});var st=r(Q);tt(g.$$.fragment,st),st.forEach(a),it.forEach(a),U=I(M),x=o(M,"SPAN",{});var lt=r(x);B=p(lt,"Quantization"),lt.forEach(a),M.forEach(a),C=I(t),l=o(t,"P",{});var q=r(l);G=p(q,"\u{1F917} Optimum provides an "),A=o(q,"CODE",{});var ut=r(A);J=p(ut,"neural_compressor"),ut.forEach(a),R=p(q," package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),y=o(q,"A",{href:!0,rel:!0});var pt=r(y);V=p(pt,"Intel Neural Compressor"),pt.forEach(a),j=p(q," quantization API."),q.forEach(a),S=I(t),f=o(t,"H2",{class:!0});var T=r(f);v=o(T,"A",{id:!0,class:!0,href:!0});var ct=r(v);P=o(ct,"SPAN",{});var mt=r(P);tt(z.$$.fragment,mt),mt.forEach(a),ct.forEach(a),F=I(T),k=o(T,"SPAN",{});var ht=r(k);K=p(ht,"IncQuantizer"),ht.forEach(a),T.forEach(a),O=I(t),_=o(t,"P",{});var H=r(_);W=p(H,"The "),E=o(H,"A",{href:!0});var ft=r(E);X=p(ft,"IncQuantizer"),ft.forEach(a),Y=p(H," class allows to apply different quantization approaches such as static, dynamic and aware training quantization using pytorch eager or fx graph mode."),H.forEach(a),D=I(t),$=o(t,"DIV",{class:!0});var dt=r($);tt(w.$$.fragment,dt),dt.forEach(a),this.h()},h(){i(m,"name","hf:doc:metadata"),i(m,"content",JSON.stringify(Et)),i(d,"id","quantization"),i(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(d,"href","#quantization"),i(h,"class","relative group"),i(y,"href","https://github.com/intel/neural-compressor"),i(y,"rel","nofollow"),i(v,"id","optimum.IncQuantizer"),i(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(v,"href","#optimum.IncQuantizer"),i(f,"class","relative group"),i(E,"href","/docs/optimum.intel/pr_21/en/quantization#optimum.IncQuantizer"),i($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,s){e(document.head,m),c(t,N,s),c(t,h,s),e(h,d),e(d,Q),at(g,Q,null),e(h,U),e(h,x),e(x,B),c(t,C,s),c(t,l,s),e(l,G),e(l,A),e(A,J),e(l,R),e(l,y),e(y,V),e(l,j),c(t,S,s),c(t,f,s),e(f,v),e(v,P),at(z,P,null),e(f,F),e(f,k),e(k,K),c(t,O,s),c(t,_,s),e(_,W),e(_,E),e(E,X),e(_,Y),c(t,D,s),c(t,$,s),at(w,$,null),L=!0},p:$t,i(t){L||(et(g.$$.fragment,t),et(z.$$.fragment,t),et(w.$$.fragment,t),L=!0)},o(t){nt(g.$$.fragment,t),nt(z.$$.fragment,t),nt(w.$$.fragment,t),L=!1},d(t){a(m),t&&a(N),t&&a(h),ot(g),t&&a(C),t&&a(l),t&&a(S),t&&a(f),ot(z),t&&a(O),t&&a(_),t&&a(D),t&&a($),ot(w)}}}const Et={local:"quantization",sections:[{local:"optimum.IncQuantizer",title:"IncQuantizer"}],title:"Quantization"};function qt(rt){return wt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Pt extends _t{constructor(m){super();gt(this,m,qt,It,yt,{})}}export{Pt as default,Et as metadata};
