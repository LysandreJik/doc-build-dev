import{S as Or,i as Dr,s as zr,e as a,k as h,w as re,t as l,N as Fr,c as o,d as r,m as p,a as n,x as ae,h as s,b as i,G as t,g as c,y as oe,L as Ur,q as ne,o as le,B as se,v as Yr}from"../chunks/vendor-hf-doc-builder.js";import{I as ie}from"../chunks/IconCopyLink-hf-doc-builder.js";function Mr(er){let w,ke,y,$,fe,D,tt,ce,rt,Se,b,at,z,ot,nt,Ne,_,k,he,F,lt,pe,st,xe,S,it,U,ft,ct,Le,N,ht,ue,pt,ut,Ce,m,Y,mt,M,dt,vt,gt,me,wt,yt,de,_t,He,u,It,V,Et,At,R,Pt,$t,T,bt,kt,W,St,Nt,qe,I,x,ve,X,xt,ge,Lt,Oe,L,Ct,we,Ht,qt,De,E,C,ye,B,Ot,_e,Dt,ze,H,zt,Ie,Ft,Ut,Fe,A,q,Ee,G,Yt,Ae,Mt,Ue,d,Rt,J,Tt,Wt,j,Xt,Bt,Ye,P,O,Pe,K,Gt,$e,Jt,Me,v,jt,be,Kt,Qt,Q,Vt,Zt,Re;return D=new ie({}),F=new ie({}),X=new ie({}),B=new ie({}),G=new ie({}),K=new ie({}),{c(){w=a("meta"),ke=h(),y=a("h1"),$=a("a"),fe=a("span"),re(D.$$.fragment),tt=h(),ce=a("span"),rt=l("Inference API"),Se=h(),b=a("p"),at=l("Please refer to "),z=a("a"),ot=l("Inference API Documentation"),nt=l(" for detailed information."),Ne=h(),_=a("h2"),k=a("a"),he=a("span"),re(F.$$.fragment),lt=h(),pe=a("span"),st=l("What technology do you use to power the inference API?"),xe=h(),S=a("p"),it=l("For \u{1F917} Transformers models, "),U=a("a"),ft=l("Pipelines"),ct=l(" power the API."),Le=h(),N=a("p"),ht=l("On top of "),ue=a("code"),pt=l("Pipelines"),ut=l(" and depending on the model type, there are several production optimizations like:"),Ce=h(),m=a("ul"),Y=a("li"),mt=l("compiling models to optimized intermediary representations (e.g. "),M=a("a"),dt=l("ONNX"),vt=l("),"),gt=h(),me=a("li"),wt=l("maintaining a Least Recently Used cache, ensuring that the most popular models are always loaded,"),yt=h(),de=a("li"),_t=l("scaling the underlying compute infrastructure on the fly depending on the load constraints."),He=h(),u=a("p"),It=l("For models from "),V=a("a"),Et=l("other libraries"),At=l(", the API uses "),R=a("a"),Pt=l("Starlette"),$t=l(" and runs in "),T=a("a"),bt=l("Docker containers"),kt=l(". Each library defines the implementation of "),W=a("a"),St=l("different pipelines"),Nt=l("."),qe=h(),I=a("h2"),x=a("a"),ve=a("span"),re(X.$$.fragment),xt=h(),ge=a("span"),Lt=l("How can I turn off the inference API for my model?"),Oe=h(),L=a("p"),Ct=l("Specify "),we=a("code"),Ht=l("inference: false"),qt=l(" in your model card\u2019s metadata."),De=h(),E=a("h2"),C=a("a"),ye=a("span"),re(B.$$.fragment),Ot=h(),_e=a("span"),Dt=l("Can I send large volumes of requests? Can I get accelerated APIs?"),ze=h(),H=a("p"),zt=l("If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at "),Ie=a("code"),Ft=l("api-enterprise at huggingface.co"),Ut=l("."),Fe=h(),A=a("h2"),q=a("a"),Ee=a("span"),re(G.$$.fragment),Yt=h(),Ae=a("span"),Mt=l("How can I see my usage?"),Ue=h(),d=a("p"),Rt=l("You can head to the "),J=a("a"),Tt=l("Inference API dashboard"),Wt=l(". Learn more about it in the "),j=a("a"),Xt=l("Inference API documentation"),Bt=l("."),Ye=h(),P=a("h2"),O=a("a"),Pe=a("span"),re(K.$$.fragment),Gt=h(),$e=a("span"),Jt=l("Is there programmatic access to the Inference API?"),Me=h(),v=a("p"),jt=l("Yes, the "),be=a("code"),Kt=l("huggingface_hub"),Qt=l(" library has a client wrapper documented "),Q=a("a"),Vt=l("here"),Zt=l("."),this.h()},l(e){const f=Fr('[data-svelte="svelte-1phssyn"]',document.head);w=o(f,"META",{name:!0,content:!0}),f.forEach(r),ke=p(e),y=o(e,"H1",{class:!0});var Te=n(y);$=o(Te,"A",{id:!0,class:!0,href:!0});var tr=n($);fe=o(tr,"SPAN",{});var rr=n(fe);ae(D.$$.fragment,rr),rr.forEach(r),tr.forEach(r),tt=p(Te),ce=o(Te,"SPAN",{});var ar=n(ce);rt=s(ar,"Inference API"),ar.forEach(r),Te.forEach(r),Se=p(e),b=o(e,"P",{});var We=n(b);at=s(We,"Please refer to "),z=o(We,"A",{href:!0,rel:!0});var or=n(z);ot=s(or,"Inference API Documentation"),or.forEach(r),nt=s(We," for detailed information."),We.forEach(r),Ne=p(e),_=o(e,"H2",{class:!0});var Xe=n(_);k=o(Xe,"A",{id:!0,class:!0,href:!0});var nr=n(k);he=o(nr,"SPAN",{});var lr=n(he);ae(F.$$.fragment,lr),lr.forEach(r),nr.forEach(r),lt=p(Xe),pe=o(Xe,"SPAN",{});var sr=n(pe);st=s(sr,"What technology do you use to power the inference API?"),sr.forEach(r),Xe.forEach(r),xe=p(e),S=o(e,"P",{});var Be=n(S);it=s(Be,"For \u{1F917} Transformers models, "),U=o(Be,"A",{href:!0,rel:!0});var ir=n(U);ft=s(ir,"Pipelines"),ir.forEach(r),ct=s(Be," power the API."),Be.forEach(r),Le=p(e),N=o(e,"P",{});var Ge=n(N);ht=s(Ge,"On top of "),ue=o(Ge,"CODE",{});var fr=n(ue);pt=s(fr,"Pipelines"),fr.forEach(r),ut=s(Ge," and depending on the model type, there are several production optimizations like:"),Ge.forEach(r),Ce=p(e),m=o(e,"UL",{});var Z=n(m);Y=o(Z,"LI",{});var Je=n(Y);mt=s(Je,"compiling models to optimized intermediary representations (e.g. "),M=o(Je,"A",{href:!0,rel:!0});var cr=n(M);dt=s(cr,"ONNX"),cr.forEach(r),vt=s(Je,"),"),Je.forEach(r),gt=p(Z),me=o(Z,"LI",{});var hr=n(me);wt=s(hr,"maintaining a Least Recently Used cache, ensuring that the most popular models are always loaded,"),hr.forEach(r),yt=p(Z),de=o(Z,"LI",{});var pr=n(de);_t=s(pr,"scaling the underlying compute infrastructure on the fly depending on the load constraints."),pr.forEach(r),Z.forEach(r),He=p(e),u=o(e,"P",{});var g=n(u);It=s(g,"For models from "),V=o(g,"A",{href:!0});var ur=n(V);Et=s(ur,"other libraries"),ur.forEach(r),At=s(g,", the API uses "),R=o(g,"A",{href:!0,rel:!0});var mr=n(R);Pt=s(mr,"Starlette"),mr.forEach(r),$t=s(g," and runs in "),T=o(g,"A",{href:!0,rel:!0});var dr=n(T);bt=s(dr,"Docker containers"),dr.forEach(r),kt=s(g,". Each library defines the implementation of "),W=o(g,"A",{href:!0,rel:!0});var vr=n(W);St=s(vr,"different pipelines"),vr.forEach(r),Nt=s(g,"."),g.forEach(r),qe=p(e),I=o(e,"H2",{class:!0});var je=n(I);x=o(je,"A",{id:!0,class:!0,href:!0});var gr=n(x);ve=o(gr,"SPAN",{});var wr=n(ve);ae(X.$$.fragment,wr),wr.forEach(r),gr.forEach(r),xt=p(je),ge=o(je,"SPAN",{});var yr=n(ge);Lt=s(yr,"How can I turn off the inference API for my model?"),yr.forEach(r),je.forEach(r),Oe=p(e),L=o(e,"P",{});var Ke=n(L);Ct=s(Ke,"Specify "),we=o(Ke,"CODE",{});var _r=n(we);Ht=s(_r,"inference: false"),_r.forEach(r),qt=s(Ke," in your model card\u2019s metadata."),Ke.forEach(r),De=p(e),E=o(e,"H2",{class:!0});var Qe=n(E);C=o(Qe,"A",{id:!0,class:!0,href:!0});var Ir=n(C);ye=o(Ir,"SPAN",{});var Er=n(ye);ae(B.$$.fragment,Er),Er.forEach(r),Ir.forEach(r),Ot=p(Qe),_e=o(Qe,"SPAN",{});var Ar=n(_e);Dt=s(Ar,"Can I send large volumes of requests? Can I get accelerated APIs?"),Ar.forEach(r),Qe.forEach(r),ze=p(e),H=o(e,"P",{});var Ve=n(H);zt=s(Ve,"If you are interested in accelerated inference, higher volumes of requests, or an SLA, please contact us at "),Ie=o(Ve,"CODE",{});var Pr=n(Ie);Ft=s(Pr,"api-enterprise at huggingface.co"),Pr.forEach(r),Ut=s(Ve,"."),Ve.forEach(r),Fe=p(e),A=o(e,"H2",{class:!0});var Ze=n(A);q=o(Ze,"A",{id:!0,class:!0,href:!0});var $r=n(q);Ee=o($r,"SPAN",{});var br=n(Ee);ae(G.$$.fragment,br),br.forEach(r),$r.forEach(r),Yt=p(Ze),Ae=o(Ze,"SPAN",{});var kr=n(Ae);Mt=s(kr,"How can I see my usage?"),kr.forEach(r),Ze.forEach(r),Ue=p(e),d=o(e,"P",{});var ee=n(d);Rt=s(ee,"You can head to the "),J=o(ee,"A",{href:!0,rel:!0});var Sr=n(J);Tt=s(Sr,"Inference API dashboard"),Sr.forEach(r),Wt=s(ee,". Learn more about it in the "),j=o(ee,"A",{href:!0,rel:!0});var Nr=n(j);Xt=s(Nr,"Inference API documentation"),Nr.forEach(r),Bt=s(ee,"."),ee.forEach(r),Ye=p(e),P=o(e,"H2",{class:!0});var et=n(P);O=o(et,"A",{id:!0,class:!0,href:!0});var xr=n(O);Pe=o(xr,"SPAN",{});var Lr=n(Pe);ae(K.$$.fragment,Lr),Lr.forEach(r),xr.forEach(r),Gt=p(et),$e=o(et,"SPAN",{});var Cr=n($e);Jt=s(Cr,"Is there programmatic access to the Inference API?"),Cr.forEach(r),et.forEach(r),Me=p(e),v=o(e,"P",{});var te=n(v);jt=s(te,"Yes, the "),be=o(te,"CODE",{});var Hr=n(be);Kt=s(Hr,"huggingface_hub"),Hr.forEach(r),Qt=s(te," library has a client wrapper documented "),Q=o(te,"A",{href:!0,rel:!0});var qr=n(Q);Vt=s(qr,"here"),qr.forEach(r),Zt=s(te,"."),te.forEach(r),this.h()},h(){i(w,"name","hf:doc:metadata"),i(w,"content",JSON.stringify(Rr)),i($,"id","inference-api"),i($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i($,"href","#inference-api"),i(y,"class","relative group"),i(z,"href","https://huggingface.co/docs/api-inference"),i(z,"rel","nofollow"),i(k,"id","what-technology-do-you-use-to-power-the-inference-api"),i(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(k,"href","#what-technology-do-you-use-to-power-the-inference-api"),i(_,"class","relative group"),i(U,"href","https://huggingface.co/transformers/main_classes/pipelines.html"),i(U,"rel","nofollow"),i(M,"href","https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333"),i(M,"rel","nofollow"),i(V,"href","./models-libraries"),i(R,"href","https://www.starlette.io"),i(R,"rel","nofollow"),i(T,"href","https://github.com/huggingface/api-inference-community/tree/main/docker_images"),i(T,"rel","nofollow"),i(W,"href","https://github.com/huggingface/api-inference-community/tree/main/docker_images/sentence_transformers/app/pipelines"),i(W,"rel","nofollow"),i(x,"id","how-can-i-turn-off-the-inference-api-for-my-model"),i(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(x,"href","#how-can-i-turn-off-the-inference-api-for-my-model"),i(I,"class","relative group"),i(C,"id","can-i-send-large-volumes-of-requests-can-i-get-accelerated-apis"),i(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(C,"href","#can-i-send-large-volumes-of-requests-can-i-get-accelerated-apis"),i(E,"class","relative group"),i(q,"id","how-can-i-see-my-usage"),i(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(q,"href","#how-can-i-see-my-usage"),i(A,"class","relative group"),i(J,"href","https://api-inference.huggingface.co/dashboard/"),i(J,"rel","nofollow"),i(j,"href","https://huggingface.co/docs/api-inference/usage"),i(j,"rel","nofollow"),i(O,"id","is-there-programmatic-access-to-the-inference-api"),i(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(O,"href","#is-there-programmatic-access-to-the-inference-api"),i(P,"class","relative group"),i(Q,"href","https://huggingface.co/docs/huggingface_hub/how-to-inference"),i(Q,"rel","nofollow")},m(e,f){t(document.head,w),c(e,ke,f),c(e,y,f),t(y,$),t($,fe),oe(D,fe,null),t(y,tt),t(y,ce),t(ce,rt),c(e,Se,f),c(e,b,f),t(b,at),t(b,z),t(z,ot),t(b,nt),c(e,Ne,f),c(e,_,f),t(_,k),t(k,he),oe(F,he,null),t(_,lt),t(_,pe),t(pe,st),c(e,xe,f),c(e,S,f),t(S,it),t(S,U),t(U,ft),t(S,ct),c(e,Le,f),c(e,N,f),t(N,ht),t(N,ue),t(ue,pt),t(N,ut),c(e,Ce,f),c(e,m,f),t(m,Y),t(Y,mt),t(Y,M),t(M,dt),t(Y,vt),t(m,gt),t(m,me),t(me,wt),t(m,yt),t(m,de),t(de,_t),c(e,He,f),c(e,u,f),t(u,It),t(u,V),t(V,Et),t(u,At),t(u,R),t(R,Pt),t(u,$t),t(u,T),t(T,bt),t(u,kt),t(u,W),t(W,St),t(u,Nt),c(e,qe,f),c(e,I,f),t(I,x),t(x,ve),oe(X,ve,null),t(I,xt),t(I,ge),t(ge,Lt),c(e,Oe,f),c(e,L,f),t(L,Ct),t(L,we),t(we,Ht),t(L,qt),c(e,De,f),c(e,E,f),t(E,C),t(C,ye),oe(B,ye,null),t(E,Ot),t(E,_e),t(_e,Dt),c(e,ze,f),c(e,H,f),t(H,zt),t(H,Ie),t(Ie,Ft),t(H,Ut),c(e,Fe,f),c(e,A,f),t(A,q),t(q,Ee),oe(G,Ee,null),t(A,Yt),t(A,Ae),t(Ae,Mt),c(e,Ue,f),c(e,d,f),t(d,Rt),t(d,J),t(J,Tt),t(d,Wt),t(d,j),t(j,Xt),t(d,Bt),c(e,Ye,f),c(e,P,f),t(P,O),t(O,Pe),oe(K,Pe,null),t(P,Gt),t(P,$e),t($e,Jt),c(e,Me,f),c(e,v,f),t(v,jt),t(v,be),t(be,Kt),t(v,Qt),t(v,Q),t(Q,Vt),t(v,Zt),Re=!0},p:Ur,i(e){Re||(ne(D.$$.fragment,e),ne(F.$$.fragment,e),ne(X.$$.fragment,e),ne(B.$$.fragment,e),ne(G.$$.fragment,e),ne(K.$$.fragment,e),Re=!0)},o(e){le(D.$$.fragment,e),le(F.$$.fragment,e),le(X.$$.fragment,e),le(B.$$.fragment,e),le(G.$$.fragment,e),le(K.$$.fragment,e),Re=!1},d(e){r(w),e&&r(ke),e&&r(y),se(D),e&&r(Se),e&&r(b),e&&r(Ne),e&&r(_),se(F),e&&r(xe),e&&r(S),e&&r(Le),e&&r(N),e&&r(Ce),e&&r(m),e&&r(He),e&&r(u),e&&r(qe),e&&r(I),se(X),e&&r(Oe),e&&r(L),e&&r(De),e&&r(E),se(B),e&&r(ze),e&&r(H),e&&r(Fe),e&&r(A),se(G),e&&r(Ue),e&&r(d),e&&r(Ye),e&&r(P),se(K),e&&r(Me),e&&r(v)}}}const Rr={local:"inference-api",sections:[{local:"what-technology-do-you-use-to-power-the-inference-api",title:"What technology do you use to power the inference API?"},{local:"how-can-i-turn-off-the-inference-api-for-my-model",title:"How can I turn off the inference API for my model?"},{local:"can-i-send-large-volumes-of-requests-can-i-get-accelerated-apis",title:"Can I send large volumes of requests? Can I get accelerated APIs?"},{local:"how-can-i-see-my-usage",title:"How can I see my usage?"},{local:"is-there-programmatic-access-to-the-inference-api",title:"Is there programmatic access to the Inference API?"}],title:"Inference API"};function Tr(er){return Yr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Br extends Or{constructor(w){super();Dr(this,w,Tr,Mr,zr,{})}}export{Br as default,Rr as metadata};
