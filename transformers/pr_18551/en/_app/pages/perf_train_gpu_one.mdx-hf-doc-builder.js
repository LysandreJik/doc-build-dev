import{S as yk,i as bk,s as $k,e as r,k as h,w as d,t as n,M as Ek,c as o,d as t,m,a as s,x as c,h as l,b as f,N as xo,G as a,g as p,y as u,q as g,o as w,B as v,v as kk}from"../chunks/vendor-hf-doc-builder.js";import{T as _k}from"../chunks/Tip-hf-doc-builder.js";import{I as y}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as _}from"../chunks/CodeBlock-hf-doc-builder.js";function Pk(Go){let b,T;return{c(){b=r("p"),T=n("Note: In order to properly clear the memory after experiments we need restart the Python kernel between experiments. Run all steps above and then just one of the experiments below.")},l(E){b=o(E,"P",{});var j=s(b);T=l(j,"Note: In order to properly clear the memory after experiments we need restart the Python kernel between experiments. Run all steps above and then just one of the experiments below."),j.forEach(t)},m(E,j){p(E,b,j),a(b,T)},d(E){E&&t(b)}}}function Ak(Go){let b;return{c(){b=n(`Note that in order to use the 8-bit optimizer with an existing pretrained model a change to the embedding layer is needed.
Read [this issue](https://github.com/huggingface/transformers/issues/14819) for more information.`)},l(T){b=l(T,`Note that in order to use the 8-bit optimizer with an existing pretrained model a change to the embedding layer is needed.
Read [this issue](https://github.com/huggingface/transformers/issues/14819) for more information.`)},m(T,E){p(T,b,E)},d(T){T&&t(b)}}}function Tk(Go){let b,T,E,j,jn,ra,Du,Sn,Nu,$h,He,Mu,zo,qu,Cu,Eh,q,Ou,Uo,Lu,Bu,oa,Fu,Wu,kh,Re,In,ie,jo,Hu,Ru,So,Vu,Yu,Io,Xu,Zu,k,ne,Do,Qu,Ju,No,Ku,eg,Mo,tg,ag,le,qo,rg,og,Co,sg,ig,Oo,ng,lg,pe,Lo,pg,hg,Bo,mg,fg,Fo,dg,cg,he,Wo,ug,gg,Ho,wg,vg,Ro,_g,yg,me,Vo,bg,$g,Yo,Eg,kg,Xo,Pg,Ag,fe,Zo,Tg,xg,Qo,Gg,zg,Jo,Ug,jg,de,Ko,Sg,Ig,es,Dg,Ng,ts,Mg,Ph,as,qg,Ah,sa,Th,C,Cg,Dn,Og,Lg,Nn,Bg,Fg,xh,Ve,Wg,ia,Hg,Rg,Gh,na,zh,Ye,Vg,rs,Yg,Xg,Uh,la,jh,os,Zg,Sh,pa,Ih,ss,Qg,Dh,ha,Nh,is,Jg,Mh,ce,Xe,Mn,ma,Kg,qn,ew,qh,Ze,tw,Cn,aw,rw,Ch,fa,Oh,Qe,ow,On,sw,iw,Lh,da,Bh,ca,Fh,ns,nw,Wh,ua,Hh,Je,Rh,ue,Ke,Ln,ga,lw,Bn,pw,Vh,et,hw,ls,mw,fw,Yh,wa,Xh,va,Zh,ps,dw,Qh,ge,tt,Fn,_a,cw,Wn,uw,Jh,hs,gw,Kh,O,ya,Hn,Rn,ww,vw,ba,_w,Vn,yw,bw,$w,$a,Yn,Xn,Ew,kw,Ea,Pw,Zn,Aw,Tw,xw,ka,Qn,Jn,Gw,zw,Pa,Uw,Kn,jw,Sw,em,ms,Iw,tm,Aa,Dw,Ta,Nw,am,we,at,el,xa,Mw,tl,qw,rm,fs,Cw,om,ds,Ow,sm,cs,al,Lw,im,rt,rl,Bw,Fw,ol,Ww,nm,us,sl,Hw,lm,L,il,Rw,Vw,gs,Yw,Ga,Xw,Zw,nl,Qw,pm,ws,ll,Jw,hm,vs,pl,Kw,mm,_s,hl,ev,fm,ys,ml,tv,dm,bs,av,cm,$s,fl,rv,um,Es,ov,gm,ks,dl,sv,wm,Ps,iv,vm,As,ot,cl,nv,lv,ul,pv,hv,_m,Ts,mv,ym,xs,fv,bm,ve,st,gl,za,dv,wl,cv,$m,Gs,uv,Em,B,gv,Ua,wv,vv,ja,_v,yv,km,Sa,Ia,bv,$v,Pm,it,Ev,Da,kv,Pv,Am,_e,nt,vl,Na,Av,_l,Tv,Tm,zs,xv,xm,S,Gv,Us,zv,Uv,yl,jv,Sv,js,Iv,Dv,Gm,Ma,zm,qa,Um,x,Nv,bl,Mv,qv,$l,Cv,Ov,El,Lv,Bv,kl,Fv,Wv,jm,F,Hv,Ca,Rv,Vv,Oa,Yv,Xv,Sm,Ss,Zv,Im,ye,lt,Pl,La,Qv,Al,Jv,Dm,Is,Kv,Nm,pt,e_,Ba,t_,a_,Mm,W,r_,Ds,o_,s_,Ns,i_,n_,qm,Fa,Cm,Wa,Om,Ms,l_,Lm,be,ht,Tl,Ha,p_,xl,h_,Bm,qs,m_,Fm,I,Ra,f_,Gl,d_,c_,u_,Va,g_,zl,w_,v_,__,Ya,y_,Ul,b_,$_,E_,jl,k_,Wm,Cs,P_,Hm,$e,Os,R3,A_,Xa,T_,x_,Rm,Ls,G_,Vm,Ee,mt,Sl,Za,z_,Il,U_,Ym,H,j_,Dl,S_,I_,Nl,D_,N_,Xm,Qa,Zm,Ja,Qm,Bs,M_,Jm,Ka,Km,er,ef,Fs,q_,tf,ke,ft,Ml,tr,C_,ql,O_,af,Ws,L_,rf,ar,of,Pe,dt,Cl,rr,B_,Ol,F_,sf,Hs,W_,nf,or,lf,Rs,H_,pf,ct,R_,sr,V_,Y_,hf,Vs,X_,mf,Ys,Z_,ff,ir,df,Xs,Q_,cf,R,J_,Ll,K_,ey,Bl,ty,ay,uf,ut,ry,Fl,oy,sy,gf,V,iy,nr,ny,ly,lr,py,hy,wf,Zs,my,vf,Ae,gt,Wl,pr,fy,Hl,dy,_f,Qs,cy,yf,wt,uy,Rl,gy,wy,bf,Js,vy,$f,hr,Ef,Y,_y,mr,yy,by,Vl,$y,Ey,kf,X,ky,fr,Py,Ay,dr,Ty,xy,Pf,vt,Gy,Yl,zy,Uy,Af,Z,cr,jy,Xl,Sy,Iy,Dy,ur,Ny,Zl,My,qy,Cy,gr,Oy,Ql,Ly,By,Tf,Ks,Fy,xf,Te,_t,Jl,wr,Wy,Kl,Hy,Gf,yt,Ry,ep,Vy,Yy,zf,vr,Uf,_r,jf,ei,Xy,Sf,yr,If,br,Df,ti,Zy,Nf,xe,bt,tp,$r,Qy,ap,Jy,Mf,ai,Ky,qf,G,eb,ri,tb,ab,oi,rb,ob,Er,sb,ib,rp,nb,lb,Cf,si,pb,Of,$t,Lf,kr,Bf,Et,hb,op,mb,fb,Ff,Pr,Wf,Ar,Hf,ii,db,Rf,Tr,Vf,xr,Yf,ni,cb,Xf,li,pi,V3,Zf,Ge,kt,sp,Gr,ub,ip,np,gb,Qf,ze,Pt,lp,zr,wb,pp,vb,Jf,Q,_b,hi,yb,bb,mi,$b,Eb,Kf,Ur,ed,fi,kb,td,jr,ad,P,Pb,Sr,hp,Ab,Tb,di,xb,Gb,Ir,mp,zb,Ub,fp,jb,Sb,Dr,dp,Ib,Db,rd,At,Nb,cp,Mb,qb,od,Nr,sd,Tt,Cb,Mr,Ob,Lb,id,Ue,xt,up,qr,Bb,gp,Fb,nd,ci,Wb,ld,Gt,ui,wp,Hb,Rb,Vb,gi,vp,Yb,Xb,pd,je,zt,_p,Cr,Zb,yp,Qb,hd,Ut,Jb,wi,Kb,e1,md,vi,t1,fd,jt,bp,a1,r1,$p,o1,dd,St,s1,_i,i1,n1,cd,yi,l1,ud,D,Ep,Or,p1,bi,h1,m1,f1,Lr,kp,d1,c1,Br,u1,Pp,Fr,g1,$i,w1,v1,_1,Ap,Se,y1,Wr,b1,$1,Tp,E1,k1,gd,Ie,It,xp,Hr,P1,Gp,A1,wd,Ei,T1,vd,De,Dt,zp,Rr,x1,Up,G1,_d,ki,z1,yd,Nt,jp,U1,j1,Sp,S1,bd,Pi,I1,$d,Ai,D1,Ed,Ne,Mt,Ip,Vr,N1,Dp,M1,kd,qt,q1,Yr,C1,O1,Pd,Ct,L1,Np,B1,F1,Ad,Ti,W1,Td,J,H1,Xr,R1,V1,Zr,Y1,X1,xd,xi,Z1,Gd,Me,Ot,Mp,Qr,Q1,qp,J1,zd,qe,Lt,Cp,Jr,K1,Op,e2,Ud,Gi,t2,jd,zi,a2,Sd,Ui,r2,Id,ji,Si,Y3,Dd,Bt,o2,Kr,s2,i2,Nd,Ii,n2,Md,Di,l2,qd,Ni,p2,Cd,Mi,h2,Od,K,Lp,eo,m2,f2,Bp,to,d2,c2,Fp,ao,u2,Ld,$,g2,ro,w2,v2,oo,_2,y2,so,b2,$2,io,E2,k2,no,P2,A2,qi,T2,x2,Bd,Ce,Ft,Wp,lo,G2,Hp,z2,Fd,Ci,U2,Wd,Wt,j2,Oi,S2,I2,Hd,Oe,Ht,Rp,po,D2,Vp,N2,Rd,Rt,M2,ho,q2,C2,Vd,mo,Yd,Li,O2,Xd,ee,Yp,fo,L2,B2,Xp,co,F2,W2,Zp,uo,H2,Zd,Le,Vt,Qp,go,R2,Bi,Jp,V2,Y2,Qd,A,X2,wo,Kp,Z2,Q2,eh,J2,K2,th,e3,t3,ah,a3,r3,rh,o3,s3,Jd,Fi,Wi,X3,Kd,te,i3,vo,n3,l3,_o,p3,h3,ec,Hi,Ri,Z3,tc,Yt,m3,Vi,f3,d3,ac,Be,Xt,oh,yo,c3,sh,u3,rc,ae,ih,g3,w3,Zt,v3,nh,_3,y3,lh,b3,$3,Qt,E3,ph,k3,P3,hh,A3,oc,Fe,Jt,mh,bo,T3,fh,x3,sc,Yi,G3,ic,$o,nc,re,z3,dh,U3,j3,ch,S3,I3,lc,Eo,pc,Xi,D3,hc,We,Kt,uh,ko,N3,gh,M3,mc,Zi,q3,fc,Qi,Po,Ji,Q3,dc,Ki,C3,cc,en,Ao,tn,J3,uc;return ra=new y({}),sa=new _({props:{code:"pip install transformers datasets accelerate nvidia-ml-py3",highlighted:"pip install transformers datasets accelerate nvidia-ml-py3"}}),na=new _({props:{code:`import numpy as np
from datasets import Dataset


seq_len, dataset_size = 512, 512
dummy_data = {
    "input_ids": np.random.randint(100, 30000, (dataset_size, seq_len)),
    "labels": np.random.randint(0, 1, (dataset_size)),
}
ds = Dataset.from_dict(dummy_data)
ds.set_format("pt")`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset


seq_len, dataset_size = <span class="hljs-number">512</span>, <span class="hljs-number">512</span>
dummy_data = {
    <span class="hljs-string">&quot;input_ids&quot;</span>: np.random.randint(<span class="hljs-number">100</span>, <span class="hljs-number">30000</span>, (dataset_size, seq_len)),
    <span class="hljs-string">&quot;labels&quot;</span>: np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (dataset_size)),
}
ds = Dataset.from_dict(dummy_data)
ds.set_format(<span class="hljs-string">&quot;pt&quot;</span>)`}}),la=new _({props:{code:`from pynvml import *


def print_gpu_utilization():
    nvmlInit()
    handle = nvmlDeviceGetHandleByIndex(0)
    info = nvmlDeviceGetMemoryInfo(handle)
    print(f"GPU memory occupied: {info.used//1024**2} MB.")


def print_summary(result):
    print(f"Time: {result.metrics['train_runtime']:.2f}")
    print(f"Samples/second: {result.metrics['train_samples_per_second']:.2f}")
    print_gpu_utilization()`,highlighted:`<span class="hljs-keyword">from</span> pynvml <span class="hljs-keyword">import</span> *


<span class="hljs-keyword">def</span> <span class="hljs-title function_">print_gpu_utilization</span>():
    nvmlInit()
    handle = nvmlDeviceGetHandleByIndex(<span class="hljs-number">0</span>)
    info = nvmlDeviceGetMemoryInfo(handle)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;GPU memory occupied: <span class="hljs-subst">{info.used//<span class="hljs-number">1024</span>**<span class="hljs-number">2</span>}</span> MB.&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">print_summary</span>(<span class="hljs-params">result</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Time: <span class="hljs-subst">{result.metrics[<span class="hljs-string">&#x27;train_runtime&#x27;</span>]:<span class="hljs-number">.2</span>f}</span>&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Samples/second: <span class="hljs-subst">{result.metrics[<span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>]:<span class="hljs-number">.2</span>f}</span>&quot;</span>)
    print_gpu_utilization()`}}),pa=new _({props:{code:"print_gpu_utilization()",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>print_gpu_utilization()
GPU memory occupied: <span class="hljs-number">0</span> MB.`}}),ha=new _({props:{code:`import torch


torch.ones((1, 1)).to("cuda")
print_gpu_utilization()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch


<span class="hljs-meta">&gt;&gt;&gt; </span>torch.ones((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).to(<span class="hljs-string">&quot;cuda&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>print_gpu_utilization()
GPU memory occupied: <span class="hljs-number">1343</span> MB.`}}),ma=new y({}),fa=new _({props:{code:`from transformers import AutoModelForSequenceClassification


model = AutoModelForSequenceClassification.from_pretrained("bert-large-uncased").to("cuda")
print_gpu_utilization()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification


<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-large-uncased&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>print_gpu_utilization()
GPU memory occupied: <span class="hljs-number">2631</span> MB.`}}),da=new _({props:{code:"nvidia-smi",highlighted:"nvidia-smi"}}),ca=new _({props:{code:`Tue Jan 11 08:58:05 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P0    39W / 300W |   2631MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |
+-----------------------------------------------------------------------------+`,highlighted:`Tue Jan 11 08:58:05 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P0    39W / 300W |   2631MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3721      C   ...nvs/codeparrot/bin/python     2629MiB |
+-----------------------------------------------------------------------------+`}}),ua=new _({props:{code:`default_args = {
    "output_dir": "tmp",
    "evaluation_strategy": "steps",
    "num_train_epochs": 1,
    "log_level": "error",
    "report_to": "none",
}`,highlighted:`default_args = {
    <span class="hljs-string">&quot;output_dir&quot;</span>: <span class="hljs-string">&quot;tmp&quot;</span>,
    <span class="hljs-string">&quot;evaluation_strategy&quot;</span>: <span class="hljs-string">&quot;steps&quot;</span>,
    <span class="hljs-string">&quot;num_train_epochs&quot;</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">&quot;log_level&quot;</span>: <span class="hljs-string">&quot;error&quot;</span>,
    <span class="hljs-string">&quot;report_to&quot;</span>: <span class="hljs-string">&quot;none&quot;</span>,
}`}}),Je=new _k({props:{$$slots:{default:[Pk]},$$scope:{ctx:Go}}}),ga=new y({}),wa=new _({props:{code:`from transformers import TrainingArguments, Trainer, logging

logging.set_verbosity_error()


training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)
trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer, logging

logging.set_verbosity_error()


training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">4</span>, **default_args)
trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),va=new _({props:{code:`Time: 57.82
Samples/second: 8.86
GPU memory occupied: 14949 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 57.82
Samples/second: 8.86
GPU memory occupied: 14949 MB.`}}),_a=new y({}),xa=new y({}),za=new y({}),Na=new y({}),Ma=new _({props:{code:`training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">1</span>, gradient_accumulation_steps=<span class="hljs-number">4</span>, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),qa=new _({props:{code:`Time: 66.03
Samples/second: 7.75
GPU memory occupied: 8681 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 66.03
Samples/second: 7.75
GPU memory occupied: 8681 MB.`}}),La=new y({}),Fa=new _({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>, gradient_accumulation_steps=<span class="hljs-number">4</span>, gradient_checkpointing=<span class="hljs-literal">True</span>, **default_args
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),Wa=new _({props:{code:`Time: 85.47
Samples/second: 5.99
GPU memory occupied: 6775 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 85.47
Samples/second: 5.99
GPU memory occupied: 6775 MB.`}}),Ha=new y({}),Za=new y({}),Qa=new _({props:{code:`training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">4</span>, fp16=<span class="hljs-literal">True</span>, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),Ja=new _({props:{code:`Time: 27.46
Samples/second: 18.64
GPU memory occupied: 13939 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 27.46
Samples/second: 18.64
GPU memory occupied: 13939 MB.`}}),Ka=new _({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>,
    gradient_accumulation_steps=<span class="hljs-number">4</span>,
    gradient_checkpointing=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),er=new _({props:{code:`Time: 50.76
Samples/second: 10.09
GPU memory occupied: 7275 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 50.76
Samples/second: 10.09
GPU memory occupied: 7275 MB.`}}),tr=new y({}),ar=new _({props:{code:"TrainingArguments(bf16=True)",highlighted:'TrainingArguments(bf16=<span class="hljs-literal">True</span>)'}}),rr=new y({}),or=new _({props:{code:`import torch
torch.backends.cuda.matmul.allow_tf32 = True`,highlighted:`import torch
torch<span class="hljs-selector-class">.backends</span><span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.matmul</span><span class="hljs-selector-class">.allow_tf32</span> = True`}}),ir=new _({props:{code:"TrainingArguments(tf32=True)",highlighted:'TrainingArguments(tf32=<span class="hljs-literal">True</span>)'}}),pr=new y({}),hr=new _({props:{code:`$ python examples/pytorch/translation/run_translation.py -h | grep "\\-optim"
         [--optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor}]`,highlighted:`$ python examples/pytorch/translation/run_translation.py -h | grep <span class="hljs-string">&quot;\\-optim&quot;</span>
         [--optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor}]`}}),wr=new y({}),vr=new _({props:{code:`training_args = TrainingArguments(per_device_train_batch_size=4, optim="adafactor", **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">4</span>, optim=<span class="hljs-string">&quot;adafactor&quot;</span>, **default_args)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),_r=new _({props:{code:`Time: 64.31
Samples/second: 7.96
GPU memory occupied: 12295 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 64.31
Samples/second: 7.96
GPU memory occupied: 12295 MB.`}}),yr=new _({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    optim="adafactor",
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>,
    gradient_accumulation_steps=<span class="hljs-number">4</span>,
    gradient_checkpointing=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    optim=<span class="hljs-string">&quot;adafactor&quot;</span>,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds)
result = trainer.train()
print_summary(result)`}}),br=new _({props:{code:`Time: 56.54
Samples/second: 9.06
GPU memory occupied: 4847 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 56.54
Samples/second: 9.06
GPU memory occupied: 4847 MB.`}}),$r=new y({}),$t=new _k({props:{$$slots:{default:[Ak]},$$scope:{ctx:Go}}}),kr=new _({props:{code:`import bitsandbytes as bnb
from torch import nn
from transformers.trainer_pt_utils import get_parameter_names

training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)

decay_parameters = get_parameter_names(model, [nn.LayerNorm])
decay_parameters = [name for name in decay_parameters if "bias" not in name]
optimizer_grouped_parameters = [
    {
        "params": [p for n, p in model.named_parameters() if n in decay_parameters],
        "weight_decay": training_args.weight_decay,
    },
    {
        "params": [p for n, p in model.named_parameters() if n not in decay_parameters],
        "weight_decay": 0.0,
    },
]

optimizer_kwargs = {
    "betas": (training_args.adam_beta1, training_args.adam_beta2),
    "eps": training_args.adam_epsilon,
}
optimizer_kwargs["lr"] = training_args.learning_rate
adam_bnb_optim = bnb.optim.Adam8bit(
    optimizer_grouped_parameters,
    betas=(training_args.adam_beta1, training_args.adam_beta2),
    eps=training_args.adam_epsilon,
    lr=training_args.learning_rate,
)`,highlighted:`<span class="hljs-keyword">import</span> bitsandbytes <span class="hljs-keyword">as</span> bnb
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> transformers.trainer_pt_utils <span class="hljs-keyword">import</span> get_parameter_names

training_args = TrainingArguments(per_device_train_batch_size=<span class="hljs-number">4</span>, **default_args)

decay_parameters = get_parameter_names(model, [nn.LayerNorm])
decay_parameters = [name <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> decay_parameters <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;bias&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> name]
optimizer_grouped_parameters = [
    {
        <span class="hljs-string">&quot;params&quot;</span>: [p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters() <span class="hljs-keyword">if</span> n <span class="hljs-keyword">in</span> decay_parameters],
        <span class="hljs-string">&quot;weight_decay&quot;</span>: training_args.weight_decay,
    },
    {
        <span class="hljs-string">&quot;params&quot;</span>: [p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> model.named_parameters() <span class="hljs-keyword">if</span> n <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> decay_parameters],
        <span class="hljs-string">&quot;weight_decay&quot;</span>: <span class="hljs-number">0.0</span>,
    },
]

optimizer_kwargs = {
    <span class="hljs-string">&quot;betas&quot;</span>: (training_args.adam_beta1, training_args.adam_beta2),
    <span class="hljs-string">&quot;eps&quot;</span>: training_args.adam_epsilon,
}
optimizer_kwargs[<span class="hljs-string">&quot;lr&quot;</span>] = training_args.learning_rate
adam_bnb_optim = bnb.optim.Adam8bit(
    optimizer_grouped_parameters,
    betas=(training_args.adam_beta1, training_args.adam_beta2),
    eps=training_args.adam_epsilon,
    lr=training_args.learning_rate,
)`}}),Pr=new _({props:{code:`trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))
result = trainer.train()
print_summary(result)`,highlighted:`trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, <span class="hljs-literal">None</span>))
result = trainer.train()
print_summary(result)`}}),Ar=new _({props:{code:`Time: 55.95
Samples/second: 9.15
GPU memory occupied: 13085 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 55.95
Samples/second: 9.15
GPU memory occupied: 13085 MB.`}}),Tr=new _({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))
result = trainer.train()
print_summary(result)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>,
    gradient_accumulation_steps=<span class="hljs-number">4</span>,
    gradient_checkpointing=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    **default_args,
)

trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, <span class="hljs-literal">None</span>))
result = trainer.train()
print_summary(result)`}}),xr=new _({props:{code:`Time: 49.46
Samples/second: 10.35
GPU memory occupied: 5363 MB.`,highlighted:`<span class="hljs-keyword">Time:</span> 49.46
Samples/second: 10.35
GPU memory occupied: 5363 MB.`}}),Gr=new y({}),zr=new y({}),Ur=new _({props:{code:`training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    fp16=True,
    **default_args,
)`,highlighted:`training_args = TrainingArguments(
    per_device_train_batch_size=<span class="hljs-number">1</span>,
    gradient_accumulation_steps=<span class="hljs-number">4</span>,
    gradient_checkpointing=<span class="hljs-literal">True</span>,
    fp16=<span class="hljs-literal">True</span>,
    **default_args,
)`}}),jr=new _({props:{code:`from accelerate import Accelerator
from torch.utils.data.dataloader import DataLoader

dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)

if training_args.gradient_checkpointing:
    model.gradient_checkpointing_enable()

accelerator = Accelerator(fp16=training_args.fp16)
model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)

model.train()
for step, batch in enumerate(dataloader, start=1):
    loss = model(**batch).loss
    loss = loss / training_args.gradient_accumulation_steps
    accelerator.backward(loss)
    if step % training_args.gradient_accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator
<span class="hljs-keyword">from</span> torch.utils.data.dataloader <span class="hljs-keyword">import</span> DataLoader

dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)

<span class="hljs-keyword">if</span> training_args.gradient_checkpointing:
    model.gradient_checkpointing_enable()

accelerator = Accelerator(fp16=training_args.fp16)
model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)

model.train()
<span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader, start=<span class="hljs-number">1</span>):
    loss = model(**batch).loss
    loss = loss / training_args.gradient_accumulation_steps
    accelerator.backward(loss)
    <span class="hljs-keyword">if</span> step % training_args.gradient_accumulation_steps == <span class="hljs-number">0</span>:
        optimizer.step()
        optimizer.zero_grad()`}}),Nr=new _({props:{code:"print_gpu_utilization()",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>print_gpu_utilization()
GPU memory occupied: <span class="hljs-number">5363</span> MB.`}}),qr=new y({}),Cr=new y({}),Br=new _({props:{code:'TrainingArguments(deepspeed="/path/to/ds_config.json")',highlighted:'TrainingArguments(deepspeed=<span class="hljs-string">&quot;/path/to/ds_config.json&quot;</span>)'}}),Hr=new y({}),Rr=new y({}),Vr=new y({}),Qr=new y({}),Jr=new y({}),lo=new y({}),po=new y({}),mo=new _({props:{code:`TrainingArguments(torchdynamo="eager")      #enable eager model GPU. No performance boost
TrainingArguments(torchdynamo="nvfuser")    #enable nvfuser
TrainingArguments(torchdynamo="fx2trt")     #enable tensorRT fp32
TrainingArguments(torchdynamo="fx2trt-f16") #enable tensorRT fp16`,highlighted:`TrainingArguments(<span class="hljs-attribute">torchdynamo</span>=<span class="hljs-string">&quot;eager&quot;</span>)      #<span class="hljs-built_in">enable</span> eager model GPU. <span class="hljs-literal">No</span> performance boost
TrainingArguments(<span class="hljs-attribute">torchdynamo</span>=<span class="hljs-string">&quot;nvfuser&quot;</span>)    #<span class="hljs-built_in">enable</span> nvfuser
TrainingArguments(<span class="hljs-attribute">torchdynamo</span>=<span class="hljs-string">&quot;fx2trt&quot;</span>)     #<span class="hljs-built_in">enable</span> tensorRT fp32
TrainingArguments(<span class="hljs-attribute">torchdynamo</span>=<span class="hljs-string">&quot;fx2trt-f16&quot;</span>) #<span class="hljs-built_in">enable</span> tensorRT fp16`}}),go=new y({}),yo=new y({}),bo=new y({}),$o=new _({props:{code:`model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)`,highlighted:`model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`}}),Eo=new _({props:{code:`max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)`,highlighted:`max_memory_mapping = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;1GB&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;2GB&quot;</span>}
model_name = <span class="hljs-string">&quot;bigscience/bloom-3b&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, max_memory=max_memory_mapping
)`}}),ko=new y({}),{c(){b=r("meta"),T=h(),E=r("h1"),j=r("a"),jn=r("span"),d(ra.$$.fragment),Du=h(),Sn=r("span"),Nu=n("Efficient Training on a Single GPU"),$h=h(),He=r("p"),Mu=n("This guide focuses on training large models efficiently on a single GPU. These approaches are still valid if you have access to a machine with multiple GPUs but you will also have access to additional methods outlined in the "),zo=r("a"),qu=n("multi-GPU section"),Cu=n("."),Eh=h(),q=r("p"),Ou=n("In this section we have a look at a few tricks to reduce the memory footprint and speed up training for large models and how they are integrated in the "),Uo=r("a"),Lu=n("Trainer"),Bu=n(" and "),oa=r("a"),Fu=n("\u{1F917} Accelerate"),Wu=n(". Each method can improve speed or memory usage which is summarized in the table below:"),kh=h(),Re=r("table"),In=r("thead"),ie=r("tr"),jo=r("th"),Hu=n("Method"),Ru=h(),So=r("th"),Vu=n("Speed"),Yu=h(),Io=r("th"),Xu=n("Memory"),Zu=h(),k=r("tbody"),ne=r("tr"),Do=r("td"),Qu=n("Gradient accumulation"),Ju=h(),No=r("td"),Ku=n("No"),eg=h(),Mo=r("td"),tg=n("Yes"),ag=h(),le=r("tr"),qo=r("td"),rg=n("Gradient checkpointing"),og=h(),Co=r("td"),sg=n("No"),ig=h(),Oo=r("td"),ng=n("Yes"),lg=h(),pe=r("tr"),Lo=r("td"),pg=n("Mixed precision training"),hg=h(),Bo=r("td"),mg=n("Yes"),fg=h(),Fo=r("td"),dg=n("(No)"),cg=h(),he=r("tr"),Wo=r("td"),ug=n("Batch size"),gg=h(),Ho=r("td"),wg=n("Yes"),vg=h(),Ro=r("td"),_g=n("Yes"),yg=h(),me=r("tr"),Vo=r("td"),bg=n("Optimizer choice"),$g=h(),Yo=r("td"),Eg=n("Yes"),kg=h(),Xo=r("td"),Pg=n("Yes"),Ag=h(),fe=r("tr"),Zo=r("td"),Tg=n("DataLoader"),xg=h(),Qo=r("td"),Gg=n("Yes"),zg=h(),Jo=r("td"),Ug=n("No"),jg=h(),de=r("tr"),Ko=r("td"),Sg=n("DeepSpeed Zero"),Ig=h(),es=r("td"),Dg=n("No"),Ng=h(),ts=r("td"),Mg=n("Yes"),Ph=h(),as=r("p"),qg=n("A bracket means that it might not be strictly the case but is usually either not a main concern or negligable. Before we start make sure you have installed the following libraries:"),Ah=h(),d(sa.$$.fragment),Th=h(),C=r("p"),Cg=n("The "),Dn=r("code"),Og=n("nvidia-ml-py3"),Lg=n(" library allows us to monitor the memory usage of the models from within Python. You might be familiar with the "),Nn=r("code"),Bg=n("nvidia-smi"),Fg=n(" command in the terminal - this library allows to access the same information in Python directly."),xh=h(),Ve=r("p"),Wg=n("Then we create some dummy data. We create random token IDs between 100 and 30000 and binary labels for a classifier. In total we get 512 sequences each with length 512 and store them in a "),ia=r("a"),Hg=n("Dataset"),Rg=n(" with PyTorch format."),Gh=h(),d(na.$$.fragment),zh=h(),Ye=r("p"),Vg=n("We want to print some summary statistics for the GPU utilization and the training run with the "),rs=r("a"),Yg=n("Trainer"),Xg=n(". We setup a two helper functions to do just that:"),Uh=h(),d(la.$$.fragment),jh=h(),os=r("p"),Zg=n("Let\u2019s verify that we start with a free GPU memory:"),Sh=h(),d(pa.$$.fragment),Ih=h(),ss=r("p"),Qg=n("That looks good: the GPU memory is not occupied as we would expect before we load any models. If that\u2019s not the case on your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by the user. When a model is loaded to the GPU also the kernels are loaded which can take up 1-2GB of memory. To see how much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well."),Dh=h(),d(ha.$$.fragment),Nh=h(),is=r("p"),Jg=n("We see that the kernels alone take up 1.3GB of GPU memory. Now let\u2019s see how much space the model uses."),Mh=h(),ce=r("h2"),Xe=r("a"),Mn=r("span"),d(ma.$$.fragment),Kg=h(),qn=r("span"),ew=n("Load Model"),qh=h(),Ze=r("p"),tw=n("First, we load the "),Cn=r("code"),aw=n("bert-large-uncased"),rw=n(" model. We load the model weights directly to the GPU so that we can check how much space just weights use."),Ch=h(),d(fa.$$.fragment),Oh=h(),Qe=r("p"),ow=n("We can see that the model weights alone take up 1.3 GB of the GPU memory. The exact number depends on the specific GPU you are using. Note that on newer GPUs a model can sometimes take up more space since the weights are loaded in an optimized fashion that speeds up the usage of the model. Now we can also quickly check if we get the same result as with "),On=r("code"),sw=n("nvidia-smi"),iw=n(" CLI:"),Lh=h(),d(da.$$.fragment),Bh=h(),d(ca.$$.fragment),Fh=h(),ns=r("p"),nw=n("We get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can start training the model and see how the GPU memory consumption changes. First, we set up a few standard training arguments that we will use across all our experiments:"),Wh=h(),d(ua.$$.fragment),Hh=h(),d(Je.$$.fragment),Rh=h(),ue=r("h2"),Ke=r("a"),Ln=r("span"),d(ga.$$.fragment),lw=h(),Bn=r("span"),pw=n("Vanilla Training"),Vh=h(),et=r("p"),hw=n("As a first experiment we will use the "),ls=r("a"),mw=n("Trainer"),fw=n(" and train the model without any further modifications and a batch size of 4:"),Yh=h(),d(wa.$$.fragment),Xh=h(),d(va.$$.fragment),Zh=h(),ps=r("p"),dw=n("We see that already a relatively small batch size almost fills up our GPU\u2019s entire memory. However, a larger batch size can often result in faster model convergence or better end performance. So ideally we want to tune the batch size to our model\u2019s needs and not to the GPU limitations. What\u2019s interesting is that we use much more memory than the size of the model. To understand a bit better why this is the case let\u2019s have look at a model\u2019s operations and memory needs."),Qh=h(),ge=r("h2"),tt=r("a"),Fn=r("span"),d(_a.$$.fragment),cw=h(),Wn=r("span"),uw=n("Anatomy of Model's Operations"),Jh=h(),hs=r("p"),gw=n("Transformers architecture includes 3 main groups of operations grouped below by compute-intensity."),Kh=h(),O=r("ol"),ya=r("li"),Hn=r("p"),Rn=r("strong"),ww=n("Tensor Contractions"),vw=h(),ba=r("p"),_w=n("Linear layers and components of Multi-Head Attention all do batched "),Vn=r("strong"),yw=n("matrix-matrix multiplications"),bw=n(". These operations are the most compute-intensive part of training a transformer."),$w=h(),$a=r("li"),Yn=r("p"),Xn=r("strong"),Ew=n("Statistical Normalizations"),kw=h(),Ea=r("p"),Pw=n("Softmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more "),Zn=r("strong"),Aw=n("reduction operations"),Tw=n(", the result of which is then applied via a map."),xw=h(),ka=r("li"),Qn=r("p"),Jn=r("strong"),Gw=n("Element-wise Operators"),zw=h(),Pa=r("p"),Uw=n("These are the remaining operators: "),Kn=r("strong"),jw=n("biases, dropout, activations, and residual connections"),Sw=n(". These are the least compute-intensive operations."),em=h(),ms=r("p"),Iw=n("This knowledge can be helpful to know when analyzing performance bottlenecks."),tm=h(),Aa=r("p"),Dw=n("This summary is derived from "),Ta=r("a"),Nw=n("Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020"),am=h(),we=r("h2"),at=r("a"),el=r("span"),d(xa.$$.fragment),Mw=h(),tl=r("span"),qw=n("Anatomy of Model's Memory"),rm=n(`

We've seen that training the model uses much more memory than just putting the model on the GPU. This is because there are many components during training that use GPU memory. The components on GPU memory are the following:
1. model weights
2. optimizer states
3. gradients
4. forward activations saved for gradient computation
5. temporary buffers
6. functionality-specific memory
`),fs=r("p"),Cw=n("A typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory. For inference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per model parameter for mixed precision inference, plus activation memory."),om=h(),ds=r("p"),Ow=n("Let\u2019s look at the details."),sm=h(),cs=r("p"),al=r("strong"),Lw=n("Model Weights:"),im=h(),rt=r("ul"),rl=r("li"),Bw=n("4 bytes * number of parameters for fp32 training"),Fw=h(),ol=r("li"),Ww=n("6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16 in memory)"),nm=h(),us=r("p"),sl=r("strong"),Hw=n("Optimizer States:"),lm=h(),L=r("ul"),il=r("li"),Rw=n("8 bytes * number of parameters for normal AdamW (maintains 2 states)"),Vw=h(),gs=r("li"),Yw=n("2 bytes * number of parameters for 8-bit AdamW optimizers like "),Ga=r("a"),Xw=n("bitsandbytes"),Zw=h(),nl=r("li"),Qw=n("4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state)"),pm=h(),ws=r("p"),ll=r("strong"),Jw=n("Gradients"),hm=h(),vs=r("ul"),pl=r("li"),Kw=n("4 bytes * number of parameters for either fp32 or mixed precision training (gradients are always kept in fp32)"),mm=h(),_s=r("p"),hl=r("strong"),ev=n("Forward Activations"),fm=h(),ys=r("ul"),ml=r("li"),tv=n("size depends on many factors, the key ones being sequence length, hidden size and batch size."),dm=h(),bs=r("p"),av=n("There are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation."),cm=h(),$s=r("p"),fl=r("strong"),rv=n("Temporary Memory"),um=h(),Es=r("p"),ov=n("Additionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it\u2019s crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed."),gm=h(),ks=r("p"),dl=r("strong"),sv=n("Functionality-specific memory"),wm=h(),Ps=r("p"),iv=n("Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs."),vm=h(),As=r("p"),ot=r("strong"),cl=r("code"),nv=n("forward"),lv=n(" vs "),ul=r("code"),pv=n("backward"),hv=n(" Execution Speed"),_m=h(),Ts=r("p"),mv=n("For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it\u2019s typical for an activation to have to read more data in the backward than in the forward (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput)."),ym=h(),xs=r("p"),fv=n("So there are potentially a few places where we could save GPU memory or speed up operations. Let\u2019s start with a simple optimization: choosing the right batch size."),bm=h(),ve=r("h2"),st=r("a"),gl=r("span"),d(za.$$.fragment),dv=h(),wl=r("span"),cv=n("Batch sizes"),$m=h(),Gs=r("p"),uv=n("One gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number, which typically starts at 8, but can be much higher as well. That number varies a lot depending on the specific hardware being used and the dtype of the model."),Em=h(),B=r("p"),gv=n("For example for fully connected layers (which correspond to GEMMs), NVIDIA provides recommendations for "),Ua=r("a"),wv=n("input/output neuron counts"),vv=n(" and "),ja=r("a"),_v=n("batch size"),yv=n("."),km=h(),Sa=r("p"),Ia=r("a"),bv=n("Tensor Core Requirements"),$v=n(" define the multiplier based on the dtype and the hardware. For example, for fp16 a multiple of 8 is recommended, but on A100 it\u2019s 64!"),Pm=h(),it=r("p"),Ev=n("For parameters that are small, there is also "),Da=r("a"),kv=n("Dimension Quantization Effects"),Pv=n(" to consider, this is where tiling happens and the right multiplier can have a significant speedup."),Am=h(),_e=r("h2"),nt=r("a"),vl=r("span"),d(Na.$$.fragment),Av=h(),_l=r("span"),Tv=n("Gradient Accumulation"),Tm=h(),zs=r("p"),xv=n("The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model\u2019s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU\u2019s memory. In turn, however, the added forward and backward passes can slow down the training a bit."),xm=h(),S=r("p"),Gv=n("We can use gradient accumulation in the "),Us=r("a"),zv=n("Trainer"),Uv=n(" by simply adding the "),yl=r("code"),jv=n("gradient_accumulation_steps"),Sv=n(" argument to "),js=r("a"),Iv=n("TrainingArguments"),Dv=n(". Let\u2019s see how it impacts the models memory footprint:"),Gm=h(),d(Ma.$$.fragment),zm=h(),d(qa.$$.fragment),Um=h(),x=r("p"),Nv=n("We can see that the memory footprint was dramatically reduced at the cost of being only slightly slower than the vanilla run. Of course, this would change as you increase the number of accumulation steps. In general you would want to max out the GPU usage as much as possible. So in our case, the batch_size of 4 was already pretty close to the GPU\u2019s limit. If we wanted to train with a batch size of 64 we should not use "),bl=r("code"),Mv=n("per_device_train_batch_size=1"),qv=n(" and "),$l=r("code"),Cv=n("gradient_accumulation_steps=64"),Ov=n(" but instead "),El=r("code"),Lv=n("per_device_train_batch_size=4"),Bv=n(" and "),kl=r("code"),Fv=n("gradient_accumulation_steps=16"),Wv=n(" which has the same effective batch size while making better use of the available GPU resources."),jm=h(),F=r("p"),Hv=n("For more details see the benchmarks for "),Ca=r("a"),Rv=n("RTX-3090"),Vv=n(`
and `),Oa=r("a"),Yv=n("A100"),Xv=n("."),Sm=h(),Ss=r("p"),Zv=n("Next we have a look at another trick to save a little bit more GPU memory called gradient checkpointing."),Im=h(),ye=r("h2"),lt=r("a"),Pl=r("span"),d(La.$$.fragment),Qv=h(),Al=r("span"),Jv=n("Gradient Checkpointing"),Dm=h(),Is=r("p"),Kv=n("Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training."),Nm=h(),pt=r("p"),e_=n("Gradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See "),Ba=r("a"),t_=n("this great article"),a_=n(" explaining the ideas behind gradient checkpointing."),Mm=h(),W=r("p"),r_=n("To enable gradient checkpointing in the "),Ds=r("a"),o_=n("Trainer"),s_=n(" we only need ot pass it as a flag to the "),Ns=r("a"),i_=n("TrainingArguments"),n_=n(". Everything else is handled under the hood:"),qm=h(),d(Fa.$$.fragment),Cm=h(),d(Wa.$$.fragment),Om=h(),Ms=r("p"),l_=n("We can see that this saved some more memory but at the same time training became a bit slower. A general rule of thumb is that gradient checkpointing slows down training by about 20%. Let\u2019s have a look at another method with which we can regain some speed: mixed precision training."),Lm=h(),be=r("h2"),ht=r("a"),Tl=r("span"),d(Ha.$$.fragment),p_=h(),xl=r("span"),h_=n("Floating Data Types"),Bm=h(),qs=r("p"),m_=n("The idea of mixed precision training is that no all variables need to be stored in full (32-bit) floating point precision. If we can reduce the precision the variales and their computations are faster. Here are the commonly used floating point data types choice of which impacts both memory usage and throughput:"),Fm=h(),I=r("ul"),Ra=r("li"),f_=n("fp32 ("),Gl=r("code"),d_=n("float32"),c_=n(")"),u_=h(),Va=r("li"),g_=n("fp16 ("),zl=r("code"),w_=n("float16"),v_=n(")"),__=h(),Ya=r("li"),y_=n("bf16 ("),Ul=r("code"),b_=n("bfloat16"),$_=n(")"),E_=h(),jl=r("li"),k_=n("tf32 (CUDA internal data type)"),Wm=h(),Cs=r("p"),P_=n("Here is a diagram that shows how these data types correlate to each other."),Hm=h(),$e=r("p"),Os=r("img"),A_=n(`
(source: `),Xa=r("a"),T_=n("NVIDIA Blog"),x_=n(")"),Rm=h(),Ls=r("p"),G_=n("While fp16 and fp32 have been around for quite some time, bf16 and tf32 are only available on the Ampere architecture GPUS and TPUs support bf16 as well. Let\u2019s start with the most commonly used method which is FP16 training/"),Vm=h(),Ee=r("h3"),mt=r("a"),Sl=r("span"),d(Za.$$.fragment),z_=h(),Il=r("span"),U_=n("FP16 Training"),Ym=h(),H=r("p"),j_=n("The idea of mixed precision training is that no all variables need to be stored in full (32-bit) floating point precision. If we can reduce the precision the variales and their computations are faster. The main advantage comes from saving the activations in half (16-bit) precision. Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. Since the model is present on the GPU in both 16-bit and 32-bit precision this can use more GPU memory (1.5x the original model is on the GPU), especially for small batch sizes. Since some computations are performed in full and some in half precision this approach is also called mixed precision training. Enabling mixed precision training is also just a matter of setting the "),Dl=r("code"),S_=n("fp16"),I_=n(" flag to "),Nl=r("code"),D_=n("True"),N_=n(":"),Xm=h(),d(Qa.$$.fragment),Zm=h(),d(Ja.$$.fragment),Qm=h(),Bs=r("p"),M_=n("We can see that this is almost twice as fast as the vanilla training. Let\u2019s add it to the mix of the previous methods:"),Jm=h(),d(Ka.$$.fragment),Km=h(),d(er.$$.fragment),ef=h(),Fs=r("p"),q_=n("We can see that with these tweaks we use about half the GPU memory as at the beginning while also being slightly faster."),tf=h(),ke=r("h3"),ft=r("a"),Ml=r("span"),d(tr.$$.fragment),C_=h(),ql=r("span"),O_=n("BF16"),af=n("\n\nIf you have access to a Ampere or newer hardware you can use bf16 for your training and evaluation. While bf16 has a worse precision than fp16, it has a much much bigger dynamic range. Therefore, if in the past you were experiencing overflow issues while training the model, bf16 will prevent this from happening most of the time. Remember that in fp16 the biggest number you can have is `65535` and any number above that will overflow. A bf16 number can be as large as `3.39e+38` (!) which is about the same as fp32 - because both have 8-bits used for the numerical range.\n"),Ws=r("p"),L_=n("You can enable BF16 in the \u{1F917} Trainer with:"),rf=h(),d(ar.$$.fragment),of=h(),Pe=r("h3"),dt=r("a"),Cl=r("span"),d(rr.$$.fragment),B_=h(),Ol=r("span"),F_=n("TF32"),sf=n(`

The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total.
`),Hs=r("p"),W_=n("It\u2019s magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add this to your code:"),nf=h(),d(or.$$.fragment),lf=h(),Rs=r("p"),H_=n("When this is done CUDA will automatically switch to using tf32 instead of fp32 where it\u2019s possible. This, of course, assumes that the used GPU is from the Ampere series."),pf=h(),ct=r("p"),R_=n("Like all cases with reduced precision this may or may not be satisfactory for your needs, so you have to experiment and see. According to "),sr=r("a"),V_=n("NVIDIA research"),Y_=n(" the majority of machine learning training shouldn\u2019t be impacted and showed the same perplexity and convergence as the fp32 training."),hf=h(),Vs=r("p"),X_=n("If you\u2019re already using fp16 or bf16 mixed precision it may help with the throughput as well."),mf=h(),Ys=r("p"),Z_=n("You can enable this mode in the \u{1F917} Trainer with:"),ff=h(),d(ir.$$.fragment),df=h(),Xs=r("p"),Q_=n("By default the PyTorch default is used."),cf=h(),R=r("p"),J_=n("Note: tf32 mode is internal to CUDA and can\u2019t be accessed directly via "),Ll=r("code"),K_=n("tensor.to(dtype=torch.tf32)"),ey=n(" as "),Bl=r("code"),ty=n("torch.tf32"),ay=n(" doesn\u2019t exist."),uf=h(),ut=r("p"),ry=n("Note: you need "),Fl=r("code"),oy=n("torch>=1.7"),sy=n(" to enjoy this feature."),gf=h(),V=r("p"),iy=n(`You can also see a variety of benchmarks on tf32 vs other precisions:
`),nr=r("a"),ny=n("RTX-3090"),ly=n(` and
`),lr=r("a"),py=n("A100"),hy=n("."),wf=h(),Zs=r("p"),my=n("We\u2019ve now seen how we can change the floating types to increase throughput, but we are not done, yet! There is another area where we can save GPU memory: the optimizer."),vf=h(),Ae=r("h2"),gt=r("a"),Wl=r("span"),d(pr.$$.fragment),fy=h(),Hl=r("span"),dy=n("Optimizer"),_f=h(),Qs=r("p"),cy=n("The most common optimizer used to train transformer model is Adam or AdamW (Adam with weight decay). Adam achieves good convergence by storing the rolling average of the previous gradients which, however, adds an additional memory footprint of the order of the number of model parameters. One remedy to this is to use an alternative optimizer such as Adafactor, which works well for some models but often it has instability issues."),yf=h(),wt=r("p"),uy=n("HF Trainer integrates a variety of optimisers that can be used out of box. To activate the desired optimizer simply pass the "),Rl=r("code"),gy=n("--optim"),wy=n(" flag to the command line."),bf=h(),Js=r("p"),vy=n("To see which optimizers are currently supported:"),$f=h(),d(hr.$$.fragment),Ef=h(),Y=r("p"),_y=n("For example, if you have "),mr=r("a"),yy=n("NVIDIA/apex"),by=n(" installed "),Vl=r("code"),$y=n("--optim adamw_apex_fused"),Ey=n(" will give you the fastest training experience among all supported AdamW optimizers."),kf=h(),X=r("p"),ky=n("On the other hand "),fr=r("a"),Py=n("8bit BNB optimizer"),Ay=n(" can save 3/4 of memory normally used by a typical AdamW optimizer if it is configured to quantize all optimizer states, but in some situations only some optimizer states are quintized and then more memory is used. XXX: update once  "),dr=r("a"),Ty=n("https://github.com/huggingface/transformers/pull/15622"),xy=n(" is merged."),Pf=h(),vt=r("p"),Gy=n("Let\u2019s get a feel for the numbers and use for example use a 3B-parameter model, like "),Yl=r("code"),zy=n("t5-3b"),Uy=n(". Note that since a Gigabyte correpsonds to a billion bytes we can simply multiply the parameters (in billions) with the number of necessary bytes per parameter to get Gigabytes of GPU memory usage:"),Af=h(),Z=r("ul"),cr=r("li"),jy=n("A standard AdamW uses 8 bytes for each parameter, here the optimizer will need ("),Xl=r("code"),Sy=n("8*3"),Iy=n(") 24GB of GPU memory."),Dy=h(),ur=r("li"),Ny=n("Adafactor uses slightly more than 4 bytes, so ("),Zl=r("code"),My=n("4*3"),qy=n(") 12GB and then some extra."),Cy=h(),gr=r("li"),Oy=n("8bit BNB quantized optimizer will use only ("),Ql=r("code"),Ly=n("2*3"),By=n(") 6GB if all optimizer states are quantized."),Tf=h(),Ks=r("p"),Fy=n("Let\u2019s have a look at Adafactor first."),xf=h(),Te=r("h3"),_t=r("a"),Jl=r("span"),d(wr.$$.fragment),Wy=h(),Kl=r("span"),Hy=n("Adafactor"),Gf=h(),yt=r("p"),Ry=n("Instead of keeping the rolling average for each element in the weight matrices Adafactor only stores aggregated information (row- and column-wise sums of the rolling averages) which reduces the footprint considerably. One downside of Adafactor is that in some instances convergence can be slower than Adam\u2019s so some experimentation is advised here. We can use Adafactor simply by setting "),ep=r("code"),Vy=n('optim="adafactor"'),Yy=n(":"),zf=h(),d(vr.$$.fragment),Uf=h(),d(_r.$$.fragment),jf=h(),ei=r("p"),Xy=n("We can see that this saves a few more GB on the GPU. Let\u2019s see how it looks when we add it to the other methods we introduced earlier:"),Sf=h(),d(yr.$$.fragment),If=h(),d(br.$$.fragment),Df=h(),ti=r("p"),Zy=n("We went from 15 GB memory usage to 5 GB - a 3x improvement while maintaining the throughput! However, as mentioned before, the convergence of Adafactor can be worse than Adam. There is an alternative to Adafactor called 8-bit Adam that takes a slightly different approach."),Nf=h(),xe=r("h3"),bt=r("a"),tp=r("span"),d($r.$$.fragment),Qy=h(),ap=r("span"),Jy=n("8-bit Adam"),Mf=h(),ai=r("p"),Ky=n("Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind FP16 training where using variables with lower precision saves memory."),qf=h(),G=r("p"),eb=n("In contrast to the previous approaches is this one not integrated into the "),ri=r("a"),tb=n("Trainer"),ab=n(" as a simple flag. We need to install the 8-bit optimizer and then pass it as a custom optimizer to the "),oi=r("a"),rb=n("Trainer"),ob=n(". Follow the installation guide in the Github "),Er=r("a"),sb=n("repo"),ib=n(" to install the "),rp=r("code"),nb=n("bitsandbytes"),lb=n(" library that implements the 8-bit Adam optimizer."),Cf=h(),si=r("p"),pb=n("Once installed, we just need to initialize the the optimizer. Although this looks like a considerable amount of work it actually just involves two steps: first we need to group the model\u2019s parameters into two groups where to one group we apply weight decay and to the other we don\u2019t. Usually, biases and layer norm parameters are not weight decayed. Then in a second step we just do some argument housekeeping to use the same parameters as the previously used AdamW optimizer."),Of=h(),d($t.$$.fragment),Lf=h(),d(kr.$$.fragment),Bf=h(),Et=r("p"),hb=n("We can now pass the custom optimizer as an argument to the "),op=r("code"),mb=n("Trainer"),fb=n(":"),Ff=h(),d(Pr.$$.fragment),Wf=h(),d(Ar.$$.fragment),Hf=h(),ii=r("p"),db=n("We can see that we get a similar memory improvement as with Adafactor while keeping the full rolling average of the gradients. Let\u2019s repeat the experiment with the full settings:"),Rf=h(),d(Tr.$$.fragment),Vf=h(),d(xr.$$.fragment),Yf=h(),ni=r("p"),cb=n("Again, we get about a 3x memory improvement and even slightly higher throughput as using Adafactor. So we have seen how we can optimize the memory footprint of large models. The following plot summarizes all our experiments:"),Xf=h(),li=r("p"),pi=r("img"),Zf=h(),Ge=r("h3"),kt=r("a"),sp=r("span"),d(Gr.$$.fragment),ub=h(),ip=r("span"),np=r("code"),gb=n("_multi_tensor"),Qf=n(`

pytorch-nightly introduced \`torch.optim._multi_tensor\` which should significantly speed up the optimizers for situations with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner and don't mind using the bleed-edge, see: https://github.com/huggingface/transformers/issues/9965
`),ze=r("h2"),Pt=r("a"),lp=r("span"),d(zr.$$.fragment),wb=h(),pp=r("span"),vb=n("Using \u{1F917} Accelerate"),Jf=h(),Q=r("p"),_b=n("So far we have used the "),hi=r("a"),yb=n("Trainer"),bb=n(" to run the experiments but a more flexible alternative to that approach is to use \u{1F917} Accelerate. With \u{1F917} Accelerate you have full control over the training loop and can essentially write the loop in pure PyTorch with some minor modifications. In turn it allows you to easily scale across different infrastructures such as CPUs, GPUs, TPUs, or distributed multi-GPU setups without changing any code. Let\u2019s see what it takes to implement all of the above tweaks in \u{1F917} Accelerate. We can still use the "),mi=r("a"),$b=n("TrainingArguments"),Eb=n(" to wrap the training settings:"),Kf=h(),d(Ur.$$.fragment),ed=h(),fi=r("p"),kb=n("The full example training loop with \u{1F917} Accelerate is only a handful of lines of code long:"),td=h(),d(jr.$$.fragment),ad=h(),P=r("p"),Pb=n("First we wrap the dataset in a "),Sr=r("a"),hp=r("code"),Ab=n("DataLoader"),Tb=n(". Then we can enable gradient checkpointing by calling the model\u2019s "),di=r("a"),xb=n("gradient_checkpointing_enable()"),Gb=n(" method. When we initialize the "),Ir=r("a"),mp=r("code"),zb=n("Accelerator"),Ub=n(" we can specifiy if we want to use mixed precision training and it will take care of it for us in the "),fp=r("code"),jb=n("prepare"),Sb=n(" call. During the "),Dr=r("a"),dp=r("code"),Ib=n("prepare"),Db=n(" call the dataloader will also be distributed across workers should we use multiple GPUs. We use the same 8-bit optimizer from the earlier experiments."),rd=h(),At=r("p"),Nb=n("Finally, we can write the main training loop. Note that the "),cp=r("code"),Mb=n("backward"),qb=n(" call is handled by \u{1F917} Accelerate. We can also see how gradient accumulation works: we normalize the loss so we get the average at the end of accumulation and once we have enough steps we run the optimization. Now the question is: does this use the same amount of memory as the previous steps? Let\u2019s check:"),od=h(),d(Nr.$$.fragment),sd=h(),Tt=r("p"),Cb=n("Indeed it does. Implementing these optimization techniques with \u{1F917} Accelerate only takes a handful of lines of code and comes with the benefit of more flexiblity in the training loop. For a full documentation of all features have a look at the "),Mr=r("a"),Ob=n("Accelerate documentation"),Lb=n("."),id=h(),Ue=r("h2"),xt=r("a"),up=r("span"),d(qr.$$.fragment),Bb=h(),gp=r("span"),Fb=n("DataLoader"),nd=h(),ci=r("p"),Wb=n("One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default everything happens in the main process and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization."),ld=h(),Gt=r("ul"),ui=r("li"),wp=r("code"),Hb=n("DataLoader(pin_memory=True, ...)"),Rb=n(" which ensures that the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory."),Vb=h(),gi=r("li"),vp=r("code"),Yb=n("DataLoader(num_workers=4, ...)"),Xb=n(" - spawn several workers to pre-load data faster - during training watch the GPU utilization stats and if it\u2019s far from 100% experiment with raising the number of workers. Of course, the problem could be elsewhere so a very big number of workers won\u2019t necessarily lead to a better performance."),pd=h(),je=r("h2"),zt=r("a"),_p=r("span"),d(Cr.$$.fragment),Zb=h(),yp=r("span"),Qb=n("DeepSpeed ZeRO"),hd=h(),Ut=r("p"),Jb=n("The in-depth details on how to use Deepspeed can be found "),wi=r("a"),Kb=n("here"),e1=n("."),md=h(),vi=r("p"),t1=n("First, a quick decision tree:"),fd=h(),jt=r("ol"),bp=r("li"),a1=n("Model fits onto a single GPU and you have enough space to fit a small batch size - you don\u2019t need to use Deepspeed as it\u2019ll only slow things down in this use case."),r1=h(),$p=r("li"),o1=n("Model doesn\u2019t fit onto a single GPU or you can\u2019t fit a small batch - use DeepSpeed ZeRO + CPU Offload and for much larger models NVMe Offload."),dd=h(),St=r("p"),s1=n("Now if the decision tree suggested you use DeepSpeed first you need to "),_i=r("a"),i1=n("install it"),n1=n(", then follow one of the following guides to create a configuration file and launch DeepSpeed."),cd=h(),yi=r("p"),l1=n("Activation:"),ud=h(),D=r("ul"),Ep=r("li"),Or=r("p"),p1=n("HF Trainer-based examples: see this "),bi=r("a"),h1=n("guide"),m1=n("."),f1=h(),Lr=r("li"),kp=r("p"),d1=n("Custom HF Trainer-based program: Same as above, but pass:"),c1=h(),d(Br.$$.fragment),u1=h(),Pp=r("li"),Fr=r("p"),g1=n("Deployment in Notebooks: see this "),$i=r("a"),w1=n("guide"),v1=n("."),_1=h(),Ap=r("li"),Se=r("p"),y1=n("Custom training loop: This is somewhat complex but you can study how this is implemented in "),Wr=r("a"),b1=n("HF Trainer"),$1=n(" - simply search for "),Tp=r("code"),E1=n("deepspeed"),k1=n(" in the code."),gd=h(),Ie=r("h2"),It=r("a"),xp=r("span"),d(Hr.$$.fragment),P1=h(),Gp=r("span"),A1=n("Choice of GPU"),wd=n(`

Sometimes, even when applying all the above tweaks the throughput on a given GPU might still not be good enough. One easy solution is to change the type of GPU. For example switching from let's say a K80 (which you typically get on Google Colab) to a fancier GPU such as the V100 or A100. Although they are more expensive they are usually more cost effective than cheaper GPUs due to their larger memory and faster architecture.
`),Ei=r("p"),T1=n("Now, let\u2019s take a step back and discuss what we should optimize for when scaling the training of large models."),vd=h(),De=r("h2"),Dt=r("a"),zp=r("span"),d(Rr.$$.fragment),x1=h(),Up=r("span"),G1=n("How to scale"),_d=h(),ki=r("p"),z1=n("When we train models there are a two aspects we want to optimize at the same time:"),yd=h(),Nt=r("ul"),jp=r("li"),U1=n("Data throughput/training time"),j1=h(),Sp=r("li"),S1=n("Model performance"),bd=h(),Pi=r("p"),I1=n("We have seen that each method changes the memory usage and throughput. In general we want to maximize the throughput (samples/second) to minimize the training cost. This is generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit. For example, as mentioned earlier, we only employ gradient accumulation when we want to use a batch size beyond the size of the GPU memory. If the desired batch size fits into memory then there is no reason to apply gradient accumulation which will only slow down training."),$d=h(),Ai=r("p"),D1=n("The second objective is model performance. Just because we can does not mean we should use a large batch size. As part of hyperparameter tuning you should determine which batch size yields the best result and then optimize the throughput accordingly."),Ed=h(),Ne=r("h2"),Mt=r("a"),Ip=r("span"),d(Vr.$$.fragment),N1=h(),Dp=r("span"),M1=n("Efficient Software Prebuilds"),kd=h(),qt=r("p"),q1=n("PyTorch\u2019s "),Yr=r("a"),C1=n("pip and conda builds"),O1=n(" come prebuit with the cuda toolkit which is enough to run PyTorch, but it is insufficient if you need to build cuda extensions."),Pd=h(),Ct=r("p"),L1=n("At times it may take an additional effort to pre-build some components, e.g., if you\u2019re using libraries like "),Np=r("code"),B1=n("apex"),F1=n(" that don\u2019t come pre-compiled. In other situations figuring out how to install the right cuda toolkit system-wide can be complicated. To address these users\u2019 needs PyTorch and NVIDIA release a new version of NGC docker container which already comes with everything prebuilt and you just need to install your programs on it and it will run out of the box."),Ad=h(),Ti=r("p"),W1=n("This approach is also useful if you want to tweak the pytorch source and/or make a new customized build."),Td=h(),J=r("p"),H1=n("To find the docker image version you want start "),Xr=r("a"),R1=n("here"),V1=n(", choose one of the latest monthly releases. Go into the release\u2019s notes for the desired release, check that the environment\u2019s components are matching your needs (including NVIDIA Driver requirements!) and then at the very top of that document go to the corresponding NGC page. If for some reason you get lost, here is "),Zr=r("a"),Y1=n("the index of all PyTorch NGC images"),X1=n("."),xd=h(),xi=r("p"),Z1=n("Next follow the instructions to download and deploy the docker image."),Gd=h(),Me=r("h2"),Ot=r("a"),Mp=r("span"),d(Qr.$$.fragment),Q1=h(),qp=r("span"),J1=n("Sparsity"),zd=h(),qe=r("h3"),Lt=r("a"),Cp=r("span"),d(Jr.$$.fragment),K1=h(),Op=r("span"),e2=n("Mixture of Experts"),Ud=h(),Gi=r("p"),t2=n(`Quite a few of the recent papers reported a 4-5x training speedup and a faster inference by integrating
Mixture of Experts (MoE) into the Transformer models.`),jd=h(),zi=r("p"),a2=n("Since it has been discovered that more parameters lead to better performance, this technique allows to increase the number of parameters by an order of magnitude without increasing training costs."),Sd=h(),Ui=r("p"),r2=n("In this approach every other FFN layer is replaced with a MoE Layer which consists of many experts, with a gated function that trains each expert in a balanced way depending on the input token\u2019s position in a sequence."),Id=h(),ji=r("p"),Si=r("img"),Dd=h(),Bt=r("p"),o2=n("(source: "),Kr=r("a"),s2=n("GLAM"),i2=n(")"),Nd=h(),Ii=r("p"),n2=n("You can find exhaustive details and comparison tables in the papers listed at the end of this section."),Md=h(),Di=r("p"),l2=n("The main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude larger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements."),qd=h(),Ni=r("p"),p2=n("There is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or hundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the memory requirements moderately as well."),Cd=h(),Mi=r("p"),h2=n("Most related papers and implementations are built around Tensorflow/TPUs:"),Od=h(),K=r("ul"),Lp=r("li"),eo=r("a"),m2=n("GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"),f2=h(),Bp=r("li"),to=r("a"),d2=n("Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"),c2=h(),Fp=r("li"),ao=r("a"),u2=n("GLaM: Generalist Language Model (GLaM)"),Ld=h(),$=r("p"),g2=n("And for Pytorch DeepSpeed has built one as well: "),ro=r("a"),w2=n("DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"),v2=n(", "),oo=r("a"),_2=n("Mixture of Experts"),y2=n(" - blog posts:  "),so=r("a"),b2=n("1"),$2=n(", "),io=r("a"),E2=n("2"),k2=n(" and specific deployment with large transformer-based natural language generation models: "),no=r("a"),P2=n("blog post"),A2=n(", "),qi=r("a"),T2=n("Megatron-Deepspeed branch"),x2=n("."),Bd=h(),Ce=r("h2"),Ft=r("a"),Wp=r("span"),d(lo.$$.fragment),G2=h(),Hp=r("span"),z2=n("Scaling beyond a single GPU"),Fd=h(),Ci=r("p"),U2=n("For some applications, such as pretraining large language models, applying all the approaches above might still not be fast enough. In this case you want to scale your experiment to several GPUs."),Wd=h(),Wt=r("p"),j2=n("Another use case for training on many GPUs is if the model does not fit on a single GPU with all the mentioned tricks. There are still more methods we can apply although life starts to get a bit more complicated. This usually involves some form of pipeline or tensor parallelism where the model itself is distributed across several GPUs. One can also make use of DeepSpeed which implements some of these parallelism strategies along with some more optimization to reduce the memory footprint such as partitioning the optimizer states. You can read more about this in the "),Oi=r("a"),S2=n("\u201CMulti-GPU training\u201D section"),I2=n("."),Hd=h(),Oe=r("h2"),Ht=r("a"),Rp=r("span"),d(po.$$.fragment),D2=h(),Vp=r("span"),N2=n("Inference with torchdynamo"),Rd=h(),Rt=r("p"),M2=n("TorchDynamo is a new tracer that uses Python\u2019s frame evaluation API to automatically create FX traces from existing PyTorch programs. After capturing the FX graph, different backends can be deployed to lower the graph to an optimized engine. One solution is using the "),ho=r("a"),q2=n("TensorRT"),C2=n(" or NVFuser as backend. You can choose one option below for performance boost."),Vd=h(),d(mo.$$.fragment),Yd=h(),Li=r("p"),O2=n("This feature involves 3 different libraries. To install them, please follow the instructions below:"),Xd=h(),ee=r("ul"),Yp=r("li"),fo=r("a"),L2=n("Torchdynamo installation"),B2=h(),Xp=r("li"),co=r("a"),F2=n("Functorch installation"),W2=h(),Zp=r("li"),uo=r("a"),H2=n("Torch-TensorRT(FX) installation"),Zd=h(),Le=r("h2"),Vt=r("a"),Qp=r("span"),d(go.$$.fragment),R2=h(),Bi=r("span"),Jp=r("code"),V2=n("bitsandbytes"),Y2=n(" integration for Int8 mixed-precision matrix decomposition"),Qd=h(),A=r("p"),X2=n("From the paper "),wo=r("a"),Kp=r("code"),Z2=n("LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale"),Q2=n(`, we support HuggingFace integration for all models in the Hub with a few lines of code.
The method reduce `),eh=r("code"),J2=n("nn.Linear"),K2=n(" size by 2 for "),th=r("code"),e3=n("float16"),t3=n(" and "),ah=r("code"),a3=n("bfloat16"),r3=n(" weights and by 4 for "),rh=r("code"),o3=n("float32"),s3=n(" weights, with close to no impact to the quality by operating on the outliers in half-precision."),Jd=h(),Fi=r("p"),Wi=r("img"),Kd=h(),te=r("p"),i3=n(`Int8 mixed-precision matrix decomposition works by separating a matrix multiplication into two streams: (1) a systematic feature outlier stream matrix multiplied in fp16 (0.01%), (2) a regular stream of int8 matrix multiplication (99.9%). With this method, int8 inference with no predictive degradation is possible for very large models.
For more details regarding the method, check out the `),vo=r("a"),n3=n("paper"),l3=n(" or our "),_o=r("a"),p3=n("blogpost about the integration"),h3=n("."),ec=h(),Hi=r("p"),Ri=r("img"),tc=h(),Yt=r("p"),m3=n(`Note, that you would require a GPU to run mixed-8bit models as the kernels have been compiled for GPUs only. Make sure that you have enough GPU memory to store the quarter (or half if your model weights are in half precision) of the model before using this feature.
Below are some notes to help you use this module, or follow the demos on `),Vi=r("a"),f3=n("Google colab"),d3=n("."),ac=h(),Be=r("h3"),Xt=r("a"),oh=r("span"),d(yo.$$.fragment),c3=h(),sh=r("span"),u3=n("Requirements"),rc=h(),ae=r("ul"),ih=r("li"),g3=n("Make sure you run that on NVIDIA GPUs that support 8-bit tensor cores (Turing, Ampere or newer architectures - e.g. T4, RTX20s RTX30s, A40-A100)."),w3=h(),Zt=r("li"),v3=n("Install the correct version of "),nh=r("code"),_3=n("bitsandbytes"),y3=n(` by running:
`),lh=r("code"),b3=n("pip install bitsandbytes>=0.31.5"),$3=h(),Qt=r("li"),E3=n("Install "),ph=r("code"),k3=n("accelerate"),P3=h(),hh=r("code"),A3=n("pip install accelerate>=0.12.0"),oc=h(),Fe=r("h3"),Jt=r("a"),mh=r("span"),d(bo.$$.fragment),T3=h(),fh=r("span"),x3=n("Running mixed-int8 models"),sc=h(),Yi=r("p"),G3=n("After installing the required libraries, the way to load your mixed 8-bit model is as follows:"),ic=h(),d($o.$$.fragment),nc=h(),re=r("p"),z3=n("The current implementation supports a multi-GPU setup when using "),dh=r("code"),U3=n("accelerate"),j3=n(". If you want to control the GPU memory you want to allocate for each GPU use the "),ch=r("code"),S3=n("max_memory"),I3=n(" argument as follows:"),lc=h(),d(Eo.$$.fragment),pc=h(),Xi=r("p"),D3=n("In this example, the first GPU will use 1GB of memory and the second 2GB."),hc=h(),We=r("h3"),Kt=r("a"),uh=r("span"),d(ko.$$.fragment),N3=h(),gh=r("span"),M3=n("Colab demos"),mc=h(),Zi=r("p"),q3=n(`With this method you can infer on models that were not possible to infer on a Google Colab before.
Check out the demo for running T5-11b (42GB in fp32)! Using 8-bit quantization on Google Colab:`),fc=h(),Qi=r("p"),Po=r("a"),Ji=r("img"),dc=h(),Ki=r("p"),C3=n("Or this demo for BLOOM-3B:"),cc=h(),en=r("p"),Ao=r("a"),tn=r("img"),this.h()},l(e){const i=Ek('[data-svelte="svelte-1phssyn"]',document.head);b=o(i,"META",{name:!0,content:!0}),i.forEach(t),T=m(e),E=o(e,"H1",{class:!0});var To=s(E);j=o(To,"A",{id:!0,class:!0,href:!0});var wh=s(j);jn=o(wh,"SPAN",{});var K3=s(jn);c(ra.$$.fragment,K3),K3.forEach(t),wh.forEach(t),Du=m(To),Sn=o(To,"SPAN",{});var e$=s(Sn);Nu=l(e$,"Efficient Training on a Single GPU"),e$.forEach(t),To.forEach(t),$h=m(e),He=o(e,"P",{});var gc=s(He);Mu=l(gc,"This guide focuses on training large models efficiently on a single GPU. These approaches are still valid if you have access to a machine with multiple GPUs but you will also have access to additional methods outlined in the "),zo=o(gc,"A",{href:!0});var t$=s(zo);qu=l(t$,"multi-GPU section"),t$.forEach(t),Cu=l(gc,"."),gc.forEach(t),Eh=m(e),q=o(e,"P",{});var an=s(q);Ou=l(an,"In this section we have a look at a few tricks to reduce the memory footprint and speed up training for large models and how they are integrated in the "),Uo=o(an,"A",{href:!0});var a$=s(Uo);Lu=l(a$,"Trainer"),a$.forEach(t),Bu=l(an," and "),oa=o(an,"A",{href:!0,rel:!0});var r$=s(oa);Fu=l(r$,"\u{1F917} Accelerate"),r$.forEach(t),Wu=l(an,". Each method can improve speed or memory usage which is summarized in the table below:"),an.forEach(t),kh=m(e),Re=o(e,"TABLE",{});var wc=s(Re);In=o(wc,"THEAD",{});var o$=s(In);ie=o(o$,"TR",{});var rn=s(ie);jo=o(rn,"TH",{align:!0});var s$=s(jo);Hu=l(s$,"Method"),s$.forEach(t),Ru=m(rn),So=o(rn,"TH",{align:!0});var i$=s(So);Vu=l(i$,"Speed"),i$.forEach(t),Yu=m(rn),Io=o(rn,"TH",{align:!0});var n$=s(Io);Xu=l(n$,"Memory"),n$.forEach(t),rn.forEach(t),o$.forEach(t),Zu=m(wc),k=o(wc,"TBODY",{});var z=s(k);ne=o(z,"TR",{});var on=s(ne);Do=o(on,"TD",{align:!0});var l$=s(Do);Qu=l(l$,"Gradient accumulation"),l$.forEach(t),Ju=m(on),No=o(on,"TD",{align:!0});var p$=s(No);Ku=l(p$,"No"),p$.forEach(t),eg=m(on),Mo=o(on,"TD",{align:!0});var h$=s(Mo);tg=l(h$,"Yes"),h$.forEach(t),on.forEach(t),ag=m(z),le=o(z,"TR",{});var sn=s(le);qo=o(sn,"TD",{align:!0});var m$=s(qo);rg=l(m$,"Gradient checkpointing"),m$.forEach(t),og=m(sn),Co=o(sn,"TD",{align:!0});var f$=s(Co);sg=l(f$,"No"),f$.forEach(t),ig=m(sn),Oo=o(sn,"TD",{align:!0});var d$=s(Oo);ng=l(d$,"Yes"),d$.forEach(t),sn.forEach(t),lg=m(z),pe=o(z,"TR",{});var nn=s(pe);Lo=o(nn,"TD",{align:!0});var c$=s(Lo);pg=l(c$,"Mixed precision training"),c$.forEach(t),hg=m(nn),Bo=o(nn,"TD",{align:!0});var u$=s(Bo);mg=l(u$,"Yes"),u$.forEach(t),fg=m(nn),Fo=o(nn,"TD",{align:!0});var g$=s(Fo);dg=l(g$,"(No)"),g$.forEach(t),nn.forEach(t),cg=m(z),he=o(z,"TR",{});var ln=s(he);Wo=o(ln,"TD",{align:!0});var w$=s(Wo);ug=l(w$,"Batch size"),w$.forEach(t),gg=m(ln),Ho=o(ln,"TD",{align:!0});var v$=s(Ho);wg=l(v$,"Yes"),v$.forEach(t),vg=m(ln),Ro=o(ln,"TD",{align:!0});var _$=s(Ro);_g=l(_$,"Yes"),_$.forEach(t),ln.forEach(t),yg=m(z),me=o(z,"TR",{});var pn=s(me);Vo=o(pn,"TD",{align:!0});var y$=s(Vo);bg=l(y$,"Optimizer choice"),y$.forEach(t),$g=m(pn),Yo=o(pn,"TD",{align:!0});var b$=s(Yo);Eg=l(b$,"Yes"),b$.forEach(t),kg=m(pn),Xo=o(pn,"TD",{align:!0});var $$=s(Xo);Pg=l($$,"Yes"),$$.forEach(t),pn.forEach(t),Ag=m(z),fe=o(z,"TR",{});var hn=s(fe);Zo=o(hn,"TD",{align:!0});var E$=s(Zo);Tg=l(E$,"DataLoader"),E$.forEach(t),xg=m(hn),Qo=o(hn,"TD",{align:!0});var k$=s(Qo);Gg=l(k$,"Yes"),k$.forEach(t),zg=m(hn),Jo=o(hn,"TD",{align:!0});var P$=s(Jo);Ug=l(P$,"No"),P$.forEach(t),hn.forEach(t),jg=m(z),de=o(z,"TR",{});var mn=s(de);Ko=o(mn,"TD",{align:!0});var A$=s(Ko);Sg=l(A$,"DeepSpeed Zero"),A$.forEach(t),Ig=m(mn),es=o(mn,"TD",{align:!0});var T$=s(es);Dg=l(T$,"No"),T$.forEach(t),Ng=m(mn),ts=o(mn,"TD",{align:!0});var x$=s(ts);Mg=l(x$,"Yes"),x$.forEach(t),mn.forEach(t),z.forEach(t),wc.forEach(t),Ph=m(e),as=o(e,"P",{});var G$=s(as);qg=l(G$,"A bracket means that it might not be strictly the case but is usually either not a main concern or negligable. Before we start make sure you have installed the following libraries:"),G$.forEach(t),Ah=m(e),c(sa.$$.fragment,e),Th=m(e),C=o(e,"P",{});var fn=s(C);Cg=l(fn,"The "),Dn=o(fn,"CODE",{});var z$=s(Dn);Og=l(z$,"nvidia-ml-py3"),z$.forEach(t),Lg=l(fn," library allows us to monitor the memory usage of the models from within Python. You might be familiar with the "),Nn=o(fn,"CODE",{});var U$=s(Nn);Bg=l(U$,"nvidia-smi"),U$.forEach(t),Fg=l(fn," command in the terminal - this library allows to access the same information in Python directly."),fn.forEach(t),xh=m(e),Ve=o(e,"P",{});var vc=s(Ve);Wg=l(vc,"Then we create some dummy data. We create random token IDs between 100 and 30000 and binary labels for a classifier. In total we get 512 sequences each with length 512 and store them in a "),ia=o(vc,"A",{href:!0,rel:!0});var j$=s(ia);Hg=l(j$,"Dataset"),j$.forEach(t),Rg=l(vc," with PyTorch format."),vc.forEach(t),Gh=m(e),c(na.$$.fragment,e),zh=m(e),Ye=o(e,"P",{});var _c=s(Ye);Vg=l(_c,"We want to print some summary statistics for the GPU utilization and the training run with the "),rs=o(_c,"A",{href:!0});var S$=s(rs);Yg=l(S$,"Trainer"),S$.forEach(t),Xg=l(_c,". We setup a two helper functions to do just that:"),_c.forEach(t),Uh=m(e),c(la.$$.fragment,e),jh=m(e),os=o(e,"P",{});var I$=s(os);Zg=l(I$,"Let\u2019s verify that we start with a free GPU memory:"),I$.forEach(t),Sh=m(e),c(pa.$$.fragment,e),Ih=m(e),ss=o(e,"P",{});var D$=s(ss);Qg=l(D$,"That looks good: the GPU memory is not occupied as we would expect before we load any models. If that\u2019s not the case on your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by the user. When a model is loaded to the GPU also the kernels are loaded which can take up 1-2GB of memory. To see how much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well."),D$.forEach(t),Dh=m(e),c(ha.$$.fragment,e),Nh=m(e),is=o(e,"P",{});var N$=s(is);Jg=l(N$,"We see that the kernels alone take up 1.3GB of GPU memory. Now let\u2019s see how much space the model uses."),N$.forEach(t),Mh=m(e),ce=o(e,"H2",{class:!0});var yc=s(ce);Xe=o(yc,"A",{id:!0,class:!0,href:!0});var M$=s(Xe);Mn=o(M$,"SPAN",{});var q$=s(Mn);c(ma.$$.fragment,q$),q$.forEach(t),M$.forEach(t),Kg=m(yc),qn=o(yc,"SPAN",{});var C$=s(qn);ew=l(C$,"Load Model"),C$.forEach(t),yc.forEach(t),qh=m(e),Ze=o(e,"P",{});var bc=s(Ze);tw=l(bc,"First, we load the "),Cn=o(bc,"CODE",{});var O$=s(Cn);aw=l(O$,"bert-large-uncased"),O$.forEach(t),rw=l(bc," model. We load the model weights directly to the GPU so that we can check how much space just weights use."),bc.forEach(t),Ch=m(e),c(fa.$$.fragment,e),Oh=m(e),Qe=o(e,"P",{});var $c=s(Qe);ow=l($c,"We can see that the model weights alone take up 1.3 GB of the GPU memory. The exact number depends on the specific GPU you are using. Note that on newer GPUs a model can sometimes take up more space since the weights are loaded in an optimized fashion that speeds up the usage of the model. Now we can also quickly check if we get the same result as with "),On=o($c,"CODE",{});var L$=s(On);sw=l(L$,"nvidia-smi"),L$.forEach(t),iw=l($c," CLI:"),$c.forEach(t),Lh=m(e),c(da.$$.fragment,e),Bh=m(e),c(ca.$$.fragment,e),Fh=m(e),ns=o(e,"P",{});var B$=s(ns);nw=l(B$,"We get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can start training the model and see how the GPU memory consumption changes. First, we set up a few standard training arguments that we will use across all our experiments:"),B$.forEach(t),Wh=m(e),c(ua.$$.fragment,e),Hh=m(e),c(Je.$$.fragment,e),Rh=m(e),ue=o(e,"H2",{class:!0});var Ec=s(ue);Ke=o(Ec,"A",{id:!0,class:!0,href:!0});var F$=s(Ke);Ln=o(F$,"SPAN",{});var W$=s(Ln);c(ga.$$.fragment,W$),W$.forEach(t),F$.forEach(t),lw=m(Ec),Bn=o(Ec,"SPAN",{});var H$=s(Bn);pw=l(H$,"Vanilla Training"),H$.forEach(t),Ec.forEach(t),Vh=m(e),et=o(e,"P",{});var kc=s(et);hw=l(kc,"As a first experiment we will use the "),ls=o(kc,"A",{href:!0});var R$=s(ls);mw=l(R$,"Trainer"),R$.forEach(t),fw=l(kc," and train the model without any further modifications and a batch size of 4:"),kc.forEach(t),Yh=m(e),c(wa.$$.fragment,e),Xh=m(e),c(va.$$.fragment,e),Zh=m(e),ps=o(e,"P",{});var V$=s(ps);dw=l(V$,"We see that already a relatively small batch size almost fills up our GPU\u2019s entire memory. However, a larger batch size can often result in faster model convergence or better end performance. So ideally we want to tune the batch size to our model\u2019s needs and not to the GPU limitations. What\u2019s interesting is that we use much more memory than the size of the model. To understand a bit better why this is the case let\u2019s have look at a model\u2019s operations and memory needs."),V$.forEach(t),Qh=m(e),ge=o(e,"H2",{class:!0});var Pc=s(ge);tt=o(Pc,"A",{id:!0,class:!0,href:!0});var Y$=s(tt);Fn=o(Y$,"SPAN",{});var X$=s(Fn);c(_a.$$.fragment,X$),X$.forEach(t),Y$.forEach(t),cw=m(Pc),Wn=o(Pc,"SPAN",{});var Z$=s(Wn);uw=l(Z$,"Anatomy of Model's Operations"),Z$.forEach(t),Pc.forEach(t),Jh=m(e),hs=o(e,"P",{});var Q$=s(hs);gw=l(Q$,"Transformers architecture includes 3 main groups of operations grouped below by compute-intensity."),Q$.forEach(t),Kh=m(e),O=o(e,"OL",{});var dn=s(O);ya=o(dn,"LI",{});var Ac=s(ya);Hn=o(Ac,"P",{});var J$=s(Hn);Rn=o(J$,"STRONG",{});var K$=s(Rn);ww=l(K$,"Tensor Contractions"),K$.forEach(t),J$.forEach(t),vw=m(Ac),ba=o(Ac,"P",{});var Tc=s(ba);_w=l(Tc,"Linear layers and components of Multi-Head Attention all do batched "),Vn=o(Tc,"STRONG",{});var e5=s(Vn);yw=l(e5,"matrix-matrix multiplications"),e5.forEach(t),bw=l(Tc,". These operations are the most compute-intensive part of training a transformer."),Tc.forEach(t),Ac.forEach(t),$w=m(dn),$a=o(dn,"LI",{});var xc=s($a);Yn=o(xc,"P",{});var t5=s(Yn);Xn=o(t5,"STRONG",{});var a5=s(Xn);Ew=l(a5,"Statistical Normalizations"),a5.forEach(t),t5.forEach(t),kw=m(xc),Ea=o(xc,"P",{});var Gc=s(Ea);Pw=l(Gc,"Softmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more "),Zn=o(Gc,"STRONG",{});var r5=s(Zn);Aw=l(r5,"reduction operations"),r5.forEach(t),Tw=l(Gc,", the result of which is then applied via a map."),Gc.forEach(t),xc.forEach(t),xw=m(dn),ka=o(dn,"LI",{});var zc=s(ka);Qn=o(zc,"P",{});var o5=s(Qn);Jn=o(o5,"STRONG",{});var s5=s(Jn);Gw=l(s5,"Element-wise Operators"),s5.forEach(t),o5.forEach(t),zw=m(zc),Pa=o(zc,"P",{});var Uc=s(Pa);Uw=l(Uc,"These are the remaining operators: "),Kn=o(Uc,"STRONG",{});var i5=s(Kn);jw=l(i5,"biases, dropout, activations, and residual connections"),i5.forEach(t),Sw=l(Uc,". These are the least compute-intensive operations."),Uc.forEach(t),zc.forEach(t),dn.forEach(t),em=m(e),ms=o(e,"P",{});var n5=s(ms);Iw=l(n5,"This knowledge can be helpful to know when analyzing performance bottlenecks."),n5.forEach(t),tm=m(e),Aa=o(e,"P",{});var O3=s(Aa);Dw=l(O3,"This summary is derived from "),Ta=o(O3,"A",{href:!0,rel:!0});var l5=s(Ta);Nw=l(l5,"Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020"),l5.forEach(t),O3.forEach(t),am=m(e),we=o(e,"H2",{class:!0});var jc=s(we);at=o(jc,"A",{id:!0,class:!0,href:!0});var p5=s(at);el=o(p5,"SPAN",{});var h5=s(el);c(xa.$$.fragment,h5),h5.forEach(t),p5.forEach(t),Mw=m(jc),tl=o(jc,"SPAN",{});var m5=s(tl);qw=l(m5,"Anatomy of Model's Memory"),m5.forEach(t),jc.forEach(t),rm=l(e,`

We've seen that training the model uses much more memory than just putting the model on the GPU. This is because there are many components during training that use GPU memory. The components on GPU memory are the following:
1. model weights
2. optimizer states
3. gradients
4. forward activations saved for gradient computation
5. temporary buffers
6. functionality-specific memory
`),fs=o(e,"P",{});var f5=s(fs);Cw=l(f5,"A typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory. For inference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per model parameter for mixed precision inference, plus activation memory."),f5.forEach(t),om=m(e),ds=o(e,"P",{});var d5=s(ds);Ow=l(d5,"Let\u2019s look at the details."),d5.forEach(t),sm=m(e),cs=o(e,"P",{});var c5=s(cs);al=o(c5,"STRONG",{});var u5=s(al);Lw=l(u5,"Model Weights:"),u5.forEach(t),c5.forEach(t),im=m(e),rt=o(e,"UL",{});var Sc=s(rt);rl=o(Sc,"LI",{});var g5=s(rl);Bw=l(g5,"4 bytes * number of parameters for fp32 training"),g5.forEach(t),Fw=m(Sc),ol=o(Sc,"LI",{});var w5=s(ol);Ww=l(w5,"6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16 in memory)"),w5.forEach(t),Sc.forEach(t),nm=m(e),us=o(e,"P",{});var v5=s(us);sl=o(v5,"STRONG",{});var _5=s(sl);Hw=l(_5,"Optimizer States:"),_5.forEach(t),v5.forEach(t),lm=m(e),L=o(e,"UL",{});var cn=s(L);il=o(cn,"LI",{});var y5=s(il);Rw=l(y5,"8 bytes * number of parameters for normal AdamW (maintains 2 states)"),y5.forEach(t),Vw=m(cn),gs=o(cn,"LI",{});var L3=s(gs);Yw=l(L3,"2 bytes * number of parameters for 8-bit AdamW optimizers like "),Ga=o(L3,"A",{href:!0,rel:!0});var b5=s(Ga);Xw=l(b5,"bitsandbytes"),b5.forEach(t),L3.forEach(t),Zw=m(cn),nl=o(cn,"LI",{});var $5=s(nl);Qw=l($5,"4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state)"),$5.forEach(t),cn.forEach(t),pm=m(e),ws=o(e,"P",{});var E5=s(ws);ll=o(E5,"STRONG",{});var k5=s(ll);Jw=l(k5,"Gradients"),k5.forEach(t),E5.forEach(t),hm=m(e),vs=o(e,"UL",{});var P5=s(vs);pl=o(P5,"LI",{});var A5=s(pl);Kw=l(A5,"4 bytes * number of parameters for either fp32 or mixed precision training (gradients are always kept in fp32)"),A5.forEach(t),P5.forEach(t),mm=m(e),_s=o(e,"P",{});var T5=s(_s);hl=o(T5,"STRONG",{});var x5=s(hl);ev=l(x5,"Forward Activations"),x5.forEach(t),T5.forEach(t),fm=m(e),ys=o(e,"UL",{});var G5=s(ys);ml=o(G5,"LI",{});var z5=s(ml);tv=l(z5,"size depends on many factors, the key ones being sequence length, hidden size and batch size."),z5.forEach(t),G5.forEach(t),dm=m(e),bs=o(e,"P",{});var U5=s(bs);av=l(U5,"There are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation."),U5.forEach(t),cm=m(e),$s=o(e,"P",{});var j5=s($s);fl=o(j5,"STRONG",{});var S5=s(fl);rv=l(S5,"Temporary Memory"),S5.forEach(t),j5.forEach(t),um=m(e),Es=o(e,"P",{});var I5=s(Es);ov=l(I5,"Additionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it\u2019s crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed."),I5.forEach(t),gm=m(e),ks=o(e,"P",{});var D5=s(ks);dl=o(D5,"STRONG",{});var N5=s(dl);sv=l(N5,"Functionality-specific memory"),N5.forEach(t),D5.forEach(t),wm=m(e),Ps=o(e,"P",{});var M5=s(Ps);iv=l(M5,"Then your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs."),M5.forEach(t),vm=m(e),As=o(e,"P",{});var q5=s(As);ot=o(q5,"STRONG",{});var vh=s(ot);cl=o(vh,"CODE",{});var C5=s(cl);nv=l(C5,"forward"),C5.forEach(t),lv=l(vh," vs "),ul=o(vh,"CODE",{});var O5=s(ul);pv=l(O5,"backward"),O5.forEach(t),hv=l(vh," Execution Speed"),vh.forEach(t),q5.forEach(t),_m=m(e),Ts=o(e,"P",{});var L5=s(Ts);mv=l(L5,"For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it\u2019s typical for an activation to have to read more data in the backward than in the forward (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput)."),L5.forEach(t),ym=m(e),xs=o(e,"P",{});var B5=s(xs);fv=l(B5,"So there are potentially a few places where we could save GPU memory or speed up operations. Let\u2019s start with a simple optimization: choosing the right batch size."),B5.forEach(t),bm=m(e),ve=o(e,"H2",{class:!0});var Ic=s(ve);st=o(Ic,"A",{id:!0,class:!0,href:!0});var F5=s(st);gl=o(F5,"SPAN",{});var W5=s(gl);c(za.$$.fragment,W5),W5.forEach(t),F5.forEach(t),dv=m(Ic),wl=o(Ic,"SPAN",{});var H5=s(wl);cv=l(H5,"Batch sizes"),H5.forEach(t),Ic.forEach(t),$m=m(e),Gs=o(e,"P",{});var R5=s(Gs);uv=l(R5,"One gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number, which typically starts at 8, but can be much higher as well. That number varies a lot depending on the specific hardware being used and the dtype of the model."),R5.forEach(t),Em=m(e),B=o(e,"P",{});var un=s(B);gv=l(un,"For example for fully connected layers (which correspond to GEMMs), NVIDIA provides recommendations for "),Ua=o(un,"A",{href:!0,rel:!0});var V5=s(Ua);wv=l(V5,"input/output neuron counts"),V5.forEach(t),vv=l(un," and "),ja=o(un,"A",{href:!0,rel:!0});var Y5=s(ja);_v=l(Y5,"batch size"),Y5.forEach(t),yv=l(un,"."),un.forEach(t),km=m(e),Sa=o(e,"P",{});var B3=s(Sa);Ia=o(B3,"A",{href:!0,rel:!0});var X5=s(Ia);bv=l(X5,"Tensor Core Requirements"),X5.forEach(t),$v=l(B3," define the multiplier based on the dtype and the hardware. For example, for fp16 a multiple of 8 is recommended, but on A100 it\u2019s 64!"),B3.forEach(t),Pm=m(e),it=o(e,"P",{});var Dc=s(it);Ev=l(Dc,"For parameters that are small, there is also "),Da=o(Dc,"A",{href:!0,rel:!0});var Z5=s(Da);kv=l(Z5,"Dimension Quantization Effects"),Z5.forEach(t),Pv=l(Dc," to consider, this is where tiling happens and the right multiplier can have a significant speedup."),Dc.forEach(t),Am=m(e),_e=o(e,"H2",{class:!0});var Nc=s(_e);nt=o(Nc,"A",{id:!0,class:!0,href:!0});var Q5=s(nt);vl=o(Q5,"SPAN",{});var J5=s(vl);c(Na.$$.fragment,J5),J5.forEach(t),Q5.forEach(t),Av=m(Nc),_l=o(Nc,"SPAN",{});var K5=s(_l);Tv=l(K5,"Gradient Accumulation"),K5.forEach(t),Nc.forEach(t),Tm=m(e),zs=o(e,"P",{});var e0=s(zs);xv=l(e0,"The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model\u2019s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU\u2019s memory. In turn, however, the added forward and backward passes can slow down the training a bit."),e0.forEach(t),xm=m(e),S=o(e,"P",{});var ea=s(S);Gv=l(ea,"We can use gradient accumulation in the "),Us=o(ea,"A",{href:!0});var t0=s(Us);zv=l(t0,"Trainer"),t0.forEach(t),Uv=l(ea," by simply adding the "),yl=o(ea,"CODE",{});var a0=s(yl);jv=l(a0,"gradient_accumulation_steps"),a0.forEach(t),Sv=l(ea," argument to "),js=o(ea,"A",{href:!0});var r0=s(js);Iv=l(r0,"TrainingArguments"),r0.forEach(t),Dv=l(ea,". Let\u2019s see how it impacts the models memory footprint:"),ea.forEach(t),Gm=m(e),c(Ma.$$.fragment,e),zm=m(e),c(qa.$$.fragment,e),Um=m(e),x=o(e,"P",{});var oe=s(x);Nv=l(oe,"We can see that the memory footprint was dramatically reduced at the cost of being only slightly slower than the vanilla run. Of course, this would change as you increase the number of accumulation steps. In general you would want to max out the GPU usage as much as possible. So in our case, the batch_size of 4 was already pretty close to the GPU\u2019s limit. If we wanted to train with a batch size of 64 we should not use "),bl=o(oe,"CODE",{});var o0=s(bl);Mv=l(o0,"per_device_train_batch_size=1"),o0.forEach(t),qv=l(oe," and "),$l=o(oe,"CODE",{});var s0=s($l);Cv=l(s0,"gradient_accumulation_steps=64"),s0.forEach(t),Ov=l(oe," but instead "),El=o(oe,"CODE",{});var i0=s(El);Lv=l(i0,"per_device_train_batch_size=4"),i0.forEach(t),Bv=l(oe," and "),kl=o(oe,"CODE",{});var n0=s(kl);Fv=l(n0,"gradient_accumulation_steps=16"),n0.forEach(t),Wv=l(oe," which has the same effective batch size while making better use of the available GPU resources."),oe.forEach(t),jm=m(e),F=o(e,"P",{});var gn=s(F);Hv=l(gn,"For more details see the benchmarks for "),Ca=o(gn,"A",{href:!0,rel:!0});var l0=s(Ca);Rv=l(l0,"RTX-3090"),l0.forEach(t),Vv=l(gn,`
and `),Oa=o(gn,"A",{href:!0,rel:!0});var p0=s(Oa);Yv=l(p0,"A100"),p0.forEach(t),Xv=l(gn,"."),gn.forEach(t),Sm=m(e),Ss=o(e,"P",{});var h0=s(Ss);Zv=l(h0,"Next we have a look at another trick to save a little bit more GPU memory called gradient checkpointing."),h0.forEach(t),Im=m(e),ye=o(e,"H2",{class:!0});var Mc=s(ye);lt=o(Mc,"A",{id:!0,class:!0,href:!0});var m0=s(lt);Pl=o(m0,"SPAN",{});var f0=s(Pl);c(La.$$.fragment,f0),f0.forEach(t),m0.forEach(t),Qv=m(Mc),Al=o(Mc,"SPAN",{});var d0=s(Al);Jv=l(d0,"Gradient Checkpointing"),d0.forEach(t),Mc.forEach(t),Dm=m(e),Is=o(e,"P",{});var c0=s(Is);Kv=l(c0,"Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training."),c0.forEach(t),Nm=m(e),pt=o(e,"P",{});var qc=s(pt);e_=l(qc,"Gradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See "),Ba=o(qc,"A",{href:!0,rel:!0});var u0=s(Ba);t_=l(u0,"this great article"),u0.forEach(t),a_=l(qc," explaining the ideas behind gradient checkpointing."),qc.forEach(t),Mm=m(e),W=o(e,"P",{});var wn=s(W);r_=l(wn,"To enable gradient checkpointing in the "),Ds=o(wn,"A",{href:!0});var g0=s(Ds);o_=l(g0,"Trainer"),g0.forEach(t),s_=l(wn," we only need ot pass it as a flag to the "),Ns=o(wn,"A",{href:!0});var w0=s(Ns);i_=l(w0,"TrainingArguments"),w0.forEach(t),n_=l(wn,". Everything else is handled under the hood:"),wn.forEach(t),qm=m(e),c(Fa.$$.fragment,e),Cm=m(e),c(Wa.$$.fragment,e),Om=m(e),Ms=o(e,"P",{});var v0=s(Ms);l_=l(v0,"We can see that this saved some more memory but at the same time training became a bit slower. A general rule of thumb is that gradient checkpointing slows down training by about 20%. Let\u2019s have a look at another method with which we can regain some speed: mixed precision training."),v0.forEach(t),Lm=m(e),be=o(e,"H2",{class:!0});var Cc=s(be);ht=o(Cc,"A",{id:!0,class:!0,href:!0});var _0=s(ht);Tl=o(_0,"SPAN",{});var y0=s(Tl);c(Ha.$$.fragment,y0),y0.forEach(t),_0.forEach(t),p_=m(Cc),xl=o(Cc,"SPAN",{});var b0=s(xl);h_=l(b0,"Floating Data Types"),b0.forEach(t),Cc.forEach(t),Bm=m(e),qs=o(e,"P",{});var $0=s(qs);m_=l($0,"The idea of mixed precision training is that no all variables need to be stored in full (32-bit) floating point precision. If we can reduce the precision the variales and their computations are faster. Here are the commonly used floating point data types choice of which impacts both memory usage and throughput:"),$0.forEach(t),Fm=m(e),I=o(e,"UL",{});var ta=s(I);Ra=o(ta,"LI",{});var Oc=s(Ra);f_=l(Oc,"fp32 ("),Gl=o(Oc,"CODE",{});var E0=s(Gl);d_=l(E0,"float32"),E0.forEach(t),c_=l(Oc,")"),Oc.forEach(t),u_=m(ta),Va=o(ta,"LI",{});var Lc=s(Va);g_=l(Lc,"fp16 ("),zl=o(Lc,"CODE",{});var k0=s(zl);w_=l(k0,"float16"),k0.forEach(t),v_=l(Lc,")"),Lc.forEach(t),__=m(ta),Ya=o(ta,"LI",{});var Bc=s(Ya);y_=l(Bc,"bf16 ("),Ul=o(Bc,"CODE",{});var P0=s(Ul);b_=l(P0,"bfloat16"),P0.forEach(t),$_=l(Bc,")"),Bc.forEach(t),E_=m(ta),jl=o(ta,"LI",{});var A0=s(jl);k_=l(A0,"tf32 (CUDA internal data type)"),A0.forEach(t),ta.forEach(t),Wm=m(e),Cs=o(e,"P",{});var T0=s(Cs);P_=l(T0,"Here is a diagram that shows how these data types correlate to each other."),T0.forEach(t),Hm=m(e),$e=o(e,"P",{});var _h=s($e);Os=o(_h,"IMG",{src:!0,alt:!0}),A_=l(_h,`
(source: `),Xa=o(_h,"A",{href:!0,rel:!0});var x0=s(Xa);T_=l(x0,"NVIDIA Blog"),x0.forEach(t),x_=l(_h,")"),_h.forEach(t),Rm=m(e),Ls=o(e,"P",{});var G0=s(Ls);G_=l(G0,"While fp16 and fp32 have been around for quite some time, bf16 and tf32 are only available on the Ampere architecture GPUS and TPUs support bf16 as well. Let\u2019s start with the most commonly used method which is FP16 training/"),G0.forEach(t),Vm=m(e),Ee=o(e,"H3",{class:!0});var Fc=s(Ee);mt=o(Fc,"A",{id:!0,class:!0,href:!0});var z0=s(mt);Sl=o(z0,"SPAN",{});var U0=s(Sl);c(Za.$$.fragment,U0),U0.forEach(t),z0.forEach(t),z_=m(Fc),Il=o(Fc,"SPAN",{});var j0=s(Il);U_=l(j0,"FP16 Training"),j0.forEach(t),Fc.forEach(t),Ym=m(e),H=o(e,"P",{});var vn=s(H);j_=l(vn,"The idea of mixed precision training is that no all variables need to be stored in full (32-bit) floating point precision. If we can reduce the precision the variales and their computations are faster. The main advantage comes from saving the activations in half (16-bit) precision. Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. Since the model is present on the GPU in both 16-bit and 32-bit precision this can use more GPU memory (1.5x the original model is on the GPU), especially for small batch sizes. Since some computations are performed in full and some in half precision this approach is also called mixed precision training. Enabling mixed precision training is also just a matter of setting the "),Dl=o(vn,"CODE",{});var S0=s(Dl);S_=l(S0,"fp16"),S0.forEach(t),I_=l(vn," flag to "),Nl=o(vn,"CODE",{});var I0=s(Nl);D_=l(I0,"True"),I0.forEach(t),N_=l(vn,":"),vn.forEach(t),Xm=m(e),c(Qa.$$.fragment,e),Zm=m(e),c(Ja.$$.fragment,e),Qm=m(e),Bs=o(e,"P",{});var D0=s(Bs);M_=l(D0,"We can see that this is almost twice as fast as the vanilla training. Let\u2019s add it to the mix of the previous methods:"),D0.forEach(t),Jm=m(e),c(Ka.$$.fragment,e),Km=m(e),c(er.$$.fragment,e),ef=m(e),Fs=o(e,"P",{});var N0=s(Fs);q_=l(N0,"We can see that with these tweaks we use about half the GPU memory as at the beginning while also being slightly faster."),N0.forEach(t),tf=m(e),ke=o(e,"H3",{class:!0});var Wc=s(ke);ft=o(Wc,"A",{id:!0,class:!0,href:!0});var M0=s(ft);Ml=o(M0,"SPAN",{});var q0=s(Ml);c(tr.$$.fragment,q0),q0.forEach(t),M0.forEach(t),C_=m(Wc),ql=o(Wc,"SPAN",{});var C0=s(ql);O_=l(C0,"BF16"),C0.forEach(t),Wc.forEach(t),af=l(e,"\n\nIf you have access to a Ampere or newer hardware you can use bf16 for your training and evaluation. While bf16 has a worse precision than fp16, it has a much much bigger dynamic range. Therefore, if in the past you were experiencing overflow issues while training the model, bf16 will prevent this from happening most of the time. Remember that in fp16 the biggest number you can have is `65535` and any number above that will overflow. A bf16 number can be as large as `3.39e+38` (!) which is about the same as fp32 - because both have 8-bits used for the numerical range.\n"),Ws=o(e,"P",{});var O0=s(Ws);L_=l(O0,"You can enable BF16 in the \u{1F917} Trainer with:"),O0.forEach(t),rf=m(e),c(ar.$$.fragment,e),of=m(e),Pe=o(e,"H3",{class:!0});var Hc=s(Pe);dt=o(Hc,"A",{id:!0,class:!0,href:!0});var L0=s(dt);Cl=o(L0,"SPAN",{});var B0=s(Cl);c(rr.$$.fragment,B0),B0.forEach(t),L0.forEach(t),B_=m(Hc),Ol=o(Hc,"SPAN",{});var F0=s(Ol);F_=l(F0,"TF32"),F0.forEach(t),Hc.forEach(t),sf=l(e,`

The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead of 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total.
`),Hs=o(e,"P",{});var W0=s(Hs);W_=l(W0,"It\u2019s magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add this to your code:"),W0.forEach(t),nf=m(e),c(or.$$.fragment,e),lf=m(e),Rs=o(e,"P",{});var H0=s(Rs);H_=l(H0,"When this is done CUDA will automatically switch to using tf32 instead of fp32 where it\u2019s possible. This, of course, assumes that the used GPU is from the Ampere series."),H0.forEach(t),pf=m(e),ct=o(e,"P",{});var Rc=s(ct);R_=l(Rc,"Like all cases with reduced precision this may or may not be satisfactory for your needs, so you have to experiment and see. According to "),sr=o(Rc,"A",{href:!0,rel:!0});var R0=s(sr);V_=l(R0,"NVIDIA research"),R0.forEach(t),Y_=l(Rc," the majority of machine learning training shouldn\u2019t be impacted and showed the same perplexity and convergence as the fp32 training."),Rc.forEach(t),hf=m(e),Vs=o(e,"P",{});var V0=s(Vs);X_=l(V0,"If you\u2019re already using fp16 or bf16 mixed precision it may help with the throughput as well."),V0.forEach(t),mf=m(e),Ys=o(e,"P",{});var Y0=s(Ys);Z_=l(Y0,"You can enable this mode in the \u{1F917} Trainer with:"),Y0.forEach(t),ff=m(e),c(ir.$$.fragment,e),df=m(e),Xs=o(e,"P",{});var X0=s(Xs);Q_=l(X0,"By default the PyTorch default is used."),X0.forEach(t),cf=m(e),R=o(e,"P",{});var _n=s(R);J_=l(_n,"Note: tf32 mode is internal to CUDA and can\u2019t be accessed directly via "),Ll=o(_n,"CODE",{});var Z0=s(Ll);K_=l(Z0,"tensor.to(dtype=torch.tf32)"),Z0.forEach(t),ey=l(_n," as "),Bl=o(_n,"CODE",{});var Q0=s(Bl);ty=l(Q0,"torch.tf32"),Q0.forEach(t),ay=l(_n," doesn\u2019t exist."),_n.forEach(t),uf=m(e),ut=o(e,"P",{});var Vc=s(ut);ry=l(Vc,"Note: you need "),Fl=o(Vc,"CODE",{});var J0=s(Fl);oy=l(J0,"torch>=1.7"),J0.forEach(t),sy=l(Vc," to enjoy this feature."),Vc.forEach(t),gf=m(e),V=o(e,"P",{});var yn=s(V);iy=l(yn,`You can also see a variety of benchmarks on tf32 vs other precisions:
`),nr=o(yn,"A",{href:!0,rel:!0});var K0=s(nr);ny=l(K0,"RTX-3090"),K0.forEach(t),ly=l(yn,` and
`),lr=o(yn,"A",{href:!0,rel:!0});var eE=s(lr);py=l(eE,"A100"),eE.forEach(t),hy=l(yn,"."),yn.forEach(t),wf=m(e),Zs=o(e,"P",{});var tE=s(Zs);my=l(tE,"We\u2019ve now seen how we can change the floating types to increase throughput, but we are not done, yet! There is another area where we can save GPU memory: the optimizer."),tE.forEach(t),vf=m(e),Ae=o(e,"H2",{class:!0});var Yc=s(Ae);gt=o(Yc,"A",{id:!0,class:!0,href:!0});var aE=s(gt);Wl=o(aE,"SPAN",{});var rE=s(Wl);c(pr.$$.fragment,rE),rE.forEach(t),aE.forEach(t),fy=m(Yc),Hl=o(Yc,"SPAN",{});var oE=s(Hl);dy=l(oE,"Optimizer"),oE.forEach(t),Yc.forEach(t),_f=m(e),Qs=o(e,"P",{});var sE=s(Qs);cy=l(sE,"The most common optimizer used to train transformer model is Adam or AdamW (Adam with weight decay). Adam achieves good convergence by storing the rolling average of the previous gradients which, however, adds an additional memory footprint of the order of the number of model parameters. One remedy to this is to use an alternative optimizer such as Adafactor, which works well for some models but often it has instability issues."),sE.forEach(t),yf=m(e),wt=o(e,"P",{});var Xc=s(wt);uy=l(Xc,"HF Trainer integrates a variety of optimisers that can be used out of box. To activate the desired optimizer simply pass the "),Rl=o(Xc,"CODE",{});var iE=s(Rl);gy=l(iE,"--optim"),iE.forEach(t),wy=l(Xc," flag to the command line."),Xc.forEach(t),bf=m(e),Js=o(e,"P",{});var nE=s(Js);vy=l(nE,"To see which optimizers are currently supported:"),nE.forEach(t),$f=m(e),c(hr.$$.fragment,e),Ef=m(e),Y=o(e,"P",{});var bn=s(Y);_y=l(bn,"For example, if you have "),mr=o(bn,"A",{href:!0,rel:!0});var lE=s(mr);yy=l(lE,"NVIDIA/apex"),lE.forEach(t),by=l(bn," installed "),Vl=o(bn,"CODE",{});var pE=s(Vl);$y=l(pE,"--optim adamw_apex_fused"),pE.forEach(t),Ey=l(bn," will give you the fastest training experience among all supported AdamW optimizers."),bn.forEach(t),kf=m(e),X=o(e,"P",{});var $n=s(X);ky=l($n,"On the other hand "),fr=o($n,"A",{href:!0,rel:!0});var hE=s(fr);Py=l(hE,"8bit BNB optimizer"),hE.forEach(t),Ay=l($n," can save 3/4 of memory normally used by a typical AdamW optimizer if it is configured to quantize all optimizer states, but in some situations only some optimizer states are quintized and then more memory is used. XXX: update once  "),dr=o($n,"A",{href:!0,rel:!0});var mE=s(dr);Ty=l(mE,"https://github.com/huggingface/transformers/pull/15622"),mE.forEach(t),xy=l($n," is merged."),$n.forEach(t),Pf=m(e),vt=o(e,"P",{});var Zc=s(vt);Gy=l(Zc,"Let\u2019s get a feel for the numbers and use for example use a 3B-parameter model, like "),Yl=o(Zc,"CODE",{});var fE=s(Yl);zy=l(fE,"t5-3b"),fE.forEach(t),Uy=l(Zc,". Note that since a Gigabyte correpsonds to a billion bytes we can simply multiply the parameters (in billions) with the number of necessary bytes per parameter to get Gigabytes of GPU memory usage:"),Zc.forEach(t),Af=m(e),Z=o(e,"UL",{});var En=s(Z);cr=o(En,"LI",{});var Qc=s(cr);jy=l(Qc,"A standard AdamW uses 8 bytes for each parameter, here the optimizer will need ("),Xl=o(Qc,"CODE",{});var dE=s(Xl);Sy=l(dE,"8*3"),dE.forEach(t),Iy=l(Qc,") 24GB of GPU memory."),Qc.forEach(t),Dy=m(En),ur=o(En,"LI",{});var Jc=s(ur);Ny=l(Jc,"Adafactor uses slightly more than 4 bytes, so ("),Zl=o(Jc,"CODE",{});var cE=s(Zl);My=l(cE,"4*3"),cE.forEach(t),qy=l(Jc,") 12GB and then some extra."),Jc.forEach(t),Cy=m(En),gr=o(En,"LI",{});var Kc=s(gr);Oy=l(Kc,"8bit BNB quantized optimizer will use only ("),Ql=o(Kc,"CODE",{});var uE=s(Ql);Ly=l(uE,"2*3"),uE.forEach(t),By=l(Kc,") 6GB if all optimizer states are quantized."),Kc.forEach(t),En.forEach(t),Tf=m(e),Ks=o(e,"P",{});var gE=s(Ks);Fy=l(gE,"Let\u2019s have a look at Adafactor first."),gE.forEach(t),xf=m(e),Te=o(e,"H3",{class:!0});var eu=s(Te);_t=o(eu,"A",{id:!0,class:!0,href:!0});var wE=s(_t);Jl=o(wE,"SPAN",{});var vE=s(Jl);c(wr.$$.fragment,vE),vE.forEach(t),wE.forEach(t),Wy=m(eu),Kl=o(eu,"SPAN",{});var _E=s(Kl);Hy=l(_E,"Adafactor"),_E.forEach(t),eu.forEach(t),Gf=m(e),yt=o(e,"P",{});var tu=s(yt);Ry=l(tu,"Instead of keeping the rolling average for each element in the weight matrices Adafactor only stores aggregated information (row- and column-wise sums of the rolling averages) which reduces the footprint considerably. One downside of Adafactor is that in some instances convergence can be slower than Adam\u2019s so some experimentation is advised here. We can use Adafactor simply by setting "),ep=o(tu,"CODE",{});var yE=s(ep);Vy=l(yE,'optim="adafactor"'),yE.forEach(t),Yy=l(tu,":"),tu.forEach(t),zf=m(e),c(vr.$$.fragment,e),Uf=m(e),c(_r.$$.fragment,e),jf=m(e),ei=o(e,"P",{});var bE=s(ei);Xy=l(bE,"We can see that this saves a few more GB on the GPU. Let\u2019s see how it looks when we add it to the other methods we introduced earlier:"),bE.forEach(t),Sf=m(e),c(yr.$$.fragment,e),If=m(e),c(br.$$.fragment,e),Df=m(e),ti=o(e,"P",{});var $E=s(ti);Zy=l($E,"We went from 15 GB memory usage to 5 GB - a 3x improvement while maintaining the throughput! However, as mentioned before, the convergence of Adafactor can be worse than Adam. There is an alternative to Adafactor called 8-bit Adam that takes a slightly different approach."),$E.forEach(t),Nf=m(e),xe=o(e,"H3",{class:!0});var au=s(xe);bt=o(au,"A",{id:!0,class:!0,href:!0});var EE=s(bt);tp=o(EE,"SPAN",{});var kE=s(tp);c($r.$$.fragment,kE),kE.forEach(t),EE.forEach(t),Qy=m(au),ap=o(au,"SPAN",{});var PE=s(ap);Jy=l(PE,"8-bit Adam"),PE.forEach(t),au.forEach(t),Mf=m(e),ai=o(e,"P",{});var AE=s(ai);Ky=l(AE,"Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind FP16 training where using variables with lower precision saves memory."),AE.forEach(t),qf=m(e),G=o(e,"P",{});var se=s(G);eb=l(se,"In contrast to the previous approaches is this one not integrated into the "),ri=o(se,"A",{href:!0});var TE=s(ri);tb=l(TE,"Trainer"),TE.forEach(t),ab=l(se," as a simple flag. We need to install the 8-bit optimizer and then pass it as a custom optimizer to the "),oi=o(se,"A",{href:!0});var xE=s(oi);rb=l(xE,"Trainer"),xE.forEach(t),ob=l(se,". Follow the installation guide in the Github "),Er=o(se,"A",{href:!0,rel:!0});var GE=s(Er);sb=l(GE,"repo"),GE.forEach(t),ib=l(se," to install the "),rp=o(se,"CODE",{});var zE=s(rp);nb=l(zE,"bitsandbytes"),zE.forEach(t),lb=l(se," library that implements the 8-bit Adam optimizer."),se.forEach(t),Cf=m(e),si=o(e,"P",{});var UE=s(si);pb=l(UE,"Once installed, we just need to initialize the the optimizer. Although this looks like a considerable amount of work it actually just involves two steps: first we need to group the model\u2019s parameters into two groups where to one group we apply weight decay and to the other we don\u2019t. Usually, biases and layer norm parameters are not weight decayed. Then in a second step we just do some argument housekeeping to use the same parameters as the previously used AdamW optimizer."),UE.forEach(t),Of=m(e),c($t.$$.fragment,e),Lf=m(e),c(kr.$$.fragment,e),Bf=m(e),Et=o(e,"P",{});var ru=s(Et);hb=l(ru,"We can now pass the custom optimizer as an argument to the "),op=o(ru,"CODE",{});var jE=s(op);mb=l(jE,"Trainer"),jE.forEach(t),fb=l(ru,":"),ru.forEach(t),Ff=m(e),c(Pr.$$.fragment,e),Wf=m(e),c(Ar.$$.fragment,e),Hf=m(e),ii=o(e,"P",{});var SE=s(ii);db=l(SE,"We can see that we get a similar memory improvement as with Adafactor while keeping the full rolling average of the gradients. Let\u2019s repeat the experiment with the full settings:"),SE.forEach(t),Rf=m(e),c(Tr.$$.fragment,e),Vf=m(e),c(xr.$$.fragment,e),Yf=m(e),ni=o(e,"P",{});var IE=s(ni);cb=l(IE,"Again, we get about a 3x memory improvement and even slightly higher throughput as using Adafactor. So we have seen how we can optimize the memory footprint of large models. The following plot summarizes all our experiments:"),IE.forEach(t),Xf=m(e),li=o(e,"P",{});var DE=s(li);pi=o(DE,"IMG",{src:!0,alt:!0}),DE.forEach(t),Zf=m(e),Ge=o(e,"H3",{class:!0});var ou=s(Ge);kt=o(ou,"A",{id:!0,class:!0,href:!0});var NE=s(kt);sp=o(NE,"SPAN",{});var ME=s(sp);c(Gr.$$.fragment,ME),ME.forEach(t),NE.forEach(t),ub=m(ou),ip=o(ou,"SPAN",{});var qE=s(ip);np=o(qE,"CODE",{});var CE=s(np);gb=l(CE,"_multi_tensor"),CE.forEach(t),qE.forEach(t),ou.forEach(t),Qf=l(e,`

pytorch-nightly introduced \`torch.optim._multi_tensor\` which should significantly speed up the optimizers for situations with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner and don't mind using the bleed-edge, see: https://github.com/huggingface/transformers/issues/9965
`),ze=o(e,"H2",{class:!0});var su=s(ze);Pt=o(su,"A",{id:!0,class:!0,href:!0});var OE=s(Pt);lp=o(OE,"SPAN",{});var LE=s(lp);c(zr.$$.fragment,LE),LE.forEach(t),OE.forEach(t),wb=m(su),pp=o(su,"SPAN",{});var BE=s(pp);vb=l(BE,"Using \u{1F917} Accelerate"),BE.forEach(t),su.forEach(t),Jf=m(e),Q=o(e,"P",{});var kn=s(Q);_b=l(kn,"So far we have used the "),hi=o(kn,"A",{href:!0});var FE=s(hi);yb=l(FE,"Trainer"),FE.forEach(t),bb=l(kn," to run the experiments but a more flexible alternative to that approach is to use \u{1F917} Accelerate. With \u{1F917} Accelerate you have full control over the training loop and can essentially write the loop in pure PyTorch with some minor modifications. In turn it allows you to easily scale across different infrastructures such as CPUs, GPUs, TPUs, or distributed multi-GPU setups without changing any code. Let\u2019s see what it takes to implement all of the above tweaks in \u{1F917} Accelerate. We can still use the "),mi=o(kn,"A",{href:!0});var WE=s(mi);$b=l(WE,"TrainingArguments"),WE.forEach(t),Eb=l(kn," to wrap the training settings:"),kn.forEach(t),Kf=m(e),c(Ur.$$.fragment,e),ed=m(e),fi=o(e,"P",{});var HE=s(fi);kb=l(HE,"The full example training loop with \u{1F917} Accelerate is only a handful of lines of code long:"),HE.forEach(t),td=m(e),c(jr.$$.fragment,e),ad=m(e),P=o(e,"P",{});var N=s(P);Pb=l(N,"First we wrap the dataset in a "),Sr=o(N,"A",{href:!0,rel:!0});var RE=s(Sr);hp=o(RE,"CODE",{});var VE=s(hp);Ab=l(VE,"DataLoader"),VE.forEach(t),RE.forEach(t),Tb=l(N,". Then we can enable gradient checkpointing by calling the model\u2019s "),di=o(N,"A",{href:!0});var YE=s(di);xb=l(YE,"gradient_checkpointing_enable()"),YE.forEach(t),Gb=l(N," method. When we initialize the "),Ir=o(N,"A",{href:!0,rel:!0});var XE=s(Ir);mp=o(XE,"CODE",{});var ZE=s(mp);zb=l(ZE,"Accelerator"),ZE.forEach(t),XE.forEach(t),Ub=l(N," we can specifiy if we want to use mixed precision training and it will take care of it for us in the "),fp=o(N,"CODE",{});var QE=s(fp);jb=l(QE,"prepare"),QE.forEach(t),Sb=l(N," call. During the "),Dr=o(N,"A",{href:!0,rel:!0});var JE=s(Dr);dp=o(JE,"CODE",{});var KE=s(dp);Ib=l(KE,"prepare"),KE.forEach(t),JE.forEach(t),Db=l(N," call the dataloader will also be distributed across workers should we use multiple GPUs. We use the same 8-bit optimizer from the earlier experiments."),N.forEach(t),rd=m(e),At=o(e,"P",{});var iu=s(At);Nb=l(iu,"Finally, we can write the main training loop. Note that the "),cp=o(iu,"CODE",{});var e6=s(cp);Mb=l(e6,"backward"),e6.forEach(t),qb=l(iu," call is handled by \u{1F917} Accelerate. We can also see how gradient accumulation works: we normalize the loss so we get the average at the end of accumulation and once we have enough steps we run the optimization. Now the question is: does this use the same amount of memory as the previous steps? Let\u2019s check:"),iu.forEach(t),od=m(e),c(Nr.$$.fragment,e),sd=m(e),Tt=o(e,"P",{});var nu=s(Tt);Cb=l(nu,"Indeed it does. Implementing these optimization techniques with \u{1F917} Accelerate only takes a handful of lines of code and comes with the benefit of more flexiblity in the training loop. For a full documentation of all features have a look at the "),Mr=o(nu,"A",{href:!0,rel:!0});var t6=s(Mr);Ob=l(t6,"Accelerate documentation"),t6.forEach(t),Lb=l(nu,"."),nu.forEach(t),id=m(e),Ue=o(e,"H2",{class:!0});var lu=s(Ue);xt=o(lu,"A",{id:!0,class:!0,href:!0});var a6=s(xt);up=o(a6,"SPAN",{});var r6=s(up);c(qr.$$.fragment,r6),r6.forEach(t),a6.forEach(t),Bb=m(lu),gp=o(lu,"SPAN",{});var o6=s(gp);Fb=l(o6,"DataLoader"),o6.forEach(t),lu.forEach(t),nd=m(e),ci=o(e,"P",{});var s6=s(ci);Wb=l(s6,"One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default everything happens in the main process and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization."),s6.forEach(t),ld=m(e),Gt=o(e,"UL",{});var pu=s(Gt);ui=o(pu,"LI",{});var F3=s(ui);wp=o(F3,"CODE",{});var i6=s(wp);Hb=l(i6,"DataLoader(pin_memory=True, ...)"),i6.forEach(t),Rb=l(F3," which ensures that the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory."),F3.forEach(t),Vb=m(pu),gi=o(pu,"LI",{});var W3=s(gi);vp=o(W3,"CODE",{});var n6=s(vp);Yb=l(n6,"DataLoader(num_workers=4, ...)"),n6.forEach(t),Xb=l(W3," - spawn several workers to pre-load data faster - during training watch the GPU utilization stats and if it\u2019s far from 100% experiment with raising the number of workers. Of course, the problem could be elsewhere so a very big number of workers won\u2019t necessarily lead to a better performance."),W3.forEach(t),pu.forEach(t),pd=m(e),je=o(e,"H2",{class:!0});var hu=s(je);zt=o(hu,"A",{id:!0,class:!0,href:!0});var l6=s(zt);_p=o(l6,"SPAN",{});var p6=s(_p);c(Cr.$$.fragment,p6),p6.forEach(t),l6.forEach(t),Zb=m(hu),yp=o(hu,"SPAN",{});var h6=s(yp);Qb=l(h6,"DeepSpeed ZeRO"),h6.forEach(t),hu.forEach(t),hd=m(e),Ut=o(e,"P",{});var mu=s(Ut);Jb=l(mu,"The in-depth details on how to use Deepspeed can be found "),wi=o(mu,"A",{href:!0});var m6=s(wi);Kb=l(m6,"here"),m6.forEach(t),e1=l(mu,"."),mu.forEach(t),md=m(e),vi=o(e,"P",{});var f6=s(vi);t1=l(f6,"First, a quick decision tree:"),f6.forEach(t),fd=m(e),jt=o(e,"OL",{});var fu=s(jt);bp=o(fu,"LI",{});var d6=s(bp);a1=l(d6,"Model fits onto a single GPU and you have enough space to fit a small batch size - you don\u2019t need to use Deepspeed as it\u2019ll only slow things down in this use case."),d6.forEach(t),r1=m(fu),$p=o(fu,"LI",{});var c6=s($p);o1=l(c6,"Model doesn\u2019t fit onto a single GPU or you can\u2019t fit a small batch - use DeepSpeed ZeRO + CPU Offload and for much larger models NVMe Offload."),c6.forEach(t),fu.forEach(t),dd=m(e),St=o(e,"P",{});var du=s(St);s1=l(du,"Now if the decision tree suggested you use DeepSpeed first you need to "),_i=o(du,"A",{href:!0});var u6=s(_i);i1=l(u6,"install it"),u6.forEach(t),n1=l(du,", then follow one of the following guides to create a configuration file and launch DeepSpeed."),du.forEach(t),cd=m(e),yi=o(e,"P",{});var g6=s(yi);l1=l(g6,"Activation:"),g6.forEach(t),ud=m(e),D=o(e,"UL",{});var aa=s(D);Ep=o(aa,"LI",{});var w6=s(Ep);Or=o(w6,"P",{});var cu=s(Or);p1=l(cu,"HF Trainer-based examples: see this "),bi=o(cu,"A",{href:!0});var v6=s(bi);h1=l(v6,"guide"),v6.forEach(t),m1=l(cu,"."),cu.forEach(t),w6.forEach(t),f1=m(aa),Lr=o(aa,"LI",{});var uu=s(Lr);kp=o(uu,"P",{});var _6=s(kp);d1=l(_6,"Custom HF Trainer-based program: Same as above, but pass:"),_6.forEach(t),c1=m(uu),c(Br.$$.fragment,uu),uu.forEach(t),u1=m(aa),Pp=o(aa,"LI",{});var y6=s(Pp);Fr=o(y6,"P",{});var gu=s(Fr);g1=l(gu,"Deployment in Notebooks: see this "),$i=o(gu,"A",{href:!0});var b6=s($i);w1=l(b6,"guide"),b6.forEach(t),v1=l(gu,"."),gu.forEach(t),y6.forEach(t),_1=m(aa),Ap=o(aa,"LI",{});var $6=s(Ap);Se=o($6,"P",{});var Pn=s(Se);y1=l(Pn,"Custom training loop: This is somewhat complex but you can study how this is implemented in "),Wr=o(Pn,"A",{href:!0,rel:!0});var E6=s(Wr);b1=l(E6,"HF Trainer"),E6.forEach(t),$1=l(Pn," - simply search for "),Tp=o(Pn,"CODE",{});var k6=s(Tp);E1=l(k6,"deepspeed"),k6.forEach(t),k1=l(Pn," in the code."),Pn.forEach(t),$6.forEach(t),aa.forEach(t),gd=m(e),Ie=o(e,"H2",{class:!0});var wu=s(Ie);It=o(wu,"A",{id:!0,class:!0,href:!0});var P6=s(It);xp=o(P6,"SPAN",{});var A6=s(xp);c(Hr.$$.fragment,A6),A6.forEach(t),P6.forEach(t),P1=m(wu),Gp=o(wu,"SPAN",{});var T6=s(Gp);A1=l(T6,"Choice of GPU"),T6.forEach(t),wu.forEach(t),wd=l(e,`

Sometimes, even when applying all the above tweaks the throughput on a given GPU might still not be good enough. One easy solution is to change the type of GPU. For example switching from let's say a K80 (which you typically get on Google Colab) to a fancier GPU such as the V100 or A100. Although they are more expensive they are usually more cost effective than cheaper GPUs due to their larger memory and faster architecture.
`),Ei=o(e,"P",{});var x6=s(Ei);T1=l(x6,"Now, let\u2019s take a step back and discuss what we should optimize for when scaling the training of large models."),x6.forEach(t),vd=m(e),De=o(e,"H2",{class:!0});var vu=s(De);Dt=o(vu,"A",{id:!0,class:!0,href:!0});var G6=s(Dt);zp=o(G6,"SPAN",{});var z6=s(zp);c(Rr.$$.fragment,z6),z6.forEach(t),G6.forEach(t),x1=m(vu),Up=o(vu,"SPAN",{});var U6=s(Up);G1=l(U6,"How to scale"),U6.forEach(t),vu.forEach(t),_d=m(e),ki=o(e,"P",{});var j6=s(ki);z1=l(j6,"When we train models there are a two aspects we want to optimize at the same time:"),j6.forEach(t),yd=m(e),Nt=o(e,"UL",{});var _u=s(Nt);jp=o(_u,"LI",{});var S6=s(jp);U1=l(S6,"Data throughput/training time"),S6.forEach(t),j1=m(_u),Sp=o(_u,"LI",{});var I6=s(Sp);S1=l(I6,"Model performance"),I6.forEach(t),_u.forEach(t),bd=m(e),Pi=o(e,"P",{});var D6=s(Pi);I1=l(D6,"We have seen that each method changes the memory usage and throughput. In general we want to maximize the throughput (samples/second) to minimize the training cost. This is generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit. For example, as mentioned earlier, we only employ gradient accumulation when we want to use a batch size beyond the size of the GPU memory. If the desired batch size fits into memory then there is no reason to apply gradient accumulation which will only slow down training."),D6.forEach(t),$d=m(e),Ai=o(e,"P",{});var N6=s(Ai);D1=l(N6,"The second objective is model performance. Just because we can does not mean we should use a large batch size. As part of hyperparameter tuning you should determine which batch size yields the best result and then optimize the throughput accordingly."),N6.forEach(t),Ed=m(e),Ne=o(e,"H2",{class:!0});var yu=s(Ne);Mt=o(yu,"A",{id:!0,class:!0,href:!0});var M6=s(Mt);Ip=o(M6,"SPAN",{});var q6=s(Ip);c(Vr.$$.fragment,q6),q6.forEach(t),M6.forEach(t),N1=m(yu),Dp=o(yu,"SPAN",{});var C6=s(Dp);M1=l(C6,"Efficient Software Prebuilds"),C6.forEach(t),yu.forEach(t),kd=m(e),qt=o(e,"P",{});var bu=s(qt);q1=l(bu,"PyTorch\u2019s "),Yr=o(bu,"A",{href:!0,rel:!0});var O6=s(Yr);C1=l(O6,"pip and conda builds"),O6.forEach(t),O1=l(bu," come prebuit with the cuda toolkit which is enough to run PyTorch, but it is insufficient if you need to build cuda extensions."),bu.forEach(t),Pd=m(e),Ct=o(e,"P",{});var $u=s(Ct);L1=l($u,"At times it may take an additional effort to pre-build some components, e.g., if you\u2019re using libraries like "),Np=o($u,"CODE",{});var L6=s(Np);B1=l(L6,"apex"),L6.forEach(t),F1=l($u," that don\u2019t come pre-compiled. In other situations figuring out how to install the right cuda toolkit system-wide can be complicated. To address these users\u2019 needs PyTorch and NVIDIA release a new version of NGC docker container which already comes with everything prebuilt and you just need to install your programs on it and it will run out of the box."),$u.forEach(t),Ad=m(e),Ti=o(e,"P",{});var B6=s(Ti);W1=l(B6,"This approach is also useful if you want to tweak the pytorch source and/or make a new customized build."),B6.forEach(t),Td=m(e),J=o(e,"P",{});var An=s(J);H1=l(An,"To find the docker image version you want start "),Xr=o(An,"A",{href:!0,rel:!0});var F6=s(Xr);R1=l(F6,"here"),F6.forEach(t),V1=l(An,", choose one of the latest monthly releases. Go into the release\u2019s notes for the desired release, check that the environment\u2019s components are matching your needs (including NVIDIA Driver requirements!) and then at the very top of that document go to the corresponding NGC page. If for some reason you get lost, here is "),Zr=o(An,"A",{href:!0,rel:!0});var W6=s(Zr);Y1=l(W6,"the index of all PyTorch NGC images"),W6.forEach(t),X1=l(An,"."),An.forEach(t),xd=m(e),xi=o(e,"P",{});var H6=s(xi);Z1=l(H6,"Next follow the instructions to download and deploy the docker image."),H6.forEach(t),Gd=m(e),Me=o(e,"H2",{class:!0});var Eu=s(Me);Ot=o(Eu,"A",{id:!0,class:!0,href:!0});var R6=s(Ot);Mp=o(R6,"SPAN",{});var V6=s(Mp);c(Qr.$$.fragment,V6),V6.forEach(t),R6.forEach(t),Q1=m(Eu),qp=o(Eu,"SPAN",{});var Y6=s(qp);J1=l(Y6,"Sparsity"),Y6.forEach(t),Eu.forEach(t),zd=m(e),qe=o(e,"H3",{class:!0});var ku=s(qe);Lt=o(ku,"A",{id:!0,class:!0,href:!0});var X6=s(Lt);Cp=o(X6,"SPAN",{});var Z6=s(Cp);c(Jr.$$.fragment,Z6),Z6.forEach(t),X6.forEach(t),K1=m(ku),Op=o(ku,"SPAN",{});var Q6=s(Op);e2=l(Q6,"Mixture of Experts"),Q6.forEach(t),ku.forEach(t),Ud=m(e),Gi=o(e,"P",{});var J6=s(Gi);t2=l(J6,`Quite a few of the recent papers reported a 4-5x training speedup and a faster inference by integrating
Mixture of Experts (MoE) into the Transformer models.`),J6.forEach(t),jd=m(e),zi=o(e,"P",{});var K6=s(zi);a2=l(K6,"Since it has been discovered that more parameters lead to better performance, this technique allows to increase the number of parameters by an order of magnitude without increasing training costs."),K6.forEach(t),Sd=m(e),Ui=o(e,"P",{});var e4=s(Ui);r2=l(e4,"In this approach every other FFN layer is replaced with a MoE Layer which consists of many experts, with a gated function that trains each expert in a balanced way depending on the input token\u2019s position in a sequence."),e4.forEach(t),Id=m(e),ji=o(e,"P",{});var t4=s(ji);Si=o(t4,"IMG",{src:!0,alt:!0}),t4.forEach(t),Dd=m(e),Bt=o(e,"P",{});var Pu=s(Bt);o2=l(Pu,"(source: "),Kr=o(Pu,"A",{href:!0,rel:!0});var a4=s(Kr);s2=l(a4,"GLAM"),a4.forEach(t),i2=l(Pu,")"),Pu.forEach(t),Nd=m(e),Ii=o(e,"P",{});var r4=s(Ii);n2=l(r4,"You can find exhaustive details and comparison tables in the papers listed at the end of this section."),r4.forEach(t),Md=m(e),Di=o(e,"P",{});var o4=s(Di);l2=l(o4,"The main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude larger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements."),o4.forEach(t),qd=m(e),Ni=o(e,"P",{});var s4=s(Ni);p2=l(s4,"There is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or hundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the memory requirements moderately as well."),s4.forEach(t),Cd=m(e),Mi=o(e,"P",{});var i4=s(Mi);h2=l(i4,"Most related papers and implementations are built around Tensorflow/TPUs:"),i4.forEach(t),Od=m(e),K=o(e,"UL",{});var Tn=s(K);Lp=o(Tn,"LI",{});var n4=s(Lp);eo=o(n4,"A",{href:!0,rel:!0});var l4=s(eo);m2=l(l4,"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"),l4.forEach(t),n4.forEach(t),f2=m(Tn),Bp=o(Tn,"LI",{});var p4=s(Bp);to=o(p4,"A",{href:!0,rel:!0});var h4=s(to);d2=l(h4,"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"),h4.forEach(t),p4.forEach(t),c2=m(Tn),Fp=o(Tn,"LI",{});var m4=s(Fp);ao=o(m4,"A",{href:!0,rel:!0});var f4=s(ao);u2=l(f4,"GLaM: Generalist Language Model (GLaM)"),f4.forEach(t),m4.forEach(t),Tn.forEach(t),Ld=m(e),$=o(e,"P",{});var U=s($);g2=l(U,"And for Pytorch DeepSpeed has built one as well: "),ro=o(U,"A",{href:!0,rel:!0});var d4=s(ro);w2=l(d4,"DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"),d4.forEach(t),v2=l(U,", "),oo=o(U,"A",{href:!0,rel:!0});var c4=s(oo);_2=l(c4,"Mixture of Experts"),c4.forEach(t),y2=l(U," - blog posts:  "),so=o(U,"A",{href:!0,rel:!0});var u4=s(so);b2=l(u4,"1"),u4.forEach(t),$2=l(U,", "),io=o(U,"A",{href:!0,rel:!0});var g4=s(io);E2=l(g4,"2"),g4.forEach(t),k2=l(U," and specific deployment with large transformer-based natural language generation models: "),no=o(U,"A",{href:!0,rel:!0});var w4=s(no);P2=l(w4,"blog post"),w4.forEach(t),A2=l(U,", "),qi=o(U,"A",{href:!0});var v4=s(qi);T2=l(v4,"Megatron-Deepspeed branch"),v4.forEach(t),x2=l(U,"."),U.forEach(t),Bd=m(e),Ce=o(e,"H2",{class:!0});var Au=s(Ce);Ft=o(Au,"A",{id:!0,class:!0,href:!0});var _4=s(Ft);Wp=o(_4,"SPAN",{});var y4=s(Wp);c(lo.$$.fragment,y4),y4.forEach(t),_4.forEach(t),G2=m(Au),Hp=o(Au,"SPAN",{});var b4=s(Hp);z2=l(b4,"Scaling beyond a single GPU"),b4.forEach(t),Au.forEach(t),Fd=m(e),Ci=o(e,"P",{});var $4=s(Ci);U2=l($4,"For some applications, such as pretraining large language models, applying all the approaches above might still not be fast enough. In this case you want to scale your experiment to several GPUs."),$4.forEach(t),Wd=m(e),Wt=o(e,"P",{});var Tu=s(Wt);j2=l(Tu,"Another use case for training on many GPUs is if the model does not fit on a single GPU with all the mentioned tricks. There are still more methods we can apply although life starts to get a bit more complicated. This usually involves some form of pipeline or tensor parallelism where the model itself is distributed across several GPUs. One can also make use of DeepSpeed which implements some of these parallelism strategies along with some more optimization to reduce the memory footprint such as partitioning the optimizer states. You can read more about this in the "),Oi=o(Tu,"A",{href:!0});var E4=s(Oi);S2=l(E4,"\u201CMulti-GPU training\u201D section"),E4.forEach(t),I2=l(Tu,"."),Tu.forEach(t),Hd=m(e),Oe=o(e,"H2",{class:!0});var xu=s(Oe);Ht=o(xu,"A",{id:!0,class:!0,href:!0});var k4=s(Ht);Rp=o(k4,"SPAN",{});var P4=s(Rp);c(po.$$.fragment,P4),P4.forEach(t),k4.forEach(t),D2=m(xu),Vp=o(xu,"SPAN",{});var A4=s(Vp);N2=l(A4,"Inference with torchdynamo"),A4.forEach(t),xu.forEach(t),Rd=m(e),Rt=o(e,"P",{});var Gu=s(Rt);M2=l(Gu,"TorchDynamo is a new tracer that uses Python\u2019s frame evaluation API to automatically create FX traces from existing PyTorch programs. After capturing the FX graph, different backends can be deployed to lower the graph to an optimized engine. One solution is using the "),ho=o(Gu,"A",{href:!0,rel:!0});var T4=s(ho);q2=l(T4,"TensorRT"),T4.forEach(t),C2=l(Gu," or NVFuser as backend. You can choose one option below for performance boost."),Gu.forEach(t),Vd=m(e),c(mo.$$.fragment,e),Yd=m(e),Li=o(e,"P",{});var x4=s(Li);O2=l(x4,"This feature involves 3 different libraries. To install them, please follow the instructions below:"),x4.forEach(t),Xd=m(e),ee=o(e,"UL",{});var xn=s(ee);Yp=o(xn,"LI",{});var G4=s(Yp);fo=o(G4,"A",{href:!0,rel:!0});var z4=s(fo);L2=l(z4,"Torchdynamo installation"),z4.forEach(t),G4.forEach(t),B2=m(xn),Xp=o(xn,"LI",{});var U4=s(Xp);co=o(U4,"A",{href:!0,rel:!0});var j4=s(co);F2=l(j4,"Functorch installation"),j4.forEach(t),U4.forEach(t),W2=m(xn),Zp=o(xn,"LI",{});var S4=s(Zp);uo=o(S4,"A",{href:!0,rel:!0});var I4=s(uo);H2=l(I4,"Torch-TensorRT(FX) installation"),I4.forEach(t),S4.forEach(t),xn.forEach(t),Zd=m(e),Le=o(e,"H2",{class:!0});var zu=s(Le);Vt=o(zu,"A",{id:!0,class:!0,href:!0});var D4=s(Vt);Qp=o(D4,"SPAN",{});var N4=s(Qp);c(go.$$.fragment,N4),N4.forEach(t),D4.forEach(t),R2=m(zu),Bi=o(zu,"SPAN",{});var H3=s(Bi);Jp=o(H3,"CODE",{});var M4=s(Jp);V2=l(M4,"bitsandbytes"),M4.forEach(t),Y2=l(H3," integration for Int8 mixed-precision matrix decomposition"),H3.forEach(t),zu.forEach(t),Qd=m(e),A=o(e,"P",{});var M=s(A);X2=l(M,"From the paper "),wo=o(M,"A",{href:!0,rel:!0});var q4=s(wo);Kp=o(q4,"CODE",{});var C4=s(Kp);Z2=l(C4,"LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale"),C4.forEach(t),q4.forEach(t),Q2=l(M,`, we support HuggingFace integration for all models in the Hub with a few lines of code.
The method reduce `),eh=o(M,"CODE",{});var O4=s(eh);J2=l(O4,"nn.Linear"),O4.forEach(t),K2=l(M," size by 2 for "),th=o(M,"CODE",{});var L4=s(th);e3=l(L4,"float16"),L4.forEach(t),t3=l(M," and "),ah=o(M,"CODE",{});var B4=s(ah);a3=l(B4,"bfloat16"),B4.forEach(t),r3=l(M," weights and by 4 for "),rh=o(M,"CODE",{});var F4=s(rh);o3=l(F4,"float32"),F4.forEach(t),s3=l(M," weights, with close to no impact to the quality by operating on the outliers in half-precision."),M.forEach(t),Jd=m(e),Fi=o(e,"P",{});var W4=s(Fi);Wi=o(W4,"IMG",{src:!0,alt:!0}),W4.forEach(t),Kd=m(e),te=o(e,"P",{});var Gn=s(te);i3=l(Gn,`Int8 mixed-precision matrix decomposition works by separating a matrix multiplication into two streams: (1) a systematic feature outlier stream matrix multiplied in fp16 (0.01%), (2) a regular stream of int8 matrix multiplication (99.9%). With this method, int8 inference with no predictive degradation is possible for very large models.
For more details regarding the method, check out the `),vo=o(Gn,"A",{href:!0,rel:!0});var H4=s(vo);n3=l(H4,"paper"),H4.forEach(t),l3=l(Gn," or our "),_o=o(Gn,"A",{href:!0,rel:!0});var R4=s(_o);p3=l(R4,"blogpost about the integration"),R4.forEach(t),h3=l(Gn,"."),Gn.forEach(t),ec=m(e),Hi=o(e,"P",{});var V4=s(Hi);Ri=o(V4,"IMG",{src:!0,alt:!0}),V4.forEach(t),tc=m(e),Yt=o(e,"P",{});var Uu=s(Yt);m3=l(Uu,`Note, that you would require a GPU to run mixed-8bit models as the kernels have been compiled for GPUs only. Make sure that you have enough GPU memory to store the quarter (or half if your model weights are in half precision) of the model before using this feature.
Below are some notes to help you use this module, or follow the demos on `),Vi=o(Uu,"A",{href:!0});var Y4=s(Vi);f3=l(Y4,"Google colab"),Y4.forEach(t),d3=l(Uu,"."),Uu.forEach(t),ac=m(e),Be=o(e,"H3",{class:!0});var ju=s(Be);Xt=o(ju,"A",{id:!0,class:!0,href:!0});var X4=s(Xt);oh=o(X4,"SPAN",{});var Z4=s(oh);c(yo.$$.fragment,Z4),Z4.forEach(t),X4.forEach(t),c3=m(ju),sh=o(ju,"SPAN",{});var Q4=s(sh);u3=l(Q4,"Requirements"),Q4.forEach(t),ju.forEach(t),rc=m(e),ae=o(e,"UL",{});var zn=s(ae);ih=o(zn,"LI",{});var J4=s(ih);g3=l(J4,"Make sure you run that on NVIDIA GPUs that support 8-bit tensor cores (Turing, Ampere or newer architectures - e.g. T4, RTX20s RTX30s, A40-A100)."),J4.forEach(t),w3=m(zn),Zt=o(zn,"LI",{});var yh=s(Zt);v3=l(yh,"Install the correct version of "),nh=o(yh,"CODE",{});var K4=s(nh);_3=l(K4,"bitsandbytes"),K4.forEach(t),y3=l(yh,` by running:
`),lh=o(yh,"CODE",{});var ek=s(lh);b3=l(ek,"pip install bitsandbytes>=0.31.5"),ek.forEach(t),yh.forEach(t),$3=m(zn),Qt=o(zn,"LI",{});var bh=s(Qt);E3=l(bh,"Install "),ph=o(bh,"CODE",{});var tk=s(ph);k3=l(tk,"accelerate"),tk.forEach(t),P3=m(bh),hh=o(bh,"CODE",{});var ak=s(hh);A3=l(ak,"pip install accelerate>=0.12.0"),ak.forEach(t),bh.forEach(t),zn.forEach(t),oc=m(e),Fe=o(e,"H3",{class:!0});var Su=s(Fe);Jt=o(Su,"A",{id:!0,class:!0,href:!0});var rk=s(Jt);mh=o(rk,"SPAN",{});var ok=s(mh);c(bo.$$.fragment,ok),ok.forEach(t),rk.forEach(t),T3=m(Su),fh=o(Su,"SPAN",{});var sk=s(fh);x3=l(sk,"Running mixed-int8 models"),sk.forEach(t),Su.forEach(t),sc=m(e),Yi=o(e,"P",{});var ik=s(Yi);G3=l(ik,"After installing the required libraries, the way to load your mixed 8-bit model is as follows:"),ik.forEach(t),ic=m(e),c($o.$$.fragment,e),nc=m(e),re=o(e,"P",{});var Un=s(re);z3=l(Un,"The current implementation supports a multi-GPU setup when using "),dh=o(Un,"CODE",{});var nk=s(dh);U3=l(nk,"accelerate"),nk.forEach(t),j3=l(Un,". If you want to control the GPU memory you want to allocate for each GPU use the "),ch=o(Un,"CODE",{});var lk=s(ch);S3=l(lk,"max_memory"),lk.forEach(t),I3=l(Un," argument as follows:"),Un.forEach(t),lc=m(e),c(Eo.$$.fragment,e),pc=m(e),Xi=o(e,"P",{});var pk=s(Xi);D3=l(pk,"In this example, the first GPU will use 1GB of memory and the second 2GB."),pk.forEach(t),hc=m(e),We=o(e,"H3",{class:!0});var Iu=s(We);Kt=o(Iu,"A",{id:!0,class:!0,href:!0});var hk=s(Kt);uh=o(hk,"SPAN",{});var mk=s(uh);c(ko.$$.fragment,mk),mk.forEach(t),hk.forEach(t),N3=m(Iu),gh=o(Iu,"SPAN",{});var fk=s(gh);M3=l(fk,"Colab demos"),fk.forEach(t),Iu.forEach(t),mc=m(e),Zi=o(e,"P",{});var dk=s(Zi);q3=l(dk,`With this method you can infer on models that were not possible to infer on a Google Colab before.
Check out the demo for running T5-11b (42GB in fp32)! Using 8-bit quantization on Google Colab:`),dk.forEach(t),fc=m(e),Qi=o(e,"P",{});var ck=s(Qi);Po=o(ck,"A",{href:!0,rel:!0});var uk=s(Po);Ji=o(uk,"IMG",{src:!0,alt:!0}),uk.forEach(t),ck.forEach(t),dc=m(e),Ki=o(e,"P",{});var gk=s(Ki);C3=l(gk,"Or this demo for BLOOM-3B:"),gk.forEach(t),cc=m(e),en=o(e,"P",{});var wk=s(en);Ao=o(wk,"A",{href:!0,rel:!0});var vk=s(Ao);tn=o(vk,"IMG",{src:!0,alt:!0}),vk.forEach(t),wk.forEach(t),this.h()},h(){f(b,"name","hf:doc:metadata"),f(b,"content",JSON.stringify(xk)),f(j,"id","efficient-training-on-a-single-gpu"),f(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(j,"href","#efficient-training-on-a-single-gpu"),f(E,"class","relative group"),f(zo,"href","perf_train_gpu_many"),f(Uo,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.Trainer"),f(oa,"href","https://huggingface.co/docs/accelerate/"),f(oa,"rel","nofollow"),f(jo,"align","left"),f(So,"align","left"),f(Io,"align","left"),f(Do,"align","left"),f(No,"align","left"),f(Mo,"align","left"),f(qo,"align","left"),f(Co,"align","left"),f(Oo,"align","left"),f(Lo,"align","left"),f(Bo,"align","left"),f(Fo,"align","left"),f(Wo,"align","left"),f(Ho,"align","left"),f(Ro,"align","left"),f(Vo,"align","left"),f(Yo,"align","left"),f(Xo,"align","left"),f(Zo,"align","left"),f(Qo,"align","left"),f(Jo,"align","left"),f(Ko,"align","left"),f(es,"align","left"),f(ts,"align","left"),f(ia,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset"),f(ia,"rel","nofollow"),f(rs,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.Trainer"),f(Xe,"id","load-model"),f(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Xe,"href","#load-model"),f(ce,"class","relative group"),f(Ke,"id","vanilla-training"),f(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ke,"href","#vanilla-training"),f(ue,"class","relative group"),f(ls,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.Trainer"),f(tt,"id","anatomy-of-models-operations"),f(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(tt,"href","#anatomy-of-models-operations"),f(ge,"class","relative group"),f(Ta,"href","https://arxiv.org/abs/2007.00072"),f(Ta,"rel","nofollow"),f(at,"id","anatomy-of-models-memory"),f(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(at,"href","#anatomy-of-models-memory"),f(we,"class","relative group"),f(Ga,"href","https://github.com/facebookresearch/bitsandbytes"),f(Ga,"rel","nofollow"),f(st,"id","batch-sizes"),f(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(st,"href","#batch-sizes"),f(ve,"class","relative group"),f(Ua,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features"),f(Ua,"rel","nofollow"),f(ja,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size"),f(ja,"rel","nofollow"),f(Ia,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc"),f(Ia,"rel","nofollow"),f(Da,"href","https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization"),f(Da,"rel","nofollow"),f(nt,"id","gradient-accumulation"),f(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(nt,"href","#gradient-accumulation"),f(_e,"class","relative group"),f(Us,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.Trainer"),f(js,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.TrainingArguments"),f(Ca,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537"),f(Ca,"rel","nofollow"),f(Oa,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957"),f(Oa,"rel","nofollow"),f(lt,"id","gradient-checkpointing"),f(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(lt,"href","#gradient-checkpointing"),f(ye,"class","relative group"),f(Ba,"href","https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9"),f(Ba,"rel","nofollow"),f(Ds,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.Trainer"),f(Ns,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.TrainingArguments"),f(ht,"id","floating-data-types"),f(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ht,"href","#floating-data-types"),f(be,"class","relative group"),xo(Os.src,R3="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tf32-bf16-fp16-fp32.png")||f(Os,"src",R3),f(Os,"alt","data types"),f(Xa,"href","https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/"),f(Xa,"rel","nofollow"),f(mt,"id","fp16-training"),f(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(mt,"href","#fp16-training"),f(Ee,"class","relative group"),f(ft,"id","bf16"),f(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ft,"href","#bf16"),f(ke,"class","relative group"),f(dt,"id","tf32"),f(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(dt,"href","#tf32"),f(Pe,"class","relative group"),f(sr,"href","https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/"),f(sr,"rel","nofollow"),f(nr,"href","https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803"),f(nr,"rel","nofollow"),f(lr,"href","https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189"),f(lr,"rel","nofollow"),f(gt,"id","optimizer"),f(gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(gt,"href","#optimizer"),f(Ae,"class","relative group"),f(mr,"href","https://github.com/NVIDIA/apex"),f(mr,"rel","nofollow"),f(fr,"href","https://github.com/facebookresearch/bitsandbytes"),f(fr,"rel","nofollow"),f(dr,"href","https://github.com/huggingface/transformers/pull/15622"),f(dr,"rel","nofollow"),f(_t,"id","adafactor"),f(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(_t,"href","#adafactor"),f(Te,"class","relative group"),f(bt,"id","8bit-adam"),f(bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(bt,"href","#8bit-adam"),f(xe,"class","relative group"),f(ri,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.Trainer"),f(oi,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.Trainer"),f(Er,"href","https://github.com/facebookresearch/bitsandbytes"),f(Er,"rel","nofollow"),xo(pi.src,V3="https://huggingface.co/datasets/lvwerra/repo-images/raw/main/gpu-memory-savings.png")||f(pi,"src",V3),f(pi,"alt","png"),f(kt,"id","multitensor"),f(kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(kt,"href","#multitensor"),f(Ge,"class","relative group"),f(Pt,"id","using-accelerate"),f(Pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Pt,"href","#using-accelerate"),f(ze,"class","relative group"),f(hi,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.Trainer"),f(mi,"href","/docs/transformers/pr_18551/en/main_classes/trainer#transformers.TrainingArguments"),f(Sr,"href","https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"),f(Sr,"rel","nofollow"),f(di,"href","/docs/transformers/pr_18551/en/main_classes/model#transformers.PreTrainedModel.gradient_checkpointing_enable"),f(Ir,"href","https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator"),f(Ir,"rel","nofollow"),f(Dr,"href","https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare"),f(Dr,"rel","nofollow"),f(Mr,"href","https://huggingface.co/docs/accelerate/index"),f(Mr,"rel","nofollow"),f(xt,"id","dataloader"),f(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(xt,"href","#dataloader"),f(Ue,"class","relative group"),f(zt,"id","deepspeed-zero"),f(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(zt,"href","#deepspeed-zero"),f(je,"class","relative group"),f(wi,"href","main_classes/deepspeed"),f(_i,"href","main_classes/deepspeed#installation"),f(bi,"href","main_classes/deepspeed#deployment-with-one-gpu"),f($i,"href","main_classes/deepspeed#deployment-in-notebooks"),f(Wr,"href","https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py"),f(Wr,"rel","nofollow"),f(It,"id","choice-of-gpu"),f(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(It,"href","#choice-of-gpu"),f(Ie,"class","relative group"),f(Dt,"id","how-to-scale"),f(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Dt,"href","#how-to-scale"),f(De,"class","relative group"),f(Mt,"id","efficient-software-prebuilds"),f(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Mt,"href","#efficient-software-prebuilds"),f(Ne,"class","relative group"),f(Yr,"href","https://pytorch.org/get-started/locally/#start-locally"),f(Yr,"rel","nofollow"),f(Xr,"href","https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/"),f(Xr,"rel","nofollow"),f(Zr,"href","https://ngc.nvidia.com/catalog/containers/nvidia:pytorch"),f(Zr,"rel","nofollow"),f(Ot,"id","sparsity"),f(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ot,"href","#sparsity"),f(Me,"class","relative group"),f(Lt,"id","mixture-of-experts"),f(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Lt,"href","#mixture-of-experts"),f(qe,"class","relative group"),xo(Si.src,Y3="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perf-moe-transformer.png")||f(Si,"src",Y3),f(Si,"alt","MoE Transformer 2x block"),f(Kr,"href","https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html"),f(Kr,"rel","nofollow"),f(eo,"href","https://arxiv.org/abs/2006.16668"),f(eo,"rel","nofollow"),f(to,"href","https://arxiv.org/abs/2101.03961"),f(to,"rel","nofollow"),f(ao,"href","https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html"),f(ao,"rel","nofollow"),f(ro,"href","https://arxiv.org/abs/2201.05596"),f(ro,"rel","nofollow"),f(oo,"href","https://www.deepspeed.ai/tutorials/mixture-of-experts/"),f(oo,"rel","nofollow"),f(so,"href","https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/"),f(so,"rel","nofollow"),f(io,"href","https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/"),f(io,"rel","nofollow"),f(no,"href","https://www.deepspeed.ai/news/2021/12/09/deepspeed-moe-nlg.html"),f(no,"rel","nofollow"),f(qi,"href","Thttps://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training"),f(Ft,"id","scaling-beyond-a-single-gpu"),f(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ft,"href","#scaling-beyond-a-single-gpu"),f(Ce,"class","relative group"),f(Oi,"href","perf_train_gpu_many"),f(Ht,"id","inference-with-torchdynamo"),f(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ht,"href","#inference-with-torchdynamo"),f(Oe,"class","relative group"),f(ho,"href","https://developer.nvidia.com/tensorrt"),f(ho,"rel","nofollow"),f(fo,"href","https://github.com/pytorch/torchdynamo#requirements-and-setup"),f(fo,"rel","nofollow"),f(co,"href","https://github.com/pytorch/functorch#install"),f(co,"rel","nofollow"),f(uo,"href","https://github.com/pytorch/TensorRT/blob/master/docsrc/tutorials/getting_started_with_fx_path.rst#installation"),f(uo,"rel","nofollow"),f(Vt,"id","bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition"),f(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Vt,"href","#bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition"),f(Le,"class","relative group"),f(wo,"href","https://arxiv.org/abs/2208.07339"),f(wo,"rel","nofollow"),xo(Wi.src,X3="https://s3.amazonaws.com/moonup/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png")||f(Wi,"src",X3),f(Wi,"alt","HFxbitsandbytes.png"),f(vo,"href","https://arxiv.org/abs/2208.07339"),f(vo,"rel","nofollow"),f(_o,"href","https://huggingface.co/blog/hf-bitsandbytes-integration"),f(_o,"rel","nofollow"),xo(Ri.src,Z3="https://s3.amazonaws.com/moonup/production/uploads/1660567469965-62441d1d9fdefb55a0b7d12c.gif")||f(Ri,"src",Z3),f(Ri,"alt","MixedInt8.gif"),f(Vi,"href","#colab-demos"),f(Xt,"id","requirements"),f(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Xt,"href","#requirements"),f(Be,"class","relative group"),f(Jt,"id","running-mixedint8-models"),f(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Jt,"href","#running-mixedint8-models"),f(Fe,"class","relative group"),f(Kt,"id","colab-demos"),f(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Kt,"href","#colab-demos"),f(We,"class","relative group"),xo(Ji.src,Q3="https://colab.research.google.com/assets/colab-badge.svg")||f(Ji,"src",Q3),f(Ji,"alt","Open In Colab: T5-11b demo"),f(Po,"href","https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing"),f(Po,"rel","nofollow"),xo(tn.src,J3="https://colab.research.google.com/assets/colab-badge.svg")||f(tn,"src",J3),f(tn,"alt","Open In Colab: BLOOM-3b demo"),f(Ao,"href","https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing"),f(Ao,"rel","nofollow")},m(e,i){a(document.head,b),p(e,T,i),p(e,E,i),a(E,j),a(j,jn),u(ra,jn,null),a(E,Du),a(E,Sn),a(Sn,Nu),p(e,$h,i),p(e,He,i),a(He,Mu),a(He,zo),a(zo,qu),a(He,Cu),p(e,Eh,i),p(e,q,i),a(q,Ou),a(q,Uo),a(Uo,Lu),a(q,Bu),a(q,oa),a(oa,Fu),a(q,Wu),p(e,kh,i),p(e,Re,i),a(Re,In),a(In,ie),a(ie,jo),a(jo,Hu),a(ie,Ru),a(ie,So),a(So,Vu),a(ie,Yu),a(ie,Io),a(Io,Xu),a(Re,Zu),a(Re,k),a(k,ne),a(ne,Do),a(Do,Qu),a(ne,Ju),a(ne,No),a(No,Ku),a(ne,eg),a(ne,Mo),a(Mo,tg),a(k,ag),a(k,le),a(le,qo),a(qo,rg),a(le,og),a(le,Co),a(Co,sg),a(le,ig),a(le,Oo),a(Oo,ng),a(k,lg),a(k,pe),a(pe,Lo),a(Lo,pg),a(pe,hg),a(pe,Bo),a(Bo,mg),a(pe,fg),a(pe,Fo),a(Fo,dg),a(k,cg),a(k,he),a(he,Wo),a(Wo,ug),a(he,gg),a(he,Ho),a(Ho,wg),a(he,vg),a(he,Ro),a(Ro,_g),a(k,yg),a(k,me),a(me,Vo),a(Vo,bg),a(me,$g),a(me,Yo),a(Yo,Eg),a(me,kg),a(me,Xo),a(Xo,Pg),a(k,Ag),a(k,fe),a(fe,Zo),a(Zo,Tg),a(fe,xg),a(fe,Qo),a(Qo,Gg),a(fe,zg),a(fe,Jo),a(Jo,Ug),a(k,jg),a(k,de),a(de,Ko),a(Ko,Sg),a(de,Ig),a(de,es),a(es,Dg),a(de,Ng),a(de,ts),a(ts,Mg),p(e,Ph,i),p(e,as,i),a(as,qg),p(e,Ah,i),u(sa,e,i),p(e,Th,i),p(e,C,i),a(C,Cg),a(C,Dn),a(Dn,Og),a(C,Lg),a(C,Nn),a(Nn,Bg),a(C,Fg),p(e,xh,i),p(e,Ve,i),a(Ve,Wg),a(Ve,ia),a(ia,Hg),a(Ve,Rg),p(e,Gh,i),u(na,e,i),p(e,zh,i),p(e,Ye,i),a(Ye,Vg),a(Ye,rs),a(rs,Yg),a(Ye,Xg),p(e,Uh,i),u(la,e,i),p(e,jh,i),p(e,os,i),a(os,Zg),p(e,Sh,i),u(pa,e,i),p(e,Ih,i),p(e,ss,i),a(ss,Qg),p(e,Dh,i),u(ha,e,i),p(e,Nh,i),p(e,is,i),a(is,Jg),p(e,Mh,i),p(e,ce,i),a(ce,Xe),a(Xe,Mn),u(ma,Mn,null),a(ce,Kg),a(ce,qn),a(qn,ew),p(e,qh,i),p(e,Ze,i),a(Ze,tw),a(Ze,Cn),a(Cn,aw),a(Ze,rw),p(e,Ch,i),u(fa,e,i),p(e,Oh,i),p(e,Qe,i),a(Qe,ow),a(Qe,On),a(On,sw),a(Qe,iw),p(e,Lh,i),u(da,e,i),p(e,Bh,i),u(ca,e,i),p(e,Fh,i),p(e,ns,i),a(ns,nw),p(e,Wh,i),u(ua,e,i),p(e,Hh,i),u(Je,e,i),p(e,Rh,i),p(e,ue,i),a(ue,Ke),a(Ke,Ln),u(ga,Ln,null),a(ue,lw),a(ue,Bn),a(Bn,pw),p(e,Vh,i),p(e,et,i),a(et,hw),a(et,ls),a(ls,mw),a(et,fw),p(e,Yh,i),u(wa,e,i),p(e,Xh,i),u(va,e,i),p(e,Zh,i),p(e,ps,i),a(ps,dw),p(e,Qh,i),p(e,ge,i),a(ge,tt),a(tt,Fn),u(_a,Fn,null),a(ge,cw),a(ge,Wn),a(Wn,uw),p(e,Jh,i),p(e,hs,i),a(hs,gw),p(e,Kh,i),p(e,O,i),a(O,ya),a(ya,Hn),a(Hn,Rn),a(Rn,ww),a(ya,vw),a(ya,ba),a(ba,_w),a(ba,Vn),a(Vn,yw),a(ba,bw),a(O,$w),a(O,$a),a($a,Yn),a(Yn,Xn),a(Xn,Ew),a($a,kw),a($a,Ea),a(Ea,Pw),a(Ea,Zn),a(Zn,Aw),a(Ea,Tw),a(O,xw),a(O,ka),a(ka,Qn),a(Qn,Jn),a(Jn,Gw),a(ka,zw),a(ka,Pa),a(Pa,Uw),a(Pa,Kn),a(Kn,jw),a(Pa,Sw),p(e,em,i),p(e,ms,i),a(ms,Iw),p(e,tm,i),p(e,Aa,i),a(Aa,Dw),a(Aa,Ta),a(Ta,Nw),p(e,am,i),p(e,we,i),a(we,at),a(at,el),u(xa,el,null),a(we,Mw),a(we,tl),a(tl,qw),p(e,rm,i),p(e,fs,i),a(fs,Cw),p(e,om,i),p(e,ds,i),a(ds,Ow),p(e,sm,i),p(e,cs,i),a(cs,al),a(al,Lw),p(e,im,i),p(e,rt,i),a(rt,rl),a(rl,Bw),a(rt,Fw),a(rt,ol),a(ol,Ww),p(e,nm,i),p(e,us,i),a(us,sl),a(sl,Hw),p(e,lm,i),p(e,L,i),a(L,il),a(il,Rw),a(L,Vw),a(L,gs),a(gs,Yw),a(gs,Ga),a(Ga,Xw),a(L,Zw),a(L,nl),a(nl,Qw),p(e,pm,i),p(e,ws,i),a(ws,ll),a(ll,Jw),p(e,hm,i),p(e,vs,i),a(vs,pl),a(pl,Kw),p(e,mm,i),p(e,_s,i),a(_s,hl),a(hl,ev),p(e,fm,i),p(e,ys,i),a(ys,ml),a(ml,tv),p(e,dm,i),p(e,bs,i),a(bs,av),p(e,cm,i),p(e,$s,i),a($s,fl),a(fl,rv),p(e,um,i),p(e,Es,i),a(Es,ov),p(e,gm,i),p(e,ks,i),a(ks,dl),a(dl,sv),p(e,wm,i),p(e,Ps,i),a(Ps,iv),p(e,vm,i),p(e,As,i),a(As,ot),a(ot,cl),a(cl,nv),a(ot,lv),a(ot,ul),a(ul,pv),a(ot,hv),p(e,_m,i),p(e,Ts,i),a(Ts,mv),p(e,ym,i),p(e,xs,i),a(xs,fv),p(e,bm,i),p(e,ve,i),a(ve,st),a(st,gl),u(za,gl,null),a(ve,dv),a(ve,wl),a(wl,cv),p(e,$m,i),p(e,Gs,i),a(Gs,uv),p(e,Em,i),p(e,B,i),a(B,gv),a(B,Ua),a(Ua,wv),a(B,vv),a(B,ja),a(ja,_v),a(B,yv),p(e,km,i),p(e,Sa,i),a(Sa,Ia),a(Ia,bv),a(Sa,$v),p(e,Pm,i),p(e,it,i),a(it,Ev),a(it,Da),a(Da,kv),a(it,Pv),p(e,Am,i),p(e,_e,i),a(_e,nt),a(nt,vl),u(Na,vl,null),a(_e,Av),a(_e,_l),a(_l,Tv),p(e,Tm,i),p(e,zs,i),a(zs,xv),p(e,xm,i),p(e,S,i),a(S,Gv),a(S,Us),a(Us,zv),a(S,Uv),a(S,yl),a(yl,jv),a(S,Sv),a(S,js),a(js,Iv),a(S,Dv),p(e,Gm,i),u(Ma,e,i),p(e,zm,i),u(qa,e,i),p(e,Um,i),p(e,x,i),a(x,Nv),a(x,bl),a(bl,Mv),a(x,qv),a(x,$l),a($l,Cv),a(x,Ov),a(x,El),a(El,Lv),a(x,Bv),a(x,kl),a(kl,Fv),a(x,Wv),p(e,jm,i),p(e,F,i),a(F,Hv),a(F,Ca),a(Ca,Rv),a(F,Vv),a(F,Oa),a(Oa,Yv),a(F,Xv),p(e,Sm,i),p(e,Ss,i),a(Ss,Zv),p(e,Im,i),p(e,ye,i),a(ye,lt),a(lt,Pl),u(La,Pl,null),a(ye,Qv),a(ye,Al),a(Al,Jv),p(e,Dm,i),p(e,Is,i),a(Is,Kv),p(e,Nm,i),p(e,pt,i),a(pt,e_),a(pt,Ba),a(Ba,t_),a(pt,a_),p(e,Mm,i),p(e,W,i),a(W,r_),a(W,Ds),a(Ds,o_),a(W,s_),a(W,Ns),a(Ns,i_),a(W,n_),p(e,qm,i),u(Fa,e,i),p(e,Cm,i),u(Wa,e,i),p(e,Om,i),p(e,Ms,i),a(Ms,l_),p(e,Lm,i),p(e,be,i),a(be,ht),a(ht,Tl),u(Ha,Tl,null),a(be,p_),a(be,xl),a(xl,h_),p(e,Bm,i),p(e,qs,i),a(qs,m_),p(e,Fm,i),p(e,I,i),a(I,Ra),a(Ra,f_),a(Ra,Gl),a(Gl,d_),a(Ra,c_),a(I,u_),a(I,Va),a(Va,g_),a(Va,zl),a(zl,w_),a(Va,v_),a(I,__),a(I,Ya),a(Ya,y_),a(Ya,Ul),a(Ul,b_),a(Ya,$_),a(I,E_),a(I,jl),a(jl,k_),p(e,Wm,i),p(e,Cs,i),a(Cs,P_),p(e,Hm,i),p(e,$e,i),a($e,Os),a($e,A_),a($e,Xa),a(Xa,T_),a($e,x_),p(e,Rm,i),p(e,Ls,i),a(Ls,G_),p(e,Vm,i),p(e,Ee,i),a(Ee,mt),a(mt,Sl),u(Za,Sl,null),a(Ee,z_),a(Ee,Il),a(Il,U_),p(e,Ym,i),p(e,H,i),a(H,j_),a(H,Dl),a(Dl,S_),a(H,I_),a(H,Nl),a(Nl,D_),a(H,N_),p(e,Xm,i),u(Qa,e,i),p(e,Zm,i),u(Ja,e,i),p(e,Qm,i),p(e,Bs,i),a(Bs,M_),p(e,Jm,i),u(Ka,e,i),p(e,Km,i),u(er,e,i),p(e,ef,i),p(e,Fs,i),a(Fs,q_),p(e,tf,i),p(e,ke,i),a(ke,ft),a(ft,Ml),u(tr,Ml,null),a(ke,C_),a(ke,ql),a(ql,O_),p(e,af,i),p(e,Ws,i),a(Ws,L_),p(e,rf,i),u(ar,e,i),p(e,of,i),p(e,Pe,i),a(Pe,dt),a(dt,Cl),u(rr,Cl,null),a(Pe,B_),a(Pe,Ol),a(Ol,F_),p(e,sf,i),p(e,Hs,i),a(Hs,W_),p(e,nf,i),u(or,e,i),p(e,lf,i),p(e,Rs,i),a(Rs,H_),p(e,pf,i),p(e,ct,i),a(ct,R_),a(ct,sr),a(sr,V_),a(ct,Y_),p(e,hf,i),p(e,Vs,i),a(Vs,X_),p(e,mf,i),p(e,Ys,i),a(Ys,Z_),p(e,ff,i),u(ir,e,i),p(e,df,i),p(e,Xs,i),a(Xs,Q_),p(e,cf,i),p(e,R,i),a(R,J_),a(R,Ll),a(Ll,K_),a(R,ey),a(R,Bl),a(Bl,ty),a(R,ay),p(e,uf,i),p(e,ut,i),a(ut,ry),a(ut,Fl),a(Fl,oy),a(ut,sy),p(e,gf,i),p(e,V,i),a(V,iy),a(V,nr),a(nr,ny),a(V,ly),a(V,lr),a(lr,py),a(V,hy),p(e,wf,i),p(e,Zs,i),a(Zs,my),p(e,vf,i),p(e,Ae,i),a(Ae,gt),a(gt,Wl),u(pr,Wl,null),a(Ae,fy),a(Ae,Hl),a(Hl,dy),p(e,_f,i),p(e,Qs,i),a(Qs,cy),p(e,yf,i),p(e,wt,i),a(wt,uy),a(wt,Rl),a(Rl,gy),a(wt,wy),p(e,bf,i),p(e,Js,i),a(Js,vy),p(e,$f,i),u(hr,e,i),p(e,Ef,i),p(e,Y,i),a(Y,_y),a(Y,mr),a(mr,yy),a(Y,by),a(Y,Vl),a(Vl,$y),a(Y,Ey),p(e,kf,i),p(e,X,i),a(X,ky),a(X,fr),a(fr,Py),a(X,Ay),a(X,dr),a(dr,Ty),a(X,xy),p(e,Pf,i),p(e,vt,i),a(vt,Gy),a(vt,Yl),a(Yl,zy),a(vt,Uy),p(e,Af,i),p(e,Z,i),a(Z,cr),a(cr,jy),a(cr,Xl),a(Xl,Sy),a(cr,Iy),a(Z,Dy),a(Z,ur),a(ur,Ny),a(ur,Zl),a(Zl,My),a(ur,qy),a(Z,Cy),a(Z,gr),a(gr,Oy),a(gr,Ql),a(Ql,Ly),a(gr,By),p(e,Tf,i),p(e,Ks,i),a(Ks,Fy),p(e,xf,i),p(e,Te,i),a(Te,_t),a(_t,Jl),u(wr,Jl,null),a(Te,Wy),a(Te,Kl),a(Kl,Hy),p(e,Gf,i),p(e,yt,i),a(yt,Ry),a(yt,ep),a(ep,Vy),a(yt,Yy),p(e,zf,i),u(vr,e,i),p(e,Uf,i),u(_r,e,i),p(e,jf,i),p(e,ei,i),a(ei,Xy),p(e,Sf,i),u(yr,e,i),p(e,If,i),u(br,e,i),p(e,Df,i),p(e,ti,i),a(ti,Zy),p(e,Nf,i),p(e,xe,i),a(xe,bt),a(bt,tp),u($r,tp,null),a(xe,Qy),a(xe,ap),a(ap,Jy),p(e,Mf,i),p(e,ai,i),a(ai,Ky),p(e,qf,i),p(e,G,i),a(G,eb),a(G,ri),a(ri,tb),a(G,ab),a(G,oi),a(oi,rb),a(G,ob),a(G,Er),a(Er,sb),a(G,ib),a(G,rp),a(rp,nb),a(G,lb),p(e,Cf,i),p(e,si,i),a(si,pb),p(e,Of,i),u($t,e,i),p(e,Lf,i),u(kr,e,i),p(e,Bf,i),p(e,Et,i),a(Et,hb),a(Et,op),a(op,mb),a(Et,fb),p(e,Ff,i),u(Pr,e,i),p(e,Wf,i),u(Ar,e,i),p(e,Hf,i),p(e,ii,i),a(ii,db),p(e,Rf,i),u(Tr,e,i),p(e,Vf,i),u(xr,e,i),p(e,Yf,i),p(e,ni,i),a(ni,cb),p(e,Xf,i),p(e,li,i),a(li,pi),p(e,Zf,i),p(e,Ge,i),a(Ge,kt),a(kt,sp),u(Gr,sp,null),a(Ge,ub),a(Ge,ip),a(ip,np),a(np,gb),p(e,Qf,i),p(e,ze,i),a(ze,Pt),a(Pt,lp),u(zr,lp,null),a(ze,wb),a(ze,pp),a(pp,vb),p(e,Jf,i),p(e,Q,i),a(Q,_b),a(Q,hi),a(hi,yb),a(Q,bb),a(Q,mi),a(mi,$b),a(Q,Eb),p(e,Kf,i),u(Ur,e,i),p(e,ed,i),p(e,fi,i),a(fi,kb),p(e,td,i),u(jr,e,i),p(e,ad,i),p(e,P,i),a(P,Pb),a(P,Sr),a(Sr,hp),a(hp,Ab),a(P,Tb),a(P,di),a(di,xb),a(P,Gb),a(P,Ir),a(Ir,mp),a(mp,zb),a(P,Ub),a(P,fp),a(fp,jb),a(P,Sb),a(P,Dr),a(Dr,dp),a(dp,Ib),a(P,Db),p(e,rd,i),p(e,At,i),a(At,Nb),a(At,cp),a(cp,Mb),a(At,qb),p(e,od,i),u(Nr,e,i),p(e,sd,i),p(e,Tt,i),a(Tt,Cb),a(Tt,Mr),a(Mr,Ob),a(Tt,Lb),p(e,id,i),p(e,Ue,i),a(Ue,xt),a(xt,up),u(qr,up,null),a(Ue,Bb),a(Ue,gp),a(gp,Fb),p(e,nd,i),p(e,ci,i),a(ci,Wb),p(e,ld,i),p(e,Gt,i),a(Gt,ui),a(ui,wp),a(wp,Hb),a(ui,Rb),a(Gt,Vb),a(Gt,gi),a(gi,vp),a(vp,Yb),a(gi,Xb),p(e,pd,i),p(e,je,i),a(je,zt),a(zt,_p),u(Cr,_p,null),a(je,Zb),a(je,yp),a(yp,Qb),p(e,hd,i),p(e,Ut,i),a(Ut,Jb),a(Ut,wi),a(wi,Kb),a(Ut,e1),p(e,md,i),p(e,vi,i),a(vi,t1),p(e,fd,i),p(e,jt,i),a(jt,bp),a(bp,a1),a(jt,r1),a(jt,$p),a($p,o1),p(e,dd,i),p(e,St,i),a(St,s1),a(St,_i),a(_i,i1),a(St,n1),p(e,cd,i),p(e,yi,i),a(yi,l1),p(e,ud,i),p(e,D,i),a(D,Ep),a(Ep,Or),a(Or,p1),a(Or,bi),a(bi,h1),a(Or,m1),a(D,f1),a(D,Lr),a(Lr,kp),a(kp,d1),a(Lr,c1),u(Br,Lr,null),a(D,u1),a(D,Pp),a(Pp,Fr),a(Fr,g1),a(Fr,$i),a($i,w1),a(Fr,v1),a(D,_1),a(D,Ap),a(Ap,Se),a(Se,y1),a(Se,Wr),a(Wr,b1),a(Se,$1),a(Se,Tp),a(Tp,E1),a(Se,k1),p(e,gd,i),p(e,Ie,i),a(Ie,It),a(It,xp),u(Hr,xp,null),a(Ie,P1),a(Ie,Gp),a(Gp,A1),p(e,wd,i),p(e,Ei,i),a(Ei,T1),p(e,vd,i),p(e,De,i),a(De,Dt),a(Dt,zp),u(Rr,zp,null),a(De,x1),a(De,Up),a(Up,G1),p(e,_d,i),p(e,ki,i),a(ki,z1),p(e,yd,i),p(e,Nt,i),a(Nt,jp),a(jp,U1),a(Nt,j1),a(Nt,Sp),a(Sp,S1),p(e,bd,i),p(e,Pi,i),a(Pi,I1),p(e,$d,i),p(e,Ai,i),a(Ai,D1),p(e,Ed,i),p(e,Ne,i),a(Ne,Mt),a(Mt,Ip),u(Vr,Ip,null),a(Ne,N1),a(Ne,Dp),a(Dp,M1),p(e,kd,i),p(e,qt,i),a(qt,q1),a(qt,Yr),a(Yr,C1),a(qt,O1),p(e,Pd,i),p(e,Ct,i),a(Ct,L1),a(Ct,Np),a(Np,B1),a(Ct,F1),p(e,Ad,i),p(e,Ti,i),a(Ti,W1),p(e,Td,i),p(e,J,i),a(J,H1),a(J,Xr),a(Xr,R1),a(J,V1),a(J,Zr),a(Zr,Y1),a(J,X1),p(e,xd,i),p(e,xi,i),a(xi,Z1),p(e,Gd,i),p(e,Me,i),a(Me,Ot),a(Ot,Mp),u(Qr,Mp,null),a(Me,Q1),a(Me,qp),a(qp,J1),p(e,zd,i),p(e,qe,i),a(qe,Lt),a(Lt,Cp),u(Jr,Cp,null),a(qe,K1),a(qe,Op),a(Op,e2),p(e,Ud,i),p(e,Gi,i),a(Gi,t2),p(e,jd,i),p(e,zi,i),a(zi,a2),p(e,Sd,i),p(e,Ui,i),a(Ui,r2),p(e,Id,i),p(e,ji,i),a(ji,Si),p(e,Dd,i),p(e,Bt,i),a(Bt,o2),a(Bt,Kr),a(Kr,s2),a(Bt,i2),p(e,Nd,i),p(e,Ii,i),a(Ii,n2),p(e,Md,i),p(e,Di,i),a(Di,l2),p(e,qd,i),p(e,Ni,i),a(Ni,p2),p(e,Cd,i),p(e,Mi,i),a(Mi,h2),p(e,Od,i),p(e,K,i),a(K,Lp),a(Lp,eo),a(eo,m2),a(K,f2),a(K,Bp),a(Bp,to),a(to,d2),a(K,c2),a(K,Fp),a(Fp,ao),a(ao,u2),p(e,Ld,i),p(e,$,i),a($,g2),a($,ro),a(ro,w2),a($,v2),a($,oo),a(oo,_2),a($,y2),a($,so),a(so,b2),a($,$2),a($,io),a(io,E2),a($,k2),a($,no),a(no,P2),a($,A2),a($,qi),a(qi,T2),a($,x2),p(e,Bd,i),p(e,Ce,i),a(Ce,Ft),a(Ft,Wp),u(lo,Wp,null),a(Ce,G2),a(Ce,Hp),a(Hp,z2),p(e,Fd,i),p(e,Ci,i),a(Ci,U2),p(e,Wd,i),p(e,Wt,i),a(Wt,j2),a(Wt,Oi),a(Oi,S2),a(Wt,I2),p(e,Hd,i),p(e,Oe,i),a(Oe,Ht),a(Ht,Rp),u(po,Rp,null),a(Oe,D2),a(Oe,Vp),a(Vp,N2),p(e,Rd,i),p(e,Rt,i),a(Rt,M2),a(Rt,ho),a(ho,q2),a(Rt,C2),p(e,Vd,i),u(mo,e,i),p(e,Yd,i),p(e,Li,i),a(Li,O2),p(e,Xd,i),p(e,ee,i),a(ee,Yp),a(Yp,fo),a(fo,L2),a(ee,B2),a(ee,Xp),a(Xp,co),a(co,F2),a(ee,W2),a(ee,Zp),a(Zp,uo),a(uo,H2),p(e,Zd,i),p(e,Le,i),a(Le,Vt),a(Vt,Qp),u(go,Qp,null),a(Le,R2),a(Le,Bi),a(Bi,Jp),a(Jp,V2),a(Bi,Y2),p(e,Qd,i),p(e,A,i),a(A,X2),a(A,wo),a(wo,Kp),a(Kp,Z2),a(A,Q2),a(A,eh),a(eh,J2),a(A,K2),a(A,th),a(th,e3),a(A,t3),a(A,ah),a(ah,a3),a(A,r3),a(A,rh),a(rh,o3),a(A,s3),p(e,Jd,i),p(e,Fi,i),a(Fi,Wi),p(e,Kd,i),p(e,te,i),a(te,i3),a(te,vo),a(vo,n3),a(te,l3),a(te,_o),a(_o,p3),a(te,h3),p(e,ec,i),p(e,Hi,i),a(Hi,Ri),p(e,tc,i),p(e,Yt,i),a(Yt,m3),a(Yt,Vi),a(Vi,f3),a(Yt,d3),p(e,ac,i),p(e,Be,i),a(Be,Xt),a(Xt,oh),u(yo,oh,null),a(Be,c3),a(Be,sh),a(sh,u3),p(e,rc,i),p(e,ae,i),a(ae,ih),a(ih,g3),a(ae,w3),a(ae,Zt),a(Zt,v3),a(Zt,nh),a(nh,_3),a(Zt,y3),a(Zt,lh),a(lh,b3),a(ae,$3),a(ae,Qt),a(Qt,E3),a(Qt,ph),a(ph,k3),a(Qt,P3),a(Qt,hh),a(hh,A3),p(e,oc,i),p(e,Fe,i),a(Fe,Jt),a(Jt,mh),u(bo,mh,null),a(Fe,T3),a(Fe,fh),a(fh,x3),p(e,sc,i),p(e,Yi,i),a(Yi,G3),p(e,ic,i),u($o,e,i),p(e,nc,i),p(e,re,i),a(re,z3),a(re,dh),a(dh,U3),a(re,j3),a(re,ch),a(ch,S3),a(re,I3),p(e,lc,i),u(Eo,e,i),p(e,pc,i),p(e,Xi,i),a(Xi,D3),p(e,hc,i),p(e,We,i),a(We,Kt),a(Kt,uh),u(ko,uh,null),a(We,N3),a(We,gh),a(gh,M3),p(e,mc,i),p(e,Zi,i),a(Zi,q3),p(e,fc,i),p(e,Qi,i),a(Qi,Po),a(Po,Ji),p(e,dc,i),p(e,Ki,i),a(Ki,C3),p(e,cc,i),p(e,en,i),a(en,Ao),a(Ao,tn),uc=!0},p(e,[i]){const To={};i&2&&(To.$$scope={dirty:i,ctx:e}),Je.$set(To);const wh={};i&2&&(wh.$$scope={dirty:i,ctx:e}),$t.$set(wh)},i(e){uc||(g(ra.$$.fragment,e),g(sa.$$.fragment,e),g(na.$$.fragment,e),g(la.$$.fragment,e),g(pa.$$.fragment,e),g(ha.$$.fragment,e),g(ma.$$.fragment,e),g(fa.$$.fragment,e),g(da.$$.fragment,e),g(ca.$$.fragment,e),g(ua.$$.fragment,e),g(Je.$$.fragment,e),g(ga.$$.fragment,e),g(wa.$$.fragment,e),g(va.$$.fragment,e),g(_a.$$.fragment,e),g(xa.$$.fragment,e),g(za.$$.fragment,e),g(Na.$$.fragment,e),g(Ma.$$.fragment,e),g(qa.$$.fragment,e),g(La.$$.fragment,e),g(Fa.$$.fragment,e),g(Wa.$$.fragment,e),g(Ha.$$.fragment,e),g(Za.$$.fragment,e),g(Qa.$$.fragment,e),g(Ja.$$.fragment,e),g(Ka.$$.fragment,e),g(er.$$.fragment,e),g(tr.$$.fragment,e),g(ar.$$.fragment,e),g(rr.$$.fragment,e),g(or.$$.fragment,e),g(ir.$$.fragment,e),g(pr.$$.fragment,e),g(hr.$$.fragment,e),g(wr.$$.fragment,e),g(vr.$$.fragment,e),g(_r.$$.fragment,e),g(yr.$$.fragment,e),g(br.$$.fragment,e),g($r.$$.fragment,e),g($t.$$.fragment,e),g(kr.$$.fragment,e),g(Pr.$$.fragment,e),g(Ar.$$.fragment,e),g(Tr.$$.fragment,e),g(xr.$$.fragment,e),g(Gr.$$.fragment,e),g(zr.$$.fragment,e),g(Ur.$$.fragment,e),g(jr.$$.fragment,e),g(Nr.$$.fragment,e),g(qr.$$.fragment,e),g(Cr.$$.fragment,e),g(Br.$$.fragment,e),g(Hr.$$.fragment,e),g(Rr.$$.fragment,e),g(Vr.$$.fragment,e),g(Qr.$$.fragment,e),g(Jr.$$.fragment,e),g(lo.$$.fragment,e),g(po.$$.fragment,e),g(mo.$$.fragment,e),g(go.$$.fragment,e),g(yo.$$.fragment,e),g(bo.$$.fragment,e),g($o.$$.fragment,e),g(Eo.$$.fragment,e),g(ko.$$.fragment,e),uc=!0)},o(e){w(ra.$$.fragment,e),w(sa.$$.fragment,e),w(na.$$.fragment,e),w(la.$$.fragment,e),w(pa.$$.fragment,e),w(ha.$$.fragment,e),w(ma.$$.fragment,e),w(fa.$$.fragment,e),w(da.$$.fragment,e),w(ca.$$.fragment,e),w(ua.$$.fragment,e),w(Je.$$.fragment,e),w(ga.$$.fragment,e),w(wa.$$.fragment,e),w(va.$$.fragment,e),w(_a.$$.fragment,e),w(xa.$$.fragment,e),w(za.$$.fragment,e),w(Na.$$.fragment,e),w(Ma.$$.fragment,e),w(qa.$$.fragment,e),w(La.$$.fragment,e),w(Fa.$$.fragment,e),w(Wa.$$.fragment,e),w(Ha.$$.fragment,e),w(Za.$$.fragment,e),w(Qa.$$.fragment,e),w(Ja.$$.fragment,e),w(Ka.$$.fragment,e),w(er.$$.fragment,e),w(tr.$$.fragment,e),w(ar.$$.fragment,e),w(rr.$$.fragment,e),w(or.$$.fragment,e),w(ir.$$.fragment,e),w(pr.$$.fragment,e),w(hr.$$.fragment,e),w(wr.$$.fragment,e),w(vr.$$.fragment,e),w(_r.$$.fragment,e),w(yr.$$.fragment,e),w(br.$$.fragment,e),w($r.$$.fragment,e),w($t.$$.fragment,e),w(kr.$$.fragment,e),w(Pr.$$.fragment,e),w(Ar.$$.fragment,e),w(Tr.$$.fragment,e),w(xr.$$.fragment,e),w(Gr.$$.fragment,e),w(zr.$$.fragment,e),w(Ur.$$.fragment,e),w(jr.$$.fragment,e),w(Nr.$$.fragment,e),w(qr.$$.fragment,e),w(Cr.$$.fragment,e),w(Br.$$.fragment,e),w(Hr.$$.fragment,e),w(Rr.$$.fragment,e),w(Vr.$$.fragment,e),w(Qr.$$.fragment,e),w(Jr.$$.fragment,e),w(lo.$$.fragment,e),w(po.$$.fragment,e),w(mo.$$.fragment,e),w(go.$$.fragment,e),w(yo.$$.fragment,e),w(bo.$$.fragment,e),w($o.$$.fragment,e),w(Eo.$$.fragment,e),w(ko.$$.fragment,e),uc=!1},d(e){t(b),e&&t(T),e&&t(E),v(ra),e&&t($h),e&&t(He),e&&t(Eh),e&&t(q),e&&t(kh),e&&t(Re),e&&t(Ph),e&&t(as),e&&t(Ah),v(sa,e),e&&t(Th),e&&t(C),e&&t(xh),e&&t(Ve),e&&t(Gh),v(na,e),e&&t(zh),e&&t(Ye),e&&t(Uh),v(la,e),e&&t(jh),e&&t(os),e&&t(Sh),v(pa,e),e&&t(Ih),e&&t(ss),e&&t(Dh),v(ha,e),e&&t(Nh),e&&t(is),e&&t(Mh),e&&t(ce),v(ma),e&&t(qh),e&&t(Ze),e&&t(Ch),v(fa,e),e&&t(Oh),e&&t(Qe),e&&t(Lh),v(da,e),e&&t(Bh),v(ca,e),e&&t(Fh),e&&t(ns),e&&t(Wh),v(ua,e),e&&t(Hh),v(Je,e),e&&t(Rh),e&&t(ue),v(ga),e&&t(Vh),e&&t(et),e&&t(Yh),v(wa,e),e&&t(Xh),v(va,e),e&&t(Zh),e&&t(ps),e&&t(Qh),e&&t(ge),v(_a),e&&t(Jh),e&&t(hs),e&&t(Kh),e&&t(O),e&&t(em),e&&t(ms),e&&t(tm),e&&t(Aa),e&&t(am),e&&t(we),v(xa),e&&t(rm),e&&t(fs),e&&t(om),e&&t(ds),e&&t(sm),e&&t(cs),e&&t(im),e&&t(rt),e&&t(nm),e&&t(us),e&&t(lm),e&&t(L),e&&t(pm),e&&t(ws),e&&t(hm),e&&t(vs),e&&t(mm),e&&t(_s),e&&t(fm),e&&t(ys),e&&t(dm),e&&t(bs),e&&t(cm),e&&t($s),e&&t(um),e&&t(Es),e&&t(gm),e&&t(ks),e&&t(wm),e&&t(Ps),e&&t(vm),e&&t(As),e&&t(_m),e&&t(Ts),e&&t(ym),e&&t(xs),e&&t(bm),e&&t(ve),v(za),e&&t($m),e&&t(Gs),e&&t(Em),e&&t(B),e&&t(km),e&&t(Sa),e&&t(Pm),e&&t(it),e&&t(Am),e&&t(_e),v(Na),e&&t(Tm),e&&t(zs),e&&t(xm),e&&t(S),e&&t(Gm),v(Ma,e),e&&t(zm),v(qa,e),e&&t(Um),e&&t(x),e&&t(jm),e&&t(F),e&&t(Sm),e&&t(Ss),e&&t(Im),e&&t(ye),v(La),e&&t(Dm),e&&t(Is),e&&t(Nm),e&&t(pt),e&&t(Mm),e&&t(W),e&&t(qm),v(Fa,e),e&&t(Cm),v(Wa,e),e&&t(Om),e&&t(Ms),e&&t(Lm),e&&t(be),v(Ha),e&&t(Bm),e&&t(qs),e&&t(Fm),e&&t(I),e&&t(Wm),e&&t(Cs),e&&t(Hm),e&&t($e),e&&t(Rm),e&&t(Ls),e&&t(Vm),e&&t(Ee),v(Za),e&&t(Ym),e&&t(H),e&&t(Xm),v(Qa,e),e&&t(Zm),v(Ja,e),e&&t(Qm),e&&t(Bs),e&&t(Jm),v(Ka,e),e&&t(Km),v(er,e),e&&t(ef),e&&t(Fs),e&&t(tf),e&&t(ke),v(tr),e&&t(af),e&&t(Ws),e&&t(rf),v(ar,e),e&&t(of),e&&t(Pe),v(rr),e&&t(sf),e&&t(Hs),e&&t(nf),v(or,e),e&&t(lf),e&&t(Rs),e&&t(pf),e&&t(ct),e&&t(hf),e&&t(Vs),e&&t(mf),e&&t(Ys),e&&t(ff),v(ir,e),e&&t(df),e&&t(Xs),e&&t(cf),e&&t(R),e&&t(uf),e&&t(ut),e&&t(gf),e&&t(V),e&&t(wf),e&&t(Zs),e&&t(vf),e&&t(Ae),v(pr),e&&t(_f),e&&t(Qs),e&&t(yf),e&&t(wt),e&&t(bf),e&&t(Js),e&&t($f),v(hr,e),e&&t(Ef),e&&t(Y),e&&t(kf),e&&t(X),e&&t(Pf),e&&t(vt),e&&t(Af),e&&t(Z),e&&t(Tf),e&&t(Ks),e&&t(xf),e&&t(Te),v(wr),e&&t(Gf),e&&t(yt),e&&t(zf),v(vr,e),e&&t(Uf),v(_r,e),e&&t(jf),e&&t(ei),e&&t(Sf),v(yr,e),e&&t(If),v(br,e),e&&t(Df),e&&t(ti),e&&t(Nf),e&&t(xe),v($r),e&&t(Mf),e&&t(ai),e&&t(qf),e&&t(G),e&&t(Cf),e&&t(si),e&&t(Of),v($t,e),e&&t(Lf),v(kr,e),e&&t(Bf),e&&t(Et),e&&t(Ff),v(Pr,e),e&&t(Wf),v(Ar,e),e&&t(Hf),e&&t(ii),e&&t(Rf),v(Tr,e),e&&t(Vf),v(xr,e),e&&t(Yf),e&&t(ni),e&&t(Xf),e&&t(li),e&&t(Zf),e&&t(Ge),v(Gr),e&&t(Qf),e&&t(ze),v(zr),e&&t(Jf),e&&t(Q),e&&t(Kf),v(Ur,e),e&&t(ed),e&&t(fi),e&&t(td),v(jr,e),e&&t(ad),e&&t(P),e&&t(rd),e&&t(At),e&&t(od),v(Nr,e),e&&t(sd),e&&t(Tt),e&&t(id),e&&t(Ue),v(qr),e&&t(nd),e&&t(ci),e&&t(ld),e&&t(Gt),e&&t(pd),e&&t(je),v(Cr),e&&t(hd),e&&t(Ut),e&&t(md),e&&t(vi),e&&t(fd),e&&t(jt),e&&t(dd),e&&t(St),e&&t(cd),e&&t(yi),e&&t(ud),e&&t(D),v(Br),e&&t(gd),e&&t(Ie),v(Hr),e&&t(wd),e&&t(Ei),e&&t(vd),e&&t(De),v(Rr),e&&t(_d),e&&t(ki),e&&t(yd),e&&t(Nt),e&&t(bd),e&&t(Pi),e&&t($d),e&&t(Ai),e&&t(Ed),e&&t(Ne),v(Vr),e&&t(kd),e&&t(qt),e&&t(Pd),e&&t(Ct),e&&t(Ad),e&&t(Ti),e&&t(Td),e&&t(J),e&&t(xd),e&&t(xi),e&&t(Gd),e&&t(Me),v(Qr),e&&t(zd),e&&t(qe),v(Jr),e&&t(Ud),e&&t(Gi),e&&t(jd),e&&t(zi),e&&t(Sd),e&&t(Ui),e&&t(Id),e&&t(ji),e&&t(Dd),e&&t(Bt),e&&t(Nd),e&&t(Ii),e&&t(Md),e&&t(Di),e&&t(qd),e&&t(Ni),e&&t(Cd),e&&t(Mi),e&&t(Od),e&&t(K),e&&t(Ld),e&&t($),e&&t(Bd),e&&t(Ce),v(lo),e&&t(Fd),e&&t(Ci),e&&t(Wd),e&&t(Wt),e&&t(Hd),e&&t(Oe),v(po),e&&t(Rd),e&&t(Rt),e&&t(Vd),v(mo,e),e&&t(Yd),e&&t(Li),e&&t(Xd),e&&t(ee),e&&t(Zd),e&&t(Le),v(go),e&&t(Qd),e&&t(A),e&&t(Jd),e&&t(Fi),e&&t(Kd),e&&t(te),e&&t(ec),e&&t(Hi),e&&t(tc),e&&t(Yt),e&&t(ac),e&&t(Be),v(yo),e&&t(rc),e&&t(ae),e&&t(oc),e&&t(Fe),v(bo),e&&t(sc),e&&t(Yi),e&&t(ic),v($o,e),e&&t(nc),e&&t(re),e&&t(lc),v(Eo,e),e&&t(pc),e&&t(Xi),e&&t(hc),e&&t(We),v(ko),e&&t(mc),e&&t(Zi),e&&t(fc),e&&t(Qi),e&&t(dc),e&&t(Ki),e&&t(cc),e&&t(en)}}}const xk={local:"efficient-training-on-a-single-gpu",sections:[{local:"load-model",title:"Load Model"},{local:"vanilla-training",title:"Vanilla Training"},{local:"anatomy-of-models-operations",title:"Anatomy of Model's Operations"},{local:"anatomy-of-models-memory",title:"Anatomy of Model's Memory"},{local:"batch-sizes",title:"Batch sizes"},{local:"gradient-accumulation",title:"Gradient Accumulation"},{local:"gradient-checkpointing",title:"Gradient Checkpointing"},{local:"floating-data-types",sections:[{local:"fp16-training",title:"FP16 Training"},{local:"bf16",title:"BF16"},{local:"tf32",title:"TF32"}],title:"Floating Data Types"},{local:"optimizer",sections:[{local:"adafactor",title:"Adafactor"},{local:"8bit-adam",title:"8-bit Adam"},{local:"multitensor",title:"`_multi_tensor`"}],title:"Optimizer"},{local:"using-accelerate",title:"Using \u{1F917} Accelerate"},{local:"dataloader",title:"DataLoader"},{local:"deepspeed-zero",title:"DeepSpeed ZeRO"},{local:"choice-of-gpu",title:"Choice of GPU"},{local:"how-to-scale",title:"How to scale"},{local:"efficient-software-prebuilds",title:"Efficient Software Prebuilds"},{local:"sparsity",sections:[{local:"mixture-of-experts",title:"Mixture of Experts"}],title:"Sparsity"},{local:"scaling-beyond-a-single-gpu",title:"Scaling beyond a single GPU"},{local:"inference-with-torchdynamo",title:"Inference with torchdynamo"},{local:"bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition",sections:[{local:"requirements",title:"Requirements"},{local:"running-mixedint8-models",title:"Running mixed-int8 models"},{local:"colab-demos",title:"Colab demos"}],title:"`bitsandbytes` integration for Int8 mixed-precision matrix decomposition"}],title:"Efficient Training on a Single GPU"};function Gk(Go){return kk(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ik extends yk{constructor(b){super();bk(this,b,Gk,Tk,$k,{})}}export{Ik as default,xk as metadata};
