import{S as Wd,i as Bd,s as Hd,e as s,k as p,w as T,t as l,M as Rd,c as o,d as r,m,a as n,x as k,h as i,b as d,G as t,g as f,y as $,q as b,o as y,B as w,v as Xd,L as me}from"../../chunks/vendor-hf-doc-builder.js";import{D as z}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ce}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as P}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pe}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Kd(M){let g,x,_,u,v;return u=new ce({props:{code:`from transformers import MT5Model, T5Tokenizer

model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
labels = tokenizer(text_target=summary, return_tensors="pt")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(a){g=o(a,"P",{});var h=n(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){y(u.$$.fragment,a),v=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function Jd(M){let g,x,_,u,v;return u=new ce({props:{code:`from transformers import MT5ForConditionalGeneration, T5Tokenizer

model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, text_target=summary, return_tensors="pt")

outputs = model(**inputs)
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, text_target=summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(a){g=o(a,"P",{});var h=n(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){y(u.$$.fragment,a),v=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function Qd(M){let g,x,_,u,v;return u=new ce({props:{code:`from transformers import MT5EncoderModel, T5Tokenizer

model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(a){g=o(a,"P",{});var h=n(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){y(u.$$.fragment,a),v=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function Yd(M){let g,x,_,u,v;return u=new ce({props:{code:`from transformers import TFMT5Model, T5Tokenizer

model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
labels = tokenizer(text_target=summary, return_tensors="tf")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(a){g=o(a,"P",{});var h=n(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){y(u.$$.fragment,a),v=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function Zd(M){let g,x,_,u,v;return u=new ce({props:{code:`from transformers import TFMT5ForConditionalGeneration, T5Tokenizer

model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, text_target=summary, return_tensors="tf")

outputs = model(**inputs)
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, text_target=summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(a){g=o(a,"P",{});var h=n(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){y(u.$$.fragment,a),v=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function ep(M){let g,x,_,u,v;return u=new ce({props:{code:`from transformers import TFMT5EncoderModel, T5Tokenizer

model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(a){g=o(a,"P",{});var h=n(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){y(u.$$.fragment,a),v=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function tp(M){let g,x,_,u,v;return u=new ce({props:{code:`from transformers import FlaxMT5Model, T5Tokenizer

model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

decoder_input_ids = tokenizer(text_target=summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(a){g=o(a,"P",{});var h=n(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){y(u.$$.fragment,a),v=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function rp(M){let g,x,_,u,v;return u=new ce({props:{code:`from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

decoder_input_ids = tokenizer(text_target=summary, return_tensors="np").input_ids

outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(a){g=o(a,"P",{});var h=n(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){y(u.$$.fragment,a),v=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function sp(M){let g,x,_,u,v;return u=new ce({props:{code:`from transformers import FlaxT5EncoderModel, T5Tokenizer

model = FlaxT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

decoder_input_ids = tokenizer(text_target=summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(a){g=o(a,"P",{});var h=n(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){y(u.$$.fragment,a),v=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function op(M){let g,x,_,u,v,a,h,Nr,Zo,Ns,K,fe,Dr,We,en,Ir,tn,Ds,ue,rn,Be,sn,on,Is,Qt,nn,Gs,Yt,Gr,an,Os,ge,ln,He,dn,pn,Vs,Zt,mn,Us,F,Or,Vr,Re,cn,fn,Ur,Wr,Xe,un,gn,Br,Hr,Ke,hn,_n,Rr,Xr,Je,vn,Tn,Kr,er,Qe,kn,$n,Ws,W,bn,Ye,yn,wn,Ze,xn,Mn,Bs,J,he,Jr,et,En,Qr,zn,Hs,C,tt,qn,A,Fn,tr,jn,Pn,rr,Cn,An,rt,Sn,Ln,Nn,Q,Dn,sr,In,Gn,or,On,Vn,Rs,Y,_e,Yr,st,Un,Zr,Wn,Xs,E,ot,Bn,nt,Hn,at,Rn,Xn,Kn,lt,Jn,nr,Qn,Yn,Zn,B,it,ea,es,ta,ra,dt,ar,sa,ts,oa,na,lr,aa,rs,la,ia,ve,pt,da,ss,pa,ma,Te,mt,ca,os,fa,ua,ke,ct,ga,ft,ha,ns,_a,va,Ks,$e,Ta,ir,ka,$a,Js,Z,be,as,ut,ba,ls,ya,Qs,q,gt,wa,ee,xa,is,Ma,Ea,ht,za,qa,Fa,_t,ja,dr,Pa,Ca,Aa,H,vt,Sa,ds,La,Na,Tt,pr,Da,ps,Ia,Ga,mr,Oa,ms,Va,Ua,ye,kt,Wa,cs,Ba,Ys,we,Ha,cr,Ra,Xa,Zs,te,xe,fs,$t,Ka,us,Ja,eo,S,bt,Qa,yt,Ya,fr,Za,el,tl,Me,to,re,Ee,gs,wt,rl,hs,sl,ro,L,xt,ol,Mt,nl,ur,al,ll,il,ze,so,se,qe,_s,Et,dl,vs,pl,oo,N,zt,ml,qt,cl,gr,fl,ul,gl,Fe,no,oe,je,Ts,Ft,hl,ks,_l,ao,D,jt,vl,Pt,Tl,hr,kl,$l,bl,Pe,lo,ne,Ce,$s,Ct,yl,bs,wl,io,I,At,xl,St,Ml,_r,El,zl,ql,Ae,po,ae,Se,ys,Lt,Fl,ws,jl,mo,G,Nt,Pl,Dt,Cl,vr,Al,Sl,Ll,Le,co,le,Ne,xs,It,Nl,Ms,Dl,fo,O,Gt,Il,Ot,Gl,Tr,Ol,Vl,Ul,De,uo,ie,Ie,Es,Vt,Wl,zs,Bl,go,V,Ut,Hl,Wt,Rl,kr,Xl,Kl,Jl,Ge,ho,de,Oe,qs,Bt,Ql,Fs,Yl,_o,U,Ht,Zl,Rt,ei,$r,ti,ri,si,Ve,vo;return a=new P({}),We=new P({}),et=new P({}),tt=new z({props:{name:"class transformers.MT5Config",anchor:"transformers.MT5Config",parameters:[{name:"vocab_size",val:" = 250112"},{name:"d_model",val:" = 512"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 1024"},{name:"num_layers",val:" = 8"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 6"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"relative_attention_max_distance",val:" = 128"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'gated-gelu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"tokenizer_class",val:" = 'T5Tokenizer'"},{name:"tie_word_embeddings",val:" = False"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"decoder_start_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MT5Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250112) &#x2014;
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_18449/en/model_doc/t5#transformers.T5Model">T5Model</a> or <a href="/docs/transformers/pr_18449/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a>.`,name:"vocab_size"},{anchor:"transformers.MT5Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.MT5Config.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. <code>d_kv</code> has to be equal to <code>d_model // num_heads</code>.`,name:"d_kv"},{anchor:"transformers.MT5Config.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the intermediate feed forward layer in each <code>T5Block</code>.`,name:"d_ff"},{anchor:"transformers.MT5Config.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.MT5Config.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not set.`,name:"num_decoder_layers"},{anchor:"transformers.MT5Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.MT5Config.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.MT5Config.relative_attention_max_distance",description:`<strong>relative_attention_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The maximum distance of the longer sequences for the bucket separation.`,name:"relative_attention_max_distance"},{anchor:"transformers.MT5Config.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.MT5Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MT5Config.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MT5Config.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;gated-gelu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.MT5Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/configuration_mt5.py#L24"}}),st=new P({}),ot=new z({props:{name:"class transformers.T5Tokenizer",anchor:"transformers.T5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.T5Tokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.T5Tokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/t5/tokenization_t5.py#L55"}}),it=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/t5/tokenization_t5.py#L249",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new z({props:{name:"convert_tokens_to_string",anchor:"transformers.T5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/t5/tokenization_t5.py#L310"}}),mt=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/t5/tokenization_t5.py#L227",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.T5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/t5/tokenization_t5.py#L188",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ut=new P({}),gt=new z({props:{name:"class transformers.T5TokenizerFast",anchor:"transformers.T5TokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5TokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5TokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5TokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5TokenizerFast.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5TokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/t5/tokenization_t5_fast.py#L65"}}),vt=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/t5/tokenization_t5_fast.py#L191",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),kt=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/t5/tokenization_t5_fast.py#L217",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),$t=new P({}),bt=new z({props:{name:"class transformers.MT5Model",anchor:"transformers.MT5Model",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/modeling_mt5.py#L28"}}),Me=new pe({props:{anchor:"transformers.MT5Model.example",$$slots:{default:[Kd]},$$scope:{ctx:M}}}),wt=new P({}),xt=new z({props:{name:"class transformers.MT5ForConditionalGeneration",anchor:"transformers.MT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/modeling_mt5.py#L61"}}),ze=new pe({props:{anchor:"transformers.MT5ForConditionalGeneration.example",$$slots:{default:[Jd]},$$scope:{ctx:M}}}),Et=new P({}),zt=new z({props:{name:"class transformers.MT5EncoderModel",anchor:"transformers.MT5EncoderModel",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/modeling_mt5.py#L91"}}),Fe=new pe({props:{anchor:"transformers.MT5EncoderModel.example",$$slots:{default:[Qd]},$$scope:{ctx:M}}}),Ft=new P({}),jt=new z({props:{name:"class transformers.TFMT5Model",anchor:"transformers.TFMT5Model",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/modeling_tf_mt5.py#L28"}}),Pe=new pe({props:{anchor:"transformers.TFMT5Model.example",$$slots:{default:[Yd]},$$scope:{ctx:M}}}),Ct=new P({}),At=new z({props:{name:"class transformers.TFMT5ForConditionalGeneration",anchor:"transformers.TFMT5ForConditionalGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/modeling_tf_mt5.py#L52"}}),Ae=new pe({props:{anchor:"transformers.TFMT5ForConditionalGeneration.example",$$slots:{default:[Zd]},$$scope:{ctx:M}}}),Lt=new P({}),Nt=new z({props:{name:"class transformers.TFMT5EncoderModel",anchor:"transformers.TFMT5EncoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/modeling_tf_mt5.py#L76"}}),Le=new pe({props:{anchor:"transformers.TFMT5EncoderModel.example",$$slots:{default:[ep]},$$scope:{ctx:M}}}),It=new P({}),Gt=new z({props:{name:"class transformers.FlaxMT5Model",anchor:"transformers.FlaxMT5Model",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/modeling_flax_mt5.py#L43"}}),De=new pe({props:{anchor:"transformers.FlaxMT5Model.example",$$slots:{default:[tp]},$$scope:{ctx:M}}}),Vt=new P({}),Ut=new z({props:{name:"class transformers.FlaxMT5ForConditionalGeneration",anchor:"transformers.FlaxMT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/modeling_flax_mt5.py#L95"}}),Ge=new pe({props:{anchor:"transformers.FlaxMT5ForConditionalGeneration.example",$$slots:{default:[rp]},$$scope:{ctx:M}}}),Bt=new P({}),Ht=new z({props:{name:"class transformers.FlaxMT5EncoderModel",anchor:"transformers.FlaxMT5EncoderModel",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18449/src/transformers/models/mt5/modeling_flax_mt5.py#L69"}}),Ve=new pe({props:{anchor:"transformers.FlaxMT5EncoderModel.example",$$slots:{default:[sp]},$$scope:{ctx:M}}}),{c(){g=s("meta"),x=p(),_=s("h1"),u=s("a"),v=s("span"),T(a.$$.fragment),h=p(),Nr=s("span"),Zo=l("mT5"),Ns=p(),K=s("h2"),fe=s("a"),Dr=s("span"),T(We.$$.fragment),en=p(),Ir=s("span"),tn=l("Overview"),Ds=p(),ue=s("p"),rn=l("The mT5 model was presented in "),Be=s("a"),sn=l("mT5: A massively multilingual pre-trained text-to-text transformer"),on=l(` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),Is=p(),Qt=s("p"),nn=l("The abstract from the paper is the following:"),Gs=p(),Yt=s("p"),Gr=s("em"),an=l(`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),Os=p(),ge=s("p"),ln=l("Note: mT5 was only pre-trained on "),He=s("a"),dn=l("mC4"),pn=l(` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),Vs=p(),Zt=s("p"),mn=l("Google has released the following variants:"),Us=p(),F=s("ul"),Or=s("li"),Vr=s("p"),Re=s("a"),cn=l("google/mt5-small"),fn=p(),Ur=s("li"),Wr=s("p"),Xe=s("a"),un=l("google/mt5-base"),gn=p(),Br=s("li"),Hr=s("p"),Ke=s("a"),hn=l("google/mt5-large"),_n=p(),Rr=s("li"),Xr=s("p"),Je=s("a"),vn=l("google/mt5-xl"),Tn=p(),Kr=s("li"),er=s("p"),Qe=s("a"),kn=l("google/mt5-xxl"),$n=l("."),Ws=p(),W=s("p"),bn=l("This model was contributed by "),Ye=s("a"),yn=l("patrickvonplaten"),wn=l(`. The original code can be
found `),Ze=s("a"),xn=l("here"),Mn=l("."),Bs=p(),J=s("h2"),he=s("a"),Jr=s("span"),T(et.$$.fragment),En=p(),Qr=s("span"),zn=l("MT5Config"),Hs=p(),C=s("div"),T(tt.$$.fragment),qn=p(),A=s("p"),Fn=l("This is the configuration class to store the configuration of a "),tr=s("a"),jn=l("MT5Model"),Pn=l(" or a "),rr=s("a"),Cn=l("TFMT5Model"),An=l(`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),rt=s("a"),Sn=l("google/mt5-small"),Ln=l(" architecture."),Nn=p(),Q=s("p"),Dn=l("Configuration objects inherit from "),sr=s("a"),In=l("PretrainedConfig"),Gn=l(` and can be used to control the model outputs. Read the
documentation from `),or=s("a"),On=l("PretrainedConfig"),Vn=l(" for more information."),Rs=p(),Y=s("h2"),_e=s("a"),Yr=s("span"),T(st.$$.fragment),Un=p(),Zr=s("span"),Wn=l("MT5Tokenizer"),Xs=p(),E=s("div"),T(ot.$$.fragment),Bn=p(),nt=s("p"),Hn=l("Construct a T5 tokenizer. Based on "),at=s("a"),Rn=l("SentencePiece"),Xn=l("."),Kn=p(),lt=s("p"),Jn=l("This tokenizer inherits from "),nr=s("a"),Qn=l("PreTrainedTokenizer"),Yn=l(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Zn=p(),B=s("div"),T(it.$$.fragment),ea=p(),es=s("p"),ta=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),ra=p(),dt=s("ul"),ar=s("li"),sa=l("single sequence: "),ts=s("code"),oa=l("X </s>"),na=p(),lr=s("li"),aa=l("pair of sequences: "),rs=s("code"),la=l("A </s> B </s>"),ia=p(),ve=s("div"),T(pt.$$.fragment),da=p(),ss=s("p"),pa=l("Converts a sequence of tokens (string) in a single string."),ma=p(),Te=s("div"),T(mt.$$.fragment),ca=p(),os=s("p"),fa=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),ua=p(),ke=s("div"),T(ct.$$.fragment),ga=p(),ft=s("p"),ha=l(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),ns=s("code"),_a=l("prepare_for_model"),va=l(" method."),Ks=p(),$e=s("p"),Ta=l("See "),ir=s("a"),ka=l("T5Tokenizer"),$a=l(" for all details."),Js=p(),Z=s("h2"),be=s("a"),as=s("span"),T(ut.$$.fragment),ba=p(),ls=s("span"),ya=l("MT5TokenizerFast"),Qs=p(),q=s("div"),T(gt.$$.fragment),wa=p(),ee=s("p"),xa=l("Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),is=s("em"),Ma=l("tokenizers"),Ea=l(` library). Based on
`),ht=s("a"),za=l("Unigram"),qa=l("."),Fa=p(),_t=s("p"),ja=l("This tokenizer inherits from "),dr=s("a"),Pa=l("PreTrainedTokenizerFast"),Ca=l(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Aa=p(),H=s("div"),T(vt.$$.fragment),Sa=p(),ds=s("p"),La=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Na=p(),Tt=s("ul"),pr=s("li"),Da=l("single sequence: "),ps=s("code"),Ia=l("X </s>"),Ga=p(),mr=s("li"),Oa=l("pair of sequences: "),ms=s("code"),Va=l("A </s> B </s>"),Ua=p(),ye=s("div"),T(kt.$$.fragment),Wa=p(),cs=s("p"),Ba=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Ys=p(),we=s("p"),Ha=l("See "),cr=s("a"),Ra=l("T5TokenizerFast"),Xa=l(" for all details."),Zs=p(),te=s("h2"),xe=s("a"),fs=s("span"),T($t.$$.fragment),Ka=p(),us=s("span"),Ja=l("MT5Model"),eo=p(),S=s("div"),T(bt.$$.fragment),Qa=p(),yt=s("p"),Ya=l("This class overrides "),fr=s("a"),Za=l("T5Model"),el=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),tl=p(),T(Me.$$.fragment),to=p(),re=s("h2"),Ee=s("a"),gs=s("span"),T(wt.$$.fragment),rl=p(),hs=s("span"),sl=l("MT5ForConditionalGeneration"),ro=p(),L=s("div"),T(xt.$$.fragment),ol=p(),Mt=s("p"),nl=l("This class overrides "),ur=s("a"),al=l("T5ForConditionalGeneration"),ll=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),il=p(),T(ze.$$.fragment),so=p(),se=s("h2"),qe=s("a"),_s=s("span"),T(Et.$$.fragment),dl=p(),vs=s("span"),pl=l("MT5EncoderModel"),oo=p(),N=s("div"),T(zt.$$.fragment),ml=p(),qt=s("p"),cl=l("This class overrides "),gr=s("a"),fl=l("T5EncoderModel"),ul=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),gl=p(),T(Fe.$$.fragment),no=p(),oe=s("h2"),je=s("a"),Ts=s("span"),T(Ft.$$.fragment),hl=p(),ks=s("span"),_l=l("TFMT5Model"),ao=p(),D=s("div"),T(jt.$$.fragment),vl=p(),Pt=s("p"),Tl=l("This class overrides "),hr=s("a"),kl=l("TFT5Model"),$l=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),bl=p(),T(Pe.$$.fragment),lo=p(),ne=s("h2"),Ce=s("a"),$s=s("span"),T(Ct.$$.fragment),yl=p(),bs=s("span"),wl=l("TFMT5ForConditionalGeneration"),io=p(),I=s("div"),T(At.$$.fragment),xl=p(),St=s("p"),Ml=l("This class overrides "),_r=s("a"),El=l("TFT5ForConditionalGeneration"),zl=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ql=p(),T(Ae.$$.fragment),po=p(),ae=s("h2"),Se=s("a"),ys=s("span"),T(Lt.$$.fragment),Fl=p(),ws=s("span"),jl=l("TFMT5EncoderModel"),mo=p(),G=s("div"),T(Nt.$$.fragment),Pl=p(),Dt=s("p"),Cl=l("This class overrides "),vr=s("a"),Al=l("TFT5EncoderModel"),Sl=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Ll=p(),T(Le.$$.fragment),co=p(),le=s("h2"),Ne=s("a"),xs=s("span"),T(It.$$.fragment),Nl=p(),Ms=s("span"),Dl=l("FlaxMT5Model"),fo=p(),O=s("div"),T(Gt.$$.fragment),Il=p(),Ot=s("p"),Gl=l("This class overrides "),Tr=s("a"),Ol=l("FlaxT5Model"),Vl=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ul=p(),T(De.$$.fragment),uo=p(),ie=s("h2"),Ie=s("a"),Es=s("span"),T(Vt.$$.fragment),Wl=p(),zs=s("span"),Bl=l("FlaxMT5ForConditionalGeneration"),go=p(),V=s("div"),T(Ut.$$.fragment),Hl=p(),Wt=s("p"),Rl=l("This class overrides "),kr=s("a"),Xl=l("FlaxT5ForConditionalGeneration"),Kl=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Jl=p(),T(Ge.$$.fragment),ho=p(),de=s("h2"),Oe=s("a"),qs=s("span"),T(Bt.$$.fragment),Ql=p(),Fs=s("span"),Yl=l("FlaxMT5EncoderModel"),_o=p(),U=s("div"),T(Ht.$$.fragment),Zl=p(),Rt=s("p"),ei=l("This class overrides "),$r=s("a"),ti=l("FlaxT5EncoderModel"),ri=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),si=p(),T(Ve.$$.fragment),this.h()},l(e){const c=Rd('[data-svelte="svelte-1phssyn"]',document.head);g=o(c,"META",{name:!0,content:!0}),c.forEach(r),x=m(e),_=o(e,"H1",{class:!0});var Xt=n(_);u=o(Xt,"A",{id:!0,class:!0,href:!0});var js=n(u);v=o(js,"SPAN",{});var Ps=n(v);k(a.$$.fragment,Ps),Ps.forEach(r),js.forEach(r),h=m(Xt),Nr=o(Xt,"SPAN",{});var Cs=n(Nr);Zo=i(Cs,"mT5"),Cs.forEach(r),Xt.forEach(r),Ns=m(e),K=o(e,"H2",{class:!0});var Kt=n(K);fe=o(Kt,"A",{id:!0,class:!0,href:!0});var As=n(fe);Dr=o(As,"SPAN",{});var Ss=n(Dr);k(We.$$.fragment,Ss),Ss.forEach(r),As.forEach(r),en=m(Kt),Ir=o(Kt,"SPAN",{});var Ls=n(Ir);tn=i(Ls,"Overview"),Ls.forEach(r),Kt.forEach(r),Ds=m(e),ue=o(e,"P",{});var Jt=n(ue);rn=i(Jt,"The mT5 model was presented in "),Be=o(Jt,"A",{href:!0,rel:!0});var di=n(Be);sn=i(di,"mT5: A massively multilingual pre-trained text-to-text transformer"),di.forEach(r),on=i(Jt,` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),Jt.forEach(r),Is=m(e),Qt=o(e,"P",{});var pi=n(Qt);nn=i(pi,"The abstract from the paper is the following:"),pi.forEach(r),Gs=m(e),Yt=o(e,"P",{});var mi=n(Yt);Gr=o(mi,"EM",{});var ci=n(Gr);an=i(ci,`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),ci.forEach(r),mi.forEach(r),Os=m(e),ge=o(e,"P",{});var To=n(ge);ln=i(To,"Note: mT5 was only pre-trained on "),He=o(To,"A",{href:!0,rel:!0});var fi=n(He);dn=i(fi,"mC4"),fi.forEach(r),pn=i(To,` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),To.forEach(r),Vs=m(e),Zt=o(e,"P",{});var ui=n(Zt);mn=i(ui,"Google has released the following variants:"),ui.forEach(r),Us=m(e),F=o(e,"UL",{});var R=n(F);Or=o(R,"LI",{});var gi=n(Or);Vr=o(gi,"P",{});var hi=n(Vr);Re=o(hi,"A",{href:!0,rel:!0});var _i=n(Re);cn=i(_i,"google/mt5-small"),_i.forEach(r),hi.forEach(r),gi.forEach(r),fn=m(R),Ur=o(R,"LI",{});var vi=n(Ur);Wr=o(vi,"P",{});var Ti=n(Wr);Xe=o(Ti,"A",{href:!0,rel:!0});var ki=n(Xe);un=i(ki,"google/mt5-base"),ki.forEach(r),Ti.forEach(r),vi.forEach(r),gn=m(R),Br=o(R,"LI",{});var $i=n(Br);Hr=o($i,"P",{});var bi=n(Hr);Ke=o(bi,"A",{href:!0,rel:!0});var yi=n(Ke);hn=i(yi,"google/mt5-large"),yi.forEach(r),bi.forEach(r),$i.forEach(r),_n=m(R),Rr=o(R,"LI",{});var wi=n(Rr);Xr=o(wi,"P",{});var xi=n(Xr);Je=o(xi,"A",{href:!0,rel:!0});var Mi=n(Je);vn=i(Mi,"google/mt5-xl"),Mi.forEach(r),xi.forEach(r),wi.forEach(r),Tn=m(R),Kr=o(R,"LI",{});var Ei=n(Kr);er=o(Ei,"P",{});var oi=n(er);Qe=o(oi,"A",{href:!0,rel:!0});var zi=n(Qe);kn=i(zi,"google/mt5-xxl"),zi.forEach(r),$n=i(oi,"."),oi.forEach(r),Ei.forEach(r),R.forEach(r),Ws=m(e),W=o(e,"P",{});var br=n(W);bn=i(br,"This model was contributed by "),Ye=o(br,"A",{href:!0,rel:!0});var qi=n(Ye);yn=i(qi,"patrickvonplaten"),qi.forEach(r),wn=i(br,`. The original code can be
found `),Ze=o(br,"A",{href:!0,rel:!0});var Fi=n(Ze);xn=i(Fi,"here"),Fi.forEach(r),Mn=i(br,"."),br.forEach(r),Bs=m(e),J=o(e,"H2",{class:!0});var ko=n(J);he=o(ko,"A",{id:!0,class:!0,href:!0});var ji=n(he);Jr=o(ji,"SPAN",{});var Pi=n(Jr);k(et.$$.fragment,Pi),Pi.forEach(r),ji.forEach(r),En=m(ko),Qr=o(ko,"SPAN",{});var Ci=n(Qr);zn=i(Ci,"MT5Config"),Ci.forEach(r),ko.forEach(r),Hs=m(e),C=o(e,"DIV",{class:!0});var yr=n(C);k(tt.$$.fragment,yr),qn=m(yr),A=o(yr,"P",{});var Ue=n(A);Fn=i(Ue,"This is the configuration class to store the configuration of a "),tr=o(Ue,"A",{href:!0});var Ai=n(tr);jn=i(Ai,"MT5Model"),Ai.forEach(r),Pn=i(Ue," or a "),rr=o(Ue,"A",{href:!0});var Si=n(rr);Cn=i(Si,"TFMT5Model"),Si.forEach(r),An=i(Ue,`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),rt=o(Ue,"A",{href:!0,rel:!0});var Li=n(rt);Sn=i(Li,"google/mt5-small"),Li.forEach(r),Ln=i(Ue," architecture."),Ue.forEach(r),Nn=m(yr),Q=o(yr,"P",{});var wr=n(Q);Dn=i(wr,"Configuration objects inherit from "),sr=o(wr,"A",{href:!0});var Ni=n(sr);In=i(Ni,"PretrainedConfig"),Ni.forEach(r),Gn=i(wr,` and can be used to control the model outputs. Read the
documentation from `),or=o(wr,"A",{href:!0});var Di=n(or);On=i(Di,"PretrainedConfig"),Di.forEach(r),Vn=i(wr," for more information."),wr.forEach(r),yr.forEach(r),Rs=m(e),Y=o(e,"H2",{class:!0});var $o=n(Y);_e=o($o,"A",{id:!0,class:!0,href:!0});var Ii=n(_e);Yr=o(Ii,"SPAN",{});var Gi=n(Yr);k(st.$$.fragment,Gi),Gi.forEach(r),Ii.forEach(r),Un=m($o),Zr=o($o,"SPAN",{});var Oi=n(Zr);Wn=i(Oi,"MT5Tokenizer"),Oi.forEach(r),$o.forEach(r),Xs=m(e),E=o(e,"DIV",{class:!0});var j=n(E);k(ot.$$.fragment,j),Bn=m(j),nt=o(j,"P",{});var bo=n(nt);Hn=i(bo,"Construct a T5 tokenizer. Based on "),at=o(bo,"A",{href:!0,rel:!0});var Vi=n(at);Rn=i(Vi,"SentencePiece"),Vi.forEach(r),Xn=i(bo,"."),bo.forEach(r),Kn=m(j),lt=o(j,"P",{});var yo=n(lt);Jn=i(yo,"This tokenizer inherits from "),nr=o(yo,"A",{href:!0});var Ui=n(nr);Qn=i(Ui,"PreTrainedTokenizer"),Ui.forEach(r),Yn=i(yo,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),yo.forEach(r),Zn=m(j),B=o(j,"DIV",{class:!0});var xr=n(B);k(it.$$.fragment,xr),ea=m(xr),es=o(xr,"P",{});var Wi=n(es);ta=i(Wi,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Wi.forEach(r),ra=m(xr),dt=o(xr,"UL",{});var wo=n(dt);ar=o(wo,"LI",{});var ni=n(ar);sa=i(ni,"single sequence: "),ts=o(ni,"CODE",{});var Bi=n(ts);oa=i(Bi,"X </s>"),Bi.forEach(r),ni.forEach(r),na=m(wo),lr=o(wo,"LI",{});var ai=n(lr);aa=i(ai,"pair of sequences: "),rs=o(ai,"CODE",{});var Hi=n(rs);la=i(Hi,"A </s> B </s>"),Hi.forEach(r),ai.forEach(r),wo.forEach(r),xr.forEach(r),ia=m(j),ve=o(j,"DIV",{class:!0});var xo=n(ve);k(pt.$$.fragment,xo),da=m(xo),ss=o(xo,"P",{});var Ri=n(ss);pa=i(Ri,"Converts a sequence of tokens (string) in a single string."),Ri.forEach(r),xo.forEach(r),ma=m(j),Te=o(j,"DIV",{class:!0});var Mo=n(Te);k(mt.$$.fragment,Mo),ca=m(Mo),os=o(Mo,"P",{});var Xi=n(os);fa=i(Xi,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Xi.forEach(r),Mo.forEach(r),ua=m(j),ke=o(j,"DIV",{class:!0});var Eo=n(ke);k(ct.$$.fragment,Eo),ga=m(Eo),ft=o(Eo,"P",{});var zo=n(ft);ha=i(zo,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),ns=o(zo,"CODE",{});var Ki=n(ns);_a=i(Ki,"prepare_for_model"),Ki.forEach(r),va=i(zo," method."),zo.forEach(r),Eo.forEach(r),j.forEach(r),Ks=m(e),$e=o(e,"P",{});var qo=n($e);Ta=i(qo,"See "),ir=o(qo,"A",{href:!0});var Ji=n(ir);ka=i(Ji,"T5Tokenizer"),Ji.forEach(r),$a=i(qo," for all details."),qo.forEach(r),Js=m(e),Z=o(e,"H2",{class:!0});var Fo=n(Z);be=o(Fo,"A",{id:!0,class:!0,href:!0});var Qi=n(be);as=o(Qi,"SPAN",{});var Yi=n(as);k(ut.$$.fragment,Yi),Yi.forEach(r),Qi.forEach(r),ba=m(Fo),ls=o(Fo,"SPAN",{});var Zi=n(ls);ya=i(Zi,"MT5TokenizerFast"),Zi.forEach(r),Fo.forEach(r),Qs=m(e),q=o(e,"DIV",{class:!0});var X=n(q);k(gt.$$.fragment,X),wa=m(X),ee=o(X,"P",{});var Mr=n(ee);xa=i(Mr,"Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),is=o(Mr,"EM",{});var ed=n(is);Ma=i(ed,"tokenizers"),ed.forEach(r),Ea=i(Mr,` library). Based on
`),ht=o(Mr,"A",{href:!0,rel:!0});var td=n(ht);za=i(td,"Unigram"),td.forEach(r),qa=i(Mr,"."),Mr.forEach(r),Fa=m(X),_t=o(X,"P",{});var jo=n(_t);ja=i(jo,"This tokenizer inherits from "),dr=o(jo,"A",{href:!0});var rd=n(dr);Pa=i(rd,"PreTrainedTokenizerFast"),rd.forEach(r),Ca=i(jo,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),jo.forEach(r),Aa=m(X),H=o(X,"DIV",{class:!0});var Er=n(H);k(vt.$$.fragment,Er),Sa=m(Er),ds=o(Er,"P",{});var sd=n(ds);La=i(sd,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),sd.forEach(r),Na=m(Er),Tt=o(Er,"UL",{});var Po=n(Tt);pr=o(Po,"LI",{});var li=n(pr);Da=i(li,"single sequence: "),ps=o(li,"CODE",{});var od=n(ps);Ia=i(od,"X </s>"),od.forEach(r),li.forEach(r),Ga=m(Po),mr=o(Po,"LI",{});var ii=n(mr);Oa=i(ii,"pair of sequences: "),ms=o(ii,"CODE",{});var nd=n(ms);Va=i(nd,"A </s> B </s>"),nd.forEach(r),ii.forEach(r),Po.forEach(r),Er.forEach(r),Ua=m(X),ye=o(X,"DIV",{class:!0});var Co=n(ye);k(kt.$$.fragment,Co),Wa=m(Co),cs=o(Co,"P",{});var ad=n(cs);Ba=i(ad,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),ad.forEach(r),Co.forEach(r),X.forEach(r),Ys=m(e),we=o(e,"P",{});var Ao=n(we);Ha=i(Ao,"See "),cr=o(Ao,"A",{href:!0});var ld=n(cr);Ra=i(ld,"T5TokenizerFast"),ld.forEach(r),Xa=i(Ao," for all details."),Ao.forEach(r),Zs=m(e),te=o(e,"H2",{class:!0});var So=n(te);xe=o(So,"A",{id:!0,class:!0,href:!0});var id=n(xe);fs=o(id,"SPAN",{});var dd=n(fs);k($t.$$.fragment,dd),dd.forEach(r),id.forEach(r),Ka=m(So),us=o(So,"SPAN",{});var pd=n(us);Ja=i(pd,"MT5Model"),pd.forEach(r),So.forEach(r),eo=m(e),S=o(e,"DIV",{class:!0});var zr=n(S);k(bt.$$.fragment,zr),Qa=m(zr),yt=o(zr,"P",{});var Lo=n(yt);Ya=i(Lo,"This class overrides "),fr=o(Lo,"A",{href:!0});var md=n(fr);Za=i(md,"T5Model"),md.forEach(r),el=i(Lo,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Lo.forEach(r),tl=m(zr),k(Me.$$.fragment,zr),zr.forEach(r),to=m(e),re=o(e,"H2",{class:!0});var No=n(re);Ee=o(No,"A",{id:!0,class:!0,href:!0});var cd=n(Ee);gs=o(cd,"SPAN",{});var fd=n(gs);k(wt.$$.fragment,fd),fd.forEach(r),cd.forEach(r),rl=m(No),hs=o(No,"SPAN",{});var ud=n(hs);sl=i(ud,"MT5ForConditionalGeneration"),ud.forEach(r),No.forEach(r),ro=m(e),L=o(e,"DIV",{class:!0});var qr=n(L);k(xt.$$.fragment,qr),ol=m(qr),Mt=o(qr,"P",{});var Do=n(Mt);nl=i(Do,"This class overrides "),ur=o(Do,"A",{href:!0});var gd=n(ur);al=i(gd,"T5ForConditionalGeneration"),gd.forEach(r),ll=i(Do,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Do.forEach(r),il=m(qr),k(ze.$$.fragment,qr),qr.forEach(r),so=m(e),se=o(e,"H2",{class:!0});var Io=n(se);qe=o(Io,"A",{id:!0,class:!0,href:!0});var hd=n(qe);_s=o(hd,"SPAN",{});var _d=n(_s);k(Et.$$.fragment,_d),_d.forEach(r),hd.forEach(r),dl=m(Io),vs=o(Io,"SPAN",{});var vd=n(vs);pl=i(vd,"MT5EncoderModel"),vd.forEach(r),Io.forEach(r),oo=m(e),N=o(e,"DIV",{class:!0});var Fr=n(N);k(zt.$$.fragment,Fr),ml=m(Fr),qt=o(Fr,"P",{});var Go=n(qt);cl=i(Go,"This class overrides "),gr=o(Go,"A",{href:!0});var Td=n(gr);fl=i(Td,"T5EncoderModel"),Td.forEach(r),ul=i(Go,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Go.forEach(r),gl=m(Fr),k(Fe.$$.fragment,Fr),Fr.forEach(r),no=m(e),oe=o(e,"H2",{class:!0});var Oo=n(oe);je=o(Oo,"A",{id:!0,class:!0,href:!0});var kd=n(je);Ts=o(kd,"SPAN",{});var $d=n(Ts);k(Ft.$$.fragment,$d),$d.forEach(r),kd.forEach(r),hl=m(Oo),ks=o(Oo,"SPAN",{});var bd=n(ks);_l=i(bd,"TFMT5Model"),bd.forEach(r),Oo.forEach(r),ao=m(e),D=o(e,"DIV",{class:!0});var jr=n(D);k(jt.$$.fragment,jr),vl=m(jr),Pt=o(jr,"P",{});var Vo=n(Pt);Tl=i(Vo,"This class overrides "),hr=o(Vo,"A",{href:!0});var yd=n(hr);kl=i(yd,"TFT5Model"),yd.forEach(r),$l=i(Vo,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Vo.forEach(r),bl=m(jr),k(Pe.$$.fragment,jr),jr.forEach(r),lo=m(e),ne=o(e,"H2",{class:!0});var Uo=n(ne);Ce=o(Uo,"A",{id:!0,class:!0,href:!0});var wd=n(Ce);$s=o(wd,"SPAN",{});var xd=n($s);k(Ct.$$.fragment,xd),xd.forEach(r),wd.forEach(r),yl=m(Uo),bs=o(Uo,"SPAN",{});var Md=n(bs);wl=i(Md,"TFMT5ForConditionalGeneration"),Md.forEach(r),Uo.forEach(r),io=m(e),I=o(e,"DIV",{class:!0});var Pr=n(I);k(At.$$.fragment,Pr),xl=m(Pr),St=o(Pr,"P",{});var Wo=n(St);Ml=i(Wo,"This class overrides "),_r=o(Wo,"A",{href:!0});var Ed=n(_r);El=i(Ed,"TFT5ForConditionalGeneration"),Ed.forEach(r),zl=i(Wo,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Wo.forEach(r),ql=m(Pr),k(Ae.$$.fragment,Pr),Pr.forEach(r),po=m(e),ae=o(e,"H2",{class:!0});var Bo=n(ae);Se=o(Bo,"A",{id:!0,class:!0,href:!0});var zd=n(Se);ys=o(zd,"SPAN",{});var qd=n(ys);k(Lt.$$.fragment,qd),qd.forEach(r),zd.forEach(r),Fl=m(Bo),ws=o(Bo,"SPAN",{});var Fd=n(ws);jl=i(Fd,"TFMT5EncoderModel"),Fd.forEach(r),Bo.forEach(r),mo=m(e),G=o(e,"DIV",{class:!0});var Cr=n(G);k(Nt.$$.fragment,Cr),Pl=m(Cr),Dt=o(Cr,"P",{});var Ho=n(Dt);Cl=i(Ho,"This class overrides "),vr=o(Ho,"A",{href:!0});var jd=n(vr);Al=i(jd,"TFT5EncoderModel"),jd.forEach(r),Sl=i(Ho,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Ho.forEach(r),Ll=m(Cr),k(Le.$$.fragment,Cr),Cr.forEach(r),co=m(e),le=o(e,"H2",{class:!0});var Ro=n(le);Ne=o(Ro,"A",{id:!0,class:!0,href:!0});var Pd=n(Ne);xs=o(Pd,"SPAN",{});var Cd=n(xs);k(It.$$.fragment,Cd),Cd.forEach(r),Pd.forEach(r),Nl=m(Ro),Ms=o(Ro,"SPAN",{});var Ad=n(Ms);Dl=i(Ad,"FlaxMT5Model"),Ad.forEach(r),Ro.forEach(r),fo=m(e),O=o(e,"DIV",{class:!0});var Ar=n(O);k(Gt.$$.fragment,Ar),Il=m(Ar),Ot=o(Ar,"P",{});var Xo=n(Ot);Gl=i(Xo,"This class overrides "),Tr=o(Xo,"A",{href:!0});var Sd=n(Tr);Ol=i(Sd,"FlaxT5Model"),Sd.forEach(r),Vl=i(Xo,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Xo.forEach(r),Ul=m(Ar),k(De.$$.fragment,Ar),Ar.forEach(r),uo=m(e),ie=o(e,"H2",{class:!0});var Ko=n(ie);Ie=o(Ko,"A",{id:!0,class:!0,href:!0});var Ld=n(Ie);Es=o(Ld,"SPAN",{});var Nd=n(Es);k(Vt.$$.fragment,Nd),Nd.forEach(r),Ld.forEach(r),Wl=m(Ko),zs=o(Ko,"SPAN",{});var Dd=n(zs);Bl=i(Dd,"FlaxMT5ForConditionalGeneration"),Dd.forEach(r),Ko.forEach(r),go=m(e),V=o(e,"DIV",{class:!0});var Sr=n(V);k(Ut.$$.fragment,Sr),Hl=m(Sr),Wt=o(Sr,"P",{});var Jo=n(Wt);Rl=i(Jo,"This class overrides "),kr=o(Jo,"A",{href:!0});var Id=n(kr);Xl=i(Id,"FlaxT5ForConditionalGeneration"),Id.forEach(r),Kl=i(Jo,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Jo.forEach(r),Jl=m(Sr),k(Ge.$$.fragment,Sr),Sr.forEach(r),ho=m(e),de=o(e,"H2",{class:!0});var Qo=n(de);Oe=o(Qo,"A",{id:!0,class:!0,href:!0});var Gd=n(Oe);qs=o(Gd,"SPAN",{});var Od=n(qs);k(Bt.$$.fragment,Od),Od.forEach(r),Gd.forEach(r),Ql=m(Qo),Fs=o(Qo,"SPAN",{});var Vd=n(Fs);Yl=i(Vd,"FlaxMT5EncoderModel"),Vd.forEach(r),Qo.forEach(r),_o=m(e),U=o(e,"DIV",{class:!0});var Lr=n(U);k(Ht.$$.fragment,Lr),Zl=m(Lr),Rt=o(Lr,"P",{});var Yo=n(Rt);ei=i(Yo,"This class overrides "),$r=o(Yo,"A",{href:!0});var Ud=n($r);ti=i(Ud,"FlaxT5EncoderModel"),Ud.forEach(r),ri=i(Yo,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Yo.forEach(r),si=m(Lr),k(Ve.$$.fragment,Lr),Lr.forEach(r),this.h()},h(){d(g,"name","hf:doc:metadata"),d(g,"content",JSON.stringify(np)),d(u,"id","mt5"),d(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(u,"href","#mt5"),d(_,"class","relative group"),d(fe,"id","overview"),d(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fe,"href","#overview"),d(K,"class","relative group"),d(Be,"href","https://arxiv.org/abs/2010.11934"),d(Be,"rel","nofollow"),d(He,"href","https://huggingface.co/datasets/mc4"),d(He,"rel","nofollow"),d(Re,"href","https://huggingface.co/google/mt5-small"),d(Re,"rel","nofollow"),d(Xe,"href","https://huggingface.co/google/mt5-base"),d(Xe,"rel","nofollow"),d(Ke,"href","https://huggingface.co/google/mt5-large"),d(Ke,"rel","nofollow"),d(Je,"href","https://huggingface.co/google/mt5-xl"),d(Je,"rel","nofollow"),d(Qe,"href","https://huggingface.co/google/mt5-xxl"),d(Qe,"rel","nofollow"),d(Ye,"href","https://huggingface.co/patrickvonplaten"),d(Ye,"rel","nofollow"),d(Ze,"href","https://github.com/google-research/multilingual-t5"),d(Ze,"rel","nofollow"),d(he,"id","transformers.MT5Config"),d(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(he,"href","#transformers.MT5Config"),d(J,"class","relative group"),d(tr,"href","/docs/transformers/pr_18449/en/model_doc/mt5#transformers.MT5Model"),d(rr,"href","/docs/transformers/pr_18449/en/model_doc/mt5#transformers.TFMT5Model"),d(rt,"href","https://huggingface.co/google/mt5-small"),d(rt,"rel","nofollow"),d(sr,"href","/docs/transformers/pr_18449/en/main_classes/configuration#transformers.PretrainedConfig"),d(or,"href","/docs/transformers/pr_18449/en/main_classes/configuration#transformers.PretrainedConfig"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_e,"id","transformers.T5Tokenizer"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#transformers.T5Tokenizer"),d(Y,"class","relative group"),d(at,"href","https://github.com/google/sentencepiece"),d(at,"rel","nofollow"),d(nr,"href","/docs/transformers/pr_18449/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ir,"href","/docs/transformers/pr_18449/en/model_doc/mt5#transformers.T5Tokenizer"),d(be,"id","transformers.T5TokenizerFast"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#transformers.T5TokenizerFast"),d(Z,"class","relative group"),d(ht,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),d(ht,"rel","nofollow"),d(dr,"href","/docs/transformers/pr_18449/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),d(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(cr,"href","/docs/transformers/pr_18449/en/model_doc/mt5#transformers.T5TokenizerFast"),d(xe,"id","transformers.MT5Model"),d(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xe,"href","#transformers.MT5Model"),d(te,"class","relative group"),d(fr,"href","/docs/transformers/pr_18449/en/model_doc/t5#transformers.T5Model"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ee,"id","transformers.MT5ForConditionalGeneration"),d(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ee,"href","#transformers.MT5ForConditionalGeneration"),d(re,"class","relative group"),d(ur,"href","/docs/transformers/pr_18449/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qe,"id","transformers.MT5EncoderModel"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#transformers.MT5EncoderModel"),d(se,"class","relative group"),d(gr,"href","/docs/transformers/pr_18449/en/model_doc/t5#transformers.T5EncoderModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(je,"id","transformers.TFMT5Model"),d(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(je,"href","#transformers.TFMT5Model"),d(oe,"class","relative group"),d(hr,"href","/docs/transformers/pr_18449/en/model_doc/t5#transformers.TFT5Model"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ce,"id","transformers.TFMT5ForConditionalGeneration"),d(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ce,"href","#transformers.TFMT5ForConditionalGeneration"),d(ne,"class","relative group"),d(_r,"href","/docs/transformers/pr_18449/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Se,"id","transformers.TFMT5EncoderModel"),d(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Se,"href","#transformers.TFMT5EncoderModel"),d(ae,"class","relative group"),d(vr,"href","/docs/transformers/pr_18449/en/model_doc/t5#transformers.TFT5EncoderModel"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ne,"id","transformers.FlaxMT5Model"),d(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ne,"href","#transformers.FlaxMT5Model"),d(le,"class","relative group"),d(Tr,"href","/docs/transformers/pr_18449/en/model_doc/t5#transformers.FlaxT5Model"),d(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ie,"id","transformers.FlaxMT5ForConditionalGeneration"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#transformers.FlaxMT5ForConditionalGeneration"),d(ie,"class","relative group"),d(kr,"href","/docs/transformers/pr_18449/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Oe,"id","transformers.FlaxMT5EncoderModel"),d(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oe,"href","#transformers.FlaxMT5EncoderModel"),d(de,"class","relative group"),d($r,"href","/docs/transformers/pr_18449/en/model_doc/t5#transformers.FlaxT5EncoderModel"),d(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,c){t(document.head,g),f(e,x,c),f(e,_,c),t(_,u),t(u,v),$(a,v,null),t(_,h),t(_,Nr),t(Nr,Zo),f(e,Ns,c),f(e,K,c),t(K,fe),t(fe,Dr),$(We,Dr,null),t(K,en),t(K,Ir),t(Ir,tn),f(e,Ds,c),f(e,ue,c),t(ue,rn),t(ue,Be),t(Be,sn),t(ue,on),f(e,Is,c),f(e,Qt,c),t(Qt,nn),f(e,Gs,c),f(e,Yt,c),t(Yt,Gr),t(Gr,an),f(e,Os,c),f(e,ge,c),t(ge,ln),t(ge,He),t(He,dn),t(ge,pn),f(e,Vs,c),f(e,Zt,c),t(Zt,mn),f(e,Us,c),f(e,F,c),t(F,Or),t(Or,Vr),t(Vr,Re),t(Re,cn),t(F,fn),t(F,Ur),t(Ur,Wr),t(Wr,Xe),t(Xe,un),t(F,gn),t(F,Br),t(Br,Hr),t(Hr,Ke),t(Ke,hn),t(F,_n),t(F,Rr),t(Rr,Xr),t(Xr,Je),t(Je,vn),t(F,Tn),t(F,Kr),t(Kr,er),t(er,Qe),t(Qe,kn),t(er,$n),f(e,Ws,c),f(e,W,c),t(W,bn),t(W,Ye),t(Ye,yn),t(W,wn),t(W,Ze),t(Ze,xn),t(W,Mn),f(e,Bs,c),f(e,J,c),t(J,he),t(he,Jr),$(et,Jr,null),t(J,En),t(J,Qr),t(Qr,zn),f(e,Hs,c),f(e,C,c),$(tt,C,null),t(C,qn),t(C,A),t(A,Fn),t(A,tr),t(tr,jn),t(A,Pn),t(A,rr),t(rr,Cn),t(A,An),t(A,rt),t(rt,Sn),t(A,Ln),t(C,Nn),t(C,Q),t(Q,Dn),t(Q,sr),t(sr,In),t(Q,Gn),t(Q,or),t(or,On),t(Q,Vn),f(e,Rs,c),f(e,Y,c),t(Y,_e),t(_e,Yr),$(st,Yr,null),t(Y,Un),t(Y,Zr),t(Zr,Wn),f(e,Xs,c),f(e,E,c),$(ot,E,null),t(E,Bn),t(E,nt),t(nt,Hn),t(nt,at),t(at,Rn),t(nt,Xn),t(E,Kn),t(E,lt),t(lt,Jn),t(lt,nr),t(nr,Qn),t(lt,Yn),t(E,Zn),t(E,B),$(it,B,null),t(B,ea),t(B,es),t(es,ta),t(B,ra),t(B,dt),t(dt,ar),t(ar,sa),t(ar,ts),t(ts,oa),t(dt,na),t(dt,lr),t(lr,aa),t(lr,rs),t(rs,la),t(E,ia),t(E,ve),$(pt,ve,null),t(ve,da),t(ve,ss),t(ss,pa),t(E,ma),t(E,Te),$(mt,Te,null),t(Te,ca),t(Te,os),t(os,fa),t(E,ua),t(E,ke),$(ct,ke,null),t(ke,ga),t(ke,ft),t(ft,ha),t(ft,ns),t(ns,_a),t(ft,va),f(e,Ks,c),f(e,$e,c),t($e,Ta),t($e,ir),t(ir,ka),t($e,$a),f(e,Js,c),f(e,Z,c),t(Z,be),t(be,as),$(ut,as,null),t(Z,ba),t(Z,ls),t(ls,ya),f(e,Qs,c),f(e,q,c),$(gt,q,null),t(q,wa),t(q,ee),t(ee,xa),t(ee,is),t(is,Ma),t(ee,Ea),t(ee,ht),t(ht,za),t(ee,qa),t(q,Fa),t(q,_t),t(_t,ja),t(_t,dr),t(dr,Pa),t(_t,Ca),t(q,Aa),t(q,H),$(vt,H,null),t(H,Sa),t(H,ds),t(ds,La),t(H,Na),t(H,Tt),t(Tt,pr),t(pr,Da),t(pr,ps),t(ps,Ia),t(Tt,Ga),t(Tt,mr),t(mr,Oa),t(mr,ms),t(ms,Va),t(q,Ua),t(q,ye),$(kt,ye,null),t(ye,Wa),t(ye,cs),t(cs,Ba),f(e,Ys,c),f(e,we,c),t(we,Ha),t(we,cr),t(cr,Ra),t(we,Xa),f(e,Zs,c),f(e,te,c),t(te,xe),t(xe,fs),$($t,fs,null),t(te,Ka),t(te,us),t(us,Ja),f(e,eo,c),f(e,S,c),$(bt,S,null),t(S,Qa),t(S,yt),t(yt,Ya),t(yt,fr),t(fr,Za),t(yt,el),t(S,tl),$(Me,S,null),f(e,to,c),f(e,re,c),t(re,Ee),t(Ee,gs),$(wt,gs,null),t(re,rl),t(re,hs),t(hs,sl),f(e,ro,c),f(e,L,c),$(xt,L,null),t(L,ol),t(L,Mt),t(Mt,nl),t(Mt,ur),t(ur,al),t(Mt,ll),t(L,il),$(ze,L,null),f(e,so,c),f(e,se,c),t(se,qe),t(qe,_s),$(Et,_s,null),t(se,dl),t(se,vs),t(vs,pl),f(e,oo,c),f(e,N,c),$(zt,N,null),t(N,ml),t(N,qt),t(qt,cl),t(qt,gr),t(gr,fl),t(qt,ul),t(N,gl),$(Fe,N,null),f(e,no,c),f(e,oe,c),t(oe,je),t(je,Ts),$(Ft,Ts,null),t(oe,hl),t(oe,ks),t(ks,_l),f(e,ao,c),f(e,D,c),$(jt,D,null),t(D,vl),t(D,Pt),t(Pt,Tl),t(Pt,hr),t(hr,kl),t(Pt,$l),t(D,bl),$(Pe,D,null),f(e,lo,c),f(e,ne,c),t(ne,Ce),t(Ce,$s),$(Ct,$s,null),t(ne,yl),t(ne,bs),t(bs,wl),f(e,io,c),f(e,I,c),$(At,I,null),t(I,xl),t(I,St),t(St,Ml),t(St,_r),t(_r,El),t(St,zl),t(I,ql),$(Ae,I,null),f(e,po,c),f(e,ae,c),t(ae,Se),t(Se,ys),$(Lt,ys,null),t(ae,Fl),t(ae,ws),t(ws,jl),f(e,mo,c),f(e,G,c),$(Nt,G,null),t(G,Pl),t(G,Dt),t(Dt,Cl),t(Dt,vr),t(vr,Al),t(Dt,Sl),t(G,Ll),$(Le,G,null),f(e,co,c),f(e,le,c),t(le,Ne),t(Ne,xs),$(It,xs,null),t(le,Nl),t(le,Ms),t(Ms,Dl),f(e,fo,c),f(e,O,c),$(Gt,O,null),t(O,Il),t(O,Ot),t(Ot,Gl),t(Ot,Tr),t(Tr,Ol),t(Ot,Vl),t(O,Ul),$(De,O,null),f(e,uo,c),f(e,ie,c),t(ie,Ie),t(Ie,Es),$(Vt,Es,null),t(ie,Wl),t(ie,zs),t(zs,Bl),f(e,go,c),f(e,V,c),$(Ut,V,null),t(V,Hl),t(V,Wt),t(Wt,Rl),t(Wt,kr),t(kr,Xl),t(Wt,Kl),t(V,Jl),$(Ge,V,null),f(e,ho,c),f(e,de,c),t(de,Oe),t(Oe,qs),$(Bt,qs,null),t(de,Ql),t(de,Fs),t(Fs,Yl),f(e,_o,c),f(e,U,c),$(Ht,U,null),t(U,Zl),t(U,Rt),t(Rt,ei),t(Rt,$r),t($r,ti),t(Rt,ri),t(U,si),$(Ve,U,null),vo=!0},p(e,[c]){const Xt={};c&2&&(Xt.$$scope={dirty:c,ctx:e}),Me.$set(Xt);const js={};c&2&&(js.$$scope={dirty:c,ctx:e}),ze.$set(js);const Ps={};c&2&&(Ps.$$scope={dirty:c,ctx:e}),Fe.$set(Ps);const Cs={};c&2&&(Cs.$$scope={dirty:c,ctx:e}),Pe.$set(Cs);const Kt={};c&2&&(Kt.$$scope={dirty:c,ctx:e}),Ae.$set(Kt);const As={};c&2&&(As.$$scope={dirty:c,ctx:e}),Le.$set(As);const Ss={};c&2&&(Ss.$$scope={dirty:c,ctx:e}),De.$set(Ss);const Ls={};c&2&&(Ls.$$scope={dirty:c,ctx:e}),Ge.$set(Ls);const Jt={};c&2&&(Jt.$$scope={dirty:c,ctx:e}),Ve.$set(Jt)},i(e){vo||(b(a.$$.fragment,e),b(We.$$.fragment,e),b(et.$$.fragment,e),b(tt.$$.fragment,e),b(st.$$.fragment,e),b(ot.$$.fragment,e),b(it.$$.fragment,e),b(pt.$$.fragment,e),b(mt.$$.fragment,e),b(ct.$$.fragment,e),b(ut.$$.fragment,e),b(gt.$$.fragment,e),b(vt.$$.fragment,e),b(kt.$$.fragment,e),b($t.$$.fragment,e),b(bt.$$.fragment,e),b(Me.$$.fragment,e),b(wt.$$.fragment,e),b(xt.$$.fragment,e),b(ze.$$.fragment,e),b(Et.$$.fragment,e),b(zt.$$.fragment,e),b(Fe.$$.fragment,e),b(Ft.$$.fragment,e),b(jt.$$.fragment,e),b(Pe.$$.fragment,e),b(Ct.$$.fragment,e),b(At.$$.fragment,e),b(Ae.$$.fragment,e),b(Lt.$$.fragment,e),b(Nt.$$.fragment,e),b(Le.$$.fragment,e),b(It.$$.fragment,e),b(Gt.$$.fragment,e),b(De.$$.fragment,e),b(Vt.$$.fragment,e),b(Ut.$$.fragment,e),b(Ge.$$.fragment,e),b(Bt.$$.fragment,e),b(Ht.$$.fragment,e),b(Ve.$$.fragment,e),vo=!0)},o(e){y(a.$$.fragment,e),y(We.$$.fragment,e),y(et.$$.fragment,e),y(tt.$$.fragment,e),y(st.$$.fragment,e),y(ot.$$.fragment,e),y(it.$$.fragment,e),y(pt.$$.fragment,e),y(mt.$$.fragment,e),y(ct.$$.fragment,e),y(ut.$$.fragment,e),y(gt.$$.fragment,e),y(vt.$$.fragment,e),y(kt.$$.fragment,e),y($t.$$.fragment,e),y(bt.$$.fragment,e),y(Me.$$.fragment,e),y(wt.$$.fragment,e),y(xt.$$.fragment,e),y(ze.$$.fragment,e),y(Et.$$.fragment,e),y(zt.$$.fragment,e),y(Fe.$$.fragment,e),y(Ft.$$.fragment,e),y(jt.$$.fragment,e),y(Pe.$$.fragment,e),y(Ct.$$.fragment,e),y(At.$$.fragment,e),y(Ae.$$.fragment,e),y(Lt.$$.fragment,e),y(Nt.$$.fragment,e),y(Le.$$.fragment,e),y(It.$$.fragment,e),y(Gt.$$.fragment,e),y(De.$$.fragment,e),y(Vt.$$.fragment,e),y(Ut.$$.fragment,e),y(Ge.$$.fragment,e),y(Bt.$$.fragment,e),y(Ht.$$.fragment,e),y(Ve.$$.fragment,e),vo=!1},d(e){r(g),e&&r(x),e&&r(_),w(a),e&&r(Ns),e&&r(K),w(We),e&&r(Ds),e&&r(ue),e&&r(Is),e&&r(Qt),e&&r(Gs),e&&r(Yt),e&&r(Os),e&&r(ge),e&&r(Vs),e&&r(Zt),e&&r(Us),e&&r(F),e&&r(Ws),e&&r(W),e&&r(Bs),e&&r(J),w(et),e&&r(Hs),e&&r(C),w(tt),e&&r(Rs),e&&r(Y),w(st),e&&r(Xs),e&&r(E),w(ot),w(it),w(pt),w(mt),w(ct),e&&r(Ks),e&&r($e),e&&r(Js),e&&r(Z),w(ut),e&&r(Qs),e&&r(q),w(gt),w(vt),w(kt),e&&r(Ys),e&&r(we),e&&r(Zs),e&&r(te),w($t),e&&r(eo),e&&r(S),w(bt),w(Me),e&&r(to),e&&r(re),w(wt),e&&r(ro),e&&r(L),w(xt),w(ze),e&&r(so),e&&r(se),w(Et),e&&r(oo),e&&r(N),w(zt),w(Fe),e&&r(no),e&&r(oe),w(Ft),e&&r(ao),e&&r(D),w(jt),w(Pe),e&&r(lo),e&&r(ne),w(Ct),e&&r(io),e&&r(I),w(At),w(Ae),e&&r(po),e&&r(ae),w(Lt),e&&r(mo),e&&r(G),w(Nt),w(Le),e&&r(co),e&&r(le),w(It),e&&r(fo),e&&r(O),w(Gt),w(De),e&&r(uo),e&&r(ie),w(Vt),e&&r(go),e&&r(V),w(Ut),w(Ge),e&&r(ho),e&&r(de),w(Bt),e&&r(_o),e&&r(U),w(Ht),w(Ve)}}}const np={local:"mt5",sections:[{local:"overview",title:"Overview"},{local:"transformers.MT5Config",title:"MT5Config"},{local:"transformers.T5Tokenizer",title:"MT5Tokenizer"},{local:"transformers.T5TokenizerFast",title:"MT5TokenizerFast"},{local:"transformers.MT5Model",title:"MT5Model"},{local:"transformers.MT5ForConditionalGeneration",title:"MT5ForConditionalGeneration"},{local:"transformers.MT5EncoderModel",title:"MT5EncoderModel"},{local:"transformers.TFMT5Model",title:"TFMT5Model"},{local:"transformers.TFMT5ForConditionalGeneration",title:"TFMT5ForConditionalGeneration"},{local:"transformers.TFMT5EncoderModel",title:"TFMT5EncoderModel"},{local:"transformers.FlaxMT5Model",title:"FlaxMT5Model"},{local:"transformers.FlaxMT5ForConditionalGeneration",title:"FlaxMT5ForConditionalGeneration"},{local:"transformers.FlaxMT5EncoderModel",title:"FlaxMT5EncoderModel"}],title:"mT5"};function ap(M){return Xd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cp extends Wd{constructor(g){super();Bd(this,g,ap,op,Hd,{})}}export{cp as default,np as metadata};
