import{S as Qo,i as Zo,s as er,e as o,k as c,w as S,t as l,M as tr,c as r,d as n,m as d,a,x as q,h as i,b as s,G as t,g as f,y as H,L as nr,q as I,o as W,B,v as or}from"../chunks/vendor-hf-doc-builder.js";import{I as wt}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ct}from"../chunks/CodeBlock-hf-doc-builder.js";function rr(to){let w,Je,C,A,Oe,K,Pt,Me,$t,Ye,ae,Dt,Qe,P,R,Ne,G,kt,Se,At,Ze,E,z,Rt,Lt,X,Ut,Ot,j,Mt,Nt,et,y,St,qe,qt,Ht,He,It,Wt,tt,L,Bt,F,Kt,Gt,nt,$,U,Ie,V,zt,We,Xt,ot,le,jt,rt,O,Be,p,ie,Ft,Vt,se,Jt,Yt,ce,Qt,Zt,de,en,tn,he,nn,on,fe,rn,an,D,_,pe,ln,sn,Ke,cn,_e,dn,hn,ue,fn,pn,me,_n,un,ve,mn,vn,u,ge,gn,xn,Ge,bn,xe,En,yn,be,Tn,wn,Ee,Cn,Pn,ye,$n,Dn,m,Te,kn,An,we,Rn,Ln,Ce,Un,On,Pe,Mn,Nn,$e,Sn,qn,ze,at,J,lt,T,Hn,Xe,In,Wn,Y,Bn,Kn,it,k,M,je,Q,Gn,Fe,zn,st,Z,Xn,ee,jn,ct,De,Fn,dt,te,ht,ke,Vn,ft,Ae,Jn,pt,ne,_t,N,Yn,Ve,Qn,Zn,ut,oe,mt;return K=new wt({}),G=new wt({}),V=new wt({}),J=new Ct({props:{code:"pip install oneccl_bind_pt=={pytorch_version} -f https://software.intel.com/ipex-whl-stable",highlighted:'pip install oneccl_bind_pt=={<span class="hljs-attribute">pytorch_version} -f https</span>://software<span class="hljs-variable">.intel</span><span class="hljs-variable">.com</span>/ipex-whl-stable'}}),Q=new wt({}),te=new Ct({props:{code:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=127.0.0.1
 mpirun -n 2 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`,highlighted:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=127.0.0.1
 mpirun -n 2 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`}}),ne=new Ct({props:{code:` cat hostfile
 xxx.xxx.xxx.xxx #node0 ip
 xxx.xxx.xxx.xxx #node1 ip`,highlighted:` cat hostfile
 xxx.xxx.xxx.xxx #node0 ip
 xxx.xxx.xxx.xxx #node1 ip`}}),oe=new Ct({props:{code:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip
 mpirun -f hostfile -n 4 -ppn 2 \\
 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`,highlighted:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip
 mpirun -f hostfile -n 4 -ppn 2 \\
 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`}}),{c(){w=o("meta"),Je=c(),C=o("h1"),A=o("a"),Oe=o("span"),S(K.$$.fragment),Pt=c(),Me=o("span"),$t=l("Efficient Training on Multiple CPUs"),Ye=c(),ae=o("p"),Dt=l("When training on a single CPU is too slow, we can use multiple CPUs. This guide focuses on PyTorch-based DDP enabling distributed CPU training efficiently."),Qe=c(),P=o("h2"),R=o("a"),Ne=o("span"),S(G.$$.fragment),kt=c(),Se=o("span"),At=l("Intel\xAE oneCCL Bindings for PyTorch"),Ze=c(),E=o("p"),z=o("a"),Rt=l("Intel\xAE oneCCL"),Lt=l(" (collective communications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the "),X=o("a"),Ut=l("oneCCL documentation"),Ot=l(" and "),j=o("a"),Mt=l("oneCCL specification"),Nt=l("."),et=c(),y=o("p"),St=l("Module "),qe=o("code"),qt=l("oneccl_bindings_for_pytorch"),Ht=l(" ("),He=o("code"),It=l("torch_ccl"),Wt=l(" before version 1.12)  implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now"),tt=c(),L=o("p"),Bt=l("Check more detailed information for "),F=o("a"),Kt=l("oneccl_bind_pt"),Gt=l("."),nt=c(),$=o("h3"),U=o("a"),Ie=o("span"),S(V.$$.fragment),zt=c(),We=o("span"),Xt=l("Intel\xAE oneCCL Bindings for PyTorch installation:"),ot=c(),le=o("p"),jt=l("Wheel files are available for the following Python versions:"),rt=c(),O=o("table"),Be=o("thead"),p=o("tr"),ie=o("th"),Ft=l("Extension Version"),Vt=c(),se=o("th"),Jt=l("Python 3.6"),Yt=c(),ce=o("th"),Qt=l("Python 3.7"),Zt=c(),de=o("th"),en=l("Python 3.8"),tn=c(),he=o("th"),nn=l("Python 3.9"),on=c(),fe=o("th"),rn=l("Python 3.10"),an=c(),D=o("tbody"),_=o("tr"),pe=o("td"),ln=l("1.12.0"),sn=c(),Ke=o("td"),cn=c(),_e=o("td"),dn=l("\u221A"),hn=c(),ue=o("td"),fn=l("\u221A"),pn=c(),me=o("td"),_n=l("\u221A"),un=c(),ve=o("td"),mn=l("\u221A"),vn=c(),u=o("tr"),ge=o("td"),gn=l("1.11.0"),xn=c(),Ge=o("td"),bn=c(),xe=o("td"),En=l("\u221A"),yn=c(),be=o("td"),Tn=l("\u221A"),wn=c(),Ee=o("td"),Cn=l("\u221A"),Pn=c(),ye=o("td"),$n=l("\u221A"),Dn=c(),m=o("tr"),Te=o("td"),kn=l("1.10.0"),An=c(),we=o("td"),Rn=l("\u221A"),Ln=c(),Ce=o("td"),Un=l("\u221A"),On=c(),Pe=o("td"),Mn=l("\u221A"),Nn=c(),$e=o("td"),Sn=l("\u221A"),qn=c(),ze=o("td"),at=c(),S(J.$$.fragment),lt=c(),T=o("p"),Hn=l("where "),Xe=o("code"),In=l("{pytorch_version}"),Wn=l(` should be your PyTorch version, for instance 1.12.0.
Check more approaches for `),Y=o("a"),Bn=l("oneccl_bind_pt installation"),Kn=l("."),it=c(),k=o("h3"),M=o("a"),je=o("span"),S(Q.$$.fragment),Gn=c(),Fe=o("span"),zn=l("Usage in Trainer"),st=l(`

To enable multi CPU distributed training in the Trainer with the ccl backend, users should add **\`--xpu_backend ccl\`** in the command arguments.
`),Z=o("p"),Xn=l("Let\u2019s see an example with the "),ee=o("a"),jn=l("question-answering example"),ct=c(),De=o("p"),Fn=l("The following command enables training with 2 processes on one Xeon node, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),dt=c(),S(te.$$.fragment),ht=c(),ke=o("p"),Vn=l("The following command enables training with a total of four processes on two Xeons (node0 and node1, taking node0 as the main process), ppn (processes per node) is set to 2, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),ft=c(),Ae=o("p"),Jn=l("In node0, you need to create a configuration file which contains the IP addresses of each node (for example hostfile) and pass that configuration file path as an argument."),pt=c(),S(ne.$$.fragment),_t=c(),N=o("p"),Yn=l("Now, run the following command in node0 and "),Ve=o("strong"),Qn=l("4DDP"),Zn=l(" will be enabled in node0 and node1:"),ut=c(),S(oe.$$.fragment),this.h()},l(e){const h=tr('[data-svelte="svelte-1phssyn"]',document.head);w=r(h,"META",{name:!0,content:!0}),h.forEach(n),Je=d(e),C=r(e,"H1",{class:!0});var vt=a(C);A=r(vt,"A",{id:!0,class:!0,href:!0});var no=a(A);Oe=r(no,"SPAN",{});var oo=a(Oe);q(K.$$.fragment,oo),oo.forEach(n),no.forEach(n),Pt=d(vt),Me=r(vt,"SPAN",{});var ro=a(Me);$t=i(ro,"Efficient Training on Multiple CPUs"),ro.forEach(n),vt.forEach(n),Ye=d(e),ae=r(e,"P",{});var ao=a(ae);Dt=i(ao,"When training on a single CPU is too slow, we can use multiple CPUs. This guide focuses on PyTorch-based DDP enabling distributed CPU training efficiently."),ao.forEach(n),Qe=d(e),P=r(e,"H2",{class:!0});var gt=a(P);R=r(gt,"A",{id:!0,class:!0,href:!0});var lo=a(R);Ne=r(lo,"SPAN",{});var io=a(Ne);q(G.$$.fragment,io),io.forEach(n),lo.forEach(n),kt=d(gt),Se=r(gt,"SPAN",{});var so=a(Se);At=i(so,"Intel\xAE oneCCL Bindings for PyTorch"),so.forEach(n),gt.forEach(n),Ze=d(e),E=r(e,"P",{});var re=a(E);z=r(re,"A",{href:!0,rel:!0});var co=a(z);Rt=i(co,"Intel\xAE oneCCL"),co.forEach(n),Lt=i(re," (collective communications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the "),X=r(re,"A",{href:!0,rel:!0});var ho=a(X);Ut=i(ho,"oneCCL documentation"),ho.forEach(n),Ot=i(re," and "),j=r(re,"A",{href:!0,rel:!0});var fo=a(j);Mt=i(fo,"oneCCL specification"),fo.forEach(n),Nt=i(re,"."),re.forEach(n),et=d(e),y=r(e,"P",{});var Re=a(y);St=i(Re,"Module "),qe=r(Re,"CODE",{});var po=a(qe);qt=i(po,"oneccl_bindings_for_pytorch"),po.forEach(n),Ht=i(Re," ("),He=r(Re,"CODE",{});var _o=a(He);It=i(_o,"torch_ccl"),_o.forEach(n),Wt=i(Re," before version 1.12)  implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now"),Re.forEach(n),tt=d(e),L=r(e,"P",{});var xt=a(L);Bt=i(xt,"Check more detailed information for "),F=r(xt,"A",{href:!0,rel:!0});var uo=a(F);Kt=i(uo,"oneccl_bind_pt"),uo.forEach(n),Gt=i(xt,"."),xt.forEach(n),nt=d(e),$=r(e,"H3",{class:!0});var bt=a($);U=r(bt,"A",{id:!0,class:!0,href:!0});var mo=a(U);Ie=r(mo,"SPAN",{});var vo=a(Ie);q(V.$$.fragment,vo),vo.forEach(n),mo.forEach(n),zt=d(bt),We=r(bt,"SPAN",{});var go=a(We);Xt=i(go,"Intel\xAE oneCCL Bindings for PyTorch installation:"),go.forEach(n),bt.forEach(n),ot=d(e),le=r(e,"P",{});var xo=a(le);jt=i(xo,"Wheel files are available for the following Python versions:"),xo.forEach(n),rt=d(e),O=r(e,"TABLE",{});var Et=a(O);Be=r(Et,"THEAD",{});var bo=a(Be);p=r(bo,"TR",{});var v=a(p);ie=r(v,"TH",{align:!0});var Eo=a(ie);Ft=i(Eo,"Extension Version"),Eo.forEach(n),Vt=d(v),se=r(v,"TH",{align:!0});var yo=a(se);Jt=i(yo,"Python 3.6"),yo.forEach(n),Yt=d(v),ce=r(v,"TH",{align:!0});var To=a(ce);Qt=i(To,"Python 3.7"),To.forEach(n),Zt=d(v),de=r(v,"TH",{align:!0});var wo=a(de);en=i(wo,"Python 3.8"),wo.forEach(n),tn=d(v),he=r(v,"TH",{align:!0});var Co=a(he);nn=i(Co,"Python 3.9"),Co.forEach(n),on=d(v),fe=r(v,"TH",{align:!0});var Po=a(fe);rn=i(Po,"Python 3.10"),Po.forEach(n),v.forEach(n),bo.forEach(n),an=d(Et),D=r(Et,"TBODY",{});var Le=a(D);_=r(Le,"TR",{});var g=a(_);pe=r(g,"TD",{align:!0});var $o=a(pe);ln=i($o,"1.12.0"),$o.forEach(n),sn=d(g),Ke=r(g,"TD",{align:!0}),a(Ke).forEach(n),cn=d(g),_e=r(g,"TD",{align:!0});var Do=a(_e);dn=i(Do,"\u221A"),Do.forEach(n),hn=d(g),ue=r(g,"TD",{align:!0});var ko=a(ue);fn=i(ko,"\u221A"),ko.forEach(n),pn=d(g),me=r(g,"TD",{align:!0});var Ao=a(me);_n=i(Ao,"\u221A"),Ao.forEach(n),un=d(g),ve=r(g,"TD",{align:!0});var Ro=a(ve);mn=i(Ro,"\u221A"),Ro.forEach(n),g.forEach(n),vn=d(Le),u=r(Le,"TR",{});var x=a(u);ge=r(x,"TD",{align:!0});var Lo=a(ge);gn=i(Lo,"1.11.0"),Lo.forEach(n),xn=d(x),Ge=r(x,"TD",{align:!0}),a(Ge).forEach(n),bn=d(x),xe=r(x,"TD",{align:!0});var Uo=a(xe);En=i(Uo,"\u221A"),Uo.forEach(n),yn=d(x),be=r(x,"TD",{align:!0});var Oo=a(be);Tn=i(Oo,"\u221A"),Oo.forEach(n),wn=d(x),Ee=r(x,"TD",{align:!0});var Mo=a(Ee);Cn=i(Mo,"\u221A"),Mo.forEach(n),Pn=d(x),ye=r(x,"TD",{align:!0});var No=a(ye);$n=i(No,"\u221A"),No.forEach(n),x.forEach(n),Dn=d(Le),m=r(Le,"TR",{});var b=a(m);Te=r(b,"TD",{align:!0});var So=a(Te);kn=i(So,"1.10.0"),So.forEach(n),An=d(b),we=r(b,"TD",{align:!0});var qo=a(we);Rn=i(qo,"\u221A"),qo.forEach(n),Ln=d(b),Ce=r(b,"TD",{align:!0});var Ho=a(Ce);Un=i(Ho,"\u221A"),Ho.forEach(n),On=d(b),Pe=r(b,"TD",{align:!0});var Io=a(Pe);Mn=i(Io,"\u221A"),Io.forEach(n),Nn=d(b),$e=r(b,"TD",{align:!0});var Wo=a($e);Sn=i(Wo,"\u221A"),Wo.forEach(n),qn=d(b),ze=r(b,"TD",{align:!0}),a(ze).forEach(n),b.forEach(n),Le.forEach(n),Et.forEach(n),at=d(e),q(J.$$.fragment,e),lt=d(e),T=r(e,"P",{});var Ue=a(T);Hn=i(Ue,"where "),Xe=r(Ue,"CODE",{});var Bo=a(Xe);In=i(Bo,"{pytorch_version}"),Bo.forEach(n),Wn=i(Ue,` should be your PyTorch version, for instance 1.12.0.
Check more approaches for `),Y=r(Ue,"A",{href:!0,rel:!0});var Ko=a(Y);Bn=i(Ko,"oneccl_bind_pt installation"),Ko.forEach(n),Kn=i(Ue,"."),Ue.forEach(n),it=d(e),k=r(e,"H3",{class:!0});var yt=a(k);M=r(yt,"A",{id:!0,class:!0,href:!0});var Go=a(M);je=r(Go,"SPAN",{});var zo=a(je);q(Q.$$.fragment,zo),zo.forEach(n),Go.forEach(n),Gn=d(yt),Fe=r(yt,"SPAN",{});var Xo=a(Fe);zn=i(Xo,"Usage in Trainer"),Xo.forEach(n),yt.forEach(n),st=i(e,`

To enable multi CPU distributed training in the Trainer with the ccl backend, users should add **\`--xpu_backend ccl\`** in the command arguments.
`),Z=r(e,"P",{});var eo=a(Z);Xn=i(eo,"Let\u2019s see an example with the "),ee=r(eo,"A",{href:!0,rel:!0});var jo=a(ee);jn=i(jo,"question-answering example"),jo.forEach(n),eo.forEach(n),ct=d(e),De=r(e,"P",{});var Fo=a(De);Fn=i(Fo,"The following command enables training with 2 processes on one Xeon node, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Fo.forEach(n),dt=d(e),q(te.$$.fragment,e),ht=d(e),ke=r(e,"P",{});var Vo=a(ke);Vn=i(Vo,"The following command enables training with a total of four processes on two Xeons (node0 and node1, taking node0 as the main process), ppn (processes per node) is set to 2, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Vo.forEach(n),ft=d(e),Ae=r(e,"P",{});var Jo=a(Ae);Jn=i(Jo,"In node0, you need to create a configuration file which contains the IP addresses of each node (for example hostfile) and pass that configuration file path as an argument."),Jo.forEach(n),pt=d(e),q(ne.$$.fragment,e),_t=d(e),N=r(e,"P",{});var Tt=a(N);Yn=i(Tt,"Now, run the following command in node0 and "),Ve=r(Tt,"STRONG",{});var Yo=a(Ve);Qn=i(Yo,"4DDP"),Yo.forEach(n),Zn=i(Tt," will be enabled in node0 and node1:"),Tt.forEach(n),ut=d(e),q(oe.$$.fragment,e),this.h()},h(){s(w,"name","hf:doc:metadata"),s(w,"content",JSON.stringify(ar)),s(A,"id","efficient-training-on-multiple-cpus"),s(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(A,"href","#efficient-training-on-multiple-cpus"),s(C,"class","relative group"),s(R,"id","intel-oneccl-bindings-for-pytorch"),s(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(R,"href","#intel-oneccl-bindings-for-pytorch"),s(P,"class","relative group"),s(z,"href","https://github.com/oneapi-src/oneCCL"),s(z,"rel","nofollow"),s(X,"href","https://spec.oneapi.com/versions/latest/elements/oneCCL/source/index.html"),s(X,"rel","nofollow"),s(j,"href","https://spec.oneapi.com/versions/latest/elements/oneCCL/source/index.html"),s(j,"rel","nofollow"),s(F,"href","https://github.com/intel/torch-ccl"),s(F,"rel","nofollow"),s(U,"id","intel-oneccl-bindings-for-pytorch-installation"),s(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(U,"href","#intel-oneccl-bindings-for-pytorch-installation"),s($,"class","relative group"),s(ie,"align","center"),s(se,"align","center"),s(ce,"align","center"),s(de,"align","center"),s(he,"align","center"),s(fe,"align","center"),s(pe,"align","center"),s(Ke,"align","center"),s(_e,"align","center"),s(ue,"align","center"),s(me,"align","center"),s(ve,"align","center"),s(ge,"align","center"),s(Ge,"align","center"),s(xe,"align","center"),s(be,"align","center"),s(Ee,"align","center"),s(ye,"align","center"),s(Te,"align","center"),s(we,"align","center"),s(Ce,"align","center"),s(Pe,"align","center"),s($e,"align","center"),s(ze,"align","center"),s(Y,"href","https://github.com/intel/torch-ccl"),s(Y,"rel","nofollow"),s(M,"id","usage-in-trainer"),s(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(M,"href","#usage-in-trainer"),s(k,"class","relative group"),s(ee,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),s(ee,"rel","nofollow")},m(e,h){t(document.head,w),f(e,Je,h),f(e,C,h),t(C,A),t(A,Oe),H(K,Oe,null),t(C,Pt),t(C,Me),t(Me,$t),f(e,Ye,h),f(e,ae,h),t(ae,Dt),f(e,Qe,h),f(e,P,h),t(P,R),t(R,Ne),H(G,Ne,null),t(P,kt),t(P,Se),t(Se,At),f(e,Ze,h),f(e,E,h),t(E,z),t(z,Rt),t(E,Lt),t(E,X),t(X,Ut),t(E,Ot),t(E,j),t(j,Mt),t(E,Nt),f(e,et,h),f(e,y,h),t(y,St),t(y,qe),t(qe,qt),t(y,Ht),t(y,He),t(He,It),t(y,Wt),f(e,tt,h),f(e,L,h),t(L,Bt),t(L,F),t(F,Kt),t(L,Gt),f(e,nt,h),f(e,$,h),t($,U),t(U,Ie),H(V,Ie,null),t($,zt),t($,We),t(We,Xt),f(e,ot,h),f(e,le,h),t(le,jt),f(e,rt,h),f(e,O,h),t(O,Be),t(Be,p),t(p,ie),t(ie,Ft),t(p,Vt),t(p,se),t(se,Jt),t(p,Yt),t(p,ce),t(ce,Qt),t(p,Zt),t(p,de),t(de,en),t(p,tn),t(p,he),t(he,nn),t(p,on),t(p,fe),t(fe,rn),t(O,an),t(O,D),t(D,_),t(_,pe),t(pe,ln),t(_,sn),t(_,Ke),t(_,cn),t(_,_e),t(_e,dn),t(_,hn),t(_,ue),t(ue,fn),t(_,pn),t(_,me),t(me,_n),t(_,un),t(_,ve),t(ve,mn),t(D,vn),t(D,u),t(u,ge),t(ge,gn),t(u,xn),t(u,Ge),t(u,bn),t(u,xe),t(xe,En),t(u,yn),t(u,be),t(be,Tn),t(u,wn),t(u,Ee),t(Ee,Cn),t(u,Pn),t(u,ye),t(ye,$n),t(D,Dn),t(D,m),t(m,Te),t(Te,kn),t(m,An),t(m,we),t(we,Rn),t(m,Ln),t(m,Ce),t(Ce,Un),t(m,On),t(m,Pe),t(Pe,Mn),t(m,Nn),t(m,$e),t($e,Sn),t(m,qn),t(m,ze),f(e,at,h),H(J,e,h),f(e,lt,h),f(e,T,h),t(T,Hn),t(T,Xe),t(Xe,In),t(T,Wn),t(T,Y),t(Y,Bn),t(T,Kn),f(e,it,h),f(e,k,h),t(k,M),t(M,je),H(Q,je,null),t(k,Gn),t(k,Fe),t(Fe,zn),f(e,st,h),f(e,Z,h),t(Z,Xn),t(Z,ee),t(ee,jn),f(e,ct,h),f(e,De,h),t(De,Fn),f(e,dt,h),H(te,e,h),f(e,ht,h),f(e,ke,h),t(ke,Vn),f(e,ft,h),f(e,Ae,h),t(Ae,Jn),f(e,pt,h),H(ne,e,h),f(e,_t,h),f(e,N,h),t(N,Yn),t(N,Ve),t(Ve,Qn),t(N,Zn),f(e,ut,h),H(oe,e,h),mt=!0},p:nr,i(e){mt||(I(K.$$.fragment,e),I(G.$$.fragment,e),I(V.$$.fragment,e),I(J.$$.fragment,e),I(Q.$$.fragment,e),I(te.$$.fragment,e),I(ne.$$.fragment,e),I(oe.$$.fragment,e),mt=!0)},o(e){W(K.$$.fragment,e),W(G.$$.fragment,e),W(V.$$.fragment,e),W(J.$$.fragment,e),W(Q.$$.fragment,e),W(te.$$.fragment,e),W(ne.$$.fragment,e),W(oe.$$.fragment,e),mt=!1},d(e){n(w),e&&n(Je),e&&n(C),B(K),e&&n(Ye),e&&n(ae),e&&n(Qe),e&&n(P),B(G),e&&n(Ze),e&&n(E),e&&n(et),e&&n(y),e&&n(tt),e&&n(L),e&&n(nt),e&&n($),B(V),e&&n(ot),e&&n(le),e&&n(rt),e&&n(O),e&&n(at),B(J,e),e&&n(lt),e&&n(T),e&&n(it),e&&n(k),B(Q),e&&n(st),e&&n(Z),e&&n(ct),e&&n(De),e&&n(dt),B(te,e),e&&n(ht),e&&n(ke),e&&n(ft),e&&n(Ae),e&&n(pt),B(ne,e),e&&n(_t),e&&n(N),e&&n(ut),B(oe,e)}}}const ar={local:"efficient-training-on-multiple-cpus",sections:[{local:"intel-oneccl-bindings-for-pytorch",sections:[{local:"intel-oneccl-bindings-for-pytorch-installation",title:"Intel\xAE oneCCL Bindings for PyTorch installation:"},{local:"usage-in-trainer",title:"Usage in Trainer"}],title:"Intel\xAE oneCCL Bindings for PyTorch"}],title:"Efficient Training on Multiple CPUs"};function lr(to){return or(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class dr extends Qo{constructor(w){super();Zo(this,w,lr,rr,er,{})}}export{dr as default,ar as metadata};
