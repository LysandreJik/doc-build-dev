import{S as Ia,i as Ua,s as Na,e as i,k,w as S,t as n,M as Ba,c as p,d as s,m as w,a as h,x as y,h as o,b as j,G as r,g as u,y as E,q as T,o as x,B as z,v as Wa,L as Ft}from"../../chunks/vendor-hf-doc-builder.js";import{T as At}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Oa}from"../../chunks/Youtube-hf-doc-builder.js";import{I as zt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as X}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as fa,M as He}from"../../chunks/Markdown-hf-doc-builder.js";function Ya(F){let t,l,a,f,_;return{c(){t=i("p"),l=n("See the translation "),a=i("a"),f=n("task page"),_=n(" for more information about its associated models, datasets, and metrics."),this.h()},l($){t=p($,"P",{});var g=h(t);l=o(g,"See the translation "),a=p(g,"A",{href:!0,rel:!0});var v=h(a);f=o(v,"task page"),v.forEach(s),_=o(g," for more information about its associated models, datasets, and metrics."),g.forEach(s),this.h()},h(){j(a,"href","https://huggingface.co/tasks/translation"),j(a,"rel","nofollow")},m($,g){u($,t,g),r(t,l),r(t,a),r(a,f),r(t,_)},d($){$&&s(t)}}}function Ha(F){let t,l,a,f,_,$,g,v;return g=new X({props:{code:`from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),{c(){t=i("p"),l=n("Load T5 with "),a=i("a"),f=n("AutoModelForSeq2SeqLM"),_=n(":"),$=k(),S(g.$$.fragment),this.h()},l(d){t=p(d,"P",{});var q=h(t);l=o(q,"Load T5 with "),a=p(q,"A",{href:!0});var P=h(a);f=o(P,"AutoModelForSeq2SeqLM"),P.forEach(s),_=o(q,":"),q.forEach(s),$=w(d),y(g.$$.fragment,d),this.h()},h(){j(a,"href","/docs/transformers/pr_18700/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM")},m(d,q){u(d,t,q),r(t,l),r(t,a),r(a,f),r(t,_),u(d,$,q),E(g,d,q),v=!0},p:Ft,i(d){v||(T(g.$$.fragment,d),v=!0)},o(d){x(g.$$.fragment,d),v=!1},d(d){d&&s(t),d&&s($),z(g,d)}}}function Za(F){let t,l;return t=new He({props:{$$slots:{default:[Ha]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(a){y(t.$$.fragment,a)},m(a,f){E(t,a,f),l=!0},p(a,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:a}),t.$set(_)},i(a){l||(T(t.$$.fragment,a),l=!0)},o(a){x(t.$$.fragment,a),l=!1},d(a){z(t,a)}}}function Ja(F){let t,l,a,f,_,$,g,v;return g=new X({props:{code:`from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),{c(){t=i("p"),l=n("Load T5 with "),a=i("a"),f=n("TFAutoModelForSeq2SeqLM"),_=n(":"),$=k(),S(g.$$.fragment),this.h()},l(d){t=p(d,"P",{});var q=h(t);l=o(q,"Load T5 with "),a=p(q,"A",{href:!0});var P=h(a);f=o(P,"TFAutoModelForSeq2SeqLM"),P.forEach(s),_=o(q,":"),q.forEach(s),$=w(d),y(g.$$.fragment,d),this.h()},h(){j(a,"href","/docs/transformers/pr_18700/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM")},m(d,q){u(d,t,q),r(t,l),r(t,a),r(a,f),r(t,_),u(d,$,q),E(g,d,q),v=!0},p:Ft,i(d){v||(T(g.$$.fragment,d),v=!0)},o(d){x(g.$$.fragment,d),v=!1},d(d){d&&s(t),d&&s($),z(g,d)}}}function Ka(F){let t,l;return t=new He({props:{$$slots:{default:[Ja]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(a){y(t.$$.fragment,a)},m(a,f){E(t,a,f),l=!0},p(a,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:a}),t.$set(_)},i(a){l||(T(t.$$.fragment,a),l=!0)},o(a){x(t.$$.fragment,a),l=!1},d(a){z(t,a)}}}function Ra(F){let t,l;return t=new X({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`}}),{c(){S(t.$$.fragment)},l(a){y(t.$$.fragment,a)},m(a,f){E(t,a,f),l=!0},p:Ft,i(a){l||(T(t.$$.fragment,a),l=!0)},o(a){x(t.$$.fragment,a),l=!1},d(a){z(t,a)}}}function Ga(F){let t,l;return t=new He({props:{$$slots:{default:[Ra]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(a){y(t.$$.fragment,a)},m(a,f){E(t,a,f),l=!0},p(a,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:a}),t.$set(_)},i(a){l||(T(t.$$.fragment,a),l=!0)},o(a){x(t.$$.fragment,a),l=!1},d(a){z(t,a)}}}function Xa(F){let t,l;return t=new X({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){S(t.$$.fragment)},l(a){y(t.$$.fragment,a)},m(a,f){E(t,a,f),l=!0},p:Ft,i(a){l||(T(t.$$.fragment,a),l=!0)},o(a){x(t.$$.fragment,a),l=!1},d(a){z(t,a)}}}function Qa(F){let t,l;return t=new He({props:{$$slots:{default:[Xa]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(a){y(t.$$.fragment,a)},m(a,f){E(t,a,f),l=!0},p(a,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:a}),t.$set(_)},i(a){l||(T(t.$$.fragment,a),l=!0)},o(a){x(t.$$.fragment,a),l=!1},d(a){z(t,a)}}}function Va(F){let t,l,a,f,_,$,g,v;return{c(){t=i("p"),l=n("If you aren\u2019t familiar with fine-tuning a model with the "),a=i("a"),f=n("Trainer"),_=n(", take a look at the basic tutorial "),$=i("a"),g=n("here"),v=n("!"),this.h()},l(d){t=p(d,"P",{});var q=h(t);l=o(q,"If you aren\u2019t familiar with fine-tuning a model with the "),a=p(q,"A",{href:!0});var P=h(a);f=o(P,"Trainer"),P.forEach(s),_=o(q,", take a look at the basic tutorial "),$=p(q,"A",{href:!0});var I=h($);g=o(I,"here"),I.forEach(s),v=o(q,"!"),q.forEach(s),this.h()},h(){j(a,"href","/docs/transformers/pr_18700/en/main_classes/trainer#transformers.Trainer"),j($,"href","../training#finetune-with-trainer")},m(d,q){u(d,t,q),r(t,l),r(t,a),r(a,f),r(t,_),r(t,$),r($,g),r(t,v)},d(d){d&&s(t)}}}function es(F){let t,l,a,f,_,$,g,v,d,q,P,I,M,re,O,U,Q,Z,Y,J,N,H,oe,B,C,ae;return t=new At({props:{$$slots:{default:[Va]},$$scope:{ctx:F}}}),C=new X({props:{code:`from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    fp16=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_books["train"],
    eval_dataset=tokenized_books["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainingArguments, Seq2SeqTrainer

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = Seq2SeqTrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Seq2SeqTrainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_books[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_books[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){S(t.$$.fragment),l=k(),a=i("p"),f=n("At this point, only three steps remain:"),_=k(),$=i("ol"),g=i("li"),v=n("Define your training hyperparameters in "),d=i("a"),q=n("Seq2SeqTrainingArguments"),P=n("."),I=k(),M=i("li"),re=n("Pass the training arguments to "),O=i("a"),U=n("Seq2SeqTrainer"),Q=n(" along with the model, dataset, tokenizer, and data collator."),Z=k(),Y=i("li"),J=n("Call "),N=i("a"),H=n("train()"),oe=n(" to fine-tune your model."),B=k(),S(C.$$.fragment),this.h()},l(b){y(t.$$.fragment,b),l=w(b),a=p(b,"P",{});var L=h(a);f=o(L,"At this point, only three steps remain:"),L.forEach(s),_=w(b),$=p(b,"OL",{});var D=h($);g=p(D,"LI",{});var W=h(g);v=o(W,"Define your training hyperparameters in "),d=p(W,"A",{href:!0});var K=h(d);q=o(K,"Seq2SeqTrainingArguments"),K.forEach(s),P=o(W,"."),W.forEach(s),I=w(D),M=p(D,"LI",{});var V=h(M);re=o(V,"Pass the training arguments to "),O=p(V,"A",{href:!0});var ee=h(O);U=o(ee,"Seq2SeqTrainer"),ee.forEach(s),Q=o(V," along with the model, dataset, tokenizer, and data collator."),V.forEach(s),Z=w(D),Y=p(D,"LI",{});var R=h(Y);J=o(R,"Call "),N=p(R,"A",{href:!0});var se=h(N);H=o(se,"train()"),se.forEach(s),oe=o(R," to fine-tune your model."),R.forEach(s),D.forEach(s),B=w(b),y(C.$$.fragment,b),this.h()},h(){j(d,"href","/docs/transformers/pr_18700/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments"),j(O,"href","/docs/transformers/pr_18700/en/main_classes/trainer#transformers.Seq2SeqTrainer"),j(N,"href","/docs/transformers/pr_18700/en/main_classes/trainer#transformers.Trainer.train")},m(b,L){E(t,b,L),u(b,l,L),u(b,a,L),r(a,f),u(b,_,L),u(b,$,L),r($,g),r(g,v),r(g,d),r(d,q),r(g,P),r($,I),r($,M),r(M,re),r(M,O),r(O,U),r(M,Q),r($,Z),r($,Y),r(Y,J),r(Y,N),r(N,H),r(Y,oe),u(b,B,L),E(C,b,L),ae=!0},p(b,L){const D={};L&2&&(D.$$scope={dirty:L,ctx:b}),t.$set(D)},i(b){ae||(T(t.$$.fragment,b),T(C.$$.fragment,b),ae=!0)},o(b){x(t.$$.fragment,b),x(C.$$.fragment,b),ae=!1},d(b){z(t,b),b&&s(l),b&&s(a),b&&s(_),b&&s($),b&&s(B),z(C,b)}}}function ts(F){let t,l;return t=new He({props:{$$slots:{default:[es]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(a){y(t.$$.fragment,a)},m(a,f){E(t,a,f),l=!0},p(a,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:a}),t.$set(_)},i(a){l||(T(t.$$.fragment,a),l=!0)},o(a){x(t.$$.fragment,a),l=!1},d(a){z(t,a)}}}function as(F){let t,l,a,f,_;return{c(){t=i("p"),l=n("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),a=i("a"),f=n("here"),_=n("!"),this.h()},l($){t=p($,"P",{});var g=h(t);l=o(g,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),a=p(g,"A",{href:!0});var v=h(a);f=o(v,"here"),v.forEach(s),_=o(g,"!"),g.forEach(s),this.h()},h(){j(a,"href","training#finetune-with-keras")},m($,g){u($,t,g),r(t,l),r(t,a),r(a,f),r(t,_)},d($){$&&s(t)}}}function ss(F){let t,l,a,f,_,$,g,v,d,q,P,I,M,re,O,U,Q,Z,Y,J,N,H,oe,B,C,ae,b,L,D,W,K,V,ee,R,se,Ee,ce,G,ue;return M=new X({props:{code:`tf_train_set = tokenized_books["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = tokenized_books["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_books[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = tokenized_books[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),O=new At({props:{$$slots:{default:[as]},$$scope:{ctx:F}}}),J=new X({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),D=new X({props:{code:"model.compile(optimizer=optimizer)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)'}}),G=new X({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){t=i("p"),l=n("To fine-tune a model in TensorFlow, start by converting your datasets to the "),a=i("code"),f=n("tf.data.Dataset"),_=n(" format with "),$=i("a"),g=n("to_tf_dataset"),v=n(". Specify inputs and labels in "),d=i("code"),q=n("columns"),P=n(", whether to shuffle the dataset order, batch size, and the data collator:"),I=k(),S(M.$$.fragment),re=k(),S(O.$$.fragment),U=k(),Q=i("p"),Z=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Y=k(),S(J.$$.fragment),N=k(),H=i("p"),oe=n("Configure the model for training with "),B=i("a"),C=i("code"),ae=n("compile"),b=n(":"),L=k(),S(D.$$.fragment),W=k(),K=i("p"),V=n("Call "),ee=i("a"),R=i("code"),se=n("fit"),Ee=n(" to fine-tune the model:"),ce=k(),S(G.$$.fragment),this.h()},l(m){t=p(m,"P",{});var A=h(t);l=o(A,"To fine-tune a model in TensorFlow, start by converting your datasets to the "),a=p(A,"CODE",{});var ie=h(a);f=o(ie,"tf.data.Dataset"),ie.forEach(s),_=o(A," format with "),$=p(A,"A",{href:!0,rel:!0});var pe=h($);g=o(pe,"to_tf_dataset"),pe.forEach(s),v=o(A,". Specify inputs and labels in "),d=p(A,"CODE",{});var Te=h(d);q=o(Te,"columns"),Te.forEach(s),P=o(A,", whether to shuffle the dataset order, batch size, and the data collator:"),A.forEach(s),I=w(m),y(M.$$.fragment,m),re=w(m),y(O.$$.fragment,m),U=w(m),Q=p(m,"P",{});var $e=h(Q);Z=o($e,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),$e.forEach(s),Y=w(m),y(J.$$.fragment,m),N=w(m),H=p(m,"P",{});var ke=h(H);oe=o(ke,"Configure the model for training with "),B=p(ke,"A",{href:!0,rel:!0});var xe=h(B);C=p(xe,"CODE",{});var fe=h(C);ae=o(fe,"compile"),fe.forEach(s),xe.forEach(s),b=o(ke,":"),ke.forEach(s),L=w(m),y(D.$$.fragment,m),W=w(m),K=p(m,"P",{});var de=h(K);V=o(de,"Call "),ee=p(de,"A",{href:!0,rel:!0});var le=h(ee);R=p(le,"CODE",{});var Ie=h(R);se=o(Ie,"fit"),Ie.forEach(s),le.forEach(s),Ee=o(de," to fine-tune the model:"),de.forEach(s),ce=w(m),y(G.$$.fragment,m),this.h()},h(){j($,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),j($,"rel","nofollow"),j(B,"href","https://keras.io/api/models/model_training_apis/#compile-method"),j(B,"rel","nofollow"),j(ee,"href","https://keras.io/api/models/model_training_apis/#fit-method"),j(ee,"rel","nofollow")},m(m,A){u(m,t,A),r(t,l),r(t,a),r(a,f),r(t,_),r(t,$),r($,g),r(t,v),r(t,d),r(d,q),r(t,P),u(m,I,A),E(M,m,A),u(m,re,A),E(O,m,A),u(m,U,A),u(m,Q,A),r(Q,Z),u(m,Y,A),E(J,m,A),u(m,N,A),u(m,H,A),r(H,oe),r(H,B),r(B,C),r(C,ae),r(H,b),u(m,L,A),E(D,m,A),u(m,W,A),u(m,K,A),r(K,V),r(K,ee),r(ee,R),r(R,se),r(K,Ee),u(m,ce,A),E(G,m,A),ue=!0},p(m,A){const ie={};A&2&&(ie.$$scope={dirty:A,ctx:m}),O.$set(ie)},i(m){ue||(T(M.$$.fragment,m),T(O.$$.fragment,m),T(J.$$.fragment,m),T(D.$$.fragment,m),T(G.$$.fragment,m),ue=!0)},o(m){x(M.$$.fragment,m),x(O.$$.fragment,m),x(J.$$.fragment,m),x(D.$$.fragment,m),x(G.$$.fragment,m),ue=!1},d(m){m&&s(t),m&&s(I),z(M,m),m&&s(re),z(O,m),m&&s(U),m&&s(Q),m&&s(Y),z(J,m),m&&s(N),m&&s(H),m&&s(L),z(D,m),m&&s(W),m&&s(K),m&&s(ce),z(G,m)}}}function rs(F){let t,l;return t=new He({props:{$$slots:{default:[ss]},$$scope:{ctx:F}}}),{c(){S(t.$$.fragment)},l(a){y(t.$$.fragment,a)},m(a,f){E(t,a,f),l=!0},p(a,f){const _={};f&2&&(_.$$scope={dirty:f,ctx:a}),t.$set(_)},i(a){l||(T(t.$$.fragment,a),l=!0)},o(a){x(t.$$.fragment,a),l=!1},d(a){z(t,a)}}}function ns(F){let t,l,a,f,_,$,g,v;return{c(){t=i("p"),l=n(`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),a=i("a"),f=n("PyTorch notebook"),_=n(`
or `),$=i("a"),g=n("TensorFlow notebook"),v=n("."),this.h()},l(d){t=p(d,"P",{});var q=h(t);l=o(q,`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),a=p(q,"A",{href:!0,rel:!0});var P=h(a);f=o(P,"PyTorch notebook"),P.forEach(s),_=o(q,`
or `),$=p(q,"A",{href:!0,rel:!0});var I=h($);g=o(I,"TensorFlow notebook"),I.forEach(s),v=o(q,"."),q.forEach(s),this.h()},h(){j(a,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb"),j(a,"rel","nofollow"),j($,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb"),j($,"rel","nofollow")},m(d,q){u(d,t,q),r(t,l),r(t,a),r(a,f),r(t,_),r(t,$),r($,g),r(t,v)},d(d){d&&s(t)}}}function os(F){let t,l,a,f,_,$,g,v,d,q,P,I,M,re,O,U,Q,Z,Y,J,N,H,oe,B,C,ae,b,L,D,W,K,V,ee,R,se,Ee,ce,G,ue,m,A,ie,pe,Te,$e,ke,xe,fe,de,le,Ie,Ze,Pt,Lt,pt,_e,we,Je,ze,Dt,Ke,Mt,ft,Ae,mt,Ue,Ct,ht,Fe,ct,Ne,Ot,ut,me,Re,It,Ut,Ge,Nt,Bt,Pe,Wt,Xe,Yt,Ht,$t,Le,dt,ne,Zt,De,Jt,Kt,Qe,Rt,Gt,Ve,Xt,Qt,_t,Me,gt,qe,kt,te,Vt,Be,ea,ta,et,aa,sa,tt,ra,na,at,oa,la,wt,be,qt,ge,je,st,Ce,ia,rt,pa,bt,ve,jt,Se,vt;return $=new zt({}),P=new Oa({props:{id:"1JvfrvZgi6c"}}),C=new At({props:{$$slots:{default:[Ya]},$$scope:{ctx:F}}}),W=new zt({}),G=new X({props:{code:`from datasets import load_dataset

books = load_dataset("opus_books", "en-fr")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>books = load_dataset(<span class="hljs-string">&quot;opus_books&quot;</span>, <span class="hljs-string">&quot;en-fr&quot;</span>)`}}),pe=new X({props:{code:'books = books["train"].train_test_split(test_size=0.2)',highlighted:'books = books[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),fe=new X({props:{code:'books["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>books[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;90560&#x27;</span>,
 <span class="hljs-string">&#x27;translation&#x27;</span>: {<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-string">&#x27;But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.&#x27;</span>,
  <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-string">&#x27;Mais ce plateau \xE9lev\xE9 ne mesurait que quelques toises, et bient\xF4t nous f\xFBmes rentr\xE9s dans notre \xE9l\xE9ment.&#x27;</span>}}`}}),ze=new zt({}),Ae=new Oa({props:{id:"XAR8jnZZuUs"}}),Fe=new X({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),Le=new X({props:{code:`source_lang = "en"
target_lang = "fr"
prefix = "translate English to French: "


def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples["translation"]]
    targets = [example[target_lang] for example in examples["translation"]]
    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
    return model_inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>source_lang = <span class="hljs-string">&quot;en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_lang = <span class="hljs-string">&quot;fr&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prefix = <span class="hljs-string">&quot;translate English to French: &quot;</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    inputs = [prefix + example[source_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    targets = [example[target_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    model_inputs = tokenizer(inputs, text_target=targets, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> model_inputs`}}),Me=new X({props:{code:"tokenized_books = books.map(preprocess_function, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_books = books.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),qe=new fa({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ka],pytorch:[Za]},$$scope:{ctx:F}}}),be=new fa({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Qa],pytorch:[Ga]},$$scope:{ctx:F}}}),Ce=new zt({}),ve=new fa({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[rs],pytorch:[ts]},$$scope:{ctx:F}}}),Se=new At({props:{$$slots:{default:[ns]},$$scope:{ctx:F}}}),{c(){t=i("meta"),l=k(),a=i("h1"),f=i("a"),_=i("span"),S($.$$.fragment),g=k(),v=i("span"),d=n("Translation"),q=k(),S(P.$$.fragment),I=k(),M=i("p"),re=n("Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),O=k(),U=i("p"),Q=n("This guide will show you how to fine-tune "),Z=i("a"),Y=n("T5"),J=n(" on the English-French subset of the "),N=i("a"),H=n("OPUS Books"),oe=n(" dataset to translate English text to French."),B=k(),S(C.$$.fragment),ae=k(),b=i("h2"),L=i("a"),D=i("span"),S(W.$$.fragment),K=k(),V=i("span"),ee=n("Load OPUS Books dataset"),R=k(),se=i("p"),Ee=n("Load the OPUS Books dataset from the \u{1F917} Datasets library:"),ce=k(),S(G.$$.fragment),ue=k(),m=i("p"),A=n("Split this dataset into a train and test set:"),ie=k(),S(pe.$$.fragment),Te=k(),$e=i("p"),ke=n("Then take a look at an example:"),xe=k(),S(fe.$$.fragment),de=k(),le=i("p"),Ie=n("The "),Ze=i("code"),Pt=n("translation"),Lt=n(" field is a dictionary containing the English and French translations of the text."),pt=k(),_e=i("h2"),we=i("a"),Je=i("span"),S(ze.$$.fragment),Dt=k(),Ke=i("span"),Mt=n("Preprocess"),ft=k(),S(Ae.$$.fragment),mt=k(),Ue=i("p"),Ct=n("Load the T5 tokenizer to process the language pairs:"),ht=k(),S(Fe.$$.fragment),ct=k(),Ne=i("p"),Ot=n("The preprocessing function needs to:"),ut=k(),me=i("ol"),Re=i("li"),It=n("Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),Ut=k(),Ge=i("li"),Nt=n("Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),Bt=k(),Pe=i("li"),Wt=n("Truncate sequences to be no longer than the maximum length set by the "),Xe=i("code"),Yt=n("max_length"),Ht=n(" parameter."),$t=k(),S(Le.$$.fragment),dt=k(),ne=i("p"),Zt=n("Use \u{1F917} Datasets "),De=i("a"),Jt=n("map"),Kt=n(" function to apply the preprocessing function over the entire dataset. You can speed up the "),Qe=i("code"),Rt=n("map"),Gt=n(" function by setting "),Ve=i("code"),Xt=n("batched=True"),Qt=n(" to process multiple elements of the dataset at once:"),_t=k(),S(Me.$$.fragment),gt=k(),S(qe.$$.fragment),kt=k(),te=i("p"),Vt=n("Use "),Be=i("a"),ea=n("DataCollatorForSeq2Seq"),ta=n(" to create a batch of examples. It will also "),et=i("em"),aa=n("dynamically pad"),sa=n(" your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),tt=i("code"),ra=n("tokenizer"),na=n(" function by setting "),at=i("code"),oa=n("padding=True"),la=n(", dynamic padding is more efficient."),wt=k(),S(be.$$.fragment),qt=k(),ge=i("h2"),je=i("a"),st=i("span"),S(Ce.$$.fragment),ia=k(),rt=i("span"),pa=n("Train"),bt=k(),S(ve.$$.fragment),jt=k(),S(Se.$$.fragment),this.h()},l(e){const c=Ba('[data-svelte="svelte-1phssyn"]',document.head);t=p(c,"META",{name:!0,content:!0}),c.forEach(s),l=w(e),a=p(e,"H1",{class:!0});var Oe=h(a);f=p(Oe,"A",{id:!0,class:!0,href:!0});var nt=h(f);_=p(nt,"SPAN",{});var ot=h(_);y($.$$.fragment,ot),ot.forEach(s),nt.forEach(s),g=w(Oe),v=p(Oe,"SPAN",{});var lt=h(v);d=o(lt,"Translation"),lt.forEach(s),Oe.forEach(s),q=w(e),y(P.$$.fragment,e),I=w(e),M=p(e,"P",{});var it=h(M);re=o(it,"Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),it.forEach(s),O=w(e),U=p(e,"P",{});var We=h(U);Q=o(We,"This guide will show you how to fine-tune "),Z=p(We,"A",{href:!0,rel:!0});var ma=h(Z);Y=o(ma,"T5"),ma.forEach(s),J=o(We," on the English-French subset of the "),N=p(We,"A",{href:!0,rel:!0});var ha=h(N);H=o(ha,"OPUS Books"),ha.forEach(s),oe=o(We," dataset to translate English text to French."),We.forEach(s),B=w(e),y(C.$$.fragment,e),ae=w(e),b=p(e,"H2",{class:!0});var St=h(b);L=p(St,"A",{id:!0,class:!0,href:!0});var ca=h(L);D=p(ca,"SPAN",{});var ua=h(D);y(W.$$.fragment,ua),ua.forEach(s),ca.forEach(s),K=w(St),V=p(St,"SPAN",{});var $a=h(V);ee=o($a,"Load OPUS Books dataset"),$a.forEach(s),St.forEach(s),R=w(e),se=p(e,"P",{});var da=h(se);Ee=o(da,"Load the OPUS Books dataset from the \u{1F917} Datasets library:"),da.forEach(s),ce=w(e),y(G.$$.fragment,e),ue=w(e),m=p(e,"P",{});var _a=h(m);A=o(_a,"Split this dataset into a train and test set:"),_a.forEach(s),ie=w(e),y(pe.$$.fragment,e),Te=w(e),$e=p(e,"P",{});var ga=h($e);ke=o(ga,"Then take a look at an example:"),ga.forEach(s),xe=w(e),y(fe.$$.fragment,e),de=w(e),le=p(e,"P",{});var yt=h(le);Ie=o(yt,"The "),Ze=p(yt,"CODE",{});var ka=h(Ze);Pt=o(ka,"translation"),ka.forEach(s),Lt=o(yt," field is a dictionary containing the English and French translations of the text."),yt.forEach(s),pt=w(e),_e=p(e,"H2",{class:!0});var Et=h(_e);we=p(Et,"A",{id:!0,class:!0,href:!0});var wa=h(we);Je=p(wa,"SPAN",{});var qa=h(Je);y(ze.$$.fragment,qa),qa.forEach(s),wa.forEach(s),Dt=w(Et),Ke=p(Et,"SPAN",{});var ba=h(Ke);Mt=o(ba,"Preprocess"),ba.forEach(s),Et.forEach(s),ft=w(e),y(Ae.$$.fragment,e),mt=w(e),Ue=p(e,"P",{});var ja=h(Ue);Ct=o(ja,"Load the T5 tokenizer to process the language pairs:"),ja.forEach(s),ht=w(e),y(Fe.$$.fragment,e),ct=w(e),Ne=p(e,"P",{});var va=h(Ne);Ot=o(va,"The preprocessing function needs to:"),va.forEach(s),ut=w(e),me=p(e,"OL",{});var Ye=h(me);Re=p(Ye,"LI",{});var Sa=h(Re);It=o(Sa,"Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),Sa.forEach(s),Ut=w(Ye),Ge=p(Ye,"LI",{});var ya=h(Ge);Nt=o(ya,"Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),ya.forEach(s),Bt=w(Ye),Pe=p(Ye,"LI",{});var Tt=h(Pe);Wt=o(Tt,"Truncate sequences to be no longer than the maximum length set by the "),Xe=p(Tt,"CODE",{});var Ea=h(Xe);Yt=o(Ea,"max_length"),Ea.forEach(s),Ht=o(Tt," parameter."),Tt.forEach(s),Ye.forEach(s),$t=w(e),y(Le.$$.fragment,e),dt=w(e),ne=p(e,"P",{});var ye=h(ne);Zt=o(ye,"Use \u{1F917} Datasets "),De=p(ye,"A",{href:!0,rel:!0});var Ta=h(De);Jt=o(Ta,"map"),Ta.forEach(s),Kt=o(ye," function to apply the preprocessing function over the entire dataset. You can speed up the "),Qe=p(ye,"CODE",{});var xa=h(Qe);Rt=o(xa,"map"),xa.forEach(s),Gt=o(ye," function by setting "),Ve=p(ye,"CODE",{});var za=h(Ve);Xt=o(za,"batched=True"),za.forEach(s),Qt=o(ye," to process multiple elements of the dataset at once:"),ye.forEach(s),_t=w(e),y(Me.$$.fragment,e),gt=w(e),y(qe.$$.fragment,e),kt=w(e),te=p(e,"P",{});var he=h(te);Vt=o(he,"Use "),Be=p(he,"A",{href:!0});var Aa=h(Be);ea=o(Aa,"DataCollatorForSeq2Seq"),Aa.forEach(s),ta=o(he," to create a batch of examples. It will also "),et=p(he,"EM",{});var Fa=h(et);aa=o(Fa,"dynamically pad"),Fa.forEach(s),sa=o(he," your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),tt=p(he,"CODE",{});var Pa=h(tt);ra=o(Pa,"tokenizer"),Pa.forEach(s),na=o(he," function by setting "),at=p(he,"CODE",{});var La=h(at);oa=o(La,"padding=True"),La.forEach(s),la=o(he,", dynamic padding is more efficient."),he.forEach(s),wt=w(e),y(be.$$.fragment,e),qt=w(e),ge=p(e,"H2",{class:!0});var xt=h(ge);je=p(xt,"A",{id:!0,class:!0,href:!0});var Da=h(je);st=p(Da,"SPAN",{});var Ma=h(st);y(Ce.$$.fragment,Ma),Ma.forEach(s),Da.forEach(s),ia=w(xt),rt=p(xt,"SPAN",{});var Ca=h(rt);pa=o(Ca,"Train"),Ca.forEach(s),xt.forEach(s),bt=w(e),y(ve.$$.fragment,e),jt=w(e),y(Se.$$.fragment,e),this.h()},h(){j(t,"name","hf:doc:metadata"),j(t,"content",JSON.stringify(ls)),j(f,"id","translation"),j(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(f,"href","#translation"),j(a,"class","relative group"),j(Z,"href","https://huggingface.co/t5-small"),j(Z,"rel","nofollow"),j(N,"href","https://huggingface.co/datasets/opus_books"),j(N,"rel","nofollow"),j(L,"id","load-opus-books-dataset"),j(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(L,"href","#load-opus-books-dataset"),j(b,"class","relative group"),j(we,"id","preprocess"),j(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(we,"href","#preprocess"),j(_e,"class","relative group"),j(De,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map"),j(De,"rel","nofollow"),j(Be,"href","/docs/transformers/pr_18700/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq"),j(je,"id","train"),j(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(je,"href","#train"),j(ge,"class","relative group")},m(e,c){r(document.head,t),u(e,l,c),u(e,a,c),r(a,f),r(f,_),E($,_,null),r(a,g),r(a,v),r(v,d),u(e,q,c),E(P,e,c),u(e,I,c),u(e,M,c),r(M,re),u(e,O,c),u(e,U,c),r(U,Q),r(U,Z),r(Z,Y),r(U,J),r(U,N),r(N,H),r(U,oe),u(e,B,c),E(C,e,c),u(e,ae,c),u(e,b,c),r(b,L),r(L,D),E(W,D,null),r(b,K),r(b,V),r(V,ee),u(e,R,c),u(e,se,c),r(se,Ee),u(e,ce,c),E(G,e,c),u(e,ue,c),u(e,m,c),r(m,A),u(e,ie,c),E(pe,e,c),u(e,Te,c),u(e,$e,c),r($e,ke),u(e,xe,c),E(fe,e,c),u(e,de,c),u(e,le,c),r(le,Ie),r(le,Ze),r(Ze,Pt),r(le,Lt),u(e,pt,c),u(e,_e,c),r(_e,we),r(we,Je),E(ze,Je,null),r(_e,Dt),r(_e,Ke),r(Ke,Mt),u(e,ft,c),E(Ae,e,c),u(e,mt,c),u(e,Ue,c),r(Ue,Ct),u(e,ht,c),E(Fe,e,c),u(e,ct,c),u(e,Ne,c),r(Ne,Ot),u(e,ut,c),u(e,me,c),r(me,Re),r(Re,It),r(me,Ut),r(me,Ge),r(Ge,Nt),r(me,Bt),r(me,Pe),r(Pe,Wt),r(Pe,Xe),r(Xe,Yt),r(Pe,Ht),u(e,$t,c),E(Le,e,c),u(e,dt,c),u(e,ne,c),r(ne,Zt),r(ne,De),r(De,Jt),r(ne,Kt),r(ne,Qe),r(Qe,Rt),r(ne,Gt),r(ne,Ve),r(Ve,Xt),r(ne,Qt),u(e,_t,c),E(Me,e,c),u(e,gt,c),E(qe,e,c),u(e,kt,c),u(e,te,c),r(te,Vt),r(te,Be),r(Be,ea),r(te,ta),r(te,et),r(et,aa),r(te,sa),r(te,tt),r(tt,ra),r(te,na),r(te,at),r(at,oa),r(te,la),u(e,wt,c),E(be,e,c),u(e,qt,c),u(e,ge,c),r(ge,je),r(je,st),E(Ce,st,null),r(ge,ia),r(ge,rt),r(rt,pa),u(e,bt,c),E(ve,e,c),u(e,jt,c),E(Se,e,c),vt=!0},p(e,[c]){const Oe={};c&2&&(Oe.$$scope={dirty:c,ctx:e}),C.$set(Oe);const nt={};c&2&&(nt.$$scope={dirty:c,ctx:e}),qe.$set(nt);const ot={};c&2&&(ot.$$scope={dirty:c,ctx:e}),be.$set(ot);const lt={};c&2&&(lt.$$scope={dirty:c,ctx:e}),ve.$set(lt);const it={};c&2&&(it.$$scope={dirty:c,ctx:e}),Se.$set(it)},i(e){vt||(T($.$$.fragment,e),T(P.$$.fragment,e),T(C.$$.fragment,e),T(W.$$.fragment,e),T(G.$$.fragment,e),T(pe.$$.fragment,e),T(fe.$$.fragment,e),T(ze.$$.fragment,e),T(Ae.$$.fragment,e),T(Fe.$$.fragment,e),T(Le.$$.fragment,e),T(Me.$$.fragment,e),T(qe.$$.fragment,e),T(be.$$.fragment,e),T(Ce.$$.fragment,e),T(ve.$$.fragment,e),T(Se.$$.fragment,e),vt=!0)},o(e){x($.$$.fragment,e),x(P.$$.fragment,e),x(C.$$.fragment,e),x(W.$$.fragment,e),x(G.$$.fragment,e),x(pe.$$.fragment,e),x(fe.$$.fragment,e),x(ze.$$.fragment,e),x(Ae.$$.fragment,e),x(Fe.$$.fragment,e),x(Le.$$.fragment,e),x(Me.$$.fragment,e),x(qe.$$.fragment,e),x(be.$$.fragment,e),x(Ce.$$.fragment,e),x(ve.$$.fragment,e),x(Se.$$.fragment,e),vt=!1},d(e){s(t),e&&s(l),e&&s(a),z($),e&&s(q),z(P,e),e&&s(I),e&&s(M),e&&s(O),e&&s(U),e&&s(B),z(C,e),e&&s(ae),e&&s(b),z(W),e&&s(R),e&&s(se),e&&s(ce),z(G,e),e&&s(ue),e&&s(m),e&&s(ie),z(pe,e),e&&s(Te),e&&s($e),e&&s(xe),z(fe,e),e&&s(de),e&&s(le),e&&s(pt),e&&s(_e),z(ze),e&&s(ft),z(Ae,e),e&&s(mt),e&&s(Ue),e&&s(ht),z(Fe,e),e&&s(ct),e&&s(Ne),e&&s(ut),e&&s(me),e&&s($t),z(Le,e),e&&s(dt),e&&s(ne),e&&s(_t),z(Me,e),e&&s(gt),z(qe,e),e&&s(kt),e&&s(te),e&&s(wt),z(be,e),e&&s(qt),e&&s(ge),z(Ce),e&&s(bt),z(ve,e),e&&s(jt),z(Se,e)}}}const ls={local:"translation",sections:[{local:"load-opus-books-dataset",title:"Load OPUS Books dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Translation"};function is(F){return Wa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $s extends Ia{constructor(t){super();Ua(this,t,is,os,Na,{})}}export{$s as default,ls as metadata};
