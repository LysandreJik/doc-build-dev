import{S as fn,i as hn,s as un,e as s,k as l,w as _,t as a,M as gn,c as n,d as r,m as d,a as o,x as k,h as i,b as c,G as e,g as h,y as w,L as _n,q as v,o as b,B as T,v as kn}from"../../chunks/vendor-hf-doc-builder.js";import{D as y}from"../../chunks/Docstring-hf-doc-builder.js";import{C as wn}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Mt}from"../../chunks/IconCopyLink-hf-doc-builder.js";function vn($s){let B,it,q,P,Pe,H,St,Re,Vt,lt,L,R,Ae,X,Ut,Ie,Wt,dt,A,Ht,Q,Xt,Qt,ct,ve,Yt,pt,be,Fe,Zt,mt,Te,Gt,ft,Y,ht,$,Jt,Z,Kt,er,G,tr,rr,ut,x,I,Ne,J,sr,je,nr,gt,m,K,or,Ce,ar,ir,ee,lr,ye,dr,cr,pr,F,te,mr,Oe,fr,hr,E,re,ur,Me,gr,_r,se,$e,kr,Se,wr,vr,Ee,br,Ve,Tr,yr,N,ne,$r,Ue,Er,zr,j,oe,Br,We,qr,Lr,C,ae,xr,ie,Dr,He,Pr,Rr,Ar,O,le,Ir,Xe,Fr,Nr,M,de,jr,Qe,Cr,_t,D,S,Ye,ce,Or,Ze,Mr,kt,f,pe,Sr,me,Vr,Ge,Ur,Wr,Hr,Je,Xr,Qr,Ke,et,Yr,Zr,fe,Gr,ze,Jr,Kr,es,z,he,ts,tt,rs,ss,ue,Be,ns,rt,os,as,qe,is,st,ls,ds,V,ge,cs,nt,ps,ms,U,_e,fs,ot,hs,us,W,ke,gs,we,_s,at,ks,ws,wt;return H=new Mt({}),X=new Mt({}),Y=new wn({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer

bertweet = AutoModel.from_pretrained("vinai/bertweet-base")

# For transformers v4.x+:
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=False)

# For transformers v3.x:
# tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")

# INPUT TWEET IS ALREADY NORMALIZED!
line = "SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:"

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = bertweet(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>bertweet = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For transformers v4.x+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>, use_fast=<span class="hljs-literal">False</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For transformers v3.x:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;)</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># INPUT TWEET IS ALREADY NORMALIZED!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(line)])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = bertweet(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With TensorFlow 2.0+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># from transformers import TFAutoModel</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># bertweet = TFAutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)</span>`}}),J=new Mt({}),K=new y({props:{name:"class transformers.BertweetTokenizer",anchor:"transformers.BertweetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"normalization",val:" = False"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertweetTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.BertweetTokenizer.normalization",description:`<strong>normalization</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply a normalization preprocess.`,name:"normalization"},{anchor:"transformers.BertweetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BertweetTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BertweetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BertweetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BertweetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BertweetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BertweetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L68"}}),te=new y({props:{name:"add_from_file",anchor:"transformers.BertweetTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L436"}}),re=new y({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L186",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ne=new y({props:{name:"convert_tokens_to_string",anchor:"transformers.BertweetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L387"}}),oe=new y({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L240",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ae=new y({props:{name:"get_special_tokens_mask",anchor:"transformers.BertweetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L212",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),le=new y({props:{name:"normalizeToken",anchor:"transformers.BertweetTokenizer.normalizeToken",parameters:[{name:"token",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L360"}}),de=new y({props:{name:"normalizeTweet",anchor:"transformers.BertweetTokenizer.normalizeTweet",parameters:[{name:"tweet",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet.py#L326"}}),ce=new Mt({}),pe=new y({props:{name:"class transformers.BertweetTokenizerFast",anchor:"transformers.BertweetTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertweetTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertweetTokenizerFast.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet_fast.py#L55"}}),he=new y({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertweetTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet_fast.py#L220",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ge=new y({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertweetTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet_fast.py#L274",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),_e=new y({props:{name:"get_added_vocab_hacking",anchor:"transformers.BertweetTokenizerFast.get_added_vocab_hacking",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet_fast.py#L112",returnDescription:`
<p>The added tokens, and their original and new ids</p>
`,returnType:`
<p><code>Dict[str, int], Dict[int, int]</code></p>
`}}),ke=new y({props:{name:"get_special_tokens_mask",anchor:"transformers.BertweetTokenizerFast.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertweetTokenizerFast.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizerFast.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertweetTokenizerFast.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/bertweet/tokenization_bertweet_fast.py#L246",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){B=s("meta"),it=l(),q=s("h1"),P=s("a"),Pe=s("span"),_(H.$$.fragment),St=l(),Re=s("span"),Vt=a("BERTweet"),lt=l(),L=s("h2"),R=s("a"),Ae=s("span"),_(X.$$.fragment),Ut=l(),Ie=s("span"),Wt=a("Overview"),dt=l(),A=s("p"),Ht=a("The BERTweet model was proposed in "),Q=s("a"),Xt=a("BERTweet: A pre-trained language model for English Tweets"),Qt=a(" by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen."),ct=l(),ve=s("p"),Yt=a("The abstract from the paper is the following:"),pt=l(),be=s("p"),Fe=s("em"),Zt=a(`We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having
the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et
al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al.,
2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks:
Part-of-speech tagging, Named-entity recognition and text classification.`),mt=l(),Te=s("p"),Gt=a("Example of use:"),ft=l(),_(Y.$$.fragment),ht=l(),$=s("p"),Jt=a("This model was contributed by "),Z=s("a"),Kt=a("dqnguyen"),er=a(". The original code can be found "),G=s("a"),tr=a("here"),rr=a("."),ut=l(),x=s("h2"),I=s("a"),Ne=s("span"),_(J.$$.fragment),sr=l(),je=s("span"),nr=a("BertweetTokenizer"),gt=l(),m=s("div"),_(K.$$.fragment),or=l(),Ce=s("p"),ar=a("Constructs a BERTweet tokenizer, using Byte-Pair-Encoding."),ir=l(),ee=s("p"),lr=a("This tokenizer inherits from "),ye=s("a"),dr=a("PreTrainedTokenizer"),cr=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),pr=l(),F=s("div"),_(te.$$.fragment),mr=l(),Oe=s("p"),fr=a("Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),hr=l(),E=s("div"),_(re.$$.fragment),ur=l(),Me=s("p"),gr=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),_r=l(),se=s("ul"),$e=s("li"),kr=a("single sequence: "),Se=s("code"),wr=a("<s> X </s>"),vr=l(),Ee=s("li"),br=a("pair of sequences: "),Ve=s("code"),Tr=a("<s> A </s></s> B </s>"),yr=l(),N=s("div"),_(ne.$$.fragment),$r=l(),Ue=s("p"),Er=a("Converts a sequence of tokens (string) in a single string."),zr=l(),j=s("div"),_(oe.$$.fragment),Br=l(),We=s("p"),qr=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),Lr=l(),C=s("div"),_(ae.$$.fragment),xr=l(),ie=s("p"),Dr=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),He=s("code"),Pr=a("prepare_for_model"),Rr=a(" method."),Ar=l(),O=s("div"),_(le.$$.fragment),Ir=l(),Xe=s("p"),Fr=a("Normalize tokens in a Tweet"),Nr=l(),M=s("div"),_(de.$$.fragment),jr=l(),Qe=s("p"),Cr=a("Normalize a raw Tweet"),_t=l(),D=s("h2"),S=s("a"),Ye=s("span"),_(ce.$$.fragment),Or=l(),Ze=s("span"),Mr=a("BertweetTokenizerFast"),kt=l(),f=s("div"),_(pe.$$.fragment),Sr=l(),me=s("p"),Vr=a("Construct a \u201CFast\u201D BPE tokenizer for BERTweet (backed by HuggingFace\u2019s "),Ge=s("em"),Ur=a("tokenizers"),Wr=a(" library)."),Hr=l(),Je=s("p"),Xr=a("Peculiarities:"),Qr=l(),Ke=s("ul"),et=s("li"),Yr=a(`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),Zr=l(),fe=s("p"),Gr=a("This tokenizer inherits from "),ze=s("a"),Jr=a("PreTrainedTokenizer"),Kr=a(` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),es=l(),z=s("div"),_(he.$$.fragment),ts=l(),tt=s("p"),rs=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),ss=l(),ue=s("ul"),Be=s("li"),ns=a("single sequence: "),rt=s("code"),os=a("<s> X </s>"),as=l(),qe=s("li"),is=a("pair of sequences: "),st=s("code"),ls=a("<s> A </s></s> B </s>"),ds=l(),V=s("div"),_(ge.$$.fragment),cs=l(),nt=s("p"),ps=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),ms=l(),U=s("div"),_(_e.$$.fragment),fs=l(),ot=s("p"),hs=a("Returns the added tokens in the vocabulary as a dictionary of token to index."),us=l(),W=s("div"),_(ke.$$.fragment),gs=l(),we=s("p"),_s=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),at=s("code"),ks=a("prepare_for_model"),ws=a(" method."),this.h()},l(t){const p=gn('[data-svelte="svelte-1phssyn"]',document.head);B=n(p,"META",{name:!0,content:!0}),p.forEach(r),it=d(t),q=n(t,"H1",{class:!0});var vt=o(q);P=n(vt,"A",{id:!0,class:!0,href:!0});var Es=o(P);Pe=n(Es,"SPAN",{});var zs=o(Pe);k(H.$$.fragment,zs),zs.forEach(r),Es.forEach(r),St=d(vt),Re=n(vt,"SPAN",{});var Bs=o(Re);Vt=i(Bs,"BERTweet"),Bs.forEach(r),vt.forEach(r),lt=d(t),L=n(t,"H2",{class:!0});var bt=o(L);R=n(bt,"A",{id:!0,class:!0,href:!0});var qs=o(R);Ae=n(qs,"SPAN",{});var Ls=o(Ae);k(X.$$.fragment,Ls),Ls.forEach(r),qs.forEach(r),Ut=d(bt),Ie=n(bt,"SPAN",{});var xs=o(Ie);Wt=i(xs,"Overview"),xs.forEach(r),bt.forEach(r),dt=d(t),A=n(t,"P",{});var Tt=o(A);Ht=i(Tt,"The BERTweet model was proposed in "),Q=n(Tt,"A",{href:!0,rel:!0});var Ds=o(Q);Xt=i(Ds,"BERTweet: A pre-trained language model for English Tweets"),Ds.forEach(r),Qt=i(Tt," by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen."),Tt.forEach(r),ct=d(t),ve=n(t,"P",{});var Ps=o(ve);Yt=i(Ps,"The abstract from the paper is the following:"),Ps.forEach(r),pt=d(t),be=n(t,"P",{});var Rs=o(be);Fe=n(Rs,"EM",{});var As=o(Fe);Zt=i(As,`We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having
the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et
al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al.,
2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks:
Part-of-speech tagging, Named-entity recognition and text classification.`),As.forEach(r),Rs.forEach(r),mt=d(t),Te=n(t,"P",{});var Is=o(Te);Gt=i(Is,"Example of use:"),Is.forEach(r),ft=d(t),k(Y.$$.fragment,t),ht=d(t),$=n(t,"P",{});var Le=o($);Jt=i(Le,"This model was contributed by "),Z=n(Le,"A",{href:!0,rel:!0});var Fs=o(Z);Kt=i(Fs,"dqnguyen"),Fs.forEach(r),er=i(Le,". The original code can be found "),G=n(Le,"A",{href:!0,rel:!0});var Ns=o(G);tr=i(Ns,"here"),Ns.forEach(r),rr=i(Le,"."),Le.forEach(r),ut=d(t),x=n(t,"H2",{class:!0});var yt=o(x);I=n(yt,"A",{id:!0,class:!0,href:!0});var js=o(I);Ne=n(js,"SPAN",{});var Cs=o(Ne);k(J.$$.fragment,Cs),Cs.forEach(r),js.forEach(r),sr=d(yt),je=n(yt,"SPAN",{});var Os=o(je);nr=i(Os,"BertweetTokenizer"),Os.forEach(r),yt.forEach(r),gt=d(t),m=n(t,"DIV",{class:!0});var u=o(m);k(K.$$.fragment,u),or=d(u),Ce=n(u,"P",{});var Ms=o(Ce);ar=i(Ms,"Constructs a BERTweet tokenizer, using Byte-Pair-Encoding."),Ms.forEach(r),ir=d(u),ee=n(u,"P",{});var $t=o(ee);lr=i($t,"This tokenizer inherits from "),ye=n($t,"A",{href:!0});var Ss=o(ye);dr=i(Ss,"PreTrainedTokenizer"),Ss.forEach(r),cr=i($t,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),$t.forEach(r),pr=d(u),F=n(u,"DIV",{class:!0});var Et=o(F);k(te.$$.fragment,Et),mr=d(Et),Oe=n(Et,"P",{});var Vs=o(Oe);fr=i(Vs,"Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),Vs.forEach(r),Et.forEach(r),hr=d(u),E=n(u,"DIV",{class:!0});var xe=o(E);k(re.$$.fragment,xe),ur=d(xe),Me=n(xe,"P",{});var Us=o(Me);gr=i(Us,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),Us.forEach(r),_r=d(xe),se=n(xe,"UL",{});var zt=o(se);$e=n(zt,"LI",{});var vs=o($e);kr=i(vs,"single sequence: "),Se=n(vs,"CODE",{});var Ws=o(Se);wr=i(Ws,"<s> X </s>"),Ws.forEach(r),vs.forEach(r),vr=d(zt),Ee=n(zt,"LI",{});var bs=o(Ee);br=i(bs,"pair of sequences: "),Ve=n(bs,"CODE",{});var Hs=o(Ve);Tr=i(Hs,"<s> A </s></s> B </s>"),Hs.forEach(r),bs.forEach(r),zt.forEach(r),xe.forEach(r),yr=d(u),N=n(u,"DIV",{class:!0});var Bt=o(N);k(ne.$$.fragment,Bt),$r=d(Bt),Ue=n(Bt,"P",{});var Xs=o(Ue);Er=i(Xs,"Converts a sequence of tokens (string) in a single string."),Xs.forEach(r),Bt.forEach(r),zr=d(u),j=n(u,"DIV",{class:!0});var qt=o(j);k(oe.$$.fragment,qt),Br=d(qt),We=n(qt,"P",{});var Qs=o(We);qr=i(Qs,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),Qs.forEach(r),qt.forEach(r),Lr=d(u),C=n(u,"DIV",{class:!0});var Lt=o(C);k(ae.$$.fragment,Lt),xr=d(Lt),ie=n(Lt,"P",{});var xt=o(ie);Dr=i(xt,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),He=n(xt,"CODE",{});var Ys=o(He);Pr=i(Ys,"prepare_for_model"),Ys.forEach(r),Rr=i(xt," method."),xt.forEach(r),Lt.forEach(r),Ar=d(u),O=n(u,"DIV",{class:!0});var Dt=o(O);k(le.$$.fragment,Dt),Ir=d(Dt),Xe=n(Dt,"P",{});var Zs=o(Xe);Fr=i(Zs,"Normalize tokens in a Tweet"),Zs.forEach(r),Dt.forEach(r),Nr=d(u),M=n(u,"DIV",{class:!0});var Pt=o(M);k(de.$$.fragment,Pt),jr=d(Pt),Qe=n(Pt,"P",{});var Gs=o(Qe);Cr=i(Gs,"Normalize a raw Tweet"),Gs.forEach(r),Pt.forEach(r),u.forEach(r),_t=d(t),D=n(t,"H2",{class:!0});var Rt=o(D);S=n(Rt,"A",{id:!0,class:!0,href:!0});var Js=o(S);Ye=n(Js,"SPAN",{});var Ks=o(Ye);k(ce.$$.fragment,Ks),Ks.forEach(r),Js.forEach(r),Or=d(Rt),Ze=n(Rt,"SPAN",{});var en=o(Ze);Mr=i(en,"BertweetTokenizerFast"),en.forEach(r),Rt.forEach(r),kt=d(t),f=n(t,"DIV",{class:!0});var g=o(f);k(pe.$$.fragment,g),Sr=d(g),me=n(g,"P",{});var At=o(me);Vr=i(At,"Construct a \u201CFast\u201D BPE tokenizer for BERTweet (backed by HuggingFace\u2019s "),Ge=n(At,"EM",{});var tn=o(Ge);Ur=i(tn,"tokenizers"),tn.forEach(r),Wr=i(At," library)."),At.forEach(r),Hr=d(g),Je=n(g,"P",{});var rn=o(Je);Xr=i(rn,"Peculiarities:"),rn.forEach(r),Qr=d(g),Ke=n(g,"UL",{});var sn=o(Ke);et=n(sn,"LI",{});var nn=o(et);Yr=i(nn,`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),nn.forEach(r),sn.forEach(r),Zr=d(g),fe=n(g,"P",{});var It=o(fe);Gr=i(It,"This tokenizer inherits from "),ze=n(It,"A",{href:!0});var on=o(ze);Jr=i(on,"PreTrainedTokenizer"),on.forEach(r),Kr=i(It,` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),It.forEach(r),es=d(g),z=n(g,"DIV",{class:!0});var De=o(z);k(he.$$.fragment,De),ts=d(De),tt=n(De,"P",{});var an=o(tt);rs=i(an,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),an.forEach(r),ss=d(De),ue=n(De,"UL",{});var Ft=o(ue);Be=n(Ft,"LI",{});var Ts=o(Be);ns=i(Ts,"single sequence: "),rt=n(Ts,"CODE",{});var ln=o(rt);os=i(ln,"<s> X </s>"),ln.forEach(r),Ts.forEach(r),as=d(Ft),qe=n(Ft,"LI",{});var ys=o(qe);is=i(ys,"pair of sequences: "),st=n(ys,"CODE",{});var dn=o(st);ls=i(dn,"<s> A </s></s> B </s>"),dn.forEach(r),ys.forEach(r),Ft.forEach(r),De.forEach(r),ds=d(g),V=n(g,"DIV",{class:!0});var Nt=o(V);k(ge.$$.fragment,Nt),cs=d(Nt),nt=n(Nt,"P",{});var cn=o(nt);ps=i(cn,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),cn.forEach(r),Nt.forEach(r),ms=d(g),U=n(g,"DIV",{class:!0});var jt=o(U);k(_e.$$.fragment,jt),fs=d(jt),ot=n(jt,"P",{});var pn=o(ot);hs=i(pn,"Returns the added tokens in the vocabulary as a dictionary of token to index."),pn.forEach(r),jt.forEach(r),us=d(g),W=n(g,"DIV",{class:!0});var Ct=o(W);k(ke.$$.fragment,Ct),gs=d(Ct),we=n(Ct,"P",{});var Ot=o(we);_s=i(Ot,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),at=n(Ot,"CODE",{});var mn=o(at);ks=i(mn,"prepare_for_model"),mn.forEach(r),ws=i(Ot," method."),Ot.forEach(r),Ct.forEach(r),g.forEach(r),this.h()},h(){c(B,"name","hf:doc:metadata"),c(B,"content",JSON.stringify(bn)),c(P,"id","bertweet"),c(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(P,"href","#bertweet"),c(q,"class","relative group"),c(R,"id","overview"),c(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(R,"href","#overview"),c(L,"class","relative group"),c(Q,"href","https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf"),c(Q,"rel","nofollow"),c(Z,"href","https://huggingface.co/dqnguyen"),c(Z,"rel","nofollow"),c(G,"href","https://github.com/VinAIResearch/BERTweet"),c(G,"rel","nofollow"),c(I,"id","transformers.BertweetTokenizer"),c(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(I,"href","#transformers.BertweetTokenizer"),c(x,"class","relative group"),c(ye,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(S,"id","transformers.BertweetTokenizerFast"),c(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S,"href","#transformers.BertweetTokenizerFast"),c(D,"class","relative group"),c(ze,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,B),h(t,it,p),h(t,q,p),e(q,P),e(P,Pe),w(H,Pe,null),e(q,St),e(q,Re),e(Re,Vt),h(t,lt,p),h(t,L,p),e(L,R),e(R,Ae),w(X,Ae,null),e(L,Ut),e(L,Ie),e(Ie,Wt),h(t,dt,p),h(t,A,p),e(A,Ht),e(A,Q),e(Q,Xt),e(A,Qt),h(t,ct,p),h(t,ve,p),e(ve,Yt),h(t,pt,p),h(t,be,p),e(be,Fe),e(Fe,Zt),h(t,mt,p),h(t,Te,p),e(Te,Gt),h(t,ft,p),w(Y,t,p),h(t,ht,p),h(t,$,p),e($,Jt),e($,Z),e(Z,Kt),e($,er),e($,G),e(G,tr),e($,rr),h(t,ut,p),h(t,x,p),e(x,I),e(I,Ne),w(J,Ne,null),e(x,sr),e(x,je),e(je,nr),h(t,gt,p),h(t,m,p),w(K,m,null),e(m,or),e(m,Ce),e(Ce,ar),e(m,ir),e(m,ee),e(ee,lr),e(ee,ye),e(ye,dr),e(ee,cr),e(m,pr),e(m,F),w(te,F,null),e(F,mr),e(F,Oe),e(Oe,fr),e(m,hr),e(m,E),w(re,E,null),e(E,ur),e(E,Me),e(Me,gr),e(E,_r),e(E,se),e(se,$e),e($e,kr),e($e,Se),e(Se,wr),e(se,vr),e(se,Ee),e(Ee,br),e(Ee,Ve),e(Ve,Tr),e(m,yr),e(m,N),w(ne,N,null),e(N,$r),e(N,Ue),e(Ue,Er),e(m,zr),e(m,j),w(oe,j,null),e(j,Br),e(j,We),e(We,qr),e(m,Lr),e(m,C),w(ae,C,null),e(C,xr),e(C,ie),e(ie,Dr),e(ie,He),e(He,Pr),e(ie,Rr),e(m,Ar),e(m,O),w(le,O,null),e(O,Ir),e(O,Xe),e(Xe,Fr),e(m,Nr),e(m,M),w(de,M,null),e(M,jr),e(M,Qe),e(Qe,Cr),h(t,_t,p),h(t,D,p),e(D,S),e(S,Ye),w(ce,Ye,null),e(D,Or),e(D,Ze),e(Ze,Mr),h(t,kt,p),h(t,f,p),w(pe,f,null),e(f,Sr),e(f,me),e(me,Vr),e(me,Ge),e(Ge,Ur),e(me,Wr),e(f,Hr),e(f,Je),e(Je,Xr),e(f,Qr),e(f,Ke),e(Ke,et),e(et,Yr),e(f,Zr),e(f,fe),e(fe,Gr),e(fe,ze),e(ze,Jr),e(fe,Kr),e(f,es),e(f,z),w(he,z,null),e(z,ts),e(z,tt),e(tt,rs),e(z,ss),e(z,ue),e(ue,Be),e(Be,ns),e(Be,rt),e(rt,os),e(ue,as),e(ue,qe),e(qe,is),e(qe,st),e(st,ls),e(f,ds),e(f,V),w(ge,V,null),e(V,cs),e(V,nt),e(nt,ps),e(f,ms),e(f,U),w(_e,U,null),e(U,fs),e(U,ot),e(ot,hs),e(f,us),e(f,W),w(ke,W,null),e(W,gs),e(W,we),e(we,_s),e(we,at),e(at,ks),e(we,ws),wt=!0},p:_n,i(t){wt||(v(H.$$.fragment,t),v(X.$$.fragment,t),v(Y.$$.fragment,t),v(J.$$.fragment,t),v(K.$$.fragment,t),v(te.$$.fragment,t),v(re.$$.fragment,t),v(ne.$$.fragment,t),v(oe.$$.fragment,t),v(ae.$$.fragment,t),v(le.$$.fragment,t),v(de.$$.fragment,t),v(ce.$$.fragment,t),v(pe.$$.fragment,t),v(he.$$.fragment,t),v(ge.$$.fragment,t),v(_e.$$.fragment,t),v(ke.$$.fragment,t),wt=!0)},o(t){b(H.$$.fragment,t),b(X.$$.fragment,t),b(Y.$$.fragment,t),b(J.$$.fragment,t),b(K.$$.fragment,t),b(te.$$.fragment,t),b(re.$$.fragment,t),b(ne.$$.fragment,t),b(oe.$$.fragment,t),b(ae.$$.fragment,t),b(le.$$.fragment,t),b(de.$$.fragment,t),b(ce.$$.fragment,t),b(pe.$$.fragment,t),b(he.$$.fragment,t),b(ge.$$.fragment,t),b(_e.$$.fragment,t),b(ke.$$.fragment,t),wt=!1},d(t){r(B),t&&r(it),t&&r(q),T(H),t&&r(lt),t&&r(L),T(X),t&&r(dt),t&&r(A),t&&r(ct),t&&r(ve),t&&r(pt),t&&r(be),t&&r(mt),t&&r(Te),t&&r(ft),T(Y,t),t&&r(ht),t&&r($),t&&r(ut),t&&r(x),T(J),t&&r(gt),t&&r(m),T(K),T(te),T(re),T(ne),T(oe),T(ae),T(le),T(de),t&&r(_t),t&&r(D),T(ce),t&&r(kt),t&&r(f),T(pe),T(he),T(ge),T(_e),T(ke)}}}const bn={local:"bertweet",sections:[{local:"overview",title:"Overview"},{local:"transformers.BertweetTokenizer",title:"BertweetTokenizer"},{local:"transformers.BertweetTokenizerFast",title:"BertweetTokenizerFast"}],title:"BERTweet"};function Tn($s){return kn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bn extends fn{constructor(B){super();hn(this,B,Tn,vn,un,{})}}export{Bn as default,bn as metadata};
