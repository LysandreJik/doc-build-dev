import{S as Js,i as Ks,s as Zs,e as s,k as l,w as g,t as a,M as en,c as n,d as o,m as d,a as r,x as k,h as i,b as c,G as e,g as m,y as b,L as tn,q as v,o as T,B as y,v as on}from"../../chunks/vendor-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as sn}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as At}from"../../chunks/IconCopyLink-hf-doc-builder.js";function nn(ps){let z,tt,q,B,qe,U,Rt,Le,It,ot,L,A,xe,W,Ft,De,Nt,st,R,Ct,X,jt,Ot,nt,_e,Mt,rt,ge,Be,Vt,at,ke,St,it,H,lt,w,Ut,G,Wt,Xt,Q,Ht,Gt,dt,x,I,Ae,Y,Qt,Re,Yt,ct,f,J,Jt,Ie,Kt,Zt,K,eo,be,to,oo,so,F,Z,no,Fe,ro,ao,$,ee,io,Ne,lo,co,te,ve,po,Ce,ho,mo,Te,fo,je,uo,_o,N,oe,go,Oe,ko,bo,C,se,vo,Me,To,yo,j,ne,Po,re,wo,Ve,$o,Eo,pt,D,O,Se,ae,zo,Ue,qo,ht,h,ie,Lo,le,xo,We,Do,Bo,Ao,Xe,Ro,Io,He,Ge,Fo,No,de,Co,ye,jo,Oo,Mo,E,ce,Vo,Qe,So,Uo,pe,Pe,Wo,Ye,Xo,Ho,we,Go,Je,Qo,Yo,M,he,Jo,Ke,Ko,Zo,V,me,es,Ze,ts,os,S,fe,ss,ue,ns,et,rs,as,mt;return U=new At({}),W=new At({}),H=new sn({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer

phobert = AutoModel.from_pretrained("vinai/phobert-base")
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!
line = "T\xF4i l\xE0 sinh_vi\xEAn tr\u01B0\u1EDDng \u0111\u1EA1i_h\u1ECDc C\xF4ng_ngh\u1EC7 ."

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = phobert(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# phobert = TFAutoModel.from_pretrained("vinai/phobert-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>phobert = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;T\xF4i l\xE0 sinh_vi\xEAn tr\u01B0\u1EDDng \u0111\u1EA1i_h\u1ECDc C\xF4ng_ngh\u1EC7 .&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(line)])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = phobert(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With TensorFlow 2.0+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># from transformers import TFAutoModel</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># phobert = TFAutoModel.from_pretrained(&quot;vinai/phobert-base&quot;)</span>`}}),Y=new At({}),J=new P({props:{name:"class transformers.PhobertTokenizer",anchor:"transformers.PhobertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.PhobertTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.PhobertTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>st</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.PhobertTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.PhobertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.PhobertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.PhobertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.PhobertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.PhobertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L68"}}),Z=new P({props:{name:"add_from_file",anchor:"transformers.PhobertTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L362"}}),ee=new P({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L166",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),oe=new P({props:{name:"convert_tokens_to_string",anchor:"transformers.PhobertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L313"}}),se=new P({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L220",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ne=new P({props:{name:"get_special_tokens_mask",anchor:"transformers.PhobertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert.py#L192",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ae=new At({}),ie=new P({props:{name:"class transformers.PhobertTokenizerFast",anchor:"transformers.PhobertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.PhobertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.PhobertTokenizerFast.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert_fast.py#L59"}}),ce=new P({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.PhobertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert_fast.py#L224",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),he=new P({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PhobertTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert_fast.py#L278",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),me=new P({props:{name:"get_added_vocab_hacking",anchor:"transformers.PhobertTokenizerFast.get_added_vocab_hacking",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert_fast.py#L116",returnDescription:`
<p>The added tokens, and their original and new ids</p>
`,returnType:`
<p><code>Dict[str, int], Dict[int, int]</code></p>
`}}),fe=new P({props:{name:"get_special_tokens_mask",anchor:"transformers.PhobertTokenizerFast.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.PhobertTokenizerFast.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizerFast.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.PhobertTokenizerFast.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/vr_17254/src/transformers/models/phobert/tokenization_phobert_fast.py#L250",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){z=s("meta"),tt=l(),q=s("h1"),B=s("a"),qe=s("span"),g(U.$$.fragment),Rt=l(),Le=s("span"),It=a("PhoBERT"),ot=l(),L=s("h2"),A=s("a"),xe=s("span"),g(W.$$.fragment),Ft=l(),De=s("span"),Nt=a("Overview"),st=l(),R=s("p"),Ct=a("The PhoBERT model was proposed in "),X=s("a"),jt=a("PhoBERT: Pre-trained language models for Vietnamese"),Ot=a(" by Dat Quoc Nguyen, Anh Tuan Nguyen."),nt=l(),_e=s("p"),Mt=a("The abstract from the paper is the following:"),rt=l(),ge=s("p"),Be=s("em"),Vt=a(`We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.`),at=l(),ke=s("p"),St=a("Example of use:"),it=l(),g(H.$$.fragment),lt=l(),w=s("p"),Ut=a("This model was contributed by "),G=s("a"),Wt=a("dqnguyen"),Xt=a(". The original code can be found "),Q=s("a"),Ht=a("here"),Gt=a("."),dt=l(),x=s("h2"),I=s("a"),Ae=s("span"),g(Y.$$.fragment),Qt=l(),Re=s("span"),Yt=a("PhobertTokenizer"),ct=l(),f=s("div"),g(J.$$.fragment),Jt=l(),Ie=s("p"),Kt=a("Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding."),Zt=l(),K=s("p"),eo=a("This tokenizer inherits from "),be=s("a"),to=a("PreTrainedTokenizer"),oo=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),so=l(),F=s("div"),g(Z.$$.fragment),no=l(),Fe=s("p"),ro=a("Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),ao=l(),$=s("div"),g(ee.$$.fragment),io=l(),Ne=s("p"),lo=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),co=l(),te=s("ul"),ve=s("li"),po=a("single sequence: "),Ce=s("code"),ho=a("<s> X </s>"),mo=l(),Te=s("li"),fo=a("pair of sequences: "),je=s("code"),uo=a("<s> A </s></s> B </s>"),_o=l(),N=s("div"),g(oe.$$.fragment),go=l(),Oe=s("p"),ko=a("Converts a sequence of tokens (string) in a single string."),bo=l(),C=s("div"),g(se.$$.fragment),vo=l(),Me=s("p"),To=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),yo=l(),j=s("div"),g(ne.$$.fragment),Po=l(),re=s("p"),wo=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ve=s("code"),$o=a("prepare_for_model"),Eo=a(" method."),pt=l(),D=s("h2"),O=s("a"),Se=s("span"),g(ae.$$.fragment),zo=l(),Ue=s("span"),qo=a("PhobertTokenizerFast"),ht=l(),h=s("div"),g(ie.$$.fragment),Lo=l(),le=s("p"),xo=a("Construct a \u201CFast\u201D BPE tokenizer for PhoBERT (backed by HuggingFace\u2019s "),We=s("em"),Do=a("tokenizers"),Bo=a(" library)."),Ao=l(),Xe=s("p"),Ro=a("Peculiarities:"),Io=l(),He=s("ul"),Ge=s("li"),Fo=a(`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),No=l(),de=s("p"),Co=a("This tokenizer inherits from "),ye=s("a"),jo=a("PreTrainedTokenizer"),Oo=a(` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),Mo=l(),E=s("div"),g(ce.$$.fragment),Vo=l(),Qe=s("p"),So=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),Uo=l(),pe=s("ul"),Pe=s("li"),Wo=a("single sequence: "),Ye=s("code"),Xo=a("<s> X </s>"),Ho=l(),we=s("li"),Go=a("pair of sequences: "),Je=s("code"),Qo=a("<s> A </s></s> B </s>"),Yo=l(),M=s("div"),g(he.$$.fragment),Jo=l(),Ke=s("p"),Ko=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),Zo=l(),V=s("div"),g(me.$$.fragment),es=l(),Ze=s("p"),ts=a("Returns the added tokens in the vocabulary as a dictionary of token to index."),os=l(),S=s("div"),g(fe.$$.fragment),ss=l(),ue=s("p"),ns=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),et=s("code"),rs=a("prepare_for_model"),as=a(" method."),this.h()},l(t){const p=en('[data-svelte="svelte-1phssyn"]',document.head);z=n(p,"META",{name:!0,content:!0}),p.forEach(o),tt=d(t),q=n(t,"H1",{class:!0});var ft=r(q);B=n(ft,"A",{id:!0,class:!0,href:!0});var hs=r(B);qe=n(hs,"SPAN",{});var ms=r(qe);k(U.$$.fragment,ms),ms.forEach(o),hs.forEach(o),Rt=d(ft),Le=n(ft,"SPAN",{});var fs=r(Le);It=i(fs,"PhoBERT"),fs.forEach(o),ft.forEach(o),ot=d(t),L=n(t,"H2",{class:!0});var ut=r(L);A=n(ut,"A",{id:!0,class:!0,href:!0});var us=r(A);xe=n(us,"SPAN",{});var _s=r(xe);k(W.$$.fragment,_s),_s.forEach(o),us.forEach(o),Ft=d(ut),De=n(ut,"SPAN",{});var gs=r(De);Nt=i(gs,"Overview"),gs.forEach(o),ut.forEach(o),st=d(t),R=n(t,"P",{});var _t=r(R);Ct=i(_t,"The PhoBERT model was proposed in "),X=n(_t,"A",{href:!0,rel:!0});var ks=r(X);jt=i(ks,"PhoBERT: Pre-trained language models for Vietnamese"),ks.forEach(o),Ot=i(_t," by Dat Quoc Nguyen, Anh Tuan Nguyen."),_t.forEach(o),nt=d(t),_e=n(t,"P",{});var bs=r(_e);Mt=i(bs,"The abstract from the paper is the following:"),bs.forEach(o),rt=d(t),ge=n(t,"P",{});var vs=r(ge);Be=n(vs,"EM",{});var Ts=r(Be);Vt=i(Ts,`We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.`),Ts.forEach(o),vs.forEach(o),at=d(t),ke=n(t,"P",{});var ys=r(ke);St=i(ys,"Example of use:"),ys.forEach(o),it=d(t),k(H.$$.fragment,t),lt=d(t),w=n(t,"P",{});var $e=r(w);Ut=i($e,"This model was contributed by "),G=n($e,"A",{href:!0,rel:!0});var Ps=r(G);Wt=i(Ps,"dqnguyen"),Ps.forEach(o),Xt=i($e,". The original code can be found "),Q=n($e,"A",{href:!0,rel:!0});var ws=r(Q);Ht=i(ws,"here"),ws.forEach(o),Gt=i($e,"."),$e.forEach(o),dt=d(t),x=n(t,"H2",{class:!0});var gt=r(x);I=n(gt,"A",{id:!0,class:!0,href:!0});var $s=r(I);Ae=n($s,"SPAN",{});var Es=r(Ae);k(Y.$$.fragment,Es),Es.forEach(o),$s.forEach(o),Qt=d(gt),Re=n(gt,"SPAN",{});var zs=r(Re);Yt=i(zs,"PhobertTokenizer"),zs.forEach(o),gt.forEach(o),ct=d(t),f=n(t,"DIV",{class:!0});var _=r(f);k(J.$$.fragment,_),Jt=d(_),Ie=n(_,"P",{});var qs=r(Ie);Kt=i(qs,"Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding."),qs.forEach(o),Zt=d(_),K=n(_,"P",{});var kt=r(K);eo=i(kt,"This tokenizer inherits from "),be=n(kt,"A",{href:!0});var Ls=r(be);to=i(Ls,"PreTrainedTokenizer"),Ls.forEach(o),oo=i(kt,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),kt.forEach(o),so=d(_),F=n(_,"DIV",{class:!0});var bt=r(F);k(Z.$$.fragment,bt),no=d(bt),Fe=n(bt,"P",{});var xs=r(Fe);ro=i(xs,"Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),xs.forEach(o),bt.forEach(o),ao=d(_),$=n(_,"DIV",{class:!0});var Ee=r($);k(ee.$$.fragment,Ee),io=d(Ee),Ne=n(Ee,"P",{});var Ds=r(Ne);lo=i(Ds,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),Ds.forEach(o),co=d(Ee),te=n(Ee,"UL",{});var vt=r(te);ve=n(vt,"LI",{});var is=r(ve);po=i(is,"single sequence: "),Ce=n(is,"CODE",{});var Bs=r(Ce);ho=i(Bs,"<s> X </s>"),Bs.forEach(o),is.forEach(o),mo=d(vt),Te=n(vt,"LI",{});var ls=r(Te);fo=i(ls,"pair of sequences: "),je=n(ls,"CODE",{});var As=r(je);uo=i(As,"<s> A </s></s> B </s>"),As.forEach(o),ls.forEach(o),vt.forEach(o),Ee.forEach(o),_o=d(_),N=n(_,"DIV",{class:!0});var Tt=r(N);k(oe.$$.fragment,Tt),go=d(Tt),Oe=n(Tt,"P",{});var Rs=r(Oe);ko=i(Rs,"Converts a sequence of tokens (string) in a single string."),Rs.forEach(o),Tt.forEach(o),bo=d(_),C=n(_,"DIV",{class:!0});var yt=r(C);k(se.$$.fragment,yt),vo=d(yt),Me=n(yt,"P",{});var Is=r(Me);To=i(Is,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),Is.forEach(o),yt.forEach(o),yo=d(_),j=n(_,"DIV",{class:!0});var Pt=r(j);k(ne.$$.fragment,Pt),Po=d(Pt),re=n(Pt,"P",{});var wt=r(re);wo=i(wt,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ve=n(wt,"CODE",{});var Fs=r(Ve);$o=i(Fs,"prepare_for_model"),Fs.forEach(o),Eo=i(wt," method."),wt.forEach(o),Pt.forEach(o),_.forEach(o),pt=d(t),D=n(t,"H2",{class:!0});var $t=r(D);O=n($t,"A",{id:!0,class:!0,href:!0});var Ns=r(O);Se=n(Ns,"SPAN",{});var Cs=r(Se);k(ae.$$.fragment,Cs),Cs.forEach(o),Ns.forEach(o),zo=d($t),Ue=n($t,"SPAN",{});var js=r(Ue);qo=i(js,"PhobertTokenizerFast"),js.forEach(o),$t.forEach(o),ht=d(t),h=n(t,"DIV",{class:!0});var u=r(h);k(ie.$$.fragment,u),Lo=d(u),le=n(u,"P",{});var Et=r(le);xo=i(Et,"Construct a \u201CFast\u201D BPE tokenizer for PhoBERT (backed by HuggingFace\u2019s "),We=n(Et,"EM",{});var Os=r(We);Do=i(Os,"tokenizers"),Os.forEach(o),Bo=i(Et," library)."),Et.forEach(o),Ao=d(u),Xe=n(u,"P",{});var Ms=r(Xe);Ro=i(Ms,"Peculiarities:"),Ms.forEach(o),Io=d(u),He=n(u,"UL",{});var Vs=r(He);Ge=n(Vs,"LI",{});var Ss=r(Ge);Fo=i(Ss,`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),Ss.forEach(o),Vs.forEach(o),No=d(u),de=n(u,"P",{});var zt=r(de);Co=i(zt,"This tokenizer inherits from "),ye=n(zt,"A",{href:!0});var Us=r(ye);jo=i(Us,"PreTrainedTokenizer"),Us.forEach(o),Oo=i(zt,` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),zt.forEach(o),Mo=d(u),E=n(u,"DIV",{class:!0});var ze=r(E);k(ce.$$.fragment,ze),Vo=d(ze),Qe=n(ze,"P",{});var Ws=r(Qe);So=i(Ws,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),Ws.forEach(o),Uo=d(ze),pe=n(ze,"UL",{});var qt=r(pe);Pe=n(qt,"LI",{});var ds=r(Pe);Wo=i(ds,"single sequence: "),Ye=n(ds,"CODE",{});var Xs=r(Ye);Xo=i(Xs,"<s> X </s>"),Xs.forEach(o),ds.forEach(o),Ho=d(qt),we=n(qt,"LI",{});var cs=r(we);Go=i(cs,"pair of sequences: "),Je=n(cs,"CODE",{});var Hs=r(Je);Qo=i(Hs,"<s> A </s></s> B </s>"),Hs.forEach(o),cs.forEach(o),qt.forEach(o),ze.forEach(o),Yo=d(u),M=n(u,"DIV",{class:!0});var Lt=r(M);k(he.$$.fragment,Lt),Jo=d(Lt),Ke=n(Lt,"P",{});var Gs=r(Ke);Ko=i(Gs,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),Gs.forEach(o),Lt.forEach(o),Zo=d(u),V=n(u,"DIV",{class:!0});var xt=r(V);k(me.$$.fragment,xt),es=d(xt),Ze=n(xt,"P",{});var Qs=r(Ze);ts=i(Qs,"Returns the added tokens in the vocabulary as a dictionary of token to index."),Qs.forEach(o),xt.forEach(o),os=d(u),S=n(u,"DIV",{class:!0});var Dt=r(S);k(fe.$$.fragment,Dt),ss=d(Dt),ue=n(Dt,"P",{});var Bt=r(ue);ns=i(Bt,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),et=n(Bt,"CODE",{});var Ys=r(et);rs=i(Ys,"prepare_for_model"),Ys.forEach(o),as=i(Bt," method."),Bt.forEach(o),Dt.forEach(o),u.forEach(o),this.h()},h(){c(z,"name","hf:doc:metadata"),c(z,"content",JSON.stringify(rn)),c(B,"id","phobert"),c(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B,"href","#phobert"),c(q,"class","relative group"),c(A,"id","overview"),c(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A,"href","#overview"),c(L,"class","relative group"),c(X,"href","https://www.aclweb.org/anthology/2020.findings-emnlp.92.pdf"),c(X,"rel","nofollow"),c(G,"href","https://huggingface.co/dqnguyen"),c(G,"rel","nofollow"),c(Q,"href","https://github.com/VinAIResearch/PhoBERT"),c(Q,"rel","nofollow"),c(I,"id","transformers.PhobertTokenizer"),c(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(I,"href","#transformers.PhobertTokenizer"),c(x,"class","relative group"),c(be,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(O,"id","transformers.PhobertTokenizerFast"),c(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(O,"href","#transformers.PhobertTokenizerFast"),c(D,"class","relative group"),c(ye,"href","/docs/transformers/pr_17254/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,z),m(t,tt,p),m(t,q,p),e(q,B),e(B,qe),b(U,qe,null),e(q,Rt),e(q,Le),e(Le,It),m(t,ot,p),m(t,L,p),e(L,A),e(A,xe),b(W,xe,null),e(L,Ft),e(L,De),e(De,Nt),m(t,st,p),m(t,R,p),e(R,Ct),e(R,X),e(X,jt),e(R,Ot),m(t,nt,p),m(t,_e,p),e(_e,Mt),m(t,rt,p),m(t,ge,p),e(ge,Be),e(Be,Vt),m(t,at,p),m(t,ke,p),e(ke,St),m(t,it,p),b(H,t,p),m(t,lt,p),m(t,w,p),e(w,Ut),e(w,G),e(G,Wt),e(w,Xt),e(w,Q),e(Q,Ht),e(w,Gt),m(t,dt,p),m(t,x,p),e(x,I),e(I,Ae),b(Y,Ae,null),e(x,Qt),e(x,Re),e(Re,Yt),m(t,ct,p),m(t,f,p),b(J,f,null),e(f,Jt),e(f,Ie),e(Ie,Kt),e(f,Zt),e(f,K),e(K,eo),e(K,be),e(be,to),e(K,oo),e(f,so),e(f,F),b(Z,F,null),e(F,no),e(F,Fe),e(Fe,ro),e(f,ao),e(f,$),b(ee,$,null),e($,io),e($,Ne),e(Ne,lo),e($,co),e($,te),e(te,ve),e(ve,po),e(ve,Ce),e(Ce,ho),e(te,mo),e(te,Te),e(Te,fo),e(Te,je),e(je,uo),e(f,_o),e(f,N),b(oe,N,null),e(N,go),e(N,Oe),e(Oe,ko),e(f,bo),e(f,C),b(se,C,null),e(C,vo),e(C,Me),e(Me,To),e(f,yo),e(f,j),b(ne,j,null),e(j,Po),e(j,re),e(re,wo),e(re,Ve),e(Ve,$o),e(re,Eo),m(t,pt,p),m(t,D,p),e(D,O),e(O,Se),b(ae,Se,null),e(D,zo),e(D,Ue),e(Ue,qo),m(t,ht,p),m(t,h,p),b(ie,h,null),e(h,Lo),e(h,le),e(le,xo),e(le,We),e(We,Do),e(le,Bo),e(h,Ao),e(h,Xe),e(Xe,Ro),e(h,Io),e(h,He),e(He,Ge),e(Ge,Fo),e(h,No),e(h,de),e(de,Co),e(de,ye),e(ye,jo),e(de,Oo),e(h,Mo),e(h,E),b(ce,E,null),e(E,Vo),e(E,Qe),e(Qe,So),e(E,Uo),e(E,pe),e(pe,Pe),e(Pe,Wo),e(Pe,Ye),e(Ye,Xo),e(pe,Ho),e(pe,we),e(we,Go),e(we,Je),e(Je,Qo),e(h,Yo),e(h,M),b(he,M,null),e(M,Jo),e(M,Ke),e(Ke,Ko),e(h,Zo),e(h,V),b(me,V,null),e(V,es),e(V,Ze),e(Ze,ts),e(h,os),e(h,S),b(fe,S,null),e(S,ss),e(S,ue),e(ue,ns),e(ue,et),e(et,rs),e(ue,as),mt=!0},p:tn,i(t){mt||(v(U.$$.fragment,t),v(W.$$.fragment,t),v(H.$$.fragment,t),v(Y.$$.fragment,t),v(J.$$.fragment,t),v(Z.$$.fragment,t),v(ee.$$.fragment,t),v(oe.$$.fragment,t),v(se.$$.fragment,t),v(ne.$$.fragment,t),v(ae.$$.fragment,t),v(ie.$$.fragment,t),v(ce.$$.fragment,t),v(he.$$.fragment,t),v(me.$$.fragment,t),v(fe.$$.fragment,t),mt=!0)},o(t){T(U.$$.fragment,t),T(W.$$.fragment,t),T(H.$$.fragment,t),T(Y.$$.fragment,t),T(J.$$.fragment,t),T(Z.$$.fragment,t),T(ee.$$.fragment,t),T(oe.$$.fragment,t),T(se.$$.fragment,t),T(ne.$$.fragment,t),T(ae.$$.fragment,t),T(ie.$$.fragment,t),T(ce.$$.fragment,t),T(he.$$.fragment,t),T(me.$$.fragment,t),T(fe.$$.fragment,t),mt=!1},d(t){o(z),t&&o(tt),t&&o(q),y(U),t&&o(ot),t&&o(L),y(W),t&&o(st),t&&o(R),t&&o(nt),t&&o(_e),t&&o(rt),t&&o(ge),t&&o(at),t&&o(ke),t&&o(it),y(H,t),t&&o(lt),t&&o(w),t&&o(dt),t&&o(x),y(Y),t&&o(ct),t&&o(f),y(J),y(Z),y(ee),y(oe),y(se),y(ne),t&&o(pt),t&&o(D),y(ae),t&&o(ht),t&&o(h),y(ie),y(ce),y(he),y(me),y(fe)}}}const rn={local:"phobert",sections:[{local:"overview",title:"Overview"},{local:"transformers.PhobertTokenizer",title:"PhobertTokenizer"},{local:"transformers.PhobertTokenizerFast",title:"PhobertTokenizerFast"}],title:"PhoBERT"};function an(ps){return on(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class hn extends Js{constructor(z){super();Ks(this,z,an,nn,Zs,{})}}export{hn as default,rn as metadata};
