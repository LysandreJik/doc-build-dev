import{S as Ta,i as qa,s as Aa,e as n,k as d,w as _,t as s,M as za,c as r,d as t,m as c,a,x as g,h as i,b as l,N as Pa,G as e,g as p,y as v,q as b,o as k,B as x,v as ja,L as Ea}from"../../chunks/vendor-hf-doc-builder.js";import{T as Na}from"../../chunks/Tip-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ca}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as ro}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Ja}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Da(U){let h,T,f,y,N;return y=new Ca({props:{code:`from transformers import JukeboxModel, JukeboxConfig

# Initializing a Jukebox configuration
configuration = JukeboxConfig()

# Initializing a model from the configuration
model = JukeboxModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JukeboxModel, JukeboxConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Jukebox configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = JukeboxConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = JukeboxModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){h=n("p"),T=s("Example:"),f=d(),_(y.$$.fragment)},l(w){h=r(w,"P",{});var D=a(h);T=i(D,"Example:"),D.forEach(t),f=c(w),g(y.$$.fragment,w)},m(w,D){p(w,h,D),e(h,T),p(w,f,D),v(y,w,D),N=!0},p:Ea,i(w){N||(b(y.$$.fragment,w),N=!0)},o(w){k(y.$$.fragment,w),N=!1},d(w){w&&t(h),w&&t(f),x(y,w)}}}function Va(U){let h,T;return h=new Ca({props:{code:`from transformers import JukeboxTokenizer
tokenizer = JukeboxTokenizer.from_pretrained("jukebox")
tokenizer("Alan Jackson", "Country Rock", "old town road")['input_ids']
tokenizer("Alan Jackson", "Country Rock")['input_ids']`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JukeboxTokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = JukeboxTokenizer.from_pretrained(<span class="hljs-string">&quot;jukebox&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer(<span class="hljs-string">&quot;Alan Jackson&quot;</span>, <span class="hljs-string">&quot;Country Rock&quot;</span>, <span class="hljs-string">&quot;old town road&quot;</span>)[<span class="hljs-string">&#x27;input_ids&#x27;</span>]</span>
## TODO UPDATE THIS OUTPUT
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer(<span class="hljs-string">&quot;Alan Jackson&quot;</span>, <span class="hljs-string">&quot;Country Rock&quot;</span>)[<span class="hljs-string">&#x27;input_ids&#x27;</span>]</span>
[6785],[546]]`}}),{c(){_(h.$$.fragment)},l(f){g(h.$$.fragment,f)},m(f,y){v(h,f,y),T=!0},p:Ea,i(f){T||(b(h.$$.fragment,f),T=!0)},o(f){k(h.$$.fragment,f),T=!1},d(f){x(h,f)}}}function Ma(U){let h,T;return{c(){h=n("p"),T=s("If nothing is provided, the genres and the artist will either be selected randomly or set to None")},l(f){h=r(f,"P",{});var y=a(h);T=i(y,"If nothing is provided, the genres and the artist will either be selected randomly or set to None"),y.forEach(t)},m(f,y){p(f,h,y),e(h,T)},d(f){f&&t(h)}}}function Ia(U){let h,T,f,y,N,w,D,ao,bt,Fo,I,H,so,ae,kt,io,xt,Oo,R,yt,se,wt,$t,So,Ve,Jt,Lo,Me,Et,Wo,Ie,Ct,Qo,E,Tt,lo,qt,At,co,zt,Pt,mo,jt,Nt,po,Dt,Vt,ho,Mt,It,fo,Ft,Ot,Uo,Fe,Oe,br,Ho,Se,St,Ro,B,uo,Lt,Wt,F,Qt,_o,Ut,Ht,go,Rt,Bt,Bo,V,Gt,ie,Zt,Kt,le,Xt,Yt,Go,O,G,vo,de,en,bo,on,Zo,q,ce,tn,me,nn,Le,rn,an,sn,j,ln,We,dn,cn,Qe,mn,pn,pe,hn,fn,un,he,_n,ko,gn,vn,bn,Z,Ko,S,K,xo,fe,kn,yo,xn,Xo,u,ue,yn,wo,wn,$n,L,$o,Jn,En,Jo,Cn,Tn,Eo,qn,An,Co,zn,Pn,_e,jn,To,Nn,Dn,Vn,X,Mn,ge,In,qo,Fn,On,Sn,Y,Ln,ve,Wn,Ue,Qn,Un,Hn,Ao,Rn,Bn,ee,be,Gn,zo,Zn,Yo,W,oe,Po,ke,Kn,jo,Xn,et,$,xe,Yn,No,er,or,ye,tr,He,nr,rr,ar,we,sr,$e,ir,lr,dr,Re,Je,cr,Be,Ee,mr,Ge,Ce,pr,Ze,Te,hr,Ke,qe,ot,Q,te,Do,Ae,fr,Vo,ur,tt,z,ze,_r,Xe,Pe,gr,Ye,je,vr,eo,Ne,nt;return w=new ro({}),ae=new ro({}),de=new ro({}),ce=new P({props:{name:"class transformers.JukeboxConfig",anchor:"transformers.JukeboxConfig",parameters:[{name:"sampling_rate",val:" = 44100"},{name:"metadata_dims",val:" = [(604, 7898), (120, 4111), (120, 4111)]"},{name:"nb_priors",val:" = 3"},{name:"timing_dims",val:" = 64"},{name:"single_enc_dec",val:" = [True, False, False]"},{name:"metadata_conditioning",val:" = True"},{name:"merged_decoder",val:" = [True, False, False]"},{name:"lyric_conditioning",val:" = [True, False, False]"},{name:"nb_relevant_lyric_tokens",val:" = [384, 0, 0]"},{name:"min_duration",val:" = 17.84"},{name:"max_duration",val:" = 600.0"},{name:"max_nb_genres",val:" = 5"},{name:"init_std",val:" = 0.2"},{name:"hop_fraction",val:" = [0.125, 0.5, 0.5]"},{name:"cond_zero_out",val:" = False"},{name:"cond_depth",val:" = [3, 16, 16]"},{name:"cond_width",val:" = [128, 1024, 1024]"},{name:"cond_dilation_growth_rate",val:" = [1, 3, 3]"},{name:"cond_dilation_cycle",val:" = [None, 8, 8]"},{name:"cond_res_scale",val:" = [None, True, False]"},{name:"cond_m_conv",val:" = 1"},{name:"cond_downs_t",val:" = (3, 2, 2)"},{name:"cond_strides_t",val:" = (2, 2, 2)"},{name:"lyric_enc_spread",val:" = None"},{name:"lyric_enc_width",val:" = [128, 128, 128]"},{name:"lyric_enc_depth",val:" = [18, 3, 3]"},{name:"lyric_enc_heads",val:" = 4"},{name:"lyric_enc_m_attn",val:" = 0.25"},{name:"lyric_enc_m_mlp",val:" = 1.0"},{name:"lyric_enc_blocks",val:" = 32"},{name:"lyric_enc_init_scale",val:" = [0.1, 0.4, 0.4]"},{name:"lyric_enc_loss_fraction",val:" = [0.4, 0.0, 0.0]"},{name:"lyric_enc_attn_order",val:" = [2, 0, 0]"},{name:"lyric_enc_attn_dropout",val:" = 0.0"},{name:"lyric_enc_resid_dropout",val:" = 0.0"},{name:"lyric_enc_emb_dropout",val:" = 0.0"},{name:"lyric_enc_zero_out",val:" = False"},{name:"lyric_enc_res_scale",val:" = False"},{name:"lyric_enc_pos_init",val:" = False"},{name:"lyric_enc_n_vocab",val:" = 79"},{name:"prior_init_scale",val:" = [0.2, 1, 1]"},{name:"prior_spread",val:" = None"},{name:"prior_zero_out",val:" = False"},{name:"prior_res_scale",val:" = False"},{name:"prior_pos_init",val:" = False"},{name:"prior_n_ctx",val:" = (6144, 8192, 8192)"},{name:"prior_latent_dim",val:" = 2048"},{name:"prior_width",val:" = [2048, 1920, 1920]"},{name:"prior_depth",val:" = [72, 72, 72]"},{name:"prior_n_heads",val:" = [2, 1, 1]"},{name:"prior_attn_order",val:" = [12, 2, 2]"},{name:"prior_blocks",val:" = 64"},{name:"prior_alignment_layer",val:" = [68, None, None]"},{name:"prior_alignment_head",val:" = [2, None, None]"},{name:"prior_m_attn",val:" = 0.25"},{name:"prior_attn_dropout",val:" = 0"},{name:"prior_resid_dropout",val:" = 0"},{name:"prior_emb_dropout",val:" = 0"},{name:"vqvae_levels",val:" = 3"},{name:"vqvae_downs_t",val:" = (3, 2, 2)"},{name:"vqvae_strides_t",val:" = (2, 2, 2)"},{name:"vqvae_emmbedding_width",val:" = 64"},{name:"vqvae_codebook_dimension",val:" = 2048"},{name:"vqvae_width",val:" = 32"},{name:"vqvae_depth",val:" = 4"},{name:"vqvae_m_conv",val:" = 1"},{name:"vqvae_dilation_growth_rate",val:" = 3"},{name:"vqvae_dilation_cycle",val:" = None"},{name:"vqvae_multipliers",val:" = (2, 1, 1)"},{name:"vqvae_lmu",val:" = 0.99"},{name:"vqvae_commit",val:" = 0.02"},{name:"vqvae_conv_block_depth",val:" = 4"},{name:"vqvae_conv_block_width",val:" = 32"},{name:"vqvae_reverse_decoder_dilation",val:" = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxConfig.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 44100) &#x2014;
Sampling rate of the raw audio.`,name:"sampling_rate"},{anchor:"transformers.JukeboxConfig.metadata_dims",description:`<strong>metadata_dims</strong> (<code>list</code>, <em>optional</em>, defaults to [(604, 7898), (120, 4111), (120, 4111)]) &#x2014;
List containing the number of genres and the number of artists that were used to train the embedding layers
of each of the prior models.`,name:"metadata_dims"},{anchor:"transformers.JukeboxConfig.nb_priors",description:`<strong>nb_priors</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of prior models that will sequentialy sample tokens. Each prior is conditional auto regressive
(decoder) model, apart from the top prior, which can include a lyric encoder. The available models were
trained using a top prior and 2 upsampler priors.`,name:"nb_priors"},{anchor:"transformers.JukeboxConfig.timing_dims",description:`<strong>timing_dims</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensions of the JukeboxRangeEmbedding layer which is equivalent to traditional positional embedding
layer. #TODO the timing embedding layer converts the absolute and relative position in the currently
sampled audio to a tensor of lenght <code>timing_dims</code> that will be added to the music tokens.`,name:"timing_dims"},{anchor:"transformers.JukeboxConfig.single_enc_dec",description:`<strong>single_enc_dec</strong> (<code>list</code>, <em>optional</em>, defaults to [True, False, False]) &#x2014;
Whether or not to use a single encoder-decoder architecture or split both modules and have a seperate
<code>lyric_encoder</code> for each of the priors.`,name:"single_enc_dec"},{anchor:"transformers.JukeboxConfig.metadata_conditioning",description:`<strong>metadata_conditioning</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether or not to use metadata conditioning, corresponding to the artist, the genre and the min/maximum
duration.`,name:"metadata_conditioning"},{anchor:"transformers.JukeboxConfig.merged_decoder",description:`<strong>merged_decoder</strong> (<code>list</code>, <em>optional</em>, defaults to [True, False, False]) &#x2014;</p>
<h1 class="relative group">
	<a id="fixme-is-that-the-same-as-single_enc_dec-??" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#fixme-is-that-the-same-as-single_enc_dec-??">
		<span><iconcopylink></iconcopylink></span>
	</a>
	<span>
		FIXME is that the same as single_enc_dec ??
	</span>
</h1>`,name:"merged_decoder"},{anchor:"transformers.JukeboxConfig.lyric_conditioning",description:`<strong>lyric_conditioning</strong> (<code>list</code>, <em>optional</em>, defaults to [True, False, False]) &#x2014;
Whether or not to use the lyrics as conditioning.`,name:"lyric_conditioning"},{anchor:"transformers.JukeboxConfig.nb_relevant_lyric_tokens",description:`<strong>nb_relevant_lyric_tokens</strong> (<code>list</code>, <em>optional</em>, defaults to [384, 0, 0]) &#x2014;
Number of tokens that are used when sampling a single window of length <code>prior_n_ctx</code>`,name:"nb_relevant_lyric_tokens"},{anchor:"transformers.JukeboxConfig.min_duration",description:`<strong>min_duration</strong> (<code>float</code>, <em>optional</em>, defaults to 17.84) &#x2014;
Minimum duration of the audios to generate`,name:"min_duration"},{anchor:"transformers.JukeboxConfig.max_duration",description:`<strong>max_duration</strong> (<code>float</code>, <em>optional</em>, defaults to 600.0) &#x2014;
Maximum duration of the audios to generate`,name:"max_duration"},{anchor:"transformers.JukeboxConfig.max_nb_genres",description:`<strong>max_nb_genres</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Maximum number of genres that can be used to condition a single sample.`,name:"max_nb_genres"},{anchor:"transformers.JukeboxConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
Standard deviation used to inital the model.`,name:"init_std"},{anchor:"transformers.JukeboxConfig.hop_fraction",description:`<strong>hop_fraction</strong> (<code>list</code>, <em>optional</em>, defaults to [0.125, 0.5, 0.5]) &#x2014;</p>
<h1 class="relative group">
	<a id="todo-detail-this-amount-of-space-between-each-of-the-sampling-windows-oif-&lt;code&gt;n_ctx&lt;/code&gt;-tokens" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#todo-detail-this-amount-of-space-between-each-of-the-sampling-windows-oif-&lt;code&gt;n_ctx&lt;/code&gt;-tokens">
		<span><iconcopylink></iconcopylink></span>
	</a>
	<span>
		TODO detail this amount of space between each of the sampling windows oif <code>n_ctx</code> tokens
	</span>
</h1>`,name:"hop_fraction"},{anchor:"transformers.JukeboxConfig.cond_zero_out",description:`<strong>cond_zero_out</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;
Zero out weights when initialising.`,name:"cond_zero_out"},{anchor:"transformers.JukeboxConfig.cond_depth",description:`<strong>cond_depth</strong> (<code>list</code>, <em>optional</em>, defaults to [3, 16, 16]) &#x2014;
Number of layers to use for the music conditioner.`,name:"cond_depth"},{anchor:"transformers.JukeboxConfig.cond_width",description:`<strong>cond_width</strong> (<code>list</code>, <em>optional</em>, defaults to [128, 1024, 1024]) &#x2014;
Width of the audio conditioning layer.`,name:"cond_width"},{anchor:"transformers.JukeboxConfig.cond_dilation_growth_rate",description:`<strong>cond_dilation_growth_rate</strong> (<code>list</code>, <em>optional</em>, defaults to [1, 3, 3]) &#x2014;
Dilation grow rate used between each convolutionnal block.`,name:"cond_dilation_growth_rate"},{anchor:"transformers.JukeboxConfig.cond_dilation_cycle",description:`<strong>cond_dilation_cycle</strong> (<code>list</code>, <em>optional</em>, defaults to [None, 8, 8]) &#x2014;
Cycle of dilation to use. Usually similar to the ones used in the VQVAE.`,name:"cond_dilation_cycle"},{anchor:"transformers.JukeboxConfig.cond_res_scale",description:`<strong>cond_res_scale</strong> (<code>list</code>, <em>optional</em>, defaults to [None, True, False]) &#x2014;
Wheter or not to scale the residuals in the audio conditionner block. Since the top level prior doeas not
have a conditionner, the default value is to None and should not be modified.`,name:"cond_res_scale"},{anchor:"transformers.JukeboxConfig.cond_m_conv",description:`<strong>cond_m_conv</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Conditionner multiplier (the input states are mulitplied by that parameter for each convolution.`,name:"cond_m_conv"},{anchor:"transformers.JukeboxConfig.cond_downs_t",description:`<strong>cond_downs_t</strong> (<code>tuple</code>, <em>optional</em>, defaults to (3, 2, 2)) &#x2014;
Downsampling &#x2026; # TODO`,name:"cond_downs_t"},{anchor:"transformers.JukeboxConfig.cond_strides_t",description:`<strong>cond_strides_t</strong> (<code>tuple</code>, <em>optional</em>, defaults to (2, 2, 2)) &#x2014;
Striding pattern to use #TODO`,name:"cond_strides_t"},{anchor:"transformers.JukeboxConfig.lyric_enc_spread",description:`<strong>lyric_enc_spread</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;
Spread used in the attention pattern #TODO check what that is actually`,name:"lyric_enc_spread"},{anchor:"transformers.JukeboxConfig.lyric_enc_width",description:`<strong>lyric_enc_width</strong> (<code>list</code>, <em>optional</em>, defaults to [128, 128, 128]) &#x2014;
Width of the lyric encoder`,name:"lyric_enc_width"},{anchor:"transformers.JukeboxConfig.lyric_enc_depth",description:`<strong>lyric_enc_depth</strong> (<code>list</code>, <em>optional</em>, defaults to [18, 3, 3]) &#x2014;
Number of blocks used in the lyric encoder is this different from lyric_enc_blocks? FIXME`,name:"lyric_enc_depth"},{anchor:"transformers.JukeboxConfig.lyric_enc_heads",description:`<strong>lyric_enc_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of heads in the lyric encoder`,name:"lyric_enc_heads"},{anchor:"transformers.JukeboxConfig.lyric_enc_m_attn",description:`<strong>lyric_enc_m_attn</strong> (<code>float</code>, <em>optional</em>, defaults to 0.25) &#x2014;</p>
<h1 class="relative group">
	<a id="again,-m_attn-and-m_mlp,-i-don&#x2019;t-really-know-how-to-rename-it" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#again,-m_attn-and-m_mlp,-i-don&#x2019;t-really-know-how-to-rename-it">
		<span><iconcopylink></iconcopylink></span>
	</a>
	<span>
		again, m_attn and m_mlp, I don&#x2019;t really know how to rename it
	</span>
</h1>`,name:"lyric_enc_m_attn"},{anchor:"transformers.JukeboxConfig.lyric_enc_m_mlp",description:`<strong>lyric_enc_m_mlp</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;</p>
<h1 class="relative group">
	<a id="again,-m_attn-and-m_mlp,-i-don&#x2019;t-really-know-how-to-rename-it" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#again,-m_attn-and-m_mlp,-i-don&#x2019;t-really-know-how-to-rename-it">
		<span><iconcopylink></iconcopylink></span>
	</a>
	<span>
		again, m_attn and m_mlp, I don&#x2019;t really know how to rename it
	</span>
</h1>`,name:"lyric_enc_m_mlp"},{anchor:"transformers.JukeboxConfig.lyric_enc_blocks",description:"<strong>lyric_enc_blocks</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;",name:"lyric_enc_blocks"},{anchor:"transformers.JukeboxConfig.lyric_enc_init_scale",description:"<strong>lyric_enc_init_scale</strong> (<code>list</code>, <em>optional</em>, defaults to [0.1, 0.4, 0.4]) &#x2014;",name:"lyric_enc_init_scale"},{anchor:"transformers.JukeboxConfig.lyric_enc_loss_fraction",description:"<strong>lyric_enc_loss_fraction</strong> (<code>list</code>, <em>optional</em>, defaults to [0.4, 0.0, 0.0]) &#x2014;",name:"lyric_enc_loss_fraction"},{anchor:"transformers.JukeboxConfig.lyric_enc_attn_order",description:`<strong>lyric_enc_attn_order</strong> (<code>list</code>, <em>optional</em>, defaults to [2, 0, 0]) &#x2014;
Which attention pattern to use for the lyric encoder`,name:"lyric_enc_attn_order"},{anchor:"transformers.JukeboxConfig.lyric_enc_attn_dropout",description:"<strong>lyric_enc_attn_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;",name:"lyric_enc_attn_dropout"},{anchor:"transformers.JukeboxConfig.lyric_enc_resid_dropout",description:"<strong>lyric_enc_resid_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;",name:"lyric_enc_resid_dropout"},{anchor:"transformers.JukeboxConfig.lyric_enc_emb_dropout",description:"<strong>lyric_enc_emb_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;",name:"lyric_enc_emb_dropout"},{anchor:"transformers.JukeboxConfig.lyric_enc_zero_out",description:"<strong>lyric_enc_zero_out</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;",name:"lyric_enc_zero_out"},{anchor:"transformers.JukeboxConfig.lyric_enc_res_scale",description:"<strong>lyric_enc_res_scale</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;",name:"lyric_enc_res_scale"},{anchor:"transformers.JukeboxConfig.lyric_enc_pos_init",description:"<strong>lyric_enc_pos_init</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;",name:"lyric_enc_pos_init"},{anchor:"transformers.JukeboxConfig.lyric_enc_n_vocab",description:"<strong>lyric_enc_n_vocab</strong> (<code>int</code>, <em>optional</em>, defaults to 79) &#x2014;",name:"lyric_enc_n_vocab"},{anchor:"transformers.JukeboxConfig.prior_init_scale",description:"<strong>prior_init_scale</strong> (<code>list</code>, <em>optional</em>, defaults to [0.2, 1, 1]) &#x2014;",name:"prior_init_scale"},{anchor:"transformers.JukeboxConfig.prior_spread",description:"<strong>prior_spread</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;",name:"prior_spread"},{anchor:"transformers.JukeboxConfig.prior_zero_out",description:"<strong>prior_zero_out</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;",name:"prior_zero_out"},{anchor:"transformers.JukeboxConfig.prior_res_scale",description:"<strong>prior_res_scale</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;",name:"prior_res_scale"},{anchor:"transformers.JukeboxConfig.prior_pos_init",description:"<strong>prior_pos_init</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;",name:"prior_pos_init"},{anchor:"transformers.JukeboxConfig.prior_n_ctx",description:`<strong>prior_n_ctx</strong> (<code>tuple</code>, <em>optional</em>, defaults to (6144, 8192, 8192)) &#x2014;
Number of context tokens for each prior. The context tokens are the music tokens that are attended to when
generating music tokens.`,name:"prior_n_ctx"},{anchor:"transformers.JukeboxConfig.prior_latent_dim",description:`<strong>prior_latent_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimension of the latent music token space. Default value match the <code>vqvae_codebook_dimension</code>.`,name:"prior_latent_dim"},{anchor:"transformers.JukeboxConfig.prior_width",description:"<strong>prior_width</strong> (<code>list</code>, <em>optional</em>, defaults to [2048, 1920, 1920]) &#x2014;",name:"prior_width"},{anchor:"transformers.JukeboxConfig.prior_depth",description:"<strong>prior_depth</strong> (<code>list</code>, <em>optional</em>, defaults to [72, 72, 72]) &#x2014;",name:"prior_depth"},{anchor:"transformers.JukeboxConfig.prior_n_heads",description:"<strong>prior_n_heads</strong> (<code>list</code>, <em>optional</em>, defaults to [2, 1, 1]) &#x2014;",name:"prior_n_heads"},{anchor:"transformers.JukeboxConfig.prior_attn_order",description:`<strong>prior_attn_order</strong> (<code>list</code>, <em>optional</em>, defaults to [12, 2, 2]) &#x2014;
Attention patterns to use in each prior. Depending on the value, cross attention, block attention and
sparse attention blocks are stacked.`,name:"prior_attn_order"},{anchor:"transformers.JukeboxConfig.prior_blocks",description:"<strong>prior_blocks</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;",name:"prior_blocks"},{anchor:"transformers.JukeboxConfig.prior_alignment_layer",description:`<strong>prior_alignment_layer</strong> (<code>list</code>, <em>optional</em>, defaults to [68, None, None]) &#x2014;
Layer corresponding to the alignemnt between the lyrics and the audio.`,name:"prior_alignment_layer"},{anchor:"transformers.JukeboxConfig.prior_alignment_head",description:`<strong>prior_alignment_head</strong> (<code>list</code>, <em>optional</em>, defaults to [2, None, None]) &#x2014;
Index of the attention head which takes care of the alignemnt between the lyrics and the audio.`,name:"prior_alignment_head"},{anchor:"transformers.JukeboxConfig.prior_m_attn",description:"<strong>prior_m_attn</strong> (<code>float</code>, <em>optional</em>, defaults to 0.25) &#x2014;",name:"prior_m_attn"},{anchor:"transformers.JukeboxConfig.prior_attn_dropout",description:"<strong>prior_attn_dropout</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;",name:"prior_attn_dropout"},{anchor:"transformers.JukeboxConfig.prior_resid_dropout",description:"<strong>prior_resid_dropout</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;",name:"prior_resid_dropout"},{anchor:"transformers.JukeboxConfig.prior_emb_dropout",description:"<strong>prior_emb_dropout</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;",name:"prior_emb_dropout"},{anchor:"transformers.JukeboxConfig.vqvae_levels",description:`<strong>vqvae_levels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of hierachical levels that used in the VQVAE.`,name:"vqvae_levels"},{anchor:"transformers.JukeboxConfig.vqvae_downs_t",description:"<strong>vqvae_downs_t</strong> (<code>tuple</code>, <em>optional</em>, defaults to (3, 2, 2)) &#x2014;",name:"vqvae_downs_t"},{anchor:"transformers.JukeboxConfig.vqvae_strides_t",description:"<strong>vqvae_strides_t</strong> (<code>tuple</code>, <em>optional</em>, defaults to (2, 2, 2)) &#x2014;",name:"vqvae_strides_t"},{anchor:"transformers.JukeboxConfig.vqvae_emmbedding_width",description:`<strong>vqvae_emmbedding_width</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimension of the codebook vectors.`,name:"vqvae_emmbedding_width"},{anchor:"transformers.JukeboxConfig.vqvae_codebook_dimension",description:`<strong>vqvae_codebook_dimension</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Number of codes to use in each of the VQVAE.`,name:"vqvae_codebook_dimension"},{anchor:"transformers.JukeboxConfig.vqvae_width",description:"<strong>vqvae_width</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;",name:"vqvae_width"},{anchor:"transformers.JukeboxConfig.vqvae_depth",description:"<strong>vqvae_depth</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;",name:"vqvae_depth"},{anchor:"transformers.JukeboxConfig.vqvae_m_conv",description:"<strong>vqvae_m_conv</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;",name:"vqvae_m_conv"},{anchor:"transformers.JukeboxConfig.vqvae_dilation_growth_rate",description:"<strong>vqvae_dilation_growth_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;",name:"vqvae_dilation_growth_rate"},{anchor:"transformers.JukeboxConfig.vqvae_dilation_cycle",description:"<strong>vqvae_dilation_cycle</strong> (<code>bool</code>, <em>optional</em>, defaults to False) &#x2014;",name:"vqvae_dilation_cycle"},{anchor:"transformers.JukeboxConfig.vqvae_multipliers",description:"<strong>vqvae_multipliers</strong> (<code>tuple</code>, <em>optional</em>, defaults to (2, 1, 1)) &#x2014;",name:"vqvae_multipliers"},{anchor:"transformers.JukeboxConfig.vqvae_lmu",description:"<strong>vqvae_lmu</strong> (<code>float</code>, <em>optional</em>, defaults to 0.99) &#x2014;",name:"vqvae_lmu"},{anchor:"transformers.JukeboxConfig.vqvae_commit",description:"<strong>vqvae_commit</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;",name:"vqvae_commit"},{anchor:"transformers.JukeboxConfig.vqvae_conv_block_depth",description:"<strong>vqvae_conv_block_depth</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;",name:"vqvae_conv_block_depth"},{anchor:"transformers.JukeboxConfig.vqvae_conv_block_width",description:"<strong>vqvae_conv_block_width</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;",name:"vqvae_conv_block_width"},{anchor:"transformers.JukeboxConfig.vqvae_reverse_decoder_dilation",description:"<strong>vqvae_reverse_decoder_dilation</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;",name:"vqvae_reverse_decoder_dilation"}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/configuration_jukebox.py#L29"}}),Z=new Ja({props:{anchor:"transformers.JukeboxConfig.example",$$slots:{default:[Da]},$$scope:{ctx:U}}}),fe=new ro({}),ue=new P({props:{name:"class transformers.JukeboxTokenizer",anchor:"transformers.JukeboxTokenizer",parameters:[{name:"artists_file",val:""},{name:"genres_file",val:""},{name:"lyrics_file",val:""},{name:"version",val:" = ['v3', 'v2', 'v2']"},{name:"max_n_lyric_tokens",val:" = 512"},{name:"n_genres",val:" = 5"},{name:"unk_token",val:" = '<|endoftext|>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file which should contain a dictionnary where the keys are &#x2018;artist&#x2019;, &#x2018;genre&#x2019; and
&#x2018;lyrics&#x2019; and the values are their corresponding vocabulary files.`,name:"vocab_file"},{anchor:"transformers.JukeboxTokenizer.n_genres",description:`<strong>n_genres</strong> (<code>int</code>, <code>optional</code>, defaults to 1) &#x2014;
Maximum number of genres to use for composition.`,name:"n_genres"},{anchor:"transformers.JukeboxTokenizer.max_n_lyric_tokens",description:`<strong>max_n_lyric_tokens</strong> (<code>int</code>, <code>optional</code>, defaults to 512) &#x2014;
Maximum number of lyric tokens to keep.`,name:"max_n_lyric_tokens"},{anchor:"transformers.JukeboxTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&lt;|endoftext|&gt;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/tokenization_jukebox.py#L108"}}),X=new Ja({props:{anchor:"transformers.JukeboxTokenizer.example",$$slots:{default:[Va]},$$scope:{ctx:U}}}),Y=new Na({props:{$$slots:{default:[Ma]},$$scope:{ctx:U}}}),be=new P({props:{name:"save_vocabulary",anchor:"transformers.JukeboxTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"transformers.JukeboxTokenizer.save_vocabulary.save_directory",description:`<strong>save_directory</strong> (<code>str</code>) &#x2014;
<em>description</em>`,name:"save_directory"},{anchor:"transformers.JukeboxTokenizer.save_vocabulary.filename_prefix",description:`<strong>filename_prefix</strong> (<code>Optional[str]</code>, <em>optional</em>, defaults to None) &#x2014;
<em>description</em>`,name:"filename_prefix"}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/tokenization_jukebox.py#L438"}}),ke=new ro({}),xe=new P({props:{name:"class transformers.JukeboxModel",anchor:"transformers.JukeboxModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.JukeboxModel.config",description:`<strong>config</strong> (<code>JukeboxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_17826/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L2731"}}),Je=new P({props:{name:"primed_sample",anchor:"transformers.JukeboxModel.primed_sample",parameters:[{name:"raw_audio",val:""},{name:"labels",val:""},{name:"**sampling_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L2963"}}),Ee=new P({props:{name:"ancestral_sample",anchor:"transformers.JukeboxModel.ancestral_sample",parameters:[{name:"labels",val:""},{name:"n_samples",val:" = 1"},{name:"**sampling_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L2942"}}),Ce=new P({props:{name:"continue_sample",anchor:"transformers.JukeboxModel.continue_sample",parameters:[{name:"music_tokens",val:""},{name:"labels",val:""},{name:"**sampling_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L2951"}}),Te=new P({props:{name:"upsample",anchor:"transformers.JukeboxModel.upsample",parameters:[{name:"music_tokens",val:""},{name:"labels",val:""},{name:"**sampling_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L2957"}}),qe=new P({props:{name:"_sample",anchor:"transformers.JukeboxModel._sample",parameters:[{name:"music_tokens",val:""},{name:"labels",val:""},{name:"sample_levels",val:""},{name:"metas",val:" = None"},{name:"chunk_size",val:" = 32"},{name:"sampling_temperature",val:" = 0.98"},{name:"lower_batch_size",val:" = 16"},{name:"max_batch_size",val:" = 16"},{name:"sample_length_in_seconds",val:" = 24"},{name:"alignments",val:" = None"},{name:"sample_tokens",val:" = None"},{name:"offset",val:" = 0"},{name:"save_results",val:" = True"},{name:"sample_length",val:" = None"},{name:"fp16",val:" = False"}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L2846"}}),Ae=new ro({}),ze=new P({props:{name:"class transformers.JukeboxVQVAE",anchor:"transformers.JukeboxVQVAE",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L528"}}),Pe=new P({props:{name:"forward",anchor:"transformers.JukeboxVQVAE.forward",parameters:[{name:"raw_audio",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L665"}}),je=new P({props:{name:"encode",anchor:"transformers.JukeboxVQVAE.encode",parameters:[{name:"input_audio",val:""},{name:"start_level",val:" = 0"},{name:"end_level",val:" = None"},{name:"bs_chunks",val:" = 1"}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L649"}}),Ne=new P({props:{name:"decode",anchor:"transformers.JukeboxVQVAE.decode",parameters:[{name:"music_tokens",val:""},{name:"start_level",val:" = 0"},{name:"end_level",val:" = None"},{name:"bs_chunks",val:" = 1"}],source:"https://github.com/huggingface/transformers/blob/vr_17826/src/transformers/models/jukebox/modeling_jukebox.py#L627"}}),{c(){h=n("meta"),T=d(),f=n("h1"),y=n("a"),N=n("span"),_(w.$$.fragment),D=d(),ao=n("span"),bt=s("Jukebox"),Fo=d(),I=n("h2"),H=n("a"),so=n("span"),_(ae.$$.fragment),kt=d(),io=n("span"),xt=s("Overview"),Oo=d(),R=n("p"),yt=s("The Jukebox model was proposed in "),se=n("a"),wt=s("Jukebox: A generative model for music"),$t=s(`
by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
Ilya Sutskever.`),So=d(),Ve=n("p"),Jt=s(`This model proposes a generative music model which can be produce minute long samples which can bne conditionned on
artist, genre and lyrics.`),Lo=d(),Me=n("p"),Et=s("The abstract from the paper is the following:"),Wo=d(),Ie=n("p"),Ct=s("We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code."),Qo=d(),E=n("p"),Tt=s("As shown on the following figure, Jukebox is made of 3 "),lo=n("code"),qt=s("priors"),At=s(" which are decoders only. They follow a particular architecture described in "),co=n("code"),zt=s("Scalable Transformers"),Pt=s(` #TODO add link to the paper.
An encoder model is used on the lyrics, on the first (also called `),mo=n("code"),jt=s("top_prior"),Nt=s(") prior\u2019s decoder attends to the last lyric hidden states. Each prior is linked to the previous by an "),po=n("code"),Dt=s("AudioConditionner"),Vt=s(" module which takes care of upsampling the generated hidden state to the correct "),ho=n("code"),Mt=s("raw_to_token"),It=s(` resolution.
The metadatas such as `),fo=n("em"),Ft=s("artist, genre and timing"),Ot=s(" are passed to each prior, in the form of a start token and positionnal embedding for the timing data.  The hidden states are mapped to the closest codebook vector from the VQVAE in order to convert them to raw audio."),Uo=d(),Fe=n("p"),Oe=n("img"),Ho=d(),Se=n("p"),St=s("Tips:"),Ro=d(),B=n("ul"),uo=n("li"),Lt=s("This model is very slow, and takes 8h to generate a minute long audio using the 5b top prior."),Wt=d(),F=n("li"),Qt=s("Primed sampling requires more memory than ancestral sampling and should be used with "),_o=n("code"),Ut=s("fp16"),Ht=s(" set to "),go=n("code"),Rt=s("True"),Bt=s("."),Bo=d(),V=n("p"),Gt=s("This model was contributed by "),ie=n("a"),Zt=s("Arthur Zucker"),Kt=s(`.
The original code can be found `),le=n("a"),Xt=s("here"),Yt=s("."),Go=d(),O=n("h2"),G=n("a"),vo=n("span"),_(de.$$.fragment),en=d(),bo=n("span"),on=s("JukeboxConfig"),Zo=d(),q=n("div"),_(ce.$$.fragment),tn=d(),me=n("p"),nn=s("This is the configuration class to store the configuration of a "),Le=n("a"),rn=s("JukeboxModel"),an=s("."),sn=d(),j=n("p"),ln=s("Configuration objects inherit from "),We=n("a"),dn=s("PretrainedConfig"),cn=s(` and can be used to control the model outputs. Read the
documentation from `),Qe=n("a"),mn=s("PretrainedConfig"),pn=s(` for more information. Instantiating a configuration with the defaults will
yield a similar configuration to that of
`),pe=n("a"),hn=s("openai/jukebox-1b-lyrics"),fn=s(" architecture."),un=d(),he=n("p"),_n=s(`The downsampling and stride are used to determine downsampling of the input sequence. For example, downsamoling =
(5,3), and strides = (2, 2) will downsample the audio by 2`),ko=n("strong"),gn=s("5 = 32 to get the first level of codes, and 2"),vn=s(`8 = 256
to get the second level codes. This is mostly true for training the top level prior and the upsamplers.`),bn=d(),_(Z.$$.fragment),Ko=d(),S=n("h2"),K=n("a"),xo=n("span"),_(fe.$$.fragment),kn=d(),yo=n("span"),xn=s("JukeboxTokenizer"),Xo=d(),u=n("div"),_(ue.$$.fragment),yn=d(),wo=n("p"),wn=s("Constructs a Jukebox tokenizer. Jukebox can be conditioned on 3 different inputs :"),$n=d(),L=n("ul"),$o=n("li"),Jn=s("Artists, unique ids are associated to each artist from the provided dictionary."),En=d(),Jo=n("li"),Cn=s("Genres, unique ids are associated to each genre from the provided dictionary."),Tn=d(),Eo=n("li"),qn=s(`Lyrics, character based tokenization. Must be initialized with the list of characters that are inside the
vocabulary.`),An=d(),Co=n("p"),zn=s(`This tokenizer is straight forward and does not require trainingg. It should be able to process a different number of inputs:
as the conditioning of the model can be done on the three different queries. If None is provided, defaults values will be used.:`),Pn=d(),_e=n("p"),jn=s("Depending on the number of genres on which the model should be conditioned ("),To=n("code"),Nn=s("n_genres"),Dn=s(")."),Vn=d(),_(X.$$.fragment),Mn=d(),ge=n("p"),In=s("You can get around that behavior by passing "),qo=n("code"),Fn=s("add_prefix_space=True"),On=s(` when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.`),Sn=d(),_(Y.$$.fragment),Ln=d(),ve=n("p"),Wn=s("This tokenizer inherits from "),Ue=n("a"),Qn=s("PreTrainedTokenizer"),Un=s(` which contains most of the main methods. Users should refer to:
this superclass for more information regarding those methods.`),Hn=d(),Ao=n("p"),Rn=s("However the code does not allow that and only supports composing from various genres."),Bn=d(),ee=n("div"),_(be.$$.fragment),Gn=d(),zo=n("p"),Zn=s("Saves the tokenizer\u2019s vocabulary dictionnary to the provided save_directory."),Yo=d(),W=n("h2"),oe=n("a"),Po=n("span"),_(ke.$$.fragment),Kn=d(),jo=n("span"),Xn=s("JukeboxModel"),et=d(),$=n("div"),_(xe.$$.fragment),Yn=d(),No=n("p"),er=s("The bare JUKEBOX Model from which you can sample"),or=d(),ye=n("p"),tr=s("This model inherits from "),He=n("a"),nr=s("PreTrainedModel"),rr=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ar=d(),we=n("p"),sr=s("This model is also a PyTorch "),$e=n("a"),ir=s("torch.nn.Module"),lr=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),dr=d(),Re=n("div"),_(Je.$$.fragment),cr=d(),Be=n("div"),_(Ee.$$.fragment),mr=d(),Ge=n("div"),_(Ce.$$.fragment),pr=d(),Ze=n("div"),_(Te.$$.fragment),hr=d(),Ke=n("div"),_(qe.$$.fragment),ot=d(),Q=n("h2"),te=n("a"),Do=n("span"),_(Ae.$$.fragment),fr=d(),Vo=n("span"),ur=s("JukeboxVQVAE"),tt=d(),z=n("div"),_(ze.$$.fragment),_r=d(),Xe=n("div"),_(Pe.$$.fragment),gr=d(),Ye=n("div"),_(je.$$.fragment),vr=d(),eo=n("div"),_(Ne.$$.fragment),this.h()},l(o){const m=za('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(t),T=c(o),f=r(o,"H1",{class:!0});var De=a(f);y=r(De,"A",{id:!0,class:!0,href:!0});var Mo=a(y);N=r(Mo,"SPAN",{});var Io=a(N);g(w.$$.fragment,Io),Io.forEach(t),Mo.forEach(t),D=c(De),ao=r(De,"SPAN",{});var kr=a(ao);bt=i(kr,"Jukebox"),kr.forEach(t),De.forEach(t),Fo=c(o),I=r(o,"H2",{class:!0});var rt=a(I);H=r(rt,"A",{id:!0,class:!0,href:!0});var xr=a(H);so=r(xr,"SPAN",{});var yr=a(so);g(ae.$$.fragment,yr),yr.forEach(t),xr.forEach(t),kt=c(rt),io=r(rt,"SPAN",{});var wr=a(io);xt=i(wr,"Overview"),wr.forEach(t),rt.forEach(t),Oo=c(o),R=r(o,"P",{});var at=a(R);yt=i(at,"The Jukebox model was proposed in "),se=r(at,"A",{href:!0,rel:!0});var $r=a(se);wt=i($r,"Jukebox: A generative model for music"),$r.forEach(t),$t=i(at,`
by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
Ilya Sutskever.`),at.forEach(t),So=c(o),Ve=r(o,"P",{});var Jr=a(Ve);Jt=i(Jr,`This model proposes a generative music model which can be produce minute long samples which can bne conditionned on
artist, genre and lyrics.`),Jr.forEach(t),Lo=c(o),Me=r(o,"P",{});var Er=a(Me);Et=i(Er,"The abstract from the paper is the following:"),Er.forEach(t),Wo=c(o),Ie=r(o,"P",{});var Cr=a(Ie);Ct=i(Cr,"We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code."),Cr.forEach(t),Qo=c(o),E=r(o,"P",{});var A=a(E);Tt=i(A,"As shown on the following figure, Jukebox is made of 3 "),lo=r(A,"CODE",{});var Tr=a(lo);qt=i(Tr,"priors"),Tr.forEach(t),At=i(A," which are decoders only. They follow a particular architecture described in "),co=r(A,"CODE",{});var qr=a(co);zt=i(qr,"Scalable Transformers"),qr.forEach(t),Pt=i(A,` #TODO add link to the paper.
An encoder model is used on the lyrics, on the first (also called `),mo=r(A,"CODE",{});var Ar=a(mo);jt=i(Ar,"top_prior"),Ar.forEach(t),Nt=i(A,") prior\u2019s decoder attends to the last lyric hidden states. Each prior is linked to the previous by an "),po=r(A,"CODE",{});var zr=a(po);Dt=i(zr,"AudioConditionner"),zr.forEach(t),Vt=i(A," module which takes care of upsampling the generated hidden state to the correct "),ho=r(A,"CODE",{});var Pr=a(ho);Mt=i(Pr,"raw_to_token"),Pr.forEach(t),It=i(A,` resolution.
The metadatas such as `),fo=r(A,"EM",{});var jr=a(fo);Ft=i(jr,"artist, genre and timing"),jr.forEach(t),Ot=i(A," are passed to each prior, in the form of a start token and positionnal embedding for the timing data.  The hidden states are mapped to the closest codebook vector from the VQVAE in order to convert them to raw audio."),A.forEach(t),Uo=c(o),Fe=r(o,"P",{});var Nr=a(Fe);Oe=r(Nr,"IMG",{src:!0,alt:!0}),Nr.forEach(t),Ho=c(o),Se=r(o,"P",{});var Dr=a(Se);St=i(Dr,"Tips:"),Dr.forEach(t),Ro=c(o),B=r(o,"UL",{});var st=a(B);uo=r(st,"LI",{});var Vr=a(uo);Lt=i(Vr,"This model is very slow, and takes 8h to generate a minute long audio using the 5b top prior."),Vr.forEach(t),Wt=c(st),F=r(st,"LI",{});var oo=a(F);Qt=i(oo,"Primed sampling requires more memory than ancestral sampling and should be used with "),_o=r(oo,"CODE",{});var Mr=a(_o);Ut=i(Mr,"fp16"),Mr.forEach(t),Ht=i(oo," set to "),go=r(oo,"CODE",{});var Ir=a(go);Rt=i(Ir,"True"),Ir.forEach(t),Bt=i(oo,"."),oo.forEach(t),st.forEach(t),Bo=c(o),V=r(o,"P",{});var to=a(V);Gt=i(to,"This model was contributed by "),ie=r(to,"A",{href:!0,rel:!0});var Fr=a(ie);Zt=i(Fr,"Arthur Zucker"),Fr.forEach(t),Kt=i(to,`.
The original code can be found `),le=r(to,"A",{href:!0,rel:!0});var Or=a(le);Xt=i(Or,"here"),Or.forEach(t),Yt=i(to,"."),to.forEach(t),Go=c(o),O=r(o,"H2",{class:!0});var it=a(O);G=r(it,"A",{id:!0,class:!0,href:!0});var Sr=a(G);vo=r(Sr,"SPAN",{});var Lr=a(vo);g(de.$$.fragment,Lr),Lr.forEach(t),Sr.forEach(t),en=c(it),bo=r(it,"SPAN",{});var Wr=a(bo);on=i(Wr,"JukeboxConfig"),Wr.forEach(t),it.forEach(t),Zo=c(o),q=r(o,"DIV",{class:!0});var M=a(q);g(ce.$$.fragment,M),tn=c(M),me=r(M,"P",{});var lt=a(me);nn=i(lt,"This is the configuration class to store the configuration of a "),Le=r(lt,"A",{href:!0});var Qr=a(Le);rn=i(Qr,"JukeboxModel"),Qr.forEach(t),an=i(lt,"."),lt.forEach(t),sn=c(M),j=r(M,"P",{});var ne=a(j);ln=i(ne,"Configuration objects inherit from "),We=r(ne,"A",{href:!0});var Ur=a(We);dn=i(Ur,"PretrainedConfig"),Ur.forEach(t),cn=i(ne,` and can be used to control the model outputs. Read the
documentation from `),Qe=r(ne,"A",{href:!0});var Hr=a(Qe);mn=i(Hr,"PretrainedConfig"),Hr.forEach(t),pn=i(ne,` for more information. Instantiating a configuration with the defaults will
yield a similar configuration to that of
`),pe=r(ne,"A",{href:!0,rel:!0});var Rr=a(pe);hn=i(Rr,"openai/jukebox-1b-lyrics"),Rr.forEach(t),fn=i(ne," architecture."),ne.forEach(t),un=c(M),he=r(M,"P",{});var dt=a(he);_n=i(dt,`The downsampling and stride are used to determine downsampling of the input sequence. For example, downsamoling =
(5,3), and strides = (2, 2) will downsample the audio by 2`),ko=r(dt,"STRONG",{});var Br=a(ko);gn=i(Br,"5 = 32 to get the first level of codes, and 2"),Br.forEach(t),vn=i(dt,`8 = 256
to get the second level codes. This is mostly true for training the top level prior and the upsamplers.`),dt.forEach(t),bn=c(M),g(Z.$$.fragment,M),M.forEach(t),Ko=c(o),S=r(o,"H2",{class:!0});var ct=a(S);K=r(ct,"A",{id:!0,class:!0,href:!0});var Gr=a(K);xo=r(Gr,"SPAN",{});var Zr=a(xo);g(fe.$$.fragment,Zr),Zr.forEach(t),Gr.forEach(t),kn=c(ct),yo=r(ct,"SPAN",{});var Kr=a(yo);xn=i(Kr,"JukeboxTokenizer"),Kr.forEach(t),ct.forEach(t),Xo=c(o),u=r(o,"DIV",{class:!0});var J=a(u);g(ue.$$.fragment,J),yn=c(J),wo=r(J,"P",{});var Xr=a(wo);wn=i(Xr,"Constructs a Jukebox tokenizer. Jukebox can be conditioned on 3 different inputs :"),Xr.forEach(t),$n=c(J),L=r(J,"UL",{});var no=a(L);$o=r(no,"LI",{});var Yr=a($o);Jn=i(Yr,"Artists, unique ids are associated to each artist from the provided dictionary."),Yr.forEach(t),En=c(no),Jo=r(no,"LI",{});var ea=a(Jo);Cn=i(ea,"Genres, unique ids are associated to each genre from the provided dictionary."),ea.forEach(t),Tn=c(no),Eo=r(no,"LI",{});var oa=a(Eo);qn=i(oa,`Lyrics, character based tokenization. Must be initialized with the list of characters that are inside the
vocabulary.`),oa.forEach(t),no.forEach(t),An=c(J),Co=r(J,"P",{});var ta=a(Co);zn=i(ta,`This tokenizer is straight forward and does not require trainingg. It should be able to process a different number of inputs:
as the conditioning of the model can be done on the three different queries. If None is provided, defaults values will be used.:`),ta.forEach(t),Pn=c(J),_e=r(J,"P",{});var mt=a(_e);jn=i(mt,"Depending on the number of genres on which the model should be conditioned ("),To=r(mt,"CODE",{});var na=a(To);Nn=i(na,"n_genres"),na.forEach(t),Dn=i(mt,")."),mt.forEach(t),Vn=c(J),g(X.$$.fragment,J),Mn=c(J),ge=r(J,"P",{});var pt=a(ge);In=i(pt,"You can get around that behavior by passing "),qo=r(pt,"CODE",{});var ra=a(qo);Fn=i(ra,"add_prefix_space=True"),ra.forEach(t),On=i(pt,` when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.`),pt.forEach(t),Sn=c(J),g(Y.$$.fragment,J),Ln=c(J),ve=r(J,"P",{});var ht=a(ve);Wn=i(ht,"This tokenizer inherits from "),Ue=r(ht,"A",{href:!0});var aa=a(Ue);Qn=i(aa,"PreTrainedTokenizer"),aa.forEach(t),Un=i(ht,` which contains most of the main methods. Users should refer to:
this superclass for more information regarding those methods.`),ht.forEach(t),Hn=c(J),Ao=r(J,"P",{});var sa=a(Ao);Rn=i(sa,"However the code does not allow that and only supports composing from various genres."),sa.forEach(t),Bn=c(J),ee=r(J,"DIV",{class:!0});var ft=a(ee);g(be.$$.fragment,ft),Gn=c(ft),zo=r(ft,"P",{});var ia=a(zo);Zn=i(ia,"Saves the tokenizer\u2019s vocabulary dictionnary to the provided save_directory."),ia.forEach(t),ft.forEach(t),J.forEach(t),Yo=c(o),W=r(o,"H2",{class:!0});var ut=a(W);oe=r(ut,"A",{id:!0,class:!0,href:!0});var la=a(oe);Po=r(la,"SPAN",{});var da=a(Po);g(ke.$$.fragment,da),da.forEach(t),la.forEach(t),Kn=c(ut),jo=r(ut,"SPAN",{});var ca=a(jo);Xn=i(ca,"JukeboxModel"),ca.forEach(t),ut.forEach(t),et=c(o),$=r(o,"DIV",{class:!0});var C=a($);g(xe.$$.fragment,C),Yn=c(C),No=r(C,"P",{});var ma=a(No);er=i(ma,"The bare JUKEBOX Model from which you can sample"),ma.forEach(t),or=c(C),ye=r(C,"P",{});var _t=a(ye);tr=i(_t,"This model inherits from "),He=r(_t,"A",{href:!0});var pa=a(He);nr=i(pa,"PreTrainedModel"),pa.forEach(t),rr=i(_t,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_t.forEach(t),ar=c(C),we=r(C,"P",{});var gt=a(we);sr=i(gt,"This model is also a PyTorch "),$e=r(gt,"A",{href:!0,rel:!0});var ha=a($e);ir=i(ha,"torch.nn.Module"),ha.forEach(t),lr=i(gt,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),gt.forEach(t),dr=c(C),Re=r(C,"DIV",{class:!0});var fa=a(Re);g(Je.$$.fragment,fa),fa.forEach(t),cr=c(C),Be=r(C,"DIV",{class:!0});var ua=a(Be);g(Ee.$$.fragment,ua),ua.forEach(t),mr=c(C),Ge=r(C,"DIV",{class:!0});var _a=a(Ge);g(Ce.$$.fragment,_a),_a.forEach(t),pr=c(C),Ze=r(C,"DIV",{class:!0});var ga=a(Ze);g(Te.$$.fragment,ga),ga.forEach(t),hr=c(C),Ke=r(C,"DIV",{class:!0});var va=a(Ke);g(qe.$$.fragment,va),va.forEach(t),C.forEach(t),ot=c(o),Q=r(o,"H2",{class:!0});var vt=a(Q);te=r(vt,"A",{id:!0,class:!0,href:!0});var ba=a(te);Do=r(ba,"SPAN",{});var ka=a(Do);g(Ae.$$.fragment,ka),ka.forEach(t),ba.forEach(t),fr=c(vt),Vo=r(vt,"SPAN",{});var xa=a(Vo);ur=i(xa,"JukeboxVQVAE"),xa.forEach(t),vt.forEach(t),tt=c(o),z=r(o,"DIV",{class:!0});var re=a(z);g(ze.$$.fragment,re),_r=c(re),Xe=r(re,"DIV",{class:!0});var ya=a(Xe);g(Pe.$$.fragment,ya),ya.forEach(t),gr=c(re),Ye=r(re,"DIV",{class:!0});var wa=a(Ye);g(je.$$.fragment,wa),wa.forEach(t),vr=c(re),eo=r(re,"DIV",{class:!0});var $a=a(eo);g(Ne.$$.fragment,$a),$a.forEach(t),re.forEach(t),this.h()},h(){l(h,"name","hf:doc:metadata"),l(h,"content",JSON.stringify(Fa)),l(y,"id","jukebox"),l(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(y,"href","#jukebox"),l(f,"class","relative group"),l(H,"id","overview"),l(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(H,"href","#overview"),l(I,"class","relative group"),l(se,"href","https://arxiv.org/pdf/2005.00341.pdf"),l(se,"rel","nofollow"),Pa(Oe.src,br="https://gist.githubusercontent.com/ArthurZucker/92c1acaae62ebf1b6a951710bdd8b6af/raw/2a75e9ab330ef27ae6565fcf28b129e033ff9bed/Jukebox.svg")||l(Oe,"src",br),l(Oe,"alt","JukeboxModel"),l(ie,"href","https://huggingface.co/ArthurZ"),l(ie,"rel","nofollow"),l(le,"href","https://github.com/openai/jukebox"),l(le,"rel","nofollow"),l(G,"id","transformers.JukeboxConfig"),l(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(G,"href","#transformers.JukeboxConfig"),l(O,"class","relative group"),l(Le,"href","/docs/transformers/pr_17826/en/model_doc/jukebox#transformers.JukeboxModel"),l(We,"href","/docs/transformers/pr_17826/en/main_classes/configuration#transformers.PretrainedConfig"),l(Qe,"href","/docs/transformers/pr_17826/en/main_classes/configuration#transformers.PretrainedConfig"),l(pe,"href","https://huggingface.co/openai/jukebox-1b-lyrics"),l(pe,"rel","nofollow"),l(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(K,"id","transformers.JukeboxTokenizer"),l(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(K,"href","#transformers.JukeboxTokenizer"),l(S,"class","relative group"),l(Ue,"href","/docs/transformers/pr_17826/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),l(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(u,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(oe,"id","transformers.JukeboxModel"),l(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(oe,"href","#transformers.JukeboxModel"),l(W,"class","relative group"),l(He,"href","/docs/transformers/pr_17826/en/main_classes/model#transformers.PreTrainedModel"),l($e,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l($e,"rel","nofollow"),l(Re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(te,"id","transformers.JukeboxVQVAE"),l(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(te,"href","#transformers.JukeboxVQVAE"),l(Q,"class","relative group"),l(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(eo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,m){e(document.head,h),p(o,T,m),p(o,f,m),e(f,y),e(y,N),v(w,N,null),e(f,D),e(f,ao),e(ao,bt),p(o,Fo,m),p(o,I,m),e(I,H),e(H,so),v(ae,so,null),e(I,kt),e(I,io),e(io,xt),p(o,Oo,m),p(o,R,m),e(R,yt),e(R,se),e(se,wt),e(R,$t),p(o,So,m),p(o,Ve,m),e(Ve,Jt),p(o,Lo,m),p(o,Me,m),e(Me,Et),p(o,Wo,m),p(o,Ie,m),e(Ie,Ct),p(o,Qo,m),p(o,E,m),e(E,Tt),e(E,lo),e(lo,qt),e(E,At),e(E,co),e(co,zt),e(E,Pt),e(E,mo),e(mo,jt),e(E,Nt),e(E,po),e(po,Dt),e(E,Vt),e(E,ho),e(ho,Mt),e(E,It),e(E,fo),e(fo,Ft),e(E,Ot),p(o,Uo,m),p(o,Fe,m),e(Fe,Oe),p(o,Ho,m),p(o,Se,m),e(Se,St),p(o,Ro,m),p(o,B,m),e(B,uo),e(uo,Lt),e(B,Wt),e(B,F),e(F,Qt),e(F,_o),e(_o,Ut),e(F,Ht),e(F,go),e(go,Rt),e(F,Bt),p(o,Bo,m),p(o,V,m),e(V,Gt),e(V,ie),e(ie,Zt),e(V,Kt),e(V,le),e(le,Xt),e(V,Yt),p(o,Go,m),p(o,O,m),e(O,G),e(G,vo),v(de,vo,null),e(O,en),e(O,bo),e(bo,on),p(o,Zo,m),p(o,q,m),v(ce,q,null),e(q,tn),e(q,me),e(me,nn),e(me,Le),e(Le,rn),e(me,an),e(q,sn),e(q,j),e(j,ln),e(j,We),e(We,dn),e(j,cn),e(j,Qe),e(Qe,mn),e(j,pn),e(j,pe),e(pe,hn),e(j,fn),e(q,un),e(q,he),e(he,_n),e(he,ko),e(ko,gn),e(he,vn),e(q,bn),v(Z,q,null),p(o,Ko,m),p(o,S,m),e(S,K),e(K,xo),v(fe,xo,null),e(S,kn),e(S,yo),e(yo,xn),p(o,Xo,m),p(o,u,m),v(ue,u,null),e(u,yn),e(u,wo),e(wo,wn),e(u,$n),e(u,L),e(L,$o),e($o,Jn),e(L,En),e(L,Jo),e(Jo,Cn),e(L,Tn),e(L,Eo),e(Eo,qn),e(u,An),e(u,Co),e(Co,zn),e(u,Pn),e(u,_e),e(_e,jn),e(_e,To),e(To,Nn),e(_e,Dn),e(u,Vn),v(X,u,null),e(u,Mn),e(u,ge),e(ge,In),e(ge,qo),e(qo,Fn),e(ge,On),e(u,Sn),v(Y,u,null),e(u,Ln),e(u,ve),e(ve,Wn),e(ve,Ue),e(Ue,Qn),e(ve,Un),e(u,Hn),e(u,Ao),e(Ao,Rn),e(u,Bn),e(u,ee),v(be,ee,null),e(ee,Gn),e(ee,zo),e(zo,Zn),p(o,Yo,m),p(o,W,m),e(W,oe),e(oe,Po),v(ke,Po,null),e(W,Kn),e(W,jo),e(jo,Xn),p(o,et,m),p(o,$,m),v(xe,$,null),e($,Yn),e($,No),e(No,er),e($,or),e($,ye),e(ye,tr),e(ye,He),e(He,nr),e(ye,rr),e($,ar),e($,we),e(we,sr),e(we,$e),e($e,ir),e(we,lr),e($,dr),e($,Re),v(Je,Re,null),e($,cr),e($,Be),v(Ee,Be,null),e($,mr),e($,Ge),v(Ce,Ge,null),e($,pr),e($,Ze),v(Te,Ze,null),e($,hr),e($,Ke),v(qe,Ke,null),p(o,ot,m),p(o,Q,m),e(Q,te),e(te,Do),v(Ae,Do,null),e(Q,fr),e(Q,Vo),e(Vo,ur),p(o,tt,m),p(o,z,m),v(ze,z,null),e(z,_r),e(z,Xe),v(Pe,Xe,null),e(z,gr),e(z,Ye),v(je,Ye,null),e(z,vr),e(z,eo),v(Ne,eo,null),nt=!0},p(o,[m]){const De={};m&2&&(De.$$scope={dirty:m,ctx:o}),Z.$set(De);const Mo={};m&2&&(Mo.$$scope={dirty:m,ctx:o}),X.$set(Mo);const Io={};m&2&&(Io.$$scope={dirty:m,ctx:o}),Y.$set(Io)},i(o){nt||(b(w.$$.fragment,o),b(ae.$$.fragment,o),b(de.$$.fragment,o),b(ce.$$.fragment,o),b(Z.$$.fragment,o),b(fe.$$.fragment,o),b(ue.$$.fragment,o),b(X.$$.fragment,o),b(Y.$$.fragment,o),b(be.$$.fragment,o),b(ke.$$.fragment,o),b(xe.$$.fragment,o),b(Je.$$.fragment,o),b(Ee.$$.fragment,o),b(Ce.$$.fragment,o),b(Te.$$.fragment,o),b(qe.$$.fragment,o),b(Ae.$$.fragment,o),b(ze.$$.fragment,o),b(Pe.$$.fragment,o),b(je.$$.fragment,o),b(Ne.$$.fragment,o),nt=!0)},o(o){k(w.$$.fragment,o),k(ae.$$.fragment,o),k(de.$$.fragment,o),k(ce.$$.fragment,o),k(Z.$$.fragment,o),k(fe.$$.fragment,o),k(ue.$$.fragment,o),k(X.$$.fragment,o),k(Y.$$.fragment,o),k(be.$$.fragment,o),k(ke.$$.fragment,o),k(xe.$$.fragment,o),k(Je.$$.fragment,o),k(Ee.$$.fragment,o),k(Ce.$$.fragment,o),k(Te.$$.fragment,o),k(qe.$$.fragment,o),k(Ae.$$.fragment,o),k(ze.$$.fragment,o),k(Pe.$$.fragment,o),k(je.$$.fragment,o),k(Ne.$$.fragment,o),nt=!1},d(o){t(h),o&&t(T),o&&t(f),x(w),o&&t(Fo),o&&t(I),x(ae),o&&t(Oo),o&&t(R),o&&t(So),o&&t(Ve),o&&t(Lo),o&&t(Me),o&&t(Wo),o&&t(Ie),o&&t(Qo),o&&t(E),o&&t(Uo),o&&t(Fe),o&&t(Ho),o&&t(Se),o&&t(Ro),o&&t(B),o&&t(Bo),o&&t(V),o&&t(Go),o&&t(O),x(de),o&&t(Zo),o&&t(q),x(ce),x(Z),o&&t(Ko),o&&t(S),x(fe),o&&t(Xo),o&&t(u),x(ue),x(X),x(Y),x(be),o&&t(Yo),o&&t(W),x(ke),o&&t(et),o&&t($),x(xe),x(Je),x(Ee),x(Ce),x(Te),x(qe),o&&t(ot),o&&t(Q),x(Ae),o&&t(tt),o&&t(z),x(ze),x(Pe),x(je),x(Ne)}}}const Fa={local:"jukebox",sections:[{local:"overview",title:"Overview"},{local:"transformers.JukeboxConfig",title:"JukeboxConfig"},{local:"transformers.JukeboxTokenizer",title:"JukeboxTokenizer"},{local:"transformers.JukeboxModel",title:"JukeboxModel"},{local:"transformers.JukeboxVQVAE",title:"JukeboxVQVAE "}],title:"Jukebox"};function Oa(U){return ja(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ra extends Ta{constructor(h){super();qa(this,h,Oa,Ia,Aa,{})}}export{Ra as default,Fa as metadata};
