import{S as Ra,i as Ja,s as Ha,e as a,k as l,w as b,t as s,M as Ba,c as o,d as r,m as d,a as n,x as y,h as i,b as p,G as e,g as v,y as $,q as E,o as w,B as F,v as Ga,L as Ca}from"../../chunks/vendor-hf-doc-builder.js";import{T as Va}from"../../chunks/Tip-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ua}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as da}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Wa}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ya(j){let m,k,u,f,M;return{c(){m=a("p"),k=s("Passing "),u=a("code"),f=s("use_auth_token=True"),M=s(" is required when you want to use a private model.")},l(c){m=o(c,"P",{});var g=n(m);k=i(g,"Passing "),u=o(g,"CODE",{});var U=n(u);f=i(U,"use_auth_token=True"),U.forEach(r),M=i(g," is required when you want to use a private model."),g.forEach(r)},m(c,g){v(c,m,g),e(m,k),e(m,u),e(u,f),e(m,M)},d(c){c&&r(m)}}}function Ka(j){let m,k,u,f,M;return f=new Ua({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){m=a("p"),k=s("Examples:"),u=l(),b(f.$$.fragment)},l(c){m=o(c,"P",{});var g=n(m);k=i(g,"Examples:"),g.forEach(r),u=d(c),y(f.$$.fragment,c)},m(c,g){v(c,m,g),e(m,k),v(c,u,g),$(f,c,g),M=!0},p:Ca,i(c){M||(E(f.$$.fragment,c),M=!0)},o(c){w(f.$$.fragment,c),M=!1},d(c){c&&r(m),c&&r(u),F(f,c)}}}function Qa(j){let m,k,u,f,M;return f=new Ua({props:{code:`from transformers import AutoFeatureExtractor

feature extractor = AutoFeatureExtractor.from_pretrained("bert-base-cased")

# Push the feature extractor to your namespace with the name "my-finetuned-bert".
feature extractor.push_to_hub("my-finetuned-bert")

# Push the feature extractor to an organization with the name "my-finetuned-bert".
feature extractor.push_to_hub("huggingface/my-finetuned-bert")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

feature extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-comment"># Push the feature extractor to your namespace with the name &quot;my-finetuned-bert&quot;.</span>
feature extractor.push_to_hub(<span class="hljs-string">&quot;my-finetuned-bert&quot;</span>)

<span class="hljs-comment"># Push the feature extractor to an organization with the name &quot;my-finetuned-bert&quot;.</span>
feature extractor.push_to_hub(<span class="hljs-string">&quot;huggingface/my-finetuned-bert&quot;</span>)`}}),{c(){m=a("p"),k=s("Examples:"),u=l(),b(f.$$.fragment)},l(c){m=o(c,"P",{});var g=n(m);k=i(g,"Examples:"),g.forEach(r),u=d(c),y(f.$$.fragment,c)},m(c,g){v(c,m,g),e(m,k),v(c,u,g),$(f,c,g),M=!0},p:Ca,i(c){M||(E(f.$$.fragment,c),M=!0)},o(c){w(f.$$.fragment,c),M=!1},d(c){c&&r(m),c&&r(u),F(f,c)}}}function Xa(j){let m,k;return{c(){m=a("p"),k=s("This API is experimental and may have some slight breaking changes in the next releases.")},l(u){m=o(u,"P",{});var f=n(m);k=i(f,"This API is experimental and may have some slight breaking changes in the next releases."),f.forEach(r)},m(u,f){v(u,m,f),e(m,k)},d(u){u&&r(m)}}}function Za(j){let m,k,u,f,M,c,g,U,At,lt,Pe,Lt,dt,Te,St,mt,A,R,We,se,Nt,Ce,Ot,pt,L,ie,Vt,S,Wt,Ue,Ct,Ut,Re,Rt,Jt,ft,N,ce,Ht,O,Bt,Je,Gt,Yt,He,Kt,Qt,ut,V,le,Xt,de,Zt,Be,er,tr,ht,W,J,Ge,me,rr,Ye,ar,_t,_,pe,or,Ke,nr,sr,H,fe,ir,ue,cr,Ie,lr,dr,mr,B,he,pr,_e,fr,De,ur,hr,_r,T,ge,gr,I,xr,je,vr,br,Qe,yr,$r,qe,Er,wr,Fr,G,kr,Y,Mr,K,xe,Pr,D,Tr,Xe,Ir,Dr,ze,jr,qr,Ze,zr,Ar,Lr,q,ve,Sr,be,Nr,et,Or,Vr,Wr,Q,Cr,z,ye,Ur,$e,Rr,tt,Jr,Hr,Br,X,Gr,Z,Ee,Yr,C,Kr,rt,Qr,Xr,Ae,Zr,ea,ta,ee,we,ra,at,aa,oa,te,Fe,na,ot,sa,ia,re,ke,ca,nt,la,gt;return c=new da({}),se=new da({}),ie=new P({props:{name:"transformers.rescale",anchor:"transformers.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": typing.Union[float, int] = 255"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"dtype",val:" = <class 'numpy.float32'>"}],parametersDescription:[{anchor:"transformers.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to rescale.`,name:"image"},{anchor:"transformers.rescale.scale",description:`<strong>scale</strong> (<code>float</code> or <code>int</code>, <em>optional</em>, defaults to 255) &#x2014;
The scale to use for rescaling the image.`,name:"scale"},{anchor:"transformers.rescale.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"},{anchor:"transformers.rescale.dtype",description:`<strong>dtype</strong> (<code>np.dtype</code>, <em>optional</em>, defaults to <code>np.float32</code>) &#x2014;
The dtype of the output image. Defaults to <code>np.float32</code>. Used for backwards compatibility with feature
extractors.`,name:"dtype"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/image_transforms.py#L76",returnDescription:`
<p>A rescaled np.ndarray image.</p>
`,returnType:`
<p>image</p>
`}}),ce=new P({props:{name:"transformers.resize",anchor:"transformers.resize",parameters:[{name:"image",val:""},{name:"size",val:": typing.Tuple[int, int]"},{name:"resample",val:" = <Resampling.BILINEAR: 2>"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"return_numpy",val:": bool = True"}],parametersDescription:[{anchor:"transformers.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.resize.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image.`,name:"size"},{anchor:"transformers.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The channel dimension format of the output image. If <code>None</code>, will use the inferred format from the input.`,name:"data_format"},{anchor:"transformers.resize.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return the resized image as a numpy array. If False a PIL.Image.Image object is returned.`,name:"return_numpy"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/image_transforms.py#L208",returnDescription:`
<p>A resized np.ndarray.</p>
`,returnType:`
<p>image</p>
`}}),le=new P({props:{name:"transformers.to_pil_image",anchor:"transformers.to_pil_image",parameters:[{name:"image",val:": typing.Union[numpy.ndarray, PIL.Image.Image, ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor'), ForwardRef('jnp.Tensor')]"},{name:"do_rescale",val:" = None"}],parametersDescription:[{anchor:"transformers.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code>, <code>numpy.ndarray</code>, <code>torch.Tensor</code>, <code>tf.Tensor</code>) &#x2014;
The image to convert to the PIL Image format.`,name:"image"},{anchor:"transformers.to_pil_image.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default
to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/image_transforms.py#L106"}}),me=new da({}),pe=new P({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L196"}}),fe=new P({props:{name:"from_dict",anchor:"transformers.FeatureExtractionMixin.from_dict",parameters:[{name:"feature_extractor_dict",val:": typing.Dict[str, typing.Any]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_dict.feature_extractor_dict",description:`<strong>feature_extractor_dict</strong> (<code>Dict[str, Any]</code>) &#x2014;
Dictionary that will be used to instantiate the feature extractor object. Such a dictionary can be
retrieved from a pretrained checkpoint by leveraging the
<a href="/docs/transformers/pr_18520/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.to_dict">to_dict()</a> method.`,name:"feature_extractor_dict"},{anchor:"transformers.FeatureExtractionMixin.from_dict.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>) &#x2014;
Additional parameters from which to initialize the feature extractor object.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L438",returnDescription:`
<p>The feature extractor object instantiated from those
parameters.</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a></p>
`}}),he=new P({props:{name:"from_json_file",anchor:"transformers.FeatureExtractionMixin.from_json_file",parameters:[{name:"json_file",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_json_file.json_file",description:`<strong>json_file</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file containing the parameters.`,name:"json_file"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L487",returnDescription:`
<p>The feature_extractor
object instantiated from that JSON file.</p>
`,returnType:`
<p>A feature extractor of type <a
  href="/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a></p>
`}}),ge=new P({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L220",returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),G=new Va({props:{$$slots:{default:[Ya]},$$scope:{ctx:j}}}),Y=new Wa({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[Ka]},$$scope:{ctx:j}}}),xe=new P({props:{name:"get_feature_extractor_dict",anchor:"transformers.FeatureExtractionMixin.get_feature_extractor_dict",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.get_feature_extractor_dict.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.`,name:"pretrained_model_name_or_path"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L348",returnDescription:`
<p>The dictionary(ies) that will be used to instantiate the feature extractor object.</p>
`,returnType:`
<p><code>Tuple[Dict, Dict]</code></p>
`}}),ve=new P({props:{name:"push_to_hub",anchor:"transformers.FeatureExtractionMixin.push_to_hub",parameters:[{name:"repo_id",val:": str"},{name:"use_temp_dir",val:": typing.Optional[bool] = None"},{name:"commit_message",val:": typing.Optional[str] = None"},{name:"private",val:": typing.Optional[bool] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"max_shard_size",val:": typing.Union[int, str, NoneType] = '10GB'"},{name:"create_pr",val:": bool = False"},{name:"**deprecated_kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.push_to_hub.repo_id",description:`<strong>repo_id</strong> (<code>str</code>) &#x2014;
The name of the repository you want to push your feature extractor to. It should contain your organization name
when pushing to a given organization.`,name:"repo_id"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.use_temp_dir",description:`<strong>use_temp_dir</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.
Will default to <code>True</code> if there is no directory named like <code>repo_id</code>, <code>False</code> otherwise.`,name:"use_temp_dir"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.commit_message",description:`<strong>commit_message</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Message to commit while pushing. Will default to <code>&quot;Upload feature extractor&quot;</code>.`,name:"commit_message"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.private",description:`<strong>private</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not the repository created should be private (requires a paying subscription).`,name:"private"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code> or <code>str</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>). Will default to <code>True</code> if <code>repo_url</code>
is not specified.`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.max_shard_size",description:`<strong>max_shard_size</strong> (<code>int</code> or <code>str</code>, <em>optional</em>, defaults to <code>&quot;10GB&quot;</code>) &#x2014;
Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard
will then be each of size lower than this size. If expressed as a string, needs to be digits followed
by a unit (like <code>&quot;5MB&quot;</code>).`,name:"max_shard_size"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.create_pr",description:`<strong>create_pr</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to create a PR with the uploaded files or directly commit.`,name:"create_pr"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/utils/hub.py#L661"}}),Q=new Wa({props:{anchor:"transformers.FeatureExtractionMixin.push_to_hub.example",$$slots:{default:[Qa]},$$scope:{ctx:j}}}),ye=new P({props:{name:"register_for_auto_class",anchor:"transformers.FeatureExtractionMixin.register_for_auto_class",parameters:[{name:"auto_class",val:" = 'AutoFeatureExtractor'"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.register_for_auto_class.auto_class",description:`<strong>auto_class</strong> (<code>str</code> or <code>type</code>, <em>optional</em>, defaults to <code>&quot;AutoFeatureExtractor&quot;</code>) &#x2014;
The auto class to register this new feature extractor with.`,name:"auto_class"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L541"}}),X=new Va({props:{warning:!0,$$slots:{default:[Xa]},$$scope:{ctx:j}}}),Ee=new P({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/pr_18520/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L304"}}),we=new P({props:{name:"to_dict",anchor:"transformers.FeatureExtractionMixin.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L475",returnDescription:`
<p>Dictionary of all the attributes that make up this feature extractor instance.</p>
`,returnType:`
<p><code>Dict[str, Any]</code></p>
`}}),Fe=new P({props:{name:"to_json_file",anchor:"transformers.FeatureExtractionMixin.to_json_file",parameters:[{name:"json_file_path",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.to_json_file.json_file_path",description:`<strong>json_file_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file in which this feature_extractor instance&#x2019;s parameters will be saved.`,name:"json_file_path"}],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L527"}}),ke=new P({props:{name:"to_json_string",anchor:"transformers.FeatureExtractionMixin.to_json_string",parameters:[],source:"https://github.com/huggingface/transformers/blob/vr_18520/src/transformers/feature_extraction_utils.py#L506",returnDescription:`
<p>String containing all the attributes that make up this feature_extractor instance in JSON format.</p>
`,returnType:`
<p><code>str</code></p>
`}}),{c(){m=a("meta"),k=l(),u=a("h1"),f=a("a"),M=a("span"),b(c.$$.fragment),g=l(),U=a("span"),At=s("Utilities for Image Processors"),lt=l(),Pe=a("p"),Lt=s(`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),dt=l(),Te=a("p"),St=s("Most of those are only useful if you are studying the code of the image processors in the library."),mt=l(),A=a("h2"),R=a("a"),We=a("span"),b(se.$$.fragment),Nt=l(),Ce=a("span"),Ot=s("Image Transformations"),pt=l(),L=a("div"),b(ie.$$.fragment),Vt=l(),S=a("p"),Wt=s("Rescales "),Ue=a("code"),Ct=s("image"),Ut=s(" by "),Re=a("code"),Rt=s("scale"),Jt=s("."),ft=l(),N=a("div"),b(ce.$$.fragment),Ht=l(),O=a("p"),Bt=s("Resizes "),Je=a("code"),Gt=s("image"),Yt=s(" to (h, w) specified by "),He=a("code"),Kt=s("size"),Qt=s(" using the PIL library."),ut=l(),V=a("div"),b(le.$$.fragment),Xt=l(),de=a("p"),Zt=s("Converts "),Be=a("code"),er=s("image"),tr=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),ht=l(),W=a("h2"),J=a("a"),Ge=a("span"),b(me.$$.fragment),rr=l(),Ye=a("span"),ar=s("ImageProcessorMixin"),_t=l(),_=a("div"),b(pe.$$.fragment),or=l(),Ke=a("p"),nr=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),sr=l(),H=a("div"),b(fe.$$.fragment),ir=l(),ue=a("p"),cr=s("Instantiates a type of "),Ie=a("a"),lr=s("FeatureExtractionMixin"),dr=s(` from a Python dictionary of
parameters.`),mr=l(),B=a("div"),b(he.$$.fragment),pr=l(),_e=a("p"),fr=s("Instantiates a feature extractor of type "),De=a("a"),ur=s("FeatureExtractionMixin"),hr=s(` from the path to
a JSON file of parameters.`),_r=l(),T=a("div"),b(ge.$$.fragment),gr=l(),I=a("p"),xr=s("Instantiate a type of "),je=a("a"),vr=s("FeatureExtractionMixin"),br=s(" from a feature extractor, "),Qe=a("em"),yr=s("e.g."),$r=s(` a
derived class of `),qe=a("a"),Er=s("SequenceFeatureExtractor"),wr=s("."),Fr=l(),b(G.$$.fragment),kr=l(),b(Y.$$.fragment),Mr=l(),K=a("div"),b(xe.$$.fragment),Pr=l(),D=a("p"),Tr=s("From a "),Xe=a("code"),Ir=s("pretrained_model_name_or_path"),Dr=s(`, resolve to a dictionary of parameters, to be used for instantiating a
feature extractor of type `),ze=a("a"),jr=s("FeatureExtractionMixin"),qr=s(" using "),Ze=a("code"),zr=s("from_dict"),Ar=s("."),Lr=l(),q=a("div"),b(ve.$$.fragment),Sr=l(),be=a("p"),Nr=s(`Upload the feature extractor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),et=a("code"),Or=s("repo_path_or_name"),Vr=s("."),Wr=l(),b(Q.$$.fragment),Cr=l(),z=a("div"),b(ye.$$.fragment),Ur=l(),$e=a("p"),Rr=s(`Register this class with a given auto class. This should only be used for custom feature extractors as the ones
in the library are already mapped with `),tt=a("code"),Jr=s("AutoFeatureExtractor"),Hr=s("."),Br=l(),b(X.$$.fragment),Gr=l(),Z=a("div"),b(Ee.$$.fragment),Yr=l(),C=a("p"),Kr=s("Save a feature_extractor object to the directory "),rt=a("code"),Qr=s("save_directory"),Xr=s(`, so that it can be re-loaded using the
`),Ae=a("a"),Zr=s("from_pretrained()"),ea=s(" class method."),ta=l(),ee=a("div"),b(we.$$.fragment),ra=l(),at=a("p"),aa=s("Serializes this instance to a Python dictionary."),oa=l(),te=a("div"),b(Fe.$$.fragment),na=l(),ot=a("p"),sa=s("Save this instance to a JSON file."),ia=l(),re=a("div"),b(ke.$$.fragment),ca=l(),nt=a("p"),la=s("Serializes this instance to a JSON string."),this.h()},l(t){const h=Ba('[data-svelte="svelte-1phssyn"]',document.head);m=o(h,"META",{name:!0,content:!0}),h.forEach(r),k=d(t),u=o(t,"H1",{class:!0});var Me=n(u);f=o(Me,"A",{id:!0,class:!0,href:!0});var st=n(f);M=o(st,"SPAN",{});var it=n(M);y(c.$$.fragment,it),it.forEach(r),st.forEach(r),g=d(Me),U=o(Me,"SPAN",{});var ct=n(U);At=i(ct,"Utilities for Image Processors"),ct.forEach(r),Me.forEach(r),lt=d(t),Pe=o(t,"P",{});var ma=n(Pe);Lt=i(ma,`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),ma.forEach(r),dt=d(t),Te=o(t,"P",{});var pa=n(Te);St=i(pa,"Most of those are only useful if you are studying the code of the image processors in the library."),pa.forEach(r),mt=d(t),A=o(t,"H2",{class:!0});var xt=n(A);R=o(xt,"A",{id:!0,class:!0,href:!0});var fa=n(R);We=o(fa,"SPAN",{});var ua=n(We);y(se.$$.fragment,ua),ua.forEach(r),fa.forEach(r),Nt=d(xt),Ce=o(xt,"SPAN",{});var ha=n(Ce);Ot=i(ha,"Image Transformations"),ha.forEach(r),xt.forEach(r),pt=d(t),L=o(t,"DIV",{class:!0});var vt=n(L);y(ie.$$.fragment,vt),Vt=d(vt),S=o(vt,"P",{});var Le=n(S);Wt=i(Le,"Rescales "),Ue=o(Le,"CODE",{});var _a=n(Ue);Ct=i(_a,"image"),_a.forEach(r),Ut=i(Le," by "),Re=o(Le,"CODE",{});var ga=n(Re);Rt=i(ga,"scale"),ga.forEach(r),Jt=i(Le,"."),Le.forEach(r),vt.forEach(r),ft=d(t),N=o(t,"DIV",{class:!0});var bt=n(N);y(ce.$$.fragment,bt),Ht=d(bt),O=o(bt,"P",{});var Se=n(O);Bt=i(Se,"Resizes "),Je=o(Se,"CODE",{});var xa=n(Je);Gt=i(xa,"image"),xa.forEach(r),Yt=i(Se," to (h, w) specified by "),He=o(Se,"CODE",{});var va=n(He);Kt=i(va,"size"),va.forEach(r),Qt=i(Se," using the PIL library."),Se.forEach(r),bt.forEach(r),ut=d(t),V=o(t,"DIV",{class:!0});var yt=n(V);y(le.$$.fragment,yt),Xt=d(yt),de=o(yt,"P",{});var $t=n(de);Zt=i($t,"Converts "),Be=o($t,"CODE",{});var ba=n(Be);er=i(ba,"image"),ba.forEach(r),tr=i($t,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),$t.forEach(r),yt.forEach(r),ht=d(t),W=o(t,"H2",{class:!0});var Et=n(W);J=o(Et,"A",{id:!0,class:!0,href:!0});var ya=n(J);Ge=o(ya,"SPAN",{});var $a=n(Ge);y(me.$$.fragment,$a),$a.forEach(r),ya.forEach(r),rr=d(Et),Ye=o(Et,"SPAN",{});var Ea=n(Ye);ar=i(Ea,"ImageProcessorMixin"),Ea.forEach(r),Et.forEach(r),_t=d(t),_=o(t,"DIV",{class:!0});var x=n(_);y(pe.$$.fragment,x),or=d(x),Ke=o(x,"P",{});var wa=n(Ke);nr=i(wa,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),wa.forEach(r),sr=d(x),H=o(x,"DIV",{class:!0});var wt=n(H);y(fe.$$.fragment,wt),ir=d(wt),ue=o(wt,"P",{});var Ft=n(ue);cr=i(Ft,"Instantiates a type of "),Ie=o(Ft,"A",{href:!0});var Fa=n(Ie);lr=i(Fa,"FeatureExtractionMixin"),Fa.forEach(r),dr=i(Ft,` from a Python dictionary of
parameters.`),Ft.forEach(r),wt.forEach(r),mr=d(x),B=o(x,"DIV",{class:!0});var kt=n(B);y(he.$$.fragment,kt),pr=d(kt),_e=o(kt,"P",{});var Mt=n(_e);fr=i(Mt,"Instantiates a feature extractor of type "),De=o(Mt,"A",{href:!0});var ka=n(De);ur=i(ka,"FeatureExtractionMixin"),ka.forEach(r),hr=i(Mt,` from the path to
a JSON file of parameters.`),Mt.forEach(r),kt.forEach(r),_r=d(x),T=o(x,"DIV",{class:!0});var ae=n(T);y(ge.$$.fragment,ae),gr=d(ae),I=o(ae,"P",{});var oe=n(I);xr=i(oe,"Instantiate a type of "),je=o(oe,"A",{href:!0});var Ma=n(je);vr=i(Ma,"FeatureExtractionMixin"),Ma.forEach(r),br=i(oe," from a feature extractor, "),Qe=o(oe,"EM",{});var Pa=n(Qe);yr=i(Pa,"e.g."),Pa.forEach(r),$r=i(oe,` a
derived class of `),qe=o(oe,"A",{href:!0});var Ta=n(qe);Er=i(Ta,"SequenceFeatureExtractor"),Ta.forEach(r),wr=i(oe,"."),oe.forEach(r),Fr=d(ae),y(G.$$.fragment,ae),kr=d(ae),y(Y.$$.fragment,ae),ae.forEach(r),Mr=d(x),K=o(x,"DIV",{class:!0});var Pt=n(K);y(xe.$$.fragment,Pt),Pr=d(Pt),D=o(Pt,"P",{});var ne=n(D);Tr=i(ne,"From a "),Xe=o(ne,"CODE",{});var Ia=n(Xe);Ir=i(Ia,"pretrained_model_name_or_path"),Ia.forEach(r),Dr=i(ne,`, resolve to a dictionary of parameters, to be used for instantiating a
feature extractor of type `),ze=o(ne,"A",{href:!0});var Da=n(ze);jr=i(Da,"FeatureExtractionMixin"),Da.forEach(r),qr=i(ne," using "),Ze=o(ne,"CODE",{});var ja=n(Ze);zr=i(ja,"from_dict"),ja.forEach(r),Ar=i(ne,"."),ne.forEach(r),Pt.forEach(r),Lr=d(x),q=o(x,"DIV",{class:!0});var Ne=n(q);y(ve.$$.fragment,Ne),Sr=d(Ne),be=o(Ne,"P",{});var Tt=n(be);Nr=i(Tt,`Upload the feature extractor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),et=o(Tt,"CODE",{});var qa=n(et);Or=i(qa,"repo_path_or_name"),qa.forEach(r),Vr=i(Tt,"."),Tt.forEach(r),Wr=d(Ne),y(Q.$$.fragment,Ne),Ne.forEach(r),Cr=d(x),z=o(x,"DIV",{class:!0});var Oe=n(z);y(ye.$$.fragment,Oe),Ur=d(Oe),$e=o(Oe,"P",{});var It=n($e);Rr=i(It,`Register this class with a given auto class. This should only be used for custom feature extractors as the ones
in the library are already mapped with `),tt=o(It,"CODE",{});var za=n(tt);Jr=i(za,"AutoFeatureExtractor"),za.forEach(r),Hr=i(It,"."),It.forEach(r),Br=d(Oe),y(X.$$.fragment,Oe),Oe.forEach(r),Gr=d(x),Z=o(x,"DIV",{class:!0});var Dt=n(Z);y(Ee.$$.fragment,Dt),Yr=d(Dt),C=o(Dt,"P",{});var Ve=n(C);Kr=i(Ve,"Save a feature_extractor object to the directory "),rt=o(Ve,"CODE",{});var Aa=n(rt);Qr=i(Aa,"save_directory"),Aa.forEach(r),Xr=i(Ve,`, so that it can be re-loaded using the
`),Ae=o(Ve,"A",{href:!0});var La=n(Ae);Zr=i(La,"from_pretrained()"),La.forEach(r),ea=i(Ve," class method."),Ve.forEach(r),Dt.forEach(r),ta=d(x),ee=o(x,"DIV",{class:!0});var jt=n(ee);y(we.$$.fragment,jt),ra=d(jt),at=o(jt,"P",{});var Sa=n(at);aa=i(Sa,"Serializes this instance to a Python dictionary."),Sa.forEach(r),jt.forEach(r),oa=d(x),te=o(x,"DIV",{class:!0});var qt=n(te);y(Fe.$$.fragment,qt),na=d(qt),ot=o(qt,"P",{});var Na=n(ot);sa=i(Na,"Save this instance to a JSON file."),Na.forEach(r),qt.forEach(r),ia=d(x),re=o(x,"DIV",{class:!0});var zt=n(re);y(ke.$$.fragment,zt),ca=d(zt),nt=o(zt,"P",{});var Oa=n(nt);la=i(Oa,"Serializes this instance to a JSON string."),Oa.forEach(r),zt.forEach(r),x.forEach(r),this.h()},h(){p(m,"name","hf:doc:metadata"),p(m,"content",JSON.stringify(eo)),p(f,"id","utilities-for-image-processors"),p(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(f,"href","#utilities-for-image-processors"),p(u,"class","relative group"),p(R,"id","transformers.rescale"),p(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(R,"href","#transformers.rescale"),p(A,"class","relative group"),p(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(J,"id","transformers.FeatureExtractionMixin"),p(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(J,"href","#transformers.FeatureExtractionMixin"),p(W,"class","relative group"),p(Ie,"href","/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),p(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(De,"href","/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),p(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(je,"href","/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),p(qe,"href","/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),p(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ze,"href","/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),p(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ae,"href","/docs/transformers/pr_18520/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained"),p(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,m),v(t,k,h),v(t,u,h),e(u,f),e(f,M),$(c,M,null),e(u,g),e(u,U),e(U,At),v(t,lt,h),v(t,Pe,h),e(Pe,Lt),v(t,dt,h),v(t,Te,h),e(Te,St),v(t,mt,h),v(t,A,h),e(A,R),e(R,We),$(se,We,null),e(A,Nt),e(A,Ce),e(Ce,Ot),v(t,pt,h),v(t,L,h),$(ie,L,null),e(L,Vt),e(L,S),e(S,Wt),e(S,Ue),e(Ue,Ct),e(S,Ut),e(S,Re),e(Re,Rt),e(S,Jt),v(t,ft,h),v(t,N,h),$(ce,N,null),e(N,Ht),e(N,O),e(O,Bt),e(O,Je),e(Je,Gt),e(O,Yt),e(O,He),e(He,Kt),e(O,Qt),v(t,ut,h),v(t,V,h),$(le,V,null),e(V,Xt),e(V,de),e(de,Zt),e(de,Be),e(Be,er),e(de,tr),v(t,ht,h),v(t,W,h),e(W,J),e(J,Ge),$(me,Ge,null),e(W,rr),e(W,Ye),e(Ye,ar),v(t,_t,h),v(t,_,h),$(pe,_,null),e(_,or),e(_,Ke),e(Ke,nr),e(_,sr),e(_,H),$(fe,H,null),e(H,ir),e(H,ue),e(ue,cr),e(ue,Ie),e(Ie,lr),e(ue,dr),e(_,mr),e(_,B),$(he,B,null),e(B,pr),e(B,_e),e(_e,fr),e(_e,De),e(De,ur),e(_e,hr),e(_,_r),e(_,T),$(ge,T,null),e(T,gr),e(T,I),e(I,xr),e(I,je),e(je,vr),e(I,br),e(I,Qe),e(Qe,yr),e(I,$r),e(I,qe),e(qe,Er),e(I,wr),e(T,Fr),$(G,T,null),e(T,kr),$(Y,T,null),e(_,Mr),e(_,K),$(xe,K,null),e(K,Pr),e(K,D),e(D,Tr),e(D,Xe),e(Xe,Ir),e(D,Dr),e(D,ze),e(ze,jr),e(D,qr),e(D,Ze),e(Ze,zr),e(D,Ar),e(_,Lr),e(_,q),$(ve,q,null),e(q,Sr),e(q,be),e(be,Nr),e(be,et),e(et,Or),e(be,Vr),e(q,Wr),$(Q,q,null),e(_,Cr),e(_,z),$(ye,z,null),e(z,Ur),e(z,$e),e($e,Rr),e($e,tt),e(tt,Jr),e($e,Hr),e(z,Br),$(X,z,null),e(_,Gr),e(_,Z),$(Ee,Z,null),e(Z,Yr),e(Z,C),e(C,Kr),e(C,rt),e(rt,Qr),e(C,Xr),e(C,Ae),e(Ae,Zr),e(C,ea),e(_,ta),e(_,ee),$(we,ee,null),e(ee,ra),e(ee,at),e(at,aa),e(_,oa),e(_,te),$(Fe,te,null),e(te,na),e(te,ot),e(ot,sa),e(_,ia),e(_,re),$(ke,re,null),e(re,ca),e(re,nt),e(nt,la),gt=!0},p(t,[h]){const Me={};h&2&&(Me.$$scope={dirty:h,ctx:t}),G.$set(Me);const st={};h&2&&(st.$$scope={dirty:h,ctx:t}),Y.$set(st);const it={};h&2&&(it.$$scope={dirty:h,ctx:t}),Q.$set(it);const ct={};h&2&&(ct.$$scope={dirty:h,ctx:t}),X.$set(ct)},i(t){gt||(E(c.$$.fragment,t),E(se.$$.fragment,t),E(ie.$$.fragment,t),E(ce.$$.fragment,t),E(le.$$.fragment,t),E(me.$$.fragment,t),E(pe.$$.fragment,t),E(fe.$$.fragment,t),E(he.$$.fragment,t),E(ge.$$.fragment,t),E(G.$$.fragment,t),E(Y.$$.fragment,t),E(xe.$$.fragment,t),E(ve.$$.fragment,t),E(Q.$$.fragment,t),E(ye.$$.fragment,t),E(X.$$.fragment,t),E(Ee.$$.fragment,t),E(we.$$.fragment,t),E(Fe.$$.fragment,t),E(ke.$$.fragment,t),gt=!0)},o(t){w(c.$$.fragment,t),w(se.$$.fragment,t),w(ie.$$.fragment,t),w(ce.$$.fragment,t),w(le.$$.fragment,t),w(me.$$.fragment,t),w(pe.$$.fragment,t),w(fe.$$.fragment,t),w(he.$$.fragment,t),w(ge.$$.fragment,t),w(G.$$.fragment,t),w(Y.$$.fragment,t),w(xe.$$.fragment,t),w(ve.$$.fragment,t),w(Q.$$.fragment,t),w(ye.$$.fragment,t),w(X.$$.fragment,t),w(Ee.$$.fragment,t),w(we.$$.fragment,t),w(Fe.$$.fragment,t),w(ke.$$.fragment,t),gt=!1},d(t){r(m),t&&r(k),t&&r(u),F(c),t&&r(lt),t&&r(Pe),t&&r(dt),t&&r(Te),t&&r(mt),t&&r(A),F(se),t&&r(pt),t&&r(L),F(ie),t&&r(ft),t&&r(N),F(ce),t&&r(ut),t&&r(V),F(le),t&&r(ht),t&&r(W),F(me),t&&r(_t),t&&r(_),F(pe),F(fe),F(he),F(ge),F(G),F(Y),F(xe),F(ve),F(Q),F(ye),F(X),F(Ee),F(we),F(Fe),F(ke)}}}const eo={local:"utilities-for-image-processors",sections:[{local:"transformers.rescale",title:"Image Transformations"},{local:"transformers.FeatureExtractionMixin",title:"ImageProcessorMixin"}],title:"Utilities for Image Processors"};function to(j){return Ga(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class co extends Ra{constructor(m){super();Ja(this,m,to,Za,Ha,{})}}export{co as default,eo as metadata};
