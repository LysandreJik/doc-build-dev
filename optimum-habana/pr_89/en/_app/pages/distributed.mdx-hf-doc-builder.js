import{S as qe,i as He,s as Oe,e as a,k as d,w as $,t as s,M as Ue,c as i,d as r,m,a as n,x as w,h as o,b as l,G as e,g as p,y,L as Me,q as E,o as D,B as x,v as Ce}from"../chunks/vendor-hf-doc-builder.js";import{D as dt}from"../chunks/Docstring-hf-doc-builder.js";import{C as ze}from"../chunks/CodeBlock-hf-doc-builder.js";import{I as Ie}from"../chunks/IconCopyLink-hf-doc-builder.js";function Fe(he){let g,mt,_,R,tt,N,St,et,jt,pt,K,Lt,ht,b,Nt,T,Tt,zt,Q,It,qt,ct,W,z,Ht,I,Ot,Ut,ft,q,bt,c,Mt,rt,Ct,Ft,H,Vt,Gt,O,Bt,Xt,gt,U,M,Jt,Y,Kt,Qt,_t,C,vt,v,P,at,F,Wt,it,Yt,$t,h,V,Zt,nt,te,ee,k,G,re,st,ae,ie,A,B,ne,ot,se,oe,S,X,le,lt,ue,de,j,J,me,ut,pe,wt;return N=new Ie({}),q=new ze({props:{code:`python gaudi_spawn.py \\
    --world_size number_of_hpu_you_have --use_mpi \\
    path_to_script.py --args1 --args2 ... --argsN`,highlighted:`python gaudi_spawn.py \\
    --world_size number_of_hpu_you_have --use_mpi \\
    path_to_script.py --args1 --args2 ... --argsN`}}),C=new ze({props:{code:`from optimum.habana.distributed import DistributedRunner
from optimum.utils import logging

world_size=8 # Number of HPUs to use (1 or 8)

# define distributed runner
distributed_runner = DistributedRunner(
    command_list=["scripts/train.py --args1 --args2 ... --argsN"],
    world_size=world_size,
    use_mpi=True,
    multi_hls=False,
)

# start job
ret_code = distributed_runner.run()`,highlighted:`<span class="hljs-keyword">from</span> optimum.habana.distributed <span class="hljs-keyword">import</span> DistributedRunner
<span class="hljs-keyword">from</span> optimum.utils <span class="hljs-keyword">import</span> logging

world_size=<span class="hljs-number">8</span> <span class="hljs-comment"># Number of HPUs to use (1 or 8)</span>

<span class="hljs-comment"># define distributed runner</span>
distributed_runner = DistributedRunner(
    command_list=[<span class="hljs-string">&quot;scripts/train.py --args1 --args2 ... --argsN&quot;</span>],
    world_size=world_size,
    use_mpi=<span class="hljs-literal">True</span>,
    multi_hls=<span class="hljs-literal">False</span>,
)

<span class="hljs-comment"># start job</span>
ret_code = distributed_runner.run()`}}),F=new Ie({}),V=new dt({props:{name:"class optimum.habana.distributed.DistributedRunner",anchor:"optimum.habana.distributed.DistributedRunner",parameters:[{name:"command_list",val:" = []"},{name:"world_size",val:" = 1"},{name:"use_mpi",val:" = False"},{name:"use_env",val:" = False"},{name:"map_by",val:" = 'socket'"},{name:"multi_hls",val:" = False"}],source:"https://github.com/huggingface/optimum.habana/blob/vr_89/src/optimum/habana/distributed/distributed_runner.py#L29"}}),G=new dt({props:{name:"create_multi_hls_setup",anchor:"optimum.habana.distributed.DistributedRunner.create_multi_hls_setup",parameters:[],source:"https://github.com/huggingface/optimum.habana/blob/vr_89/src/optimum/habana/distributed/distributed_runner.py#L149"}}),B=new dt({props:{name:"create_single_card_setup",anchor:"optimum.habana.distributed.DistributedRunner.create_single_card_setup",parameters:[],source:"https://github.com/huggingface/optimum.habana/blob/vr_89/src/optimum/habana/distributed/distributed_runner.py#L117"}}),X=new dt({props:{name:"create_single_hls_setup",anchor:"optimum.habana.distributed.DistributedRunner.create_single_hls_setup",parameters:[],source:"https://github.com/huggingface/optimum.habana/blob/vr_89/src/optimum/habana/distributed/distributed_runner.py#L137"}}),J=new dt({props:{name:"create_single_hls_setup_mpirun",anchor:"optimum.habana.distributed.DistributedRunner.create_single_hls_setup_mpirun",parameters:[],source:"https://github.com/huggingface/optimum.habana/blob/vr_89/src/optimum/habana/distributed/distributed_runner.py#L125"}}),{c(){g=a("meta"),mt=d(),_=a("h1"),R=a("a"),tt=a("span"),$(N.$$.fragment),St=d(),et=a("span"),jt=s("Distributed training with Optimum Habana"),pt=d(),K=a("p"),Lt=s("As models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude."),ht=d(),b=a("p"),Nt=s("All the "),T=a("a"),Tt=s("PyTorch examples"),zt=s(" and "),Q=a("a"),It=s("GaudiTrainer"),qt=s(` script work out of the box with distributed training.
There are two ways of launching them:`),ct=d(),W=a("ol"),z=a("li"),Ht=s("Using "),I=a("a"),Ot=s("gaudi_spawn.py"),Ut=s(" script."),ft=d(),$(q.$$.fragment),bt=d(),c=a("p"),Mt=s("where "),rt=a("code"),Ct=s("--argX"),Ft=s(` is an argument of the script to run in a distributed way.
Examples are given for question answering `),H=a("a"),Vt=s("here"),Gt=s(" and for text classification "),O=a("a"),Bt=s("here"),Xt=s("."),gt=d(),U=a("ol"),M=a("li"),Jt=s("Using the "),Y=a("a"),Kt=s("DistributedRunner"),Qt=s(" directly in code."),_t=d(),$(C.$$.fragment),vt=d(),v=a("h2"),P=a("a"),at=a("span"),$(F.$$.fragment),Wt=d(),it=a("span"),Yt=s("DistributedRunner"),$t=d(),h=a("div"),$(V.$$.fragment),Zt=d(),nt=a("p"),te=s("Set up training hardware configurations and run distributed training commands."),ee=d(),k=a("div"),$(G.$$.fragment),re=d(),st=a("p"),ae=s("Multi-node configuration setup for mpirun."),ie=d(),A=a("div"),$(B.$$.fragment),ne=d(),ot=a("p"),se=s("Single-card setup."),oe=d(),S=a("div"),$(X.$$.fragment),le=d(),lt=a("p"),ue=s("Single-node multi-cards configuration setup."),de=d(),j=a("div"),$(J.$$.fragment),me=d(),ut=a("p"),pe=s("Single-node multi-cards configuration setup for mpirun."),this.h()},l(t){const u=Ue('[data-svelte="svelte-1phssyn"]',document.head);g=i(u,"META",{name:!0,content:!0}),u.forEach(r),mt=m(t),_=i(t,"H1",{class:!0});var yt=n(_);R=i(yt,"A",{id:!0,class:!0,href:!0});var ce=n(R);tt=i(ce,"SPAN",{});var fe=n(tt);w(N.$$.fragment,fe),fe.forEach(r),ce.forEach(r),St=m(yt),et=i(yt,"SPAN",{});var be=n(et);jt=o(be,"Distributed training with Optimum Habana"),be.forEach(r),yt.forEach(r),pt=m(t),K=i(t,"P",{});var ge=n(K);Lt=o(ge,"As models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude."),ge.forEach(r),ht=m(t),b=i(t,"P",{});var Z=n(b);Nt=o(Z,"All the "),T=i(Z,"A",{href:!0,rel:!0});var _e=n(T);Tt=o(_e,"PyTorch examples"),_e.forEach(r),zt=o(Z," and "),Q=i(Z,"A",{href:!0});var ve=n(Q);It=o(ve,"GaudiTrainer"),ve.forEach(r),qt=o(Z,` script work out of the box with distributed training.
There are two ways of launching them:`),Z.forEach(r),ct=m(t),W=i(t,"OL",{});var $e=n(W);z=i($e,"LI",{});var Et=n(z);Ht=o(Et,"Using "),I=i(Et,"A",{href:!0,rel:!0});var we=n(I);Ot=o(we,"gaudi_spawn.py"),we.forEach(r),Ut=o(Et," script."),Et.forEach(r),$e.forEach(r),ft=m(t),w(q.$$.fragment,t),bt=m(t),c=i(t,"P",{});var L=n(c);Mt=o(L,"where "),rt=i(L,"CODE",{});var ye=n(rt);Ct=o(ye,"--argX"),ye.forEach(r),Ft=o(L,` is an argument of the script to run in a distributed way.
Examples are given for question answering `),H=i(L,"A",{href:!0,rel:!0});var Ee=n(H);Vt=o(Ee,"here"),Ee.forEach(r),Gt=o(L," and for text classification "),O=i(L,"A",{href:!0,rel:!0});var De=n(O);Bt=o(De,"here"),De.forEach(r),Xt=o(L,"."),L.forEach(r),gt=m(t),U=i(t,"OL",{start:!0});var xe=n(U);M=i(xe,"LI",{});var Dt=n(M);Jt=o(Dt,"Using the "),Y=i(Dt,"A",{href:!0});var Re=n(Y);Kt=o(Re,"DistributedRunner"),Re.forEach(r),Qt=o(Dt," directly in code."),Dt.forEach(r),xe.forEach(r),_t=m(t),w(C.$$.fragment,t),vt=m(t),v=i(t,"H2",{class:!0});var xt=n(v);P=i(xt,"A",{id:!0,class:!0,href:!0});var Pe=n(P);at=i(Pe,"SPAN",{});var ke=n(at);w(F.$$.fragment,ke),ke.forEach(r),Pe.forEach(r),Wt=m(xt),it=i(xt,"SPAN",{});var Ae=n(it);Yt=o(Ae,"DistributedRunner"),Ae.forEach(r),xt.forEach(r),$t=m(t),h=i(t,"DIV",{class:!0});var f=n(h);w(V.$$.fragment,f),Zt=m(f),nt=i(f,"P",{});var Se=n(nt);te=o(Se,"Set up training hardware configurations and run distributed training commands."),Se.forEach(r),ee=m(f),k=i(f,"DIV",{class:!0});var Rt=n(k);w(G.$$.fragment,Rt),re=m(Rt),st=i(Rt,"P",{});var je=n(st);ae=o(je,"Multi-node configuration setup for mpirun."),je.forEach(r),Rt.forEach(r),ie=m(f),A=i(f,"DIV",{class:!0});var Pt=n(A);w(B.$$.fragment,Pt),ne=m(Pt),ot=i(Pt,"P",{});var Le=n(ot);se=o(Le,"Single-card setup."),Le.forEach(r),Pt.forEach(r),oe=m(f),S=i(f,"DIV",{class:!0});var kt=n(S);w(X.$$.fragment,kt),le=m(kt),lt=i(kt,"P",{});var Ne=n(lt);ue=o(Ne,"Single-node multi-cards configuration setup."),Ne.forEach(r),kt.forEach(r),de=m(f),j=i(f,"DIV",{class:!0});var At=n(j);w(J.$$.fragment,At),me=m(At),ut=i(At,"P",{});var Te=n(ut);pe=o(Te,"Single-node multi-cards configuration setup for mpirun."),Te.forEach(r),At.forEach(r),f.forEach(r),this.h()},h(){l(g,"name","hf:doc:metadata"),l(g,"content",JSON.stringify(Ve)),l(R,"id","distributed-training-with-optimum-habana"),l(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(R,"href","#distributed-training-with-optimum-habana"),l(_,"class","relative group"),l(T,"href","https://github.com/huggingface/optimum-habana/tree/main/examples"),l(T,"rel","nofollow"),l(Q,"href","/docs/optimum.habana/pr_89/en/trainer#optimum.habana.GaudiTrainer"),l(I,"href","https://github.com/huggingface/optimum-habana/blob/main/examples/gaudi_spawn.py"),l(I,"rel","nofollow"),l(H,"href","https://github.com/huggingface/optimum-habana/blob/main/examples/question-answering/README.md#multi-card-training"),l(H,"rel","nofollow"),l(O,"href","https://github.com/huggingface/optimum-habana/tree/main/examples/text-classification#multi-card-training"),l(O,"rel","nofollow"),l(Y,"href","/docs/optimum.habana/pr_89/en/distributed#optimum.habana.distributed.DistributedRunner"),l(U,"start","2"),l(P,"id","optimum.habana.distributed.DistributedRunner"),l(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(P,"href","#optimum.habana.distributed.DistributedRunner"),l(v,"class","relative group"),l(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,g),p(t,mt,u),p(t,_,u),e(_,R),e(R,tt),y(N,tt,null),e(_,St),e(_,et),e(et,jt),p(t,pt,u),p(t,K,u),e(K,Lt),p(t,ht,u),p(t,b,u),e(b,Nt),e(b,T),e(T,Tt),e(b,zt),e(b,Q),e(Q,It),e(b,qt),p(t,ct,u),p(t,W,u),e(W,z),e(z,Ht),e(z,I),e(I,Ot),e(z,Ut),p(t,ft,u),y(q,t,u),p(t,bt,u),p(t,c,u),e(c,Mt),e(c,rt),e(rt,Ct),e(c,Ft),e(c,H),e(H,Vt),e(c,Gt),e(c,O),e(O,Bt),e(c,Xt),p(t,gt,u),p(t,U,u),e(U,M),e(M,Jt),e(M,Y),e(Y,Kt),e(M,Qt),p(t,_t,u),y(C,t,u),p(t,vt,u),p(t,v,u),e(v,P),e(P,at),y(F,at,null),e(v,Wt),e(v,it),e(it,Yt),p(t,$t,u),p(t,h,u),y(V,h,null),e(h,Zt),e(h,nt),e(nt,te),e(h,ee),e(h,k),y(G,k,null),e(k,re),e(k,st),e(st,ae),e(h,ie),e(h,A),y(B,A,null),e(A,ne),e(A,ot),e(ot,se),e(h,oe),e(h,S),y(X,S,null),e(S,le),e(S,lt),e(lt,ue),e(h,de),e(h,j),y(J,j,null),e(j,me),e(j,ut),e(ut,pe),wt=!0},p:Me,i(t){wt||(E(N.$$.fragment,t),E(q.$$.fragment,t),E(C.$$.fragment,t),E(F.$$.fragment,t),E(V.$$.fragment,t),E(G.$$.fragment,t),E(B.$$.fragment,t),E(X.$$.fragment,t),E(J.$$.fragment,t),wt=!0)},o(t){D(N.$$.fragment,t),D(q.$$.fragment,t),D(C.$$.fragment,t),D(F.$$.fragment,t),D(V.$$.fragment,t),D(G.$$.fragment,t),D(B.$$.fragment,t),D(X.$$.fragment,t),D(J.$$.fragment,t),wt=!1},d(t){r(g),t&&r(mt),t&&r(_),x(N),t&&r(pt),t&&r(K),t&&r(ht),t&&r(b),t&&r(ct),t&&r(W),t&&r(ft),x(q,t),t&&r(bt),t&&r(c),t&&r(gt),t&&r(U),t&&r(_t),x(C,t),t&&r(vt),t&&r(v),x(F),t&&r($t),t&&r(h),x(V),x(G),x(B),x(X),x(J)}}}const Ve={local:"distributed-training-with-optimum-habana",sections:[{local:"optimum.habana.distributed.DistributedRunner",title:"DistributedRunner"}],title:"Distributed training with Optimum Habana"};function Ge(he){return Ce(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qe extends qe{constructor(g){super();He(this,g,Ge,Fe,Oe,{})}}export{Qe as default,Ve as metadata};
